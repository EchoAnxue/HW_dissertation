nohup: ignoring input
/root/miniconda3/lib/python3.12/site-packages/torch_geometric/graphgym/config.py:19: UserWarning: Could not define global config object. Please install 'yacs' via 'pip install yacs' in order to use GraphGym
  warnings.warn("Could not define global config object. Please install "
/root/miniconda3/lib/python3.12/site-packages/torch_geometric/graphgym/imports.py:14: UserWarning: Please install 'pytorch_lightning' via  'pip install pytorch_lightning' in order to use GraphGym
  warnings.warn("Please install 'pytorch_lightning' via  "
/root/miniconda3/lib/python3.12/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling
  warnings.warn(f"Using '{self.__class__.__name__}' without a "
========== Running baseline 1/3 ==========
Training GCN with 2 layers...
可训练参数: 133385_GCN
不可训练参数: 0
batch size: (871, 871)
✅ Epoch 0: New best model saved with val_loss = 1.0715
Epoch 0, accuracy: 0.5330
Epoch 0, Train Loss: 1.1715, Val Loss: 1.0715
batch size: (884, 884)
✅ Epoch 1: New best model saved with val_loss = 1.0513
Epoch 1, accuracy: 0.5922
batch size: (895, 895)
✅ Epoch 2: New best model saved with val_loss = 1.0342
Epoch 2, accuracy: 0.6271
Epoch 2, Train Loss: 0.1422, Val Loss: 1.0342
batch size: (882, 882)
✅ Epoch 3: New best model saved with val_loss = 1.0160
Epoch 3, accuracy: 0.6543
batch size: (903, 903)
✅ Epoch 4: New best model saved with val_loss = 0.9999
Epoch 4, accuracy: 0.6607
Epoch 4, Train Loss: 0.0647, Val Loss: 0.9999
batch size: (883, 883)
✅ Epoch 5: New best model saved with val_loss = 0.9850
Epoch 5, accuracy: 0.6592
batch size: (901, 901)
✅ Epoch 6: New best model saved with val_loss = 0.9687
Epoch 6, accuracy: 0.6594
Epoch 6, Train Loss: 0.0264, Val Loss: 0.9687
batch size: (910, 910)
✅ Epoch 7: New best model saved with val_loss = 0.9551
Epoch 7, accuracy: 0.6651
batch size: (906, 906)
✅ Epoch 8: New best model saved with val_loss = 0.9433
Epoch 8, accuracy: 0.6695
Epoch 8, Train Loss: 0.0081, Val Loss: 0.9433
batch size: (903, 903)
✅ Epoch 9: New best model saved with val_loss = 0.9281
Epoch 9, accuracy: 0.6769
batch size: (886, 886)
✅ Epoch 10: New best model saved with val_loss = 0.9156
Epoch 10, accuracy: 0.6830
Epoch 10, Train Loss: 0.0187, Val Loss: 0.9156
batch size: (896, 896)
✅ Epoch 11: New best model saved with val_loss = 0.8992
Epoch 11, accuracy: 0.6868
batch size: (887, 887)
✅ Epoch 12: New best model saved with val_loss = 0.8879
Epoch 12, accuracy: 0.6994
Epoch 12, Train Loss: 0.0066, Val Loss: 0.8879
batch size: (896, 896)
✅ Epoch 13: New best model saved with val_loss = 0.8725
Epoch 13, accuracy: 0.7032
batch size: (901, 901)
✅ Epoch 14: New best model saved with val_loss = 0.8597
Epoch 14, accuracy: 0.7105
Epoch 14, Train Loss: 0.0024, Val Loss: 0.8597
batch size: (892, 892)
✅ Epoch 15: New best model saved with val_loss = 0.8471
Epoch 15, accuracy: 0.7101
batch size: (899, 899)
✅ Epoch 16: New best model saved with val_loss = 0.8328
Epoch 16, accuracy: 0.7119
Epoch 16, Train Loss: 0.0015, Val Loss: 0.8328
batch size: (883, 883)
✅ Epoch 17: New best model saved with val_loss = 0.8176
Epoch 17, accuracy: 0.7192
batch size: (925, 925)
✅ Epoch 18: New best model saved with val_loss = 0.8089
Epoch 18, accuracy: 0.7220
Epoch 18, Train Loss: 0.0017, Val Loss: 0.8089
batch size: (888, 888)
✅ Epoch 19: New best model saved with val_loss = 0.7954
Epoch 19, accuracy: 0.7202
batch size: (897, 897)
✅ Epoch 20: New best model saved with val_loss = 0.7846
Epoch 20, accuracy: 0.7263
Epoch 20, Train Loss: 0.0166, Val Loss: 0.7846
batch size: (885, 885)
✅ Epoch 21: New best model saved with val_loss = 0.7708
Epoch 21, accuracy: 0.7228
batch size: (894, 894)
✅ Epoch 22: New best model saved with val_loss = 0.7610
Epoch 22, accuracy: 0.7217
Epoch 22, Train Loss: 0.0010, Val Loss: 0.7610
batch size: (897, 897)
✅ Epoch 23: New best model saved with val_loss = 0.7528
Epoch 23, accuracy: 0.7219
batch size: (895, 895)
✅ Epoch 24: New best model saved with val_loss = 0.7470
Epoch 24, accuracy: 0.7177
Epoch 24, Train Loss: 0.0015, Val Loss: 0.7470
batch size: (906, 906)
✅ Epoch 25: New best model saved with val_loss = 0.7341
Epoch 25, accuracy: 0.7154
batch size: (896, 896)
✅ Epoch 26: New best model saved with val_loss = 0.7209
Epoch 26, accuracy: 0.7165
Epoch 26, Train Loss: 0.0017, Val Loss: 0.7209
batch size: (887, 887)
✅ Epoch 27: New best model saved with val_loss = 0.7127
Epoch 27, accuracy: 0.7151
batch size: (906, 906)
✅ Epoch 28: New best model saved with val_loss = 0.7048
Epoch 28, accuracy: 0.7123
Epoch 28, Train Loss: 0.0030, Val Loss: 0.7048
batch size: (886, 886)
✅ Epoch 29: New best model saved with val_loss = 0.6895
Epoch 29, accuracy: 0.7120
batch size: (894, 894)
✅ Epoch 30: New best model saved with val_loss = 0.6722
Epoch 30, accuracy: 0.7183
Epoch 30, Train Loss: 0.0349, Val Loss: 0.6722
batch size: (906, 906)
✅ Epoch 31: New best model saved with val_loss = 0.6626
Epoch 31, accuracy: 0.7243
batch size: (902, 902)
✅ Epoch 32: New best model saved with val_loss = 0.6470
Epoch 32, accuracy: 0.7232
Epoch 32, Train Loss: 0.0005, Val Loss: 0.6470
batch size: (913, 913)
✅ Epoch 33: New best model saved with val_loss = 0.6264
Epoch 33, accuracy: 0.7305
batch size: (900, 900)
✅ Epoch 34: New best model saved with val_loss = 0.6218
Epoch 34, accuracy: 0.7300
Epoch 34, Train Loss: 0.0010, Val Loss: 0.6218
batch size: (875, 875)
✅ Epoch 35: New best model saved with val_loss = 0.6090
Epoch 35, accuracy: 0.7310
batch size: (918, 918)
✅ Epoch 36: New best model saved with val_loss = 0.6001
Epoch 36, accuracy: 0.7309
Epoch 36, Train Loss: 0.0008, Val Loss: 0.6001
batch size: (881, 881)
✅ Epoch 37: New best model saved with val_loss = 0.5920
Epoch 37, accuracy: 0.7331
batch size: (893, 893)
Epoch 38, accuracy: 0.7281
Epoch 38, Train Loss: 0.0008, Val Loss: 0.5924
batch size: (918, 918)
✅ Epoch 39: New best model saved with val_loss = 0.5824
Epoch 39, accuracy: 0.7315
batch size: (909, 909)
Epoch 40, accuracy: 0.7321
Epoch 40, Train Loss: 0.0008, Val Loss: 0.5847
batch size: (899, 899)
Epoch 41, accuracy: 0.7272
batch size: (899, 899)
✅ Epoch 42: New best model saved with val_loss = 0.5744
Epoch 42, accuracy: 0.7290
Epoch 42, Train Loss: 0.0477, Val Loss: 0.5744
batch size: (904, 904)
✅ Epoch 43: New best model saved with val_loss = 0.5736
Epoch 43, accuracy: 0.7252
batch size: (899, 899)
Epoch 44, accuracy: 0.7225
Epoch 44, Train Loss: 0.0019, Val Loss: 0.5766
batch size: (906, 906)
Epoch 45, accuracy: 0.7154
batch size: (929, 929)
Epoch 46, accuracy: 0.7107
Epoch 46, Train Loss: 0.0013, Val Loss: 0.5886
batch size: (894, 894)
Epoch 47, accuracy: 0.7080
batch size: (896, 896)
Epoch 48, accuracy: 0.7022
Epoch 48, Train Loss: 0.0011, Val Loss: 0.6123
batch size: (888, 888)
Epoch 49, accuracy: 0.7015
batch size: (908, 908)
Epoch 50, accuracy: 0.6993
Epoch 50, Train Loss: 0.0097, Val Loss: 0.6142
batch size: (919, 919)
Epoch 51, accuracy: 0.6971
batch size: (901, 901)
Epoch 52, accuracy: 0.6981
Epoch 52, Train Loss: 0.0023, Val Loss: 0.6291
batch size: (888, 888)
Epoch 53, accuracy: 0.7008
batch size: (901, 901)
Epoch 54, accuracy: 0.6976
Epoch 54, Train Loss: 0.0009, Val Loss: 0.6377
batch size: (906, 906)
Epoch 55, accuracy: 0.7026
batch size: (893, 893)
Epoch 56, accuracy: 0.7019
Epoch 56, Train Loss: 0.0376, Val Loss: 0.6489
batch size: (889, 889)
Epoch 57, accuracy: 0.7029
batch size: (884, 884)
Epoch 58, accuracy: 0.7023
Epoch 58, Train Loss: 0.0018, Val Loss: 0.6457
batch size: (896, 896)
Epoch 59, accuracy: 0.7011
batch size: (901, 901)
Epoch 60, accuracy: 0.7016
Epoch 60, Train Loss: 0.0089, Val Loss: 0.6664
batch size: (902, 902)
Epoch 61, accuracy: 0.7017
batch size: (893, 893)
Epoch 62, accuracy: 0.7011
Epoch 62, Train Loss: 0.0050, Val Loss: 0.6925
batch size: (900, 900)
Epoch 63, accuracy: 0.7005
batch size: (892, 892)
Epoch 64, accuracy: 0.7012
Epoch 64, Train Loss: 0.0104, Val Loss: 0.7343
batch size: (909, 909)
Epoch 65, accuracy: 0.7027
batch size: (892, 892)
Epoch 66, accuracy: 0.7029
Epoch 66, Train Loss: 0.0008, Val Loss: 0.7598
batch size: (884, 884)
Epoch 67, accuracy: 0.6996
batch size: (908, 908)
Epoch 68, accuracy: 0.7038
Epoch 68, Train Loss: 0.0010, Val Loss: 0.7192
batch size: (913, 913)
Epoch 69, accuracy: 0.7021
batch size: (891, 891)
Epoch 70, accuracy: 0.7012
Epoch 70, Train Loss: 0.0016, Val Loss: 0.7894
batch size: (896, 896)
Epoch 71, accuracy: 0.7026
batch size: (908, 908)
Epoch 72, accuracy: 0.7021
Epoch 72, Train Loss: 0.0015, Val Loss: 0.7834
batch size: (893, 893)
Epoch 73, accuracy: 0.7025
batch size: (902, 902)
Epoch 74, accuracy: 0.7031
Epoch 74, Train Loss: 0.0005, Val Loss: 0.8044
batch size: (896, 896)
Epoch 75, accuracy: 0.7033
batch size: (905, 905)
Epoch 76, accuracy: 0.7034
Epoch 76, Train Loss: 0.0013, Val Loss: 0.7930
batch size: (891, 891)
Epoch 77, accuracy: 0.7041
batch size: (873, 873)
Epoch 78, accuracy: 0.7057
Epoch 78, Train Loss: 0.0011, Val Loss: 0.8388
batch size: (911, 911)
Epoch 79, accuracy: 0.7013
batch size: (899, 899)
Epoch 80, accuracy: 0.7029
Epoch 80, Train Loss: 0.0014, Val Loss: 0.8499
batch size: (899, 899)
Epoch 81, accuracy: 0.7045
batch size: (908, 908)
Epoch 82, accuracy: 0.7050
Epoch 82, Train Loss: 0.0064, Val Loss: 0.8553
batch size: (903, 903)
Epoch 83, accuracy: 0.7040
batch size: (894, 894)
Epoch 84, accuracy: 0.7030
Epoch 84, Train Loss: 0.0011, Val Loss: 0.8711
batch size: (903, 903)
Epoch 85, accuracy: 0.7033
batch size: (917, 917)
Epoch 86, accuracy: 0.7023
Epoch 86, Train Loss: 0.0013, Val Loss: 0.8324
batch size: (900, 900)
Epoch 87, accuracy: 0.7036
batch size: (890, 890)
Epoch 88, accuracy: 0.7034
Epoch 88, Train Loss: 0.0015, Val Loss: 0.8996
batch size: (879, 879)
Epoch 89, accuracy: 0.7038
batch size: (899, 899)
Epoch 90, accuracy: 0.7016
Epoch 90, Train Loss: 0.0008, Val Loss: 0.8961
batch size: (896, 896)
Epoch 91, accuracy: 0.7035
batch size: (882, 882)
Epoch 92, accuracy: 0.7034
Epoch 92, Train Loss: 0.0010, Val Loss: 0.8722
batch size: (902, 902)
Epoch 93, accuracy: 0.7017
batch size: (907, 907)
Epoch 94, accuracy: 0.7010
Epoch 94, Train Loss: 0.0008, Val Loss: 0.8947
batch size: (882, 882)
Epoch 95, accuracy: 0.7024
batch size: (894, 894)
Epoch 96, accuracy: 0.7049
Epoch 96, Train Loss: 0.0035, Val Loss: 0.9029
batch size: (922, 922)
Epoch 97, accuracy: 0.7047
batch size: (895, 895)
Epoch 98, accuracy: 0.7033
Epoch 98, Train Loss: 0.0031, Val Loss: 0.9360
batch size: (898, 898)
Epoch 99, accuracy: 0.7041
batch size: (904, 904)
Epoch 100, accuracy: 0.7053
Epoch 100, Train Loss: 0.0011, Val Loss: 0.8688
batch size: (913, 913)
Epoch 101, accuracy: 0.7019
batch size: (903, 903)
Epoch 102, accuracy: 0.7030
Epoch 102, Train Loss: 0.0013, Val Loss: 0.9017
batch size: (899, 899)
Epoch 103, accuracy: 0.7021
batch size: (902, 902)
Epoch 104, accuracy: 0.7026
Epoch 104, Train Loss: 0.0085, Val Loss: 0.9084
batch size: (905, 905)
Epoch 105, accuracy: 0.7054
batch size: (895, 895)
Epoch 106, accuracy: 0.7047
Epoch 106, Train Loss: 0.0023, Val Loss: 0.9435
batch size: (896, 896)
Epoch 107, accuracy: 0.7032
batch size: (899, 899)
Epoch 108, accuracy: 0.7030
Epoch 108, Train Loss: 0.0021, Val Loss: 0.9105
batch size: (895, 895)
Epoch 109, accuracy: 0.7047
batch size: (925, 925)
Epoch 110, accuracy: 0.7045
Epoch 110, Train Loss: 0.0006, Val Loss: 0.9587
batch size: (889, 889)
Epoch 111, accuracy: 0.7044
batch size: (885, 885)
Epoch 112, accuracy: 0.7029
Epoch 112, Train Loss: 0.0011, Val Loss: 0.8887
batch size: (901, 901)
Epoch 113, accuracy: 0.7040
batch size: (905, 905)
Epoch 114, accuracy: 0.7033
Epoch 114, Train Loss: 0.0007, Val Loss: 0.8659
batch size: (897, 897)
Epoch 115, accuracy: 0.7048
batch size: (900, 900)
Epoch 116, accuracy: 0.7043
Epoch 116, Train Loss: 0.0012, Val Loss: 0.9279
batch size: (915, 915)
Epoch 117, accuracy: 0.7029
batch size: (909, 909)
Epoch 118, accuracy: 0.7032
Epoch 118, Train Loss: 0.0012, Val Loss: 0.9214
batch size: (894, 894)
Epoch 119, accuracy: 0.7025
batch size: (899, 899)
Epoch 120, accuracy: 0.7027
Epoch 120, Train Loss: 0.0039, Val Loss: 0.8978
batch size: (886, 886)
Epoch 121, accuracy: 0.7041
batch size: (908, 908)
Epoch 122, accuracy: 0.7025
Epoch 122, Train Loss: 0.0021, Val Loss: 0.9103
batch size: (897, 897)
Epoch 123, accuracy: 0.7036
batch size: (902, 902)
Epoch 124, accuracy: 0.7027
Epoch 124, Train Loss: 0.0021, Val Loss: 0.8732
batch size: (886, 886)
Epoch 125, accuracy: 0.7011
batch size: (895, 895)
Epoch 126, accuracy: 0.7034
Epoch 126, Train Loss: 0.0013, Val Loss: 0.9125
batch size: (906, 906)
Epoch 127, accuracy: 0.7037
batch size: (907, 907)
Epoch 128, accuracy: 0.7045
Epoch 128, Train Loss: 0.0031, Val Loss: 0.9148
batch size: (912, 912)
Epoch 129, accuracy: 0.7028
batch size: (894, 894)
Epoch 130, accuracy: 0.7031
Epoch 130, Train Loss: 0.0011, Val Loss: 0.9050
batch size: (895, 895)
Epoch 131, accuracy: 0.7042
batch size: (922, 922)
Epoch 132, accuracy: 0.7044
Epoch 132, Train Loss: 0.0012, Val Loss: 0.9392
batch size: (893, 893)
Epoch 133, accuracy: 0.7037
batch size: (890, 890)
Epoch 134, accuracy: 0.7018
Epoch 134, Train Loss: 0.0011, Val Loss: 0.9232
batch size: (908, 908)
Epoch 135, accuracy: 0.7038
batch size: (894, 894)
Epoch 136, accuracy: 0.7037
Epoch 136, Train Loss: 0.0064, Val Loss: 0.9347
batch size: (904, 904)
Epoch 137, accuracy: 0.7033
batch size: (915, 915)
Epoch 138, accuracy: 0.7032
Epoch 138, Train Loss: 0.0011, Val Loss: 0.9323
batch size: (913, 913)
Epoch 139, accuracy: 0.6997
batch size: (883, 883)
Epoch 140, accuracy: 0.7018
Epoch 140, Train Loss: 0.0050, Val Loss: 0.9221
batch size: (894, 894)
Epoch 141, accuracy: 0.7030
batch size: (914, 914)
Epoch 142, accuracy: 0.7026
Epoch 142, Train Loss: 0.0011, Val Loss: 0.9061
batch size: (913, 913)
Epoch 143, accuracy: 0.7048
batch size: (896, 896)
Epoch 144, accuracy: 0.7024
Epoch 144, Train Loss: 0.0006, Val Loss: 0.8624
batch size: (910, 910)
Epoch 145, accuracy: 0.7025
batch size: (900, 900)
Epoch 146, accuracy: 0.7048
Epoch 146, Train Loss: 0.0090, Val Loss: 0.9188
batch size: (906, 906)
Epoch 147, accuracy: 0.7019
batch size: (884, 884)
Epoch 148, accuracy: 0.7025
Epoch 148, Train Loss: 0.0011, Val Loss: 0.9439
batch size: (902, 902)
Epoch 149, accuracy: 0.7041
batch size: (886, 886)
Epoch 150, accuracy: 0.7038
Epoch 150, Train Loss: 0.0011, Val Loss: 0.9006
batch size: (895, 895)
Epoch 151, accuracy: 0.7035
batch size: (882, 882)
Epoch 152, accuracy: 0.7039
Epoch 152, Train Loss: 0.0009, Val Loss: 0.9177
batch size: (893, 893)
Epoch 153, accuracy: 0.7024
batch size: (905, 905)
Epoch 154, accuracy: 0.7044
Epoch 154, Train Loss: 0.0007, Val Loss: 0.8879
batch size: (903, 903)
Epoch 155, accuracy: 0.7040
batch size: (894, 894)
Epoch 156, accuracy: 0.7030
Epoch 156, Train Loss: 0.0017, Val Loss: 0.9478
batch size: (885, 885)
Epoch 157, accuracy: 0.7026
batch size: (910, 910)
Epoch 158, accuracy: 0.7029
Epoch 158, Train Loss: 0.0011, Val Loss: 0.9378
batch size: (891, 891)
Epoch 159, accuracy: 0.7047
batch size: (893, 893)
Epoch 160, accuracy: 0.7050
Epoch 160, Train Loss: 0.0008, Val Loss: 0.9342
batch size: (897, 897)
Epoch 161, accuracy: 0.7054
batch size: (899, 899)
Epoch 162, accuracy: 0.7031
Epoch 162, Train Loss: 0.0007, Val Loss: 0.8765
batch size: (910, 910)
Epoch 163, accuracy: 0.7012
batch size: (901, 901)
Epoch 164, accuracy: 0.7031
Epoch 164, Train Loss: 0.0019, Val Loss: 0.9057
batch size: (900, 900)
Epoch 165, accuracy: 0.7036
batch size: (921, 921)
Epoch 166, accuracy: 0.7021
Epoch 166, Train Loss: 0.0010, Val Loss: 0.9586
batch size: (909, 909)
Epoch 167, accuracy: 0.7032
batch size: (906, 906)
Epoch 168, accuracy: 0.7037
Epoch 168, Train Loss: 0.0019, Val Loss: 0.9062
batch size: (882, 882)
Epoch 169, accuracy: 0.7012
batch size: (903, 903)
Epoch 170, accuracy: 0.7034
Epoch 170, Train Loss: 0.0027, Val Loss: 0.9516
batch size: (898, 898)
Epoch 171, accuracy: 0.7036
batch size: (909, 909)
Epoch 172, accuracy: 0.7024
Epoch 172, Train Loss: 0.0014, Val Loss: 0.9275
batch size: (885, 885)
Epoch 173, accuracy: 0.7042
batch size: (891, 891)
Epoch 174, accuracy: 0.7023
Epoch 174, Train Loss: 0.0007, Val Loss: 0.9754
batch size: (895, 895)
Epoch 175, accuracy: 0.7017
batch size: (905, 905)
Epoch 176, accuracy: 0.7044
Epoch 176, Train Loss: 0.0190, Val Loss: 0.9075
batch size: (900, 900)
Epoch 177, accuracy: 0.7032
batch size: (897, 897)
Epoch 178, accuracy: 0.7046
Epoch 178, Train Loss: 0.0015, Val Loss: 0.9214
batch size: (919, 919)
Epoch 179, accuracy: 0.7056
batch size: (909, 909)
Epoch 180, accuracy: 0.7028
Epoch 180, Train Loss: 0.0017, Val Loss: 0.8890
batch size: (896, 896)
Epoch 181, accuracy: 0.7022
batch size: (901, 901)
Epoch 182, accuracy: 0.7030
Epoch 182, Train Loss: 0.0018, Val Loss: 0.9143
batch size: (906, 906)
Epoch 183, accuracy: 0.7035
batch size: (891, 891)
Epoch 184, accuracy: 0.7012
Epoch 184, Train Loss: 0.0015, Val Loss: 0.8956
batch size: (905, 905)
Epoch 185, accuracy: 0.7013
batch size: (894, 894)
Epoch 186, accuracy: 0.7018
Epoch 186, Train Loss: 0.0020, Val Loss: 0.9212
batch size: (907, 907)
Epoch 187, accuracy: 0.7041
batch size: (908, 908)
Epoch 188, accuracy: 0.7046
Epoch 188, Train Loss: 0.0030, Val Loss: 0.8895
batch size: (902, 902)
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 189, accuracy: 0.7023
batch size: (912, 912)
Epoch 190, accuracy: 0.7018
Epoch 190, Train Loss: 0.0017, Val Loss: 0.8980
batch size: (901, 901)
Epoch 191, accuracy: 0.7020
batch size: (884, 884)
Epoch 192, accuracy: 0.7025
Epoch 192, Train Loss: 0.0006, Val Loss: 0.9159
batch size: (901, 901)
Epoch 193, accuracy: 0.7015
batch size: (894, 894)
Epoch 194, accuracy: 0.7041
Epoch 194, Train Loss: 0.0054, Val Loss: 0.9344
batch size: (883, 883)
Epoch 195, accuracy: 0.7039
batch size: (911, 911)
Epoch 196, accuracy: 0.7010
Epoch 196, Train Loss: 0.0021, Val Loss: 0.9538
batch size: (897, 897)
Epoch 197, accuracy: 0.7041
batch size: (913, 913)
Epoch 198, accuracy: 0.7037
Epoch 198, Train Loss: 0.0023, Val Loss: 0.9158
batch size: (894, 894)
Epoch 199, accuracy: 0.7019
Loaded best model with val_loss = 0.573627769947052
test :accuracy 0.7247, f1_macro: 0.7189, f1_micro: 0.7247, auc: 0.8732
Training GCN with 8 layers...
可训练参数: 532745_GCN
不可训练参数: 0
batch size: (901, 901)
✅ Epoch 0: New best model saved with val_loss = 1.0751
Epoch 0, accuracy: 0.4306
Epoch 0, Train Loss: 1.1883, Val Loss: 1.0751
batch size: (910, 910)
Epoch 1, accuracy: 0.4296
batch size: (910, 910)
Epoch 2, accuracy: 0.1699
Epoch 2, Train Loss: 0.9995, Val Loss: 1.3552
batch size: (883, 883)
Epoch 3, accuracy: 0.1712
batch size: (886, 886)
Epoch 4, accuracy: 0.1693
Epoch 4, Train Loss: 0.9212, Val Loss: 1.8236
batch size: (905, 905)
Epoch 5, accuracy: 0.1684
batch size: (906, 906)
Epoch 6, accuracy: 0.1704
Epoch 6, Train Loss: 0.7995, Val Loss: 2.5301
batch size: (913, 913)
Epoch 7, accuracy: 0.1698
batch size: (884, 884)
Epoch 8, accuracy: 0.1697
Epoch 8, Train Loss: 0.6735, Val Loss: 2.1970
batch size: (909, 909)
Epoch 9, accuracy: 0.1665
batch size: (916, 916)
Epoch 10, accuracy: 0.1677
Epoch 10, Train Loss: 0.8439, Val Loss: 2.2705
batch size: (895, 895)
Epoch 11, accuracy: 0.1674
batch size: (890, 890)
Epoch 12, accuracy: 0.1705
Epoch 12, Train Loss: 0.6507, Val Loss: 2.2855
batch size: (903, 903)
Epoch 13, accuracy: 0.1670
batch size: (906, 906)
Epoch 14, accuracy: 0.1662
Epoch 14, Train Loss: 0.5871, Val Loss: 2.3385
batch size: (898, 898)
Epoch 15, accuracy: 0.1643
batch size: (883, 883)
Epoch 16, accuracy: 0.1691
Epoch 16, Train Loss: 0.6684, Val Loss: 2.2281
batch size: (889, 889)
Epoch 17, accuracy: 0.1696
batch size: (879, 879)
Epoch 18, accuracy: 0.1687
Epoch 18, Train Loss: 0.6113, Val Loss: 2.4014
batch size: (878, 878)
Epoch 19, accuracy: 0.1677
batch size: (879, 879)
Epoch 20, accuracy: 0.1676
Epoch 20, Train Loss: 0.6217, Val Loss: 2.2989
batch size: (894, 894)
Epoch 21, accuracy: 0.1682
batch size: (915, 915)
Epoch 22, accuracy: 0.1655
Epoch 22, Train Loss: 0.6152, Val Loss: 2.2875
batch size: (904, 904)
Epoch 23, accuracy: 0.1674
batch size: (901, 901)
Epoch 24, accuracy: 0.1693
Epoch 24, Train Loss: 0.6822, Val Loss: 2.2322
batch size: (891, 891)
Epoch 25, accuracy: 0.1702
batch size: (894, 894)
Epoch 26, accuracy: 0.1687
Epoch 26, Train Loss: 0.6503, Val Loss: 2.2461
batch size: (910, 910)
Epoch 27, accuracy: 0.1672
batch size: (898, 898)
Epoch 28, accuracy: 0.1679
Epoch 28, Train Loss: 0.7433, Val Loss: 2.3098
batch size: (897, 897)
Epoch 29, accuracy: 0.1658
batch size: (889, 889)
Epoch 30, accuracy: 0.1670
Epoch 30, Train Loss: 0.5793, Val Loss: 2.3243
batch size: (894, 894)
Epoch 31, accuracy: 0.1689
batch size: (911, 911)
Epoch 32, accuracy: 0.1668
Epoch 32, Train Loss: 0.5850, Val Loss: 2.3300
batch size: (896, 896)
Epoch 33, accuracy: 0.1687
batch size: (873, 873)
Epoch 34, accuracy: 0.1654
Epoch 34, Train Loss: 0.7051, Val Loss: 2.3361
batch size: (901, 901)
Epoch 35, accuracy: 0.1691
batch size: (898, 898)
Epoch 36, accuracy: 0.1649
Epoch 36, Train Loss: 0.6462, Val Loss: 2.3547
batch size: (894, 894)
Epoch 37, accuracy: 0.1680
batch size: (895, 895)
Epoch 38, accuracy: 0.1674
Epoch 38, Train Loss: 0.6052, Val Loss: 2.3336
batch size: (900, 900)
Epoch 39, accuracy: 0.1664
batch size: (908, 908)
Epoch 40, accuracy: 0.1696
Epoch 40, Train Loss: 0.6488, Val Loss: 2.3457
batch size: (894, 894)
Epoch 41, accuracy: 0.1666
batch size: (907, 907)
Epoch 42, accuracy: 0.1659
Epoch 42, Train Loss: 0.6321, Val Loss: 2.3126
batch size: (918, 918)
Epoch 43, accuracy: 0.1678
batch size: (898, 898)
Epoch 44, accuracy: 0.1662
Epoch 44, Train Loss: 0.5762, Val Loss: 2.3747
batch size: (877, 877)
Epoch 45, accuracy: 0.1680
batch size: (894, 894)
Epoch 46, accuracy: 0.1689
Epoch 46, Train Loss: 0.6781, Val Loss: 2.3602
batch size: (900, 900)
Epoch 47, accuracy: 0.1702
batch size: (906, 906)
Epoch 48, accuracy: 0.1679
Epoch 48, Train Loss: 0.5533, Val Loss: 2.3432
batch size: (915, 915)
Epoch 49, accuracy: 0.1701
batch size: (886, 886)
Epoch 50, accuracy: 0.1669
Epoch 50, Train Loss: 0.7101, Val Loss: 2.2837
batch size: (899, 899)
Epoch 51, accuracy: 0.1661
batch size: (891, 891)
Epoch 52, accuracy: 0.1693
Epoch 52, Train Loss: 0.5928, Val Loss: 2.3085
batch size: (911, 911)
Epoch 53, accuracy: 0.1682
batch size: (890, 890)
Epoch 54, accuracy: 0.1671
Epoch 54, Train Loss: 0.5129, Val Loss: 2.3469
batch size: (907, 907)
Epoch 55, accuracy: 0.1690
batch size: (882, 882)
Epoch 56, accuracy: 0.1665
Epoch 56, Train Loss: 0.6895, Val Loss: 2.2828
batch size: (891, 891)
Epoch 57, accuracy: 0.1670
batch size: (916, 916)
Epoch 58, accuracy: 0.1689
Epoch 58, Train Loss: 0.7434, Val Loss: 2.3195
batch size: (890, 890)
Epoch 59, accuracy: 0.1675
batch size: (902, 902)
Epoch 60, accuracy: 0.1652
Epoch 60, Train Loss: 0.6889, Val Loss: 2.2940
batch size: (909, 909)
Epoch 61, accuracy: 0.1690
batch size: (910, 910)
Epoch 62, accuracy: 0.1669
Epoch 62, Train Loss: 0.5964, Val Loss: 2.2656
batch size: (898, 898)
Epoch 63, accuracy: 0.1671
batch size: (910, 910)
Epoch 64, accuracy: 0.1687
Epoch 64, Train Loss: 0.6609, Val Loss: 2.2832
batch size: (888, 888)
Epoch 65, accuracy: 0.1688
batch size: (888, 888)
Epoch 66, accuracy: 0.1666
Epoch 66, Train Loss: 0.7030, Val Loss: 2.2525
batch size: (907, 907)
Epoch 67, accuracy: 0.1675
batch size: (909, 909)
Epoch 68, accuracy: 0.1677
Epoch 68, Train Loss: 0.8080, Val Loss: 2.2803
batch size: (903, 903)
Epoch 69, accuracy: 0.1647
batch size: (896, 896)
Epoch 70, accuracy: 0.1661
Epoch 70, Train Loss: 0.5547, Val Loss: 2.2692
batch size: (914, 914)
Epoch 71, accuracy: 0.1689
batch size: (904, 904)
Epoch 72, accuracy: 0.1663
Epoch 72, Train Loss: 0.6833, Val Loss: 2.2896
batch size: (911, 911)
Epoch 73, accuracy: 0.1652
batch size: (907, 907)
Epoch 74, accuracy: 0.1643
Epoch 74, Train Loss: 0.5966, Val Loss: 2.3512
batch size: (891, 891)
Epoch 75, accuracy: 0.1697
batch size: (911, 911)
Epoch 76, accuracy: 0.1671
Epoch 76, Train Loss: 0.6424, Val Loss: 2.3514
batch size: (909, 909)
Epoch 77, accuracy: 0.1699
batch size: (889, 889)
Epoch 78, accuracy: 0.1647
Epoch 78, Train Loss: 0.6703, Val Loss: 2.3084
batch size: (900, 900)
Epoch 79, accuracy: 0.1671
batch size: (916, 916)
Epoch 80, accuracy: 0.1657
Epoch 80, Train Loss: 0.5038, Val Loss: 2.3346
batch size: (908, 908)
Epoch 81, accuracy: 0.1690
batch size: (888, 888)
Epoch 82, accuracy: 0.1726
Epoch 82, Train Loss: 0.6917, Val Loss: 2.3325
batch size: (903, 903)
Epoch 83, accuracy: 0.1684
batch size: (916, 916)
Epoch 84, accuracy: 0.1669
Epoch 84, Train Loss: 0.7625, Val Loss: 2.2571
batch size: (896, 896)
Epoch 85, accuracy: 0.1689
batch size: (900, 900)
Epoch 86, accuracy: 0.1701
Epoch 86, Train Loss: 0.5442, Val Loss: 2.2134
batch size: (913, 913)
Epoch 87, accuracy: 0.1672
batch size: (896, 896)
Epoch 88, accuracy: 0.1658
Epoch 88, Train Loss: 0.6733, Val Loss: 2.2837
batch size: (911, 911)
Epoch 89, accuracy: 0.1670
batch size: (903, 903)
Epoch 90, accuracy: 0.1665
Epoch 90, Train Loss: 0.7103, Val Loss: 2.3108
batch size: (900, 900)
Epoch 91, accuracy: 0.1652
batch size: (897, 897)
Epoch 92, accuracy: 0.1667
Epoch 92, Train Loss: 0.6680, Val Loss: 2.2455
batch size: (904, 904)
Epoch 93, accuracy: 0.1668
batch size: (906, 906)
Epoch 94, accuracy: 0.1688
Epoch 94, Train Loss: 0.6898, Val Loss: 2.3259
batch size: (914, 914)
Epoch 95, accuracy: 0.1656
batch size: (886, 886)
Epoch 96, accuracy: 0.1687
Epoch 96, Train Loss: 0.6838, Val Loss: 2.4267
batch size: /root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
(908, 908)
Epoch 97, accuracy: 0.1667
batch size: (898, 898)
Epoch 98, accuracy: 0.1662
Epoch 98, Train Loss: 0.5702, Val Loss: 2.3497
batch size: (911, 911)
Epoch 99, accuracy: 0.1697
batch size: (914, 914)
Epoch 100, accuracy: 0.1698
Epoch 100, Train Loss: 0.6301, Val Loss: 2.3880
batch size: (898, 898)
Epoch 101, accuracy: 0.1662
batch size: (899, 899)
Epoch 102, accuracy: 0.1695
Epoch 102, Train Loss: 0.6224, Val Loss: 2.3299
batch size: (910, 910)
Epoch 103, accuracy: 0.1676
batch size: (889, 889)
Epoch 104, accuracy: 0.1658
Epoch 104, Train Loss: 0.5290, Val Loss: 2.3811
batch size: (887, 887)
Epoch 105, accuracy: 0.1677
batch size: (898, 898)
Epoch 106, accuracy: 0.1686
Epoch 106, Train Loss: 0.6762, Val Loss: 2.2897
batch size: (884, 884)
Epoch 107, accuracy: 0.1685
batch size: (882, 882)
Epoch 108, accuracy: 0.1661
Epoch 108, Train Loss: 0.5944, Val Loss: 2.2752
batch size: (897, 897)
Epoch 109, accuracy: 0.1672
batch size: (874, 874)
Epoch 110, accuracy: 0.1681
Epoch 110, Train Loss: 0.6439, Val Loss: 2.2788
batch size: (880, 880)
Epoch 111, accuracy: 0.1651
batch size: (902, 902)
Epoch 112, accuracy: 0.1670
Epoch 112, Train Loss: 0.6546, Val Loss: 2.4182
batch size: (888, 888)
Epoch 113, accuracy: 0.1661
batch size: (889, 889)
Epoch 114, accuracy: 0.1663
Epoch 114, Train Loss: 0.5028, Val Loss: 2.3345
batch size: (907, 907)
Epoch 115, accuracy: 0.1668
batch size: (899, 899)
Epoch 116, accuracy: 0.1670
Epoch 116, Train Loss: 0.7350, Val Loss: 2.1704
batch size: (885, 885)
Epoch 117, accuracy: 0.1659
batch size: (910, 910)
Epoch 118, accuracy: 0.1659
Epoch 118, Train Loss: 0.7031, Val Loss: 2.3021
batch size: (896, 896)
Epoch 119, accuracy: 0.1663
batch size: (904, 904)
Epoch 120, accuracy: 0.1699
Epoch 120, Train Loss: 0.6827, Val Loss: 2.3872
batch size: (903, 903)
Epoch 121, accuracy: 0.1701
batch size: (899, 899)
Epoch 122, accuracy: 0.1661
Epoch 122, Train Loss: 0.5469, Val Loss: 2.3301
batch size: (900, 900)
Epoch 123, accuracy: 0.1695
batch size: (907, 907)
Epoch 124, accuracy: 0.1682
Epoch 124, Train Loss: 0.6021, Val Loss: 2.2843
batch size: (899, 899)
Epoch 125, accuracy: 0.1669
batch size: (892, 892)
Epoch 126, accuracy: 0.1636
Epoch 126, Train Loss: 0.6270, Val Loss: 2.2736
batch size: (896, 896)
Epoch 127, accuracy: 0.1668
batch size: (906, 906)
Epoch 128, accuracy: 0.1683
Epoch 128, Train Loss: 0.6146, Val Loss: 2.2573
batch size: (911, 911)
Epoch 129, accuracy: 0.1677
batch size: (899, 899)
Epoch 130, accuracy: 0.1656
Epoch 130, Train Loss: 0.7060, Val Loss: 2.3111
batch size: (889, 889)
Epoch 131, accuracy: 0.1702
batch size: (883, 883)
Epoch 132, accuracy: 0.1680
Epoch 132, Train Loss: 0.5629, Val Loss: 2.3871
batch size: (888, 888)
Epoch 133, accuracy: 0.1689
batch size: (898, 898)
Epoch 134, accuracy: 0.1675
Epoch 134, Train Loss: 0.7198, Val Loss: 2.2582
batch size: (895, 895)
Epoch 135, accuracy: 0.1674
batch size: (895, 895)
Epoch 136, accuracy: 0.1676
Epoch 136, Train Loss: 0.6334, Val Loss: 2.3021
batch size: (898, 898)
Epoch 137, accuracy: 0.1676
batch size: (904, 904)
Epoch 138, accuracy: 0.1676
Epoch 138, Train Loss: 0.5957, Val Loss: 2.3497
batch size: (904, 904)
Epoch 139, accuracy: 0.1665
batch size: (885, 885)
Epoch 140, accuracy: 0.1694
Epoch 140, Train Loss: 0.6067, Val Loss: 2.2889
batch size: (875, 875)
Epoch 141, accuracy: 0.1650
batch size: (916, 916)
Epoch 142, accuracy: 0.1660
Epoch 142, Train Loss: 0.6173, Val Loss: 2.2217
batch size: (926, 926)
Epoch 143, accuracy: 0.1659
batch size: (894, 894)
Epoch 144, accuracy: 0.1704
Epoch 144, Train Loss: 0.7702, Val Loss: 2.2635
batch size: (894, 894)
Epoch 145, accuracy: 0.1678
batch size: (890, 890)
Epoch 146, accuracy: 0.1649
Epoch 146, Train Loss: 0.6796, Val Loss: 2.3007
batch size: (901, 901)
Epoch 147, accuracy: 0.1693
batch size: (920, 920)
Epoch 148, accuracy: 0.1691
Epoch 148, Train Loss: 0.6817, Val Loss: 2.4277
batch size: (917, 917)
Epoch 149, accuracy: 0.1682
batch size: (892, 892)
Epoch 150, accuracy: 0.1657
Epoch 150, Train Loss: 0.4811, Val Loss: 2.3446
batch size: (883, 883)
Epoch 151, accuracy: 0.1665
batch size: (924, 924)
Epoch 152, accuracy: 0.1688
Epoch 152, Train Loss: 0.5960, Val Loss: 2.3417
batch size: (908, 908)
Epoch 153, accuracy: 0.1674
batch size: (908, 908)
Epoch 154, accuracy: 0.1689
Epoch 154, Train Loss: 0.5676, Val Loss: 2.2777
batch size: (909, 909)
Epoch 155, accuracy: 0.1672
batch size: (923, 923)
Epoch 156, accuracy: 0.1670
Epoch 156, Train Loss: 0.6528, Val Loss: 2.2089
batch size: (904, 904)
Epoch 157, accuracy: 0.1624
batch size: (902, 902)
Epoch 158, accuracy: 0.1673
Epoch 158, Train Loss: 0.7187, Val Loss: 2.3021
batch size: (891, 891)
Epoch 159, accuracy: 0.1680
batch size: (892, 892)
Epoch 160, accuracy: 0.1661
Epoch 160, Train Loss: 0.6250, Val Loss: 2.3275
batch size: (916, 916)
Epoch 161, accuracy: 0.1684
batch size: (894, 894)
Epoch 162, accuracy: 0.1680
Epoch 162, Train Loss: 0.6202, Val Loss: 2.3128
batch size: (919, 919)
Epoch 163, accuracy: 0.1691
batch size: (887, 887)
Epoch 164, accuracy: 0.1652
Epoch 164, Train Loss: 0.6647, Val Loss: 2.3621
batch size: (915, 915)
Epoch 165, accuracy: 0.1691
batch size: (919, 919)
Epoch 166, accuracy: 0.1691
Epoch 166, Train Loss: 0.6597, Val Loss: 2.3295
batch size: (903, 903)
Epoch 167, accuracy: 0.1681
batch size: (891, 891)
Epoch 168, accuracy: 0.1672
Epoch 168, Train Loss: 0.6316, Val Loss: 2.3157
batch size: (926, 926)
Epoch 169, accuracy: 0.1648
batch size: (889, 889)
Epoch 170, accuracy: 0.1675
Epoch 170, Train Loss: 0.6446, Val Loss: 2.3491
batch size: (910, 910)
Epoch 171, accuracy: 0.1701
batch size: (914, 914)
Epoch 172, accuracy: 0.1687
Epoch 172, Train Loss: 0.7573, Val Loss: 2.2642
batch size: (904, 904)
Epoch 173, accuracy: 0.1680
batch size: (887, 887)
Epoch 174, accuracy: 0.1653
Epoch 174, Train Loss: 0.6332, Val Loss: 2.1949
batch size: (891, 891)
Epoch 175, accuracy: 0.1690
batch size: (903, 903)
Epoch 176, accuracy: 0.1649
Epoch 176, Train Loss: 0.6899, Val Loss: 2.3067
batch size: (914, 914)
Epoch 177, accuracy: 0.1683
batch size: (903, 903)
Epoch 178, accuracy: 0.1695
Epoch 178, Train Loss: 0.6522, Val Loss: 2.2516
batch size: (899, 899)
Epoch 179, accuracy: 0.1690
batch size: (878, 878)
Epoch 180, accuracy: 0.1678
Epoch 180, Train Loss: 0.6155, Val Loss: 2.3279
batch size: (911, 911)
Epoch 181, accuracy: 0.1670
batch size: (877, 877)
Epoch 182, accuracy: 0.1655
Epoch 182, Train Loss: 0.5928, Val Loss: 2.3147
batch size: (882, 882)
Epoch 183, accuracy: 0.1680
batch size: (898, 898)
Epoch 184, accuracy: 0.1676
Epoch 184, Train Loss: 0.6646, Val Loss: 2.2530
batch size: (912, 912)
Epoch 185, accuracy: 0.1685
batch size: (888, 888)
Epoch 186, accuracy: 0.1700
Epoch 186, Train Loss: 0.7036, Val Loss: 2.4131
batch size: (903, 903)
Epoch 187, accuracy: 0.1665
batch size: (888, 888)
Epoch 188, accuracy: 0.1675
Epoch 188, Train Loss: 0.5985, Val Loss: 2.3520
batch size: (925, 925)
Epoch 189, accuracy: 0.1647
batch size: (895, 895)
Epoch 190, accuracy: 0.1684
Epoch 190, Train Loss: 0.5855, Val Loss: 2.3068
batch size: (898, 898)
Epoch 191, accuracy: 0.1663
batch size: (898, 898)
Epoch 192, accuracy: 0.1669
Epoch 192, Train Loss: 0.7036, Val Loss: 2.2184
batch size: (895, 895)
Epoch 193, accuracy: 0.1693
batch size: (920, 920)
Epoch 194, accuracy: 0.1669
Epoch 194, Train Loss: 0.7064, Val Loss: 2.2842
batch size: (905, 905)
Epoch 195, accuracy: 0.1707
batch size: (912, 912)
Epoch 196, accuracy: 0.1654
Epoch 196, Train Loss: 0.5740, Val Loss: 2.3134
batch size: (887, 887)
Epoch 197, accuracy: 0.1668
batch size: (884, 884)
Epoch 198, accuracy: 0.1688
Epoch 198, Train Loss: 0.6856, Val Loss: 2.3606
batch size: (917, 917)
Epoch 199, accuracy: 0.1653
Loaded best model with val_loss = 1.0750731229782104
test :accuracy 0.4326, f1_macro: 0.2013, f1_micro: 0.4326, auc: 0.5436
Training GCN with 32 layers...
可训练参数: 2130185_GCN
不可训练参数: 0
batch size: (922, 922)
✅ Epoch 0: New best model saved with val_loss = 1.0812
Epoch 0, accuracy: 0.4056
Epoch 0, Train Loss: 1.2322, Val Loss: 1.0812
batch size: (895, 895)
Epoch 1, accuracy: 0.3817
batch size: (901, 901)
Epoch 2, accuracy: 0.3750
Epoch 2, Train Loss: 1.1839, Val Loss: 1.1409
batch size: (891, 891)
Epoch 3, accuracy: 0.3786
batch size: (885, 885)
Epoch 4, accuracy: 0.4158
Epoch 4, Train Loss: 1.2163, Val Loss: 1.0819
batch size: (873, 873)
✅ Epoch 5: New best model saved with val_loss = 1.0706
Epoch 5, accuracy: 0.4133
batch size: (895, 895)
Epoch 6, accuracy: 0.4126
Epoch 6, Train Loss: 1.2891, Val Loss: 1.0741
batch size: (906, 906)
Epoch 7, accuracy: 0.3996
batch size: (910, 910)
Epoch 8, accuracy: 0.3961
Epoch 8, Train Loss: 1.0955, Val Loss: 1.1259
batch size: (902, 902)
Epoch 9, accuracy: 0.3971
batch size: (908, 908)
Epoch 10, accuracy: 0.4033
Epoch 10, Train Loss: 1.0716, Val Loss: 1.1356
batch size: (900, 900)
Epoch 11, accuracy: 0.3988
batch size: (915, 915)
Epoch 12, accuracy: 0.3991
Epoch 12, Train Loss: 1.3130, Val Loss: 1.1312
batch size: (887, 887)
Epoch 13, accuracy: 0.4011
batch size: (915, 915)
Epoch 14, accuracy: 0.4009
Epoch 14, Train Loss: 1.0912, Val Loss: 1.1278
batch size: (896, 896)
Epoch 15, accuracy: 0.4005
batch size: (899, 899)
Epoch 16, accuracy: 0.4041
Epoch 16, Train Loss: 1.3326, Val Loss: 1.1210
batch size: (906, 906)
Epoch 17, accuracy: 0.4048
batch size: (881, 881)
Epoch 18, accuracy: 0.4007
Epoch 18, Train Loss: 1.3404, Val Loss: 1.1340
batch size: (896, 896)
Epoch 19, accuracy: 0.4025
batch size: (884, 884)
Epoch 20, accuracy: 0.3983
Epoch 20, Train Loss: 1.3897, Val Loss: 1.1280
batch size: (877, 877)
Epoch 21, accuracy: 0.4067
batch size: (919, 919)
Epoch 22, accuracy: 0.4010
Epoch 22, Train Loss: 1.0828, Val Loss: 1.1153
batch size: (903, 903)
Epoch 23, accuracy: 0.4057
batch size: (895, 895)
Epoch 24, accuracy: 0.4047
Epoch 24, Train Loss: 1.1494, Val Loss: 1.1097
batch size: (904, 904)
Epoch 25, accuracy: 0.4020
batch size: (910, 910)
Epoch 26, accuracy: 0.4007
Epoch 26, Train Loss: 1.3351, Val Loss: 1.1106
batch size: (893, 893)
Epoch 27, accuracy: 0.3974
batch size: (882, 882)
Epoch 28, accuracy: 0.4058
Epoch 28, Train Loss: 1.2699, Val Loss: 1.1234
batch size: (891, 891)
Epoch 29, accuracy: 0.4056
batch size: (900, 900)
Epoch 30, accuracy: 0.4043
Epoch 30, Train Loss: 1.2195, Val Loss: 1.1212
batch size: (888, 888)
Epoch 31, accuracy: 0.4028
batch size: (899, 899)
Epoch 32, accuracy: 0.4050
Epoch 32, Train Loss: 1.1907, Val Loss: 1.1290
batch size: (909, 909)
Epoch 33, accuracy: 0.4036
batch size: (890, 890)
Epoch 34, accuracy: 0.4048
Epoch 34, Train Loss: 1.2380, Val Loss: 1.1144
batch size: (902, 902)
Epoch 35, accuracy: 0.3995
batch size: (904, 904)
Epoch 36, accuracy: 0.4044
Epoch 36, Train Loss: 1.3225, Val Loss: 1.1101
batch size: (897, 897)
Epoch 37, accuracy: 0.3991
batch size: (895, 895)
Epoch 38, accuracy: 0.3982
Epoch 38, Train Loss: 1.1573, Val Loss: 1.1153
batch size: (899, 899)
Epoch 39, accuracy: 0.4065
batch size: (901, 901)
Epoch 40, accuracy: 0.4006
Epoch 40, Train Loss: 1.2285, Val Loss: 1.1140
batch size: (905, 905)
Epoch 41, accuracy: 0.4042
batch size: (897, 897)
Epoch 42, accuracy: 0.4026
Epoch 42, Train Loss: 1.1435, Val Loss: 1.1304
batch size: (903, 903)
Epoch 43, accuracy: 0.4049
batch size: (914, 914)
Epoch 44, accuracy: 0.3984
Epoch 44, Train Loss: 1.1319, Val Loss: 1.1170
batch size: (905, 905)
Epoch 45, accuracy: 0.4047
batch size: (877, 877)
Epoch 46, accuracy: 0.4007
Epoch 46, Train Loss: 1.3255, Val Loss: 1.1124
batch size: (883, 883)
Epoch 47, accuracy: 0.4000
batch size: (899, 899)
Epoch 48, accuracy: 0.3988
Epoch 48, Train Loss: 1.2484, Val Loss: 1.1221
batch size: (894, 894)
Epoch 49, accuracy: 0.4038
batch size: (908, 908)
Epoch 50, accuracy: 0.3984
Epoch 50, Train Loss: 1.1863, Val Loss: 1.1176
batch size: (874, 874)
Epoch 51, accuracy: 0.4008
batch size: (900, 900)
Epoch 52, accuracy: 0.3985
Epoch 52, Train Loss: 1.2309, Val Loss: 1.1088
batch size: (885, 885)
Epoch 53, accuracy: 0.4005
batch size: (885, 885)
Epoch 54, accuracy: 0.4046
Epoch 54, Train Loss: 1.2148, Val Loss: 1.1251
batch size: (893, 893)
Epoch 55, accuracy: 0.3998
batch size: (910, 910)
Epoch 56, accuracy: 0.4021
Epoch 56, Train Loss: 1.1945, Val Loss: 1.1132
batch size: (913, 913)
Epoch 57, accuracy: 0.4018
batch size: (912, 912)
Epoch 58, accuracy: 0.4013
Epoch 58, Train Loss: 1.3347, Val Loss: 1.1313
batch size: (911, 911)
Epoch 59, accuracy: 0.4016
batch size: (907, 907)
Epoch 60, accuracy: 0.3994
Epoch 60, Train Loss: 1.1610, Val Loss: 1.1204
batch size: (882, 882)
Epoch 61, accuracy: 0.3997
batch size: (879, 879)
Epoch 62, accuracy: 0.4016
Epoch 62, Train Loss: 1.1470, Val Loss: 1.1201
batch size: (912, 912)
Epoch 63, accuracy: 0.4043
batch size: (899, 899)
Epoch 64, accuracy: 0.3984
Epoch 64, Train Loss: 1.2129, Val Loss: 1.1359
batch size: (915, 915)
Epoch 65, accuracy: 0.4022
batch size: (881, 881)
Epoch 66, accuracy: 0.4034
Epoch 66, Train Loss: 1.2610, Val Loss: 1.1151
batch size: (909, 909)
Epoch 67, accuracy: 0.4039
batch size: (893, 893)
Epoch 68, accuracy: 0.3974
Epoch 68, Train Loss: 1.1790, Val Loss: 1.1139
batch size: (896, 896)
Epoch 69, accuracy: 0.4078
batch size: (906, 906)
Epoch 70, accuracy: 0.4053
Epoch 70, Train Loss: 1.1946, Val Loss: 1.1126
batch size: (909, 909)
Epoch 71, accuracy: 0.4053
batch size: (903, 903)
Epoch 72, accuracy: 0.4015
Epoch 72, Train Loss: 1.1102, Val Loss: 1.1148
batch size: (900, 900)
Epoch 73, accuracy: 0.4035
batch size: (896, 896)
Epoch 74, accuracy: 0.4039
Epoch 74, Train Loss: 1.1945, Val Loss: 1.1194
batch size: (919, 919)
Epoch 75, accuracy: 0.4024
batch size: (902, 902)
Epoch 76, accuracy: 0.4013
Epoch 76, Train Loss: 1.1218, Val Loss: 1.1312
batch size: (905, 905)
Epoch 77, accuracy: 0.3998
batch size: (900, 900)
Epoch 78, accuracy: 0.4014
Epoch 78, Train Loss: 1.2411, Val Loss: 1.1155
batch size: (910, 910)
Epoch 79, accuracy: 0.4096
batch size: (899, 899)
Epoch 80, accuracy: 0.4001
Epoch 80, Train Loss: 1.2519, Val Loss: 1.1217
batch size: (906, 906)
Epoch 81, accuracy: 0.4038
batch size: (902, 902)
Epoch 82, accuracy: 0.4025
Epoch 82, Train Loss: 1.2432, Val Loss: 1.1138
batch size: (899, 899)
Epoch 83, accuracy: 0.4017
batch size: (891, 891)
Epoch 84, accuracy: 0.3992
Epoch 84, Train Loss: 1.1533, Val Loss: 1.1251
batch size: (903, 903)
Epoch 85, accuracy: 0.4017
batch size: (913, 913)
Epoch 86, accuracy: 0.4022
Epoch 86, Train Loss: 1.2260, Val Loss: 1.1228
batch size: (899, 899)
Epoch 87, accuracy: 0.4048
batch size: (911, 911)
Epoch 88, accuracy: 0.4006
Epoch 88, Train Loss: 1.0638, Val Loss: 1.1378
batch size: (909, 909)
Epoch 89, accuracy: 0.4019
batch size: (889, 889)
Epoch 90, accuracy: 0.4048
Epoch 90, Train Loss: 1.0941, Val Loss: 1.1321
batch size: (891, 891)
Epoch 91, accuracy: 0.4040
batch size: (891, 891)
Epoch 92, accuracy: 0.4032
Epoch 92, Train Loss: 1.1501, Val Loss: 1.1146
batch size: (900, 900)
Epoch 93, accuracy: 0.4009
batch size: (911, 911)
Epoch 94, accuracy: 0.3996
Epoch 94, Train Loss: 1.1418, Val Loss: 1.1304
batch size: (894, 894)
Epoch 95, accuracy: 0.4042
batch size: (908, 908)
Epoch 96, accuracy: 0.4027
Epoch 96, Train Loss: 1.2364, Val Loss: 1.1174
batch size: (896, 896)
Epoch 97, accuracy: 0.4000
batch size: (889, 889)
Epoch 98, accuracy: 0.4023
Epoch 98, Train Loss: 1.2919, Val Loss: 1.1184
batch size: (904, 904)
Epoch 99, accuracy: 0.4030
batch size: (896, 896)
Epoch 100, accuracy: 0.4036
Epoch 100, Train Loss: 1.1683, Val Loss: 1.1235
batch size: (896, 896)
Epoch 101, accuracy: 0.4025
batch size: (893, 893)
Epoch 102, accuracy: 0.4032
Epoch 102, Train Loss: 1.1660, Val Loss: 1.1276
batch size: (888, 888)
Epoch 103, accuracy: 0.3998
batch size: (894, 894)
Epoch 104, accuracy: 0.4018
Epoch 104, Train Loss: 1.3005, Val Loss: 1.1243
batch size: (898, 898)
Epoch 105, accuracy: 0.3980
batch size: (901, 901)
Epoch 106, accuracy: 0.3989
Epoch 106, Train Loss: 1.2137, Val Loss: 1.1194
batch size: (896, 896)
Epoch 107, accuracy: 0.3985
batch size: (903, 903)
Epoch 108, accuracy: 0.4022
Epoch 108, Train Loss: 1.2140, Val Loss: 1.1173
batch size: (919, 919)
Epoch 109, accuracy: 0.4000
batch size: (908, 908)
Epoch 110, accuracy: 0.4027
Epoch 110, Train Loss: 1.1340, Val Loss: 1.1139
batch size: (909, 909)
Epoch 111, accuracy: 0.4029
batch size: (889, 889)
Epoch 112, accuracy: 0.4010
Epoch 112, Train Loss: 1.3196, Val Loss: 1.1152
batch size: (901, 901)
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 113, accuracy: 0.4056
batch size: (895, 895)
Epoch 114, accuracy: 0.3996
Epoch 114, Train Loss: 1.1149, Val Loss: 1.1258
batch size: (895, 895)
Epoch 115, accuracy: 0.4022
batch size: (902, 902)
Epoch 116, accuracy: 0.4067
Epoch 116, Train Loss: 1.2155, Val Loss: 1.1037
batch size: (903, 903)
Epoch 117, accuracy: 0.4002
batch size: (899, 899)
Epoch 118, accuracy: 0.4058
Epoch 118, Train Loss: 1.2276, Val Loss: 1.1164
batch size: (896, 896)
Epoch 119, accuracy: 0.4017
batch size: (897, 897)
Epoch 120, accuracy: 0.4043
Epoch 120, Train Loss: 1.1677, Val Loss: 1.1096
batch size: (872, 872)
Epoch 121, accuracy: 0.4005
batch size: (902, 902)
Epoch 122, accuracy: 0.4037
Epoch 122, Train Loss: 1.0723, Val Loss: 1.1228
batch size: (888, 888)
Epoch 123, accuracy: 0.4020
batch size: (907, 907)
Epoch 124, accuracy: 0.4040
Epoch 124, Train Loss: 1.1406, Val Loss: 1.1205
batch size: (897, 897)
Epoch 125, accuracy: 0.4071
batch size: (898, 898)
Epoch 126, accuracy: 0.4007
Epoch 126, Train Loss: 1.2673, Val Loss: 1.1094
batch size: (905, 905)
Epoch 127, accuracy: 0.3999
batch size: (906, 906)
Epoch 128, accuracy: 0.4045
Epoch 128, Train Loss: 1.0663, Val Loss: 1.1203
batch size: (871, 871)
Epoch 129, accuracy: 0.3992
batch size: (898, 898)
Epoch 130, accuracy: 0.4044
Epoch 130, Train Loss: 1.2051, Val Loss: 1.1142
batch size: (899, 899)
Epoch 131, accuracy: 0.4003
batch size: (890, 890)
Epoch 132, accuracy: 0.4015
Epoch 132, Train Loss: 1.1391, Val Loss: 1.1103
batch size: (897, 897)
Epoch 133, accuracy: 0.4002
batch size: (924, 924)
Epoch 134, accuracy: 0.3993
Epoch 134, Train Loss: 1.2276, Val Loss: 1.1188
batch size: (884, 884)
Epoch 135, accuracy: 0.4014
batch size: (907, 907)
Epoch 136, accuracy: 0.3955
Epoch 136, Train Loss: 1.2868, Val Loss: 1.1348
batch size: (897, 897)
Epoch 137, accuracy: 0.4037
batch size: (887, 887)
Epoch 138, accuracy: 0.4049
Epoch 138, Train Loss: 1.2476, Val Loss: 1.1161
batch size: (882, 882)
Epoch 139, accuracy: 0.4050
batch size: (902, 902)
Epoch 140, accuracy: 0.3970
Epoch 140, Train Loss: 1.2990, Val Loss: 1.1171
batch size: (889, 889)
Epoch 141, accuracy: 0.4039
batch size: (901, 901)
Epoch 142, accuracy: 0.4042
Epoch 142, Train Loss: 1.2032, Val Loss: 1.1256
batch size: (908, 908)
Epoch 143, accuracy: 0.3969
batch size: (893, 893)
Epoch 144, accuracy: 0.4054
Epoch 144, Train Loss: 1.3597, Val Loss: 1.1139
batch size: (894, 894)
Epoch 145, accuracy: 0.4034
batch size: (886, 886)
Epoch 146, accuracy: 0.4054
Epoch 146, Train Loss: 1.1644, Val Loss: 1.1176
batch size: (895, 895)
Epoch 147, accuracy: 0.4029
batch size: (913, 913)
Epoch 148, accuracy: 0.4018
Epoch 148, Train Loss: 1.1632, Val Loss: 1.1224
batch size: (898, 898)
Epoch 149, accuracy: 0.4040
batch size: (912, 912)
Epoch 150, accuracy: 0.4039
Epoch 150, Train Loss: 1.2825, Val Loss: 1.1349
batch size: (897, 897)
Epoch 151, accuracy: 0.4045
batch size: (908, 908)
Epoch 152, accuracy: 0.4034
Epoch 152, Train Loss: 1.1328, Val Loss: 1.1273
batch size: (902, 902)
Epoch 153, accuracy: 0.4023
batch size: (895, 895)
Epoch 154, accuracy: 0.4003
Epoch 154, Train Loss: 1.3857, Val Loss: 1.1200
batch size: (899, 899)
Epoch 155, accuracy: 0.3983
batch size: (892, 892)
Epoch 156, accuracy: 0.4052
Epoch 156, Train Loss: 1.1562, Val Loss: 1.1127
batch size: (893, 893)
Epoch 157, accuracy: 0.4033
batch size: (903, 903)
Epoch 158, accuracy: 0.3989
Epoch 158, Train Loss: 1.2377, Val Loss: 1.1262
batch size: (909, 909)
Epoch 159, accuracy: 0.4014
batch size: (883, 883)
Epoch 160, accuracy: 0.4016
Epoch 160, Train Loss: 1.3650, Val Loss: 1.1187
batch size: (879, 879)
Epoch 161, accuracy: 0.4040
batch size: (885, 885)
Epoch 162, accuracy: 0.4008
Epoch 162, Train Loss: 1.1663, Val Loss: 1.1215
batch size: (918, 918)
Epoch 163, accuracy: 0.4030
batch size: (882, 882)
Epoch 164, accuracy: 0.4023
Epoch 164, Train Loss: 1.2363, Val Loss: 1.1121
batch size: (893, 893)
Epoch 165, accuracy: 0.4055
batch size: (893, 893)
Epoch 166, accuracy: 0.4067
Epoch 166, Train Loss: 1.1315, Val Loss: 1.1226
batch size: (886, 886)
Epoch 167, accuracy: 0.4049
batch size: (894, 894)
Epoch 168, accuracy: 0.4008
Epoch 168, Train Loss: 1.1517, Val Loss: 1.1121
batch size: (891, 891)
Epoch 169, accuracy: 0.3992
batch size: (902, 902)
Epoch 170, accuracy: 0.4041
Epoch 170, Train Loss: 1.2040, Val Loss: 1.1151
batch size: (904, 904)
Epoch 171, accuracy: 0.4016
batch size: (904, 904)
Epoch 172, accuracy: 0.3992
Epoch 172, Train Loss: 1.2837, Val Loss: 1.1386
batch size: (891, 891)
Epoch 173, accuracy: 0.4011
batch size: (895, 895)
Epoch 174, accuracy: 0.4039
Epoch 174, Train Loss: 1.0431, Val Loss: 1.1193
batch size: (892, 892)
Epoch 175, accuracy: 0.3999
batch size: (891, 891)
Epoch 176, accuracy: 0.4039
Epoch 176, Train Loss: 1.3558, Val Loss: 1.1169
batch size: (902, 902)
Epoch 177, accuracy: 0.4002
batch size: (916, 916)
Epoch 178, accuracy: 0.4005
Epoch 178, Train Loss: 1.2362, Val Loss: 1.1208
batch size: (903, 903)
Epoch 179, accuracy: 0.4023
batch size: (927, 927)
Epoch 180, accuracy: 0.4023
Epoch 180, Train Loss: 1.1215, Val Loss: 1.1224
batch size: (910, 910)
Epoch 181, accuracy: 0.4060
batch size: (899, 899)
Epoch 182, accuracy: 0.4026
Epoch 182, Train Loss: 1.1373, Val Loss: 1.1250
batch size: (900, 900)
Epoch 183, accuracy: 0.4027
batch size: (897, 897)
Epoch 184, accuracy: 0.4013
Epoch 184, Train Loss: 1.2457, Val Loss: 1.1330
batch size: (889, 889)
Epoch 185, accuracy: 0.4043
batch size: (901, 901)
Epoch 186, accuracy: 0.4001
Epoch 186, Train Loss: 1.1377, Val Loss: 1.1268
batch size: (918, 918)
Epoch 187, accuracy: 0.3977
batch size: (911, 911)
Epoch 188, accuracy: 0.3990
Epoch 188, Train Loss: 1.2287, Val Loss: 1.1088
batch size: (895, 895)
Epoch 189, accuracy: 0.4037
batch size: (895, 895)
Epoch 190, accuracy: 0.4002
Epoch 190, Train Loss: 1.1166, Val Loss: 1.1219
batch size: (900, 900)
Epoch 191, accuracy: 0.3968
batch size: (889, 889)
Epoch 192, accuracy: 0.4036
Epoch 192, Train Loss: 1.2070, Val Loss: 1.1082
batch size: (898, 898)
Epoch 193, accuracy: 0.3996
batch size: (918, 918)
Epoch 194, accuracy: 0.4089
Epoch 194, Train Loss: 1.1554, Val Loss: 1.1279
batch size: (902, 902)
Epoch 195, accuracy: 0.4044
batch size: (902, 902)
Epoch 196, accuracy: 0.4034
Epoch 196, Train Loss: 1.1663, Val Loss: 1.1125
batch size: (896, 896)
Epoch 197, accuracy: 0.4029
batch size: (877, 877)
Epoch 198, accuracy: 0.4046
Epoch 198, Train Loss: 1.1257, Val Loss: 1.1229
batch size: (924, 924)
Epoch 199, accuracy: 0.4037
Loaded best model with val_loss = 1.0706292390823364
test :accuracy 0.4135, f1_macro: 0.2736, f1_micro: 0.4135, auc: 0.5306
Training GraphSAGE with 2 layers...
可训练参数: 262153_GraphSAGE
不可训练参数: 0
batch size: (905, 905)
✅ Epoch 0: New best model saved with val_loss = 1.0985
Epoch 0, accuracy: 0.4666
Epoch 0, Train Loss: 1.2386, Val Loss: 1.0985
batch size: (898, 898)
✅ Epoch 1: New best model saved with val_loss = 1.0962
Epoch 1, accuracy: 0.4153
batch size: (915, 915)
✅ Epoch 2: New best model saved with val_loss = 1.0920
Epoch 2, accuracy: 0.4437
Epoch 2, Train Loss: 0.4508, Val Loss: 1.0920
batch size: (886, 886)
✅ Epoch 3: New best model saved with val_loss = 1.0862
Epoch 3, accuracy: 0.5014
batch size: (895, 895)
✅ Epoch 4: New best model saved with val_loss = 1.0777
Epoch 4, accuracy: 0.6054
Epoch 4, Train Loss: 0.2768, Val Loss: 1.0777
batch size: (899, 899)
✅ Epoch 5: New best model saved with val_loss = 1.0660
Epoch 5, accuracy: 0.6799
batch size: (890, 890)
✅ Epoch 6: New best model saved with val_loss = 1.0523
Epoch 6, accuracy: 0.6970
Epoch 6, Train Loss: 0.1643, Val Loss: 1.0523
batch size: (881, 881)
✅ Epoch 7: New best model saved with val_loss = 1.0380
Epoch 7, accuracy: 0.6969
batch size: (911, 911)
✅ Epoch 8: New best model saved with val_loss = 1.0238
Epoch 8, accuracy: 0.6851
Epoch 8, Train Loss: 0.0620, Val Loss: 1.0238
batch size: (880, 880)
✅ Epoch 9: New best model saved with val_loss = 1.0105
Epoch 9, accuracy: 0.6736
batch size: (903, 903)
✅ Epoch 10: New best model saved with val_loss = 0.9961
Epoch 10, accuracy: 0.6687
Epoch 10, Train Loss: 0.0241, Val Loss: 0.9961
batch size: (904, 904)
✅ Epoch 11: New best model saved with val_loss = 0.9819
Epoch 11, accuracy: 0.6718
batch size: (917, 917)
✅ Epoch 12: New best model saved with val_loss = 0.9681
Epoch 12, accuracy: 0.6751
Epoch 12, Train Loss: 0.0192, Val Loss: 0.9681
batch size: (894, 894)
✅ Epoch 13: New best model saved with val_loss = 0.9552
Epoch 13, accuracy: 0.6847
batch size: (892, 892)
✅ Epoch 14: New best model saved with val_loss = 0.9407
Epoch 14, accuracy: 0.6958
Epoch 14, Train Loss: 0.0040, Val Loss: 0.9407
batch size: (887, 887)
✅ Epoch 15: New best model saved with val_loss = 0.9253
Epoch 15, accuracy: 0.7005
batch size: (898, 898)
✅ Epoch 16: New best model saved with val_loss = 0.9125
Epoch 16, accuracy: 0.7055
Epoch 16, Train Loss: 0.0009, Val Loss: 0.9125
batch size: (897, 897)
✅ Epoch 17: New best model saved with val_loss = 0.8984
Epoch 17, accuracy: 0.7122
batch size: (909, 909)
✅ Epoch 18: New best model saved with val_loss = 0.8839
Epoch 18, accuracy: 0.7119
Epoch 18, Train Loss: 0.0003, Val Loss: 0.8839
batch size: (893, 893)
✅ Epoch 19: New best model saved with val_loss = 0.8706
Epoch 19, accuracy: 0.7145
batch size: (905, 905)
✅ Epoch 20: New best model saved with val_loss = 0.8590
Epoch 20, accuracy: 0.7115
Epoch 20, Train Loss: 0.0015, Val Loss: 0.8590
batch size: (917, 917)
✅ Epoch 21: New best model saved with val_loss = 0.8460
Epoch 21, accuracy: 0.7129
batch size: (905, 905)
✅ Epoch 22: New best model saved with val_loss = 0.8341
Epoch 22, accuracy: 0.7128
Epoch 22, Train Loss: 0.0002, Val Loss: 0.8341
batch size: (886, 886)
✅ Epoch 23: New best model saved with val_loss = 0.8225
Epoch 23, accuracy: 0.7129
batch size: (905, 905)
✅ Epoch 24: New best model saved with val_loss = 0.8122
Epoch 24, accuracy: 0.7099
Epoch 24, Train Loss: 0.0005, Val Loss: 0.8122
batch size: (910, 910)
✅ Epoch 25: New best model saved with val_loss = 0.7978
Epoch 25, accuracy: 0.7080
batch size: (900, 900)
✅ Epoch 26: New best model saved with val_loss = 0.7872
Epoch 26, accuracy: 0.7072
Epoch 26, Train Loss: 0.0002, Val Loss: 0.7872
batch size: (902, 902)
✅ Epoch 27: New best model saved with val_loss = 0.7751
Epoch 27, accuracy: 0.7062
batch size: (907, 907)
✅ Epoch 28: New best model saved with val_loss = 0.7622
Epoch 28, accuracy: 0.7070
Epoch 28, Train Loss: 0.0002, Val Loss: 0.7622
batch size: (901, 901)
✅ Epoch 29: New best model saved with val_loss = 0.7536
Epoch 29, accuracy: 0.7069
batch size: (897, 897)
✅ Epoch 30: New best model saved with val_loss = 0.7384
Epoch 30, accuracy: 0.7077
Epoch 30, Train Loss: 0.0031, Val Loss: 0.7384
batch size: (893, 893)
✅ Epoch 31: New best model saved with val_loss = 0.7275
Epoch 31, accuracy: 0.7065
batch size: (897, 897)
✅ Epoch 32: New best model saved with val_loss = 0.7139
Epoch 32, accuracy: 0.7069
Epoch 32, Train Loss: 0.0001, Val Loss: 0.7139
batch size: (902, 902)
✅ Epoch 33: New best model saved with val_loss = 0.7076
Epoch 33, accuracy: 0.7029
batch size: (891, 891)
✅ Epoch 34: New best model saved with val_loss = 0.6960
Epoch 34, accuracy: 0.7071
Epoch 34, Train Loss: 0.0000, Val Loss: 0.6960
batch size: (907, 907)
✅ Epoch 35: New best model saved with val_loss = 0.6860
Epoch 35, accuracy: 0.7037
batch size: (903, 903)
✅ Epoch 36: New best model saved with val_loss = 0.6801
Epoch 36, accuracy: 0.7042
Epoch 36, Train Loss: 0.0001, Val Loss: 0.6801
batch size: (901, 901)
✅ Epoch 37: New best model saved with val_loss = 0.6706
Epoch 37, accuracy: 0.7035
batch size: (909, 909)
✅ Epoch 38: New best model saved with val_loss = 0.6596
Epoch 38, accuracy: 0.7071
Epoch 38, Train Loss: 0.0001, Val Loss: 0.6596
batch size: (917, 917)
✅ Epoch 39: New best model saved with val_loss = 0.6525
Epoch 39, accuracy: 0.7056
batch size: (894, 894)
✅ Epoch 40: New best model saved with val_loss = 0.6501
Epoch 40, accuracy: 0.7059
Epoch 40, Train Loss: 0.0001, Val Loss: 0.6501
batch size: (901, 901)
✅ Epoch 41: New best model saved with val_loss = 0.6459
Epoch 41, accuracy: 0.7086
batch size: (892, 892)
✅ Epoch 42: New best model saved with val_loss = 0.6398
Epoch 42, accuracy: 0.7070
Epoch 42, Train Loss: 0.0000, Val Loss: 0.6398
batch size: (888, 888)
✅ Epoch 43: New best model saved with val_loss = 0.6348
Epoch 43, accuracy: 0.7070
batch size: (898, 898)
✅ Epoch 44: New best model saved with val_loss = 0.6328
Epoch 44, accuracy: 0.7067
Epoch 44, Train Loss: 0.0001, Val Loss: 0.6328
batch size: (900, 900)
✅ Epoch 45: New best model saved with val_loss = 0.6230
Epoch 45, accuracy: 0.7104
batch size: (894, 894)
Epoch 46, accuracy: 0.7065
Epoch 46, Train Loss: 0.0002, Val Loss: 0.6311
batch size: (905, 905)
✅ Epoch 47: New best model saved with val_loss = 0.6190
Epoch 47, accuracy: 0.7103
batch size: (918, 918)
Epoch 48, accuracy: 0.7087
Epoch 48, Train Loss: 0.0001, Val Loss: 0.6224
batch size: (876, 876)
Epoch 49, accuracy: 0.7102
batch size: (901, 901)
Epoch 50, accuracy: 0.7087
Epoch 50, Train Loss: 0.0000, Val Loss: 0.6275
batch size: (890, 890)
Epoch 51, accuracy: 0.7084
batch size: (900, 900)
Epoch 52, accuracy: 0.7093
Epoch 52, Train Loss: 0.0000, Val Loss: 0.6423
batch size: (912, 912)
Epoch 53, accuracy: 0.7089
batch size: (904, 904)
Epoch 54, accuracy: 0.7090
Epoch 54, Train Loss: 0.0001, Val Loss: 0.6443
batch size: (910, 910)
Epoch 55, accuracy: 0.7094
batch size: (892, 892)
Epoch 56, accuracy: 0.7111
Epoch 56, Train Loss: 0.0001, Val Loss: 0.6550
batch size: (910, 910)
Epoch 57, accuracy: 0.7115
batch size: (914, 914)
Epoch 58, accuracy: 0.7098
Epoch 58, Train Loss: 0.0001, Val Loss: 0.6739
batch size: (902, 902)
Epoch 59, accuracy: 0.7084
batch size: (900, 900)
Epoch 60, accuracy: 0.7092
Epoch 60, Train Loss: 0.0000, Val Loss: 0.6925
batch size: (919, 919)
Epoch 61, accuracy: 0.7073
batch size: (909, 909)
Epoch 62, accuracy: 0.7091
Epoch 62, Train Loss: 0.0001, Val Loss: 0.7217
batch size: (894, 894)
Epoch 63, accuracy: 0.7091
batch size: (889, 889)
Epoch 64, accuracy: 0.7073
Epoch 64, Train Loss: 0.0001, Val Loss: 0.7492
batch size: (908, 908)
Epoch 65, accuracy: 0.7073
batch size: (890, 890)
Epoch 66, accuracy: 0.7081
Epoch 66, Train Loss: 0.0001, Val Loss: 0.7751
batch size: (885, 885)
Epoch 67, accuracy: 0.7085
batch size: (896, 896)
Epoch 68, accuracy: 0.7118
Epoch 68, Train Loss: 0.0001, Val Loss: 0.7962
batch size: (900, 900)
Epoch 69, accuracy: 0.7058
batch size: (902, 902)
Epoch 70, accuracy: 0.7093
Epoch 70, Train Loss: 0.0001, Val Loss: 0.8383
batch size: (892, 892)
Epoch 71, accuracy: 0.7051
batch size: (898, 898)
Epoch 72, accuracy: 0.7076
Epoch 72, Train Loss: 0.0001, Val Loss: 0.8606
batch size: (914, 914)
Epoch 73, accuracy: 0.7092
batch size: (894, 894)
Epoch 74, accuracy: 0.7075
Epoch 74, Train Loss: 0.0001, Val Loss: 0.8660
batch size: (900, 900)
Epoch 75, accuracy: 0.7079
batch size: (910, 910)
Epoch 76, accuracy: 0.7086
Epoch 76, Train Loss: 0.0001, Val Loss: 0.8903
batch size: (887, 887)
Epoch 77, accuracy: 0.7062
batch size: (875, 875)
Epoch 78, accuracy: 0.7081
Epoch 78, Train Loss: 0.0001, Val Loss: 0.9092
batch size: (885, 885)
Epoch 79, accuracy: 0.7068
batch size: (927, 927)
Epoch 80, accuracy: 0.7085
Epoch 80, Train Loss: 0.0002, Val Loss: 0.9281
batch size: (912, 912)
Epoch 81, accuracy: 0.7069
batch size: (907, 907)
Epoch 82, accuracy: 0.7076
Epoch 82, Train Loss: 0.0001, Val Loss: 0.9429
batch size: (903, 903)
Epoch 83, accuracy: 0.7067
batch size: (909, 909)
Epoch 84, accuracy: 0.7112
Epoch 84, Train Loss: 0.0001, Val Loss: 0.9620
batch size: (882, 882)
Epoch 85, accuracy: 0.7086
batch size: (882, 882)
Epoch 86, accuracy: 0.7072
Epoch 86, Train Loss: 0.0001, Val Loss: 0.9647
batch size: (913, 913)
Epoch 87, accuracy: 0.7102
batch size: (897, 897)
Epoch 88, accuracy: 0.7061
Epoch 88, Train Loss: 0.0001, Val Loss: 0.9837
batch size: (879, 879)
Epoch 89, accuracy: 0.7065
batch size: (900, 900)
Epoch 90, accuracy: 0.7090
Epoch 90, Train Loss: 0.0000, Val Loss: 0.9895
batch size: (910, 910)
Epoch 91, accuracy: 0.7088
batch size: (894, 894)
Epoch 92, accuracy: 0.7058
Epoch 92, Train Loss: 0.0001, Val Loss: 0.9752
batch size: (911, 911)
Epoch 93, accuracy: 0.7100
batch size: (920, 920)
Epoch 94, accuracy: 0.7074
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 94, Train Loss: 0.0000, Val Loss: 1.0117
batch size: (915, 915)
Epoch 95, accuracy: 0.7076
batch size: (910, 910)
Epoch 96, accuracy: 0.7068
Epoch 96, Train Loss: 0.0001, Val Loss: 1.0263
batch size: (885, 885)
Epoch 97, accuracy: 0.7072
batch size: (897, 897)
Epoch 98, accuracy: 0.7053
Epoch 98, Train Loss: 0.0001, Val Loss: 1.0135
batch size: (888, 888)
Epoch 99, accuracy: 0.7079
batch size: (913, 913)
Epoch 100, accuracy: 0.7078
Epoch 100, Train Loss: 0.0000, Val Loss: 1.0059
batch size: (887, 887)
Epoch 101, accuracy: 0.7069
batch size: (918, 918)
Epoch 102, accuracy: 0.7089
Epoch 102, Train Loss: 0.0001, Val Loss: 1.0319
batch size: (900, 900)
Epoch 103, accuracy: 0.7088
batch size: (899, 899)
Epoch 104, accuracy: 0.7072
Epoch 104, Train Loss: 0.0001, Val Loss: 1.0306
batch size: (906, 906)
Epoch 105, accuracy: 0.7064
batch size: (904, 904)
Epoch 106, accuracy: 0.7052
Epoch 106, Train Loss: 0.0003, Val Loss: 1.0249
batch size: (892, 892)
Epoch 107, accuracy: 0.7076
batch size: (912, 912)
Epoch 108, accuracy: 0.7059
Epoch 108, Train Loss: 0.0000, Val Loss: 1.0348
batch size: (894, 894)
Epoch 109, accuracy: 0.7093
batch size: (886, 886)
Epoch 110, accuracy: 0.7058
Epoch 110, Train Loss: 0.0002, Val Loss: 1.0234
batch size: (882, 882)
Epoch 111, accuracy: 0.7070
batch size: (902, 902)
Epoch 112, accuracy: 0.7058
Epoch 112, Train Loss: 0.0001, Val Loss: 1.0154
batch size: (923, 923)
Epoch 113, accuracy: 0.7068
batch size: (899, 899)
Epoch 114, accuracy: 0.7056
Epoch 114, Train Loss: 0.0001, Val Loss: 1.0209
batch size: (886, 886)
Epoch 115, accuracy: 0.7057
batch size: (907, 907)
Epoch 116, accuracy: 0.7052
Epoch 116, Train Loss: 0.0001, Val Loss: 1.0132
batch size: (891, 891)
Epoch 117, accuracy: 0.7063
batch size: (901, 901)
Epoch 118, accuracy: 0.7070
Epoch 118, Train Loss: 0.0001, Val Loss: 1.0254
batch size: (912, 912)
Epoch 119, accuracy: 0.7049
batch size: (902, 902)
Epoch 120, accuracy: 0.7071
Epoch 120, Train Loss: 0.0001, Val Loss: 1.0199
batch size: (898, 898)
Epoch 121, accuracy: 0.7078
batch size: (885, 885)
Epoch 122, accuracy: 0.7088
Epoch 122, Train Loss: 0.0000, Val Loss: 1.0235
batch size: (872, 872)
Epoch 123, accuracy: 0.7083
batch size: (899, 899)
Epoch 124, accuracy: 0.7096
Epoch 124, Train Loss: 0.0001, Val Loss: 1.0207
batch size: (906, 906)
Epoch 125, accuracy: 0.7063
batch size: (897, 897)
Epoch 126, accuracy: 0.7073
Epoch 126, Train Loss: 0.0001, Val Loss: 1.0219
batch size: (915, 915)
Epoch 127, accuracy: 0.7063
batch size: (890, 890)
Epoch 128, accuracy: 0.7081
Epoch 128, Train Loss: 0.0006, Val Loss: 1.0167
batch size: (899, 899)
Epoch 129, accuracy: 0.7067
batch size: (901, 901)
Epoch 130, accuracy: 0.7104
Epoch 130, Train Loss: 0.0001, Val Loss: 1.0099
batch size: (904, 904)
Epoch 131, accuracy: 0.7057
batch size: (882, 882)
Epoch 132, accuracy: 0.7098
Epoch 132, Train Loss: 0.0001, Val Loss: 1.0399
batch size: (901, 901)
Epoch 133, accuracy: 0.7062
batch size: (909, 909)
Epoch 134, accuracy: 0.7060
Epoch 134, Train Loss: 0.0001, Val Loss: 1.0264
batch size: (894, 894)
Epoch 135, accuracy: 0.7060
batch size: (894, 894)
Epoch 136, accuracy: 0.7091
Epoch 136, Train Loss: 0.0002, Val Loss: 1.0157
batch size: (921, 921)
Epoch 137, accuracy: 0.7079
batch size: (907, 907)
Epoch 138, accuracy: 0.7067
Epoch 138, Train Loss: 0.0001, Val Loss: 1.0302
batch size: (893, 893)
Epoch 139, accuracy: 0.7063
batch size: (917, 917)
Epoch 140, accuracy: 0.7076
Epoch 140, Train Loss: 0.0001, Val Loss: 1.0091
batch size: (901, 901)
Epoch 141, accuracy: 0.7086
batch size: (897, 897)
Epoch 142, accuracy: 0.7080
Epoch 142, Train Loss: 0.0001, Val Loss: 1.0245
batch size: (897, 897)
Epoch 143, accuracy: 0.7061
batch size: (892, 892)
Epoch 144, accuracy: 0.7084
Epoch 144, Train Loss: 0.0001, Val Loss: 1.0157
batch size: (911, 911)
Epoch 145, accuracy: 0.7054
batch size: (908, 908)
Epoch 146, accuracy: 0.7057
Epoch 146, Train Loss: 0.0000, Val Loss: 1.0229
batch size: (892, 892)
Epoch 147, accuracy: 0.7067
batch size: (889, 889)
Epoch 148, accuracy: 0.7078
Epoch 148, Train Loss: 0.0000, Val Loss: 1.0432
batch size: (925, 925)
Epoch 149, accuracy: 0.7071
batch size: (885, 885)
Epoch 150, accuracy: 0.7076
Epoch 150, Train Loss: 0.0001, Val Loss: 1.0243
batch size: (899, 899)
Epoch 151, accuracy: 0.7061
batch size: (903, 903)
Epoch 152, accuracy: 0.7064
Epoch 152, Train Loss: 0.0002, Val Loss: 1.0210
batch size: (899, 899)
Epoch 153, accuracy: 0.7068
batch size: (892, 892)
Epoch 154, accuracy: 0.7046
Epoch 154, Train Loss: 0.0001, Val Loss: 1.0358
batch size: (889, 889)
Epoch 155, accuracy: 0.7057
batch size: (894, 894)
Epoch 156, accuracy: 0.7061
Epoch 156, Train Loss: 0.0001, Val Loss: 1.0157
batch size: (891, 891)
Epoch 157, accuracy: 0.7065
batch size: (892, 892)
Epoch 158, accuracy: 0.7070
Epoch 158, Train Loss: 0.0001, Val Loss: 1.0339
batch size: (884, 884)
Epoch 159, accuracy: 0.7062
batch size: (901, 901)
Epoch 160, accuracy: 0.7030
Epoch 160, Train Loss: 0.0000, Val Loss: 1.0197
batch size: (890, 890)
Epoch 161, accuracy: 0.7061
batch size: (910, 910)
Epoch 162, accuracy: 0.7091
Epoch 162, Train Loss: 0.0001, Val Loss: 1.0164
batch size: (903, 903)
Epoch 163, accuracy: 0.7083
batch size: (919, 919)
Epoch 164, accuracy: 0.7075
Epoch 164, Train Loss: 0.0001, Val Loss: 1.0308
batch size: (900, 900)
Epoch 165, accuracy: 0.7064
batch size: (891, 891)
Epoch 166, accuracy: 0.7072
Epoch 166, Train Loss: 0.0004, Val Loss: 1.0175
batch size: (899, 899)
Epoch 167, accuracy: 0.7050
batch size: (899, 899)
Epoch 168, accuracy: 0.7054
Epoch 168, Train Loss: 0.0003, Val Loss: 1.0299
batch size: (904, 904)
Epoch 169, accuracy: 0.7044
batch size: (903, 903)
Epoch 170, accuracy: 0.7080
Epoch 170, Train Loss: 0.0000, Val Loss: 1.0207
batch size: (893, 893)
Epoch 171, accuracy: 0.7063
batch size: (892, 892)
Epoch 172, accuracy: 0.7063
Epoch 172, Train Loss: 0.0002, Val Loss: 1.0214
batch size: (908, 908)
Epoch 173, accuracy: 0.7067
batch size: (887, 887)
Epoch 174, accuracy: 0.7063
Epoch 174, Train Loss: 0.0001, Val Loss: 1.0294
batch size: (903, 903)
Epoch 175, accuracy: 0.7072
batch size: (898, 898)
Epoch 176, accuracy: 0.7069
Epoch 176, Train Loss: 0.0001, Val Loss: 1.0242
batch size: (895, 895)
Epoch 177, accuracy: 0.7093
batch size: (899, 899)
Epoch 178, accuracy: 0.7069
Epoch 178, Train Loss: 0.0000, Val Loss: 1.0292
batch size: (900, 900)
Epoch 179, accuracy: 0.7078
batch size: (894, 894)
Epoch 180, accuracy: 0.7087
Epoch 180, Train Loss: 0.0000, Val Loss: 1.0290
batch size: (905, 905)
Epoch 181, accuracy: 0.7088
batch size: (906, 906)
Epoch 182, accuracy: 0.7071
Epoch 182, Train Loss: 0.0001, Val Loss: 1.0307
batch size: (905, 905)
Epoch 183, accuracy: 0.7064
batch size: (893, 893)
Epoch 184, accuracy: 0.7079
Epoch 184, Train Loss: 0.0001, Val Loss: 1.0201
batch size: (892, 892)
Epoch 185, accuracy: 0.7061
batch size: (896, 896)
Epoch 186, accuracy: 0.7068
Epoch 186, Train Loss: 0.0001, Val Loss: 1.0159
batch size: (906, 906)
Epoch 187, accuracy: 0.7061
batch size: (894, 894)
Epoch 188, accuracy: 0.7064
Epoch 188, Train Loss: 0.0001, Val Loss: 1.0482
batch size: (890, 890)
Epoch 189, accuracy: 0.7064
batch size: (897, 897)
Epoch 190, accuracy: 0.7063
Epoch 190, Train Loss: 0.0002, Val Loss: 1.0294
batch size: (889, 889)
Epoch 191, accuracy: 0.7091
batch size: (889, 889)
Epoch 192, accuracy: 0.7069
Epoch 192, Train Loss: 0.0000, Val Loss: 1.0222
batch size: (888, 888)
Epoch 193, accuracy: 0.7083
batch size: (905, 905)
Epoch 194, accuracy: 0.7052
Epoch 194, Train Loss: 0.0000, Val Loss: 1.0408
batch size: (904, 904)
Epoch 195, accuracy: 0.7077
batch size: (904, 904)
Epoch 196, accuracy: 0.7067
Epoch 196, Train Loss: 0.0000, Val Loss: 1.0212
batch size: (907, 907)
Epoch 197, accuracy: 0.7054
batch size: (912, 912)
Epoch 198, accuracy: 0.7066
Epoch 198, Train Loss: 0.0001, Val Loss: 1.0084
batch size: (925, 925)
Epoch 199, accuracy: 0.7058
Loaded best model with val_loss = 0.61900395154953
test :accuracy 0.7082, f1_macro: 0.7085, f1_micro: 0.7082, auc: 0.8507
Training GraphSAGE with 8 layers...
可训练参数: 45833_GraphSAGE
不可训练参数: 0
batch size: (895, 895)
✅ Epoch 0: New best model saved with val_loss = 1.0953
Epoch 0, accuracy: 0.4057
Epoch 0, Train Loss: 1.2204, Val Loss: 1.0953
batch size: (889, 889)
Epoch 1, accuracy: 0.3987
batch size: (901, 901)
Epoch 2, accuracy: 0.3964
Epoch 2, Train Loss: 1.1122, Val Loss: 1.0983
batch size: (905, 905)
Epoch 3, accuracy: 0.3641
batch size: (906, 906)
Epoch 4, accuracy: 0.3574
Epoch 4, Train Loss: 1.1413, Val Loss: 1.0986
batch size: (882, 882)
Epoch 5, accuracy: 0.1691
batch size: (913, 913)
Epoch 6, accuracy: 0.1683
Epoch 6, Train Loss: 1.1338, Val Loss: 1.0986
batch size: (917, 917)
Epoch 7, accuracy: 0.1665
batch size: (906, 906)
Epoch 8, accuracy: 0.1694
Epoch 8, Train Loss: 1.1403, Val Loss: 1.0986
batch size: (895, 895)
Epoch 9, accuracy: 0.1645
batch size: (904, 904)
Epoch 10, accuracy: 0.1690
Epoch 10, Train Loss: 1.0842, Val Loss: 1.0986
batch size: (893, 893)
Epoch 11, accuracy: 0.1691
batch size: (883, 883)
Epoch 12, accuracy: 0.1686
Epoch 12, Train Loss: 1.1111, Val Loss: 1.0986
batch size: (898, 898)
Epoch 13, accuracy: 0.1701
batch size: (916, 916)
Epoch 14, accuracy: 0.1679
Epoch 14, Train Loss: 1.0601, Val Loss: 1.0986
batch size: (895, 895)
Epoch 15, accuracy: 0.1678
batch size: (913, 913)
Epoch 16, accuracy: 0.1663
Epoch 16, Train Loss: 1.0978, Val Loss: 1.0986
batch size: (904, 904)
Epoch 17, accuracy: 0.1680
batch size: (889, 889)
Epoch 18, accuracy: 0.1674
Epoch 18, Train Loss: 1.1130, Val Loss: 1.0986
batch size: (881, 881)
Epoch 19, accuracy: 0.1667
batch size: (898, 898)
Epoch 20, accuracy: 0.1683
Epoch 20, Train Loss: 1.0802, Val Loss: 1.0986
batch size: (886, 886)
Epoch 21, accuracy: 0.1665
batch size: (903, 903)
Epoch 22, accuracy: 0.1689
Epoch 22, Train Loss: 1.1160, Val Loss: 1.0986
batch size: (894, 894)
Epoch 23, accuracy: 0.1675
batch size: (900, 900)
Epoch 24, accuracy: 0.1703
Epoch 24, Train Loss: 1.0850, Val Loss: 1.0986
batch size: (892, 892)
Epoch 25, accuracy: 0.1687
batch size: (904, 904)
Epoch 26, accuracy: 0.1678
Epoch 26, Train Loss: 1.1101, Val Loss: 1.0986
batch size: (906, 906)
Epoch 27, accuracy: 0.1677
batch size: (899, 899)
Epoch 28, accuracy: 0.1692
Epoch 28, Train Loss: 1.0941, Val Loss: 1.0986
batch size: (912, 912)
Epoch 29, accuracy: 0.1679
batch size: (883, 883)
Epoch 30, accuracy: 0.1691
Epoch 30, Train Loss: 1.0601, Val Loss: 1.0986
batch size: (903, 903)
Epoch 31, accuracy: 0.1667
batch size: (897, 897)
Epoch 32, accuracy: 0.1711
Epoch 32, Train Loss: 1.0966, Val Loss: 1.0986
batch size: (888, 888)
Epoch 33, accuracy: 0.1684
batch size: (868, 868)
Epoch 34, accuracy: 0.1666
Epoch 34, Train Loss: 1.0651, Val Loss: 1.0986
batch size: (915, 915)
Epoch 35, accuracy: 0.1677
batch size: (886, 886)
Epoch 36, accuracy: 0.1671
Epoch 36, Train Loss: 1.0927, Val Loss: 1.0986
batch size: (920, 920)
Epoch 37, accuracy: 0.1663
batch size: (900, 900)
Epoch 38, accuracy: 0.1684
Epoch 38, Train Loss: 1.1201, Val Loss: 1.0986
batch size: (897, 897)
Epoch 39, accuracy: 0.1656
batch size: (907, 907)
Epoch 40, accuracy: 0.1673
Epoch 40, Train Loss: 1.1049, Val Loss: 1.0986
batch size: (907, 907)
Epoch 41, accuracy: 0.1664
batch size: (901, 901)
Epoch 42, accuracy: 0.1669
Epoch 42, Train Loss: 1.1078, Val Loss: 1.0986
batch size: (902, 902)
Epoch 43, accuracy: 0.1656
batch size: (904, 904)
Epoch 44, accuracy: 0.1652
Epoch 44, Train Loss: 1.1100, Val Loss: 1.0986
batch size: (898, 898)
Epoch 45, accuracy: 0.1685
batch size: (890, 890)
Epoch 46, accuracy: 0.1694
Epoch 46, Train Loss: 1.1034, Val Loss: 1.0986
batch size: (902, 902)
Epoch 47, accuracy: 0.1655
batch size: (920, 920)
Epoch 48, accuracy: 0.1685
Epoch 48, Train Loss: 1.1249, Val Loss: 1.0986
batch size: (888, 888)
Epoch 49, accuracy: 0.1699
batch size: (898, 898)
Epoch 50, accuracy: 0.1693
Epoch 50, Train Loss: 1.0662, Val Loss: 1.0986
batch size: (892, 892)
Epoch 51, accuracy: 0.1700
batch size: (912, 912)
Epoch 52, accuracy: 0.1690
Epoch 52, Train Loss: 1.0974, Val Loss: 1.0986
batch size: (899, 899)
Epoch 53, accuracy: 0.1680
batch size: (890, 890)
Epoch 54, accuracy: 0.1671
Epoch 54, Train Loss: 1.0921, Val Loss: 1.0986
batch size: (904, 904)
Epoch 55, accuracy: 0.1635
batch size: (915, 915)
Epoch 56, accuracy: 0.1640
Epoch 56, Train Loss: 1.1080, Val Loss: 1.0986
batch size: (868, 868)
Epoch 57, accuracy: 0.1689
batch size: (896, 896)
Epoch 58, accuracy: 0.1704
Epoch 58, Train Loss: 1.0792, Val Loss: 1.0986
batch size: (889, 889)
Epoch 59, accuracy: 0.1667
batch size: (899, 899)
Epoch 60, accuracy: 0.1675
Epoch 60, Train Loss: 1.1860, Val Loss: 1.0986
batch size: (881, 881)
Epoch 61, accuracy: 0.1677
batch size: (908, 908)
Epoch 62, accuracy: 0.1659
Epoch 62, Train Loss: 1.0824, Val Loss: 1.0986
batch size: (889, 889)
Epoch 63, accuracy: 0.1644
batch size: (888, 888)
Epoch 64, accuracy: 0.1678
Epoch 64, Train Loss: 1.1028, Val Loss: 1.0986
batch size: (886, 886)
Epoch 65, accuracy: 0.1683
batch size: (895, 895)
Epoch 66, accuracy: 0.1661
Epoch 66, Train Loss: 1.0878, Val Loss: 1.0986
batch size: (904, 904)
Epoch 67, accuracy: 0.1642
batch size: (900, 900)
Epoch 68, accuracy: 0.1661
Epoch 68, Train Loss: 1.0839, Val Loss: 1.0986
batch size: (893, 893)
Epoch 69, accuracy: 0.1652
batch size: (897, 897)
Epoch 70, accuracy: 0.1675
Epoch 70, Train Loss: 1.1309, Val Loss: 1.0986
batch size: (903, 903)
Epoch 71, accuracy: 0.1654
batch size: (895, 895)
Epoch 72, accuracy: 0.1670
Epoch 72, Train Loss: 1.0945, Val Loss: 1.0986
batch size: (905, 905)
Epoch 73, accuracy: 0.1656
batch size: (900, 900)
Epoch 74, accuracy: 0.1678
Epoch 74, Train Loss: 1.0797, Val Loss: 1.0986
batch size: (916, 916)
Epoch 75, accuracy: 0.1656
batch size: (909, 909)
Epoch 76, accuracy: 0.1678
Epoch 76, Train Loss: 1.0865, Val Loss: 1.0986
batch size: (881, 881)
Epoch 77, accuracy: 0.1685
batch size: (884, 884)
Epoch 78, accuracy: 0.1660
Epoch 78, Train Loss: 1.1082, Val Loss: 1.0986
batch size: (898, 898)
Epoch 79, accuracy: 0.1707
batch size: (905, 905)
Epoch 80, accuracy: 0.1663
Epoch 80, Train Loss: 1.0844, Val Loss: 1.0986
batch size: (894, 894)
Epoch 81, accuracy: 0.1674
batch size: (910, 910)
Epoch 82, accuracy: 0.1691
Epoch 82, Train Loss: 1.1392, Val Loss: 1.0986
batch size: (918, 918)
Epoch 83, accuracy: 0.1683
batch size: (908, 908)
Epoch 84, accuracy: 0.1671
Epoch 84, Train Loss: 1.0909, Val Loss: 1.0986
batch size: (889, 889)
Epoch 85, accuracy: 0.1662
batch size: (880, 880)
Epoch 86, accuracy: 0.1655
Epoch 86, Train Loss: 1.0758, Val Loss: 1.0986
batch size: (897, 897)
Epoch 87, accuracy: 0.1665
batch size: (911, 911)
Epoch 88, accuracy: 0.1658
Epoch 88, Train Loss: 1.1249, Val Loss: 1.0986
batch size: (900, 900)
Epoch 89, accuracy: 0.1689
batch size: (894, 894)
Epoch 90, accuracy: 0.1657
Epoch 90, Train Loss: 1.0723, Val Loss: 1.0986
batch size: (879, 879)
Epoch 91, accuracy: 0.1691
batch size: (890, 890)
Epoch 92, accuracy: 0.1687
Epoch 92, Train Loss: 1.1033, Val Loss: 1.0986
batch size: (898, 898)
Epoch 93, accuracy: 0.1665
batch size: (912, 912)
Epoch 94, accuracy: 0.1678
Epoch 94, Train Loss: 1.0804, Val Loss: 1.0986
batch size: (898, 898)
Epoch 95, accuracy: 0.1681
batch size: (892, 892)
Epoch 96, accuracy: 0.1680
Epoch 96, Train Loss: 1.1121, Val Loss: 1.0986
batch size: (893, 893)
Epoch 97, accuracy: 0.1661
batch size: (908, 908)
Epoch 98, accuracy: 0.1690
Epoch 98, Train Loss: 1.0668, Val Loss: 1.0986
batch size: (896, 896)
Epoch 99, accuracy: 0.1688
batch size: (906, 906)
Epoch 100, accuracy: 0.1687
Epoch 100, Train Loss: 1.0950, Val Loss: 1.0986
batch size: (885, 885)
Epoch 101, accuracy: 0.1685
batch size: (891, 891)
Epoch 102, accuracy: 0.1672
Epoch 102, Train Loss: 1.1012, Val Loss: 1.0986
batch size: (909, 909)
Epoch 103, accuracy: 0.1656
batch size: (906, 906)
Epoch 104, accuracy: 0.1677
Epoch 104, Train Loss: 1.0984, Val Loss: 1.0986
batch size: (895, 895)
Epoch 105, accuracy: 0.1664
batch size: (896, 896)
Epoch 106, accuracy: 0.1678
Epoch 106, Train Loss: 1.1096, Val Loss: 1.0986
batch size: (906, 906)
Epoch 107, accuracy: 0.1651
batch size: (908, 908)
Epoch 108, accuracy: 0.1668
Epoch 108, Train Loss: 1.0851, Val Loss: 1.0986
batch size: (899, 899)
Epoch 109, accuracy: 0.1679
batch size: (917, 917)
Epoch 110, accuracy: 0.1679
Epoch 110, Train Loss: 1.0768, Val Loss: 1.0986
batch size: (918, 918)
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 111, accuracy: 0.1696
batch size: (879, 879)
Epoch 112, accuracy: 0.1670
Epoch 112, Train Loss: 1.1210, Val Loss: 1.0986
batch size: (907, 907)
Epoch 113, accuracy: 0.1651
batch size: (881, 881)
Epoch 114, accuracy: 0.1693
Epoch 114, Train Loss: 1.1046, Val Loss: 1.0986
batch size: (883, 883)
Epoch 115, accuracy: 0.1659
batch size: (882, 882)
Epoch 116, accuracy: 0.1680
Epoch 116, Train Loss: 1.0806, Val Loss: 1.0986
batch size: (893, 893)
Epoch 117, accuracy: 0.1659
batch size: (897, 897)
Epoch 118, accuracy: 0.1684
Epoch 118, Train Loss: 1.0626, Val Loss: 1.0986
batch size: (895, 895)
Epoch 119, accuracy: 0.1655
batch size: (879, 879)
Epoch 120, accuracy: 0.1684
Epoch 120, Train Loss: 1.0645, Val Loss: 1.0986
batch size: (880, 880)
Epoch 121, accuracy: 0.1682
batch size: (896, 896)
Epoch 122, accuracy: 0.1661
Epoch 122, Train Loss: 1.0869, Val Loss: 1.0986
batch size: (899, 899)
Epoch 123, accuracy: 0.1650
batch size: (897, 897)
Epoch 124, accuracy: 0.1661
Epoch 124, Train Loss: 1.1115, Val Loss: 1.0986
batch size: (901, 901)
Epoch 125, accuracy: 0.1680
batch size: (889, 889)
Epoch 126, accuracy: 0.1654
Epoch 126, Train Loss: 1.0953, Val Loss: 1.0986
batch size: (904, 904)
Epoch 127, accuracy: 0.1657
batch size: (907, 907)
Epoch 128, accuracy: 0.1684
Epoch 128, Train Loss: 1.1050, Val Loss: 1.0986
batch size: (899, 899)
Epoch 129, accuracy: 0.1656
batch size: (894, 894)
Epoch 130, accuracy: 0.1695
Epoch 130, Train Loss: 1.1124, Val Loss: 1.0986
batch size: (888, 888)
Epoch 131, accuracy: 0.1706
batch size: (888, 888)
Epoch 132, accuracy: 0.1690
Epoch 132, Train Loss: 1.1001, Val Loss: 1.0986
batch size: (892, 892)
Epoch 133, accuracy: 0.1657
batch size: (934, 934)
Epoch 134, accuracy: 0.1675
Epoch 134, Train Loss: 1.1131, Val Loss: 1.0986
batch size: (891, 891)
Epoch 135, accuracy: 0.1674
batch size: (912, 912)
Epoch 136, accuracy: 0.1684
Epoch 136, Train Loss: 1.1047, Val Loss: 1.0986
batch size: (909, 909)
Epoch 137, accuracy: 0.1680
batch size: (913, 913)
Epoch 138, accuracy: 0.1660
Epoch 138, Train Loss: 1.1151, Val Loss: 1.0986
batch size: (911, 911)
Epoch 139, accuracy: 0.1666
batch size: (912, 912)
Epoch 140, accuracy: 0.1621
Epoch 140, Train Loss: 1.0677, Val Loss: 1.0986
batch size: (892, 892)
Epoch 141, accuracy: 0.1680
batch size: (899, 899)
Epoch 142, accuracy: 0.1655
Epoch 142, Train Loss: 1.0880, Val Loss: 1.0986
batch size: (895, 895)
Epoch 143, accuracy: 0.1664
batch size: (926, 926)
Epoch 144, accuracy: 0.1669
Epoch 144, Train Loss: 1.1088, Val Loss: 1.0986
batch size: (889, 889)
Epoch 145, accuracy: 0.1692
batch size: (901, 901)
Epoch 146, accuracy: 0.1689
Epoch 146, Train Loss: 1.0693, Val Loss: 1.0986
batch size: (898, 898)
Epoch 147, accuracy: 0.1671
batch size: (892, 892)
Epoch 148, accuracy: 0.1697
Epoch 148, Train Loss: 1.0964, Val Loss: 1.0986
batch size: (888, 888)
Epoch 149, accuracy: 0.1704
batch size: (882, 882)
Epoch 150, accuracy: 0.1664
Epoch 150, Train Loss: 1.0866, Val Loss: 1.0986
batch size: (890, 890)
Epoch 151, accuracy: 0.1666
batch size: (896, 896)
Epoch 152, accuracy: 0.1672
Epoch 152, Train Loss: 1.1082, Val Loss: 1.0986
batch size: (912, 912)
Epoch 153, accuracy: 0.1657
batch size: (886, 886)
Epoch 154, accuracy: 0.1696
Epoch 154, Train Loss: 1.1002, Val Loss: 1.0986
batch size: (900, 900)
Epoch 155, accuracy: 0.1633
batch size: (922, 922)
Epoch 156, accuracy: 0.1683
Epoch 156, Train Loss: 1.0444, Val Loss: 1.0986
batch size: (904, 904)
Epoch 157, accuracy: 0.1689
batch size: (908, 908)
Epoch 158, accuracy: 0.1657
Epoch 158, Train Loss: 1.1136, Val Loss: 1.0986
batch size: (914, 914)
Epoch 159, accuracy: 0.1678
batch size: (883, 883)
Epoch 160, accuracy: 0.1658
Epoch 160, Train Loss: 1.1283, Val Loss: 1.0986
batch size: (921, 921)
Epoch 161, accuracy: 0.1660
batch size: (885, 885)
Epoch 162, accuracy: 0.1656
Epoch 162, Train Loss: 1.0921, Val Loss: 1.0986
batch size: (908, 908)
Epoch 163, accuracy: 0.1683
batch size: (876, 876)
Epoch 164, accuracy: 0.1671
Epoch 164, Train Loss: 1.0880, Val Loss: 1.0986
batch size: (900, 900)
Epoch 165, accuracy: 0.1661
batch size: (881, 881)
Epoch 166, accuracy: 0.1660
Epoch 166, Train Loss: 1.1173, Val Loss: 1.0986
batch size: (897, 897)
Epoch 167, accuracy: 0.1675
batch size: (900, 900)
Epoch 168, accuracy: 0.1645
Epoch 168, Train Loss: 1.1031, Val Loss: 1.0986
batch size: (907, 907)
Epoch 169, accuracy: 0.1658
batch size: (882, 882)
Epoch 170, accuracy: 0.1653
Epoch 170, Train Loss: 1.0812, Val Loss: 1.0986
batch size: (911, 911)
Epoch 171, accuracy: 0.1645
batch size: (917, 917)
Epoch 172, accuracy: 0.1632
Epoch 172, Train Loss: 1.1085, Val Loss: 1.0986
batch size: (892, 892)
Epoch 173, accuracy: 0.1696
batch size: (901, 901)
Epoch 174, accuracy: 0.1652
Epoch 174, Train Loss: 1.0906, Val Loss: 1.0986
batch size: (879, 879)
Epoch 175, accuracy: 0.1684
batch size: (894, 894)
Epoch 176, accuracy: 0.1689
Epoch 176, Train Loss: 1.1552, Val Loss: 1.0986
batch size: (904, 904)
Epoch 177, accuracy: 0.1619
batch size: (898, 898)
Epoch 178, accuracy: 0.1694
Epoch 178, Train Loss: 1.1709, Val Loss: 1.0986
batch size: (905, 905)
Epoch 179, accuracy: 0.1670
batch size: (880, 880)
Epoch 180, accuracy: 0.1656
Epoch 180, Train Loss: 1.1363, Val Loss: 1.0986
batch size: (900, 900)
Epoch 181, accuracy: 0.1668
batch size: (899, 899)
Epoch 182, accuracy: 0.1672
Epoch 182, Train Loss: 1.1153, Val Loss: 1.0986
batch size: (892, 892)
Epoch 183, accuracy: 0.1681
batch size: (881, 881)
Epoch 184, accuracy: 0.1671
Epoch 184, Train Loss: 1.0919, Val Loss: 1.0986
batch size: (889, 889)
Epoch 185, accuracy: 0.1716
batch size: (893, 893)
Epoch 186, accuracy: 0.1682
Epoch 186, Train Loss: 1.1635, Val Loss: 1.0986
batch size: (895, 895)
Epoch 187, accuracy: 0.1671
batch size: (906, 906)
Epoch 188, accuracy: 0.1662
Epoch 188, Train Loss: 1.0914, Val Loss: 1.0986
batch size: (908, 908)
Epoch 189, accuracy: 0.1687
batch size: (892, 892)
Epoch 190, accuracy: 0.1681
Epoch 190, Train Loss: 1.1313, Val Loss: 1.0986
batch size: (915, 915)
Epoch 191, accuracy: 0.1688
batch size: (891, 891)
Epoch 192, accuracy: 0.1694
Epoch 192, Train Loss: 1.1001, Val Loss: 1.0986
batch size: (902, 902)
Epoch 193, accuracy: 0.1676
batch size: (902, 902)
Epoch 194, accuracy: 0.1667
Epoch 194, Train Loss: 1.0825, Val Loss: 1.0986
batch size: (897, 897)
Epoch 195, accuracy: 0.1672
batch size: (906, 906)
Epoch 196, accuracy: 0.1687
Epoch 196, Train Loss: 1.1053, Val Loss: 1.0986
batch size: (905, 905)
Epoch 197, accuracy: 0.1683
batch size: (905, 905)
Epoch 198, accuracy: 0.1660
Epoch 198, Train Loss: 1.0876, Val Loss: 1.0986
batch size: (911, 911)
Epoch 199, accuracy: 0.1696
Loaded best model with val_loss = 1.0953006744384766
test :accuracy 0.4037, f1_macro: 0.1917, f1_micro: 0.4037, auc: 0.5016
Training GraphSAGE with 32 layers...
可训练参数: 98057_GraphSAGE
不可训练参数: 0
batch size: (893, 893)
✅ Epoch 0: New best model saved with val_loss = 1.1217
Epoch 0, accuracy: 0.1684
Epoch 0, Train Loss: 1.1238, Val Loss: 1.1217
batch size: (921, 921)
✅ Epoch 1: New best model saved with val_loss = 1.1168
Epoch 1, accuracy: 0.1674
batch size: (907, 907)
✅ Epoch 2: New best model saved with val_loss = 1.1089
Epoch 2, accuracy: 0.1677
Epoch 2, Train Loss: 1.0962, Val Loss: 1.1089
batch size: (896, 896)
✅ Epoch 3: New best model saved with val_loss = 1.1004
Epoch 3, accuracy: 0.1660
batch size: (876, 876)
✅ Epoch 4: New best model saved with val_loss = 1.0986
Epoch 4, accuracy: 0.1666
Epoch 4, Train Loss: 1.1137, Val Loss: 1.0986
batch size: (887, 887)
✅ Epoch 5: New best model saved with val_loss = 1.0986
Epoch 5, accuracy: 0.1686
batch size: (908, 908)
Epoch 6, accuracy: 0.1700
Epoch 6, Train Loss: 1.1243, Val Loss: 1.0986
batch size: (901, 901)
Epoch 7, accuracy: 0.1673
batch size: (879, 879)
Epoch 8, accuracy: 0.1678
Epoch 8, Train Loss: 1.1716, Val Loss: 1.0986
batch size: (890, 890)
Epoch 9, accuracy: 0.3558
batch size: (905, 905)
Epoch 10, accuracy: 0.3587
Epoch 10, Train Loss: 1.1416, Val Loss: 1.0986
batch size: (913, 913)
Epoch 11, accuracy: 0.3566
batch size: (901, 901)
Epoch 12, accuracy: 0.3585
Epoch 12, Train Loss: 1.1010, Val Loss: 1.0986
batch size: (913, 913)
Epoch 13, accuracy: 0.3653
batch size: (886, 886)
Epoch 14, accuracy: 0.3539
Epoch 14, Train Loss: 1.0945, Val Loss: 1.0986
batch size: (899, 899)
Epoch 15, accuracy: 0.3556
batch size: (914, 914)
Epoch 16, accuracy: 0.3696
Epoch 16, Train Loss: 1.0771, Val Loss: 1.0986
batch size: (871, 871)
Epoch 17, accuracy: 0.3673
batch size: (902, 902)
Epoch 18, accuracy: 0.1664
Epoch 18, Train Loss: 1.1066, Val Loss: 1.0986
batch size: (902, 902)
Epoch 19, accuracy: 0.3624
batch size: (902, 902)
Epoch 20, accuracy: 0.3592
Epoch 20, Train Loss: 1.1316, Val Loss: 1.0986
batch size: (894, 894)
Epoch 21, accuracy: 0.3630
batch size: (900, 900)
Epoch 22, accuracy: 0.3522
Epoch 22, Train Loss: 1.1324, Val Loss: 1.0986
batch size: (883, 883)
Epoch 23, accuracy: 0.3566
batch size: (884, 884)
Epoch 24, accuracy: 0.3684
Epoch 24, Train Loss: 1.1094, Val Loss: 1.0986
batch size: (889, 889)
Epoch 25, accuracy: 0.3523
batch size: (899, 899)
Epoch 26, accuracy: 0.3574
Epoch 26, Train Loss: 1.1140, Val Loss: 1.0986
batch size: (915, 915)
Epoch 27, accuracy: 0.3551
batch size: (894, 894)
Epoch 28, accuracy: 0.3552
Epoch 28, Train Loss: 1.1033, Val Loss: 1.0986
batch size: (909, 909)
Epoch 29, accuracy: 0.3569
batch size: (900, 900)
✅ Epoch 30: New best model saved with val_loss = 1.0984
Epoch 30, accuracy: 0.3919
Epoch 30, Train Loss: 1.1053, Val Loss: 1.0984
batch size: (899, 899)
Epoch 31, accuracy: 0.3563
batch size: (883, 883)
Epoch 32, accuracy: 0.3736
Epoch 32, Train Loss: 1.0982, Val Loss: 1.0986
batch size: (892, 892)
Epoch 33, accuracy: 0.3656
batch size: (907, 907)
Epoch 34, accuracy: 0.3831
Epoch 34, Train Loss: 1.1552, Val Loss: 1.0986
batch size: (891, 891)
Epoch 35, accuracy: 0.3842
batch size: (912, 912)
Epoch 36, accuracy: 0.3885
Epoch 36, Train Loss: 1.1506, Val Loss: 1.0985
batch size: (894, 894)
Epoch 37, accuracy: 0.3643
batch size: (915, 915)
Epoch 38, accuracy: 0.3605
Epoch 38, Train Loss: 1.0821, Val Loss: 1.0986
batch size: (898, 898)
Epoch 39, accuracy: 0.3555
batch size: (912, 912)
Epoch 40, accuracy: 0.3556
Epoch 40, Train Loss: 1.1714, Val Loss: 1.0986
batch size: (895, 895)
Epoch 41, accuracy: 0.3686
batch size: (896, 896)
Epoch 42, accuracy: 0.3549
Epoch 42, Train Loss: 1.1454, Val Loss: 1.0986
batch size: (930, 930)
Epoch 43, accuracy: 0.3562
batch size: (903, 903)
Epoch 44, accuracy: 0.3567
Epoch 44, Train Loss: 1.0944, Val Loss: 1.0986
batch size: (904, 904)
Epoch 45, accuracy: 0.3657
batch size: (888, 888)
Epoch 46, accuracy: 0.3627
Epoch 46, Train Loss: 1.1104, Val Loss: 1.0986
batch size: (905, 905)
Epoch 47, accuracy: 0.3578
batch size: (897, 897)
Epoch 48, accuracy: 0.3577
Epoch 48, Train Loss: 1.0841, Val Loss: 1.0986
batch size: (892, 892)
Epoch 49, accuracy: 0.3725
batch size: (895, 895)
Epoch 50, accuracy: 0.3598
Epoch 50, Train Loss: 1.0904, Val Loss: 1.0986
batch size: (903, 903)
Epoch 51, accuracy: 0.3534
batch size: (900, 900)
Epoch 52, accuracy: 0.3726
Epoch 52, Train Loss: 1.1064, Val Loss: 1.0986
batch size: (893, 893)
Epoch 53, accuracy: 0.3561
batch size: (889, 889)
Epoch 54, accuracy: 0.3742
Epoch 54, Train Loss: 1.0955, Val Loss: 1.0986
batch size: (903, 903)
Epoch 55, accuracy: 0.3590
batch size: (908, 908)
Epoch 56, accuracy: 0.3735
Epoch 56, Train Loss: 1.0886, Val Loss: 1.0986
batch size: (881, 881)
Epoch 57, accuracy: 0.3588
batch size: (881, 881)
Epoch 58, accuracy: 0.3695
Epoch 58, Train Loss: 1.1012, Val Loss: 1.0986
batch size: (875, 875)
Epoch 59, accuracy: 0.3531
batch size: (914, 914)
Epoch 60, accuracy: 0.3608
Epoch 60, Train Loss: 1.0774, Val Loss: 1.0986
batch size: (892, 892)
Epoch 61, accuracy: 0.3545
batch size: (888, 888)
Epoch 62, accuracy: 0.1658
Epoch 62, Train Loss: 1.0982, Val Loss: 1.0986
batch size: (885, 885)
✅ Epoch 63: New best model saved with val_loss = 1.0983
Epoch 63, accuracy: 0.3980
batch size: (891, 891)
Epoch 64, accuracy: 0.3548
Epoch 64, Train Loss: 1.0851, Val Loss: 1.0986
batch size: (891, 891)
Epoch 65, accuracy: 0.3863
batch size: (920, 920)
Epoch 66, accuracy: 0.3548
Epoch 66, Train Loss: 1.0561, Val Loss: 1.0986
batch size: (904, 904)
Epoch 67, accuracy: 0.3733
batch size: (911, 911)
Epoch 68, accuracy: 0.3537
Epoch 68, Train Loss: 1.1366, Val Loss: 1.0986
batch size: (899, 899)
Epoch 69, accuracy: 0.3590
batch size: (887, 887)
Epoch 70, accuracy: 0.3653
Epoch 70, Train Loss: 1.1058, Val Loss: 1.0986
batch size: (891, 891)
Epoch 71, accuracy: 0.3641
batch size: (908, 908)
Epoch 72, accuracy: 0.3619
Epoch 72, Train Loss: 1.1355, Val Loss: 1.0986
batch size: (908, 908)
Epoch 73, accuracy: 0.3558
batch size: (898, 898)
Epoch 74, accuracy: 0.3873
Epoch 74, Train Loss: 1.0900, Val Loss: 1.0986
batch size: (878, 878)
Epoch 75, accuracy: 0.3710
batch size: (880, 880)
Epoch 76, accuracy: 0.3567
Epoch 76, Train Loss: 1.0939, Val Loss: 1.0986
batch size: (898, 898)
Epoch 77, accuracy: 0.3790
batch size: (889, 889)
✅ Epoch 78: New best model saved with val_loss = 1.0972
Epoch 78, accuracy: 0.4044
Epoch 78, Train Loss: 1.0826, Val Loss: 1.0972
batch size: (891, 891)
Epoch 79, accuracy: 0.3666
batch size: (897, 897)
Epoch 80, accuracy: 0.3924
Epoch 80, Train Loss: 1.1783, Val Loss: 1.0984
batch size: (900, 900)
Epoch 81, accuracy: 0.3782
batch size: (907, 907)
Epoch 82, accuracy: 0.3763
Epoch 82, Train Loss: 1.1517, Val Loss: 1.0986
batch size: (908, 908)
Epoch 83, accuracy: 0.3520
batch size: (902, 902)
Epoch 84, accuracy: 0.1677
Epoch 84, Train Loss: 1.1357, Val Loss: 1.0986
batch size: (910, 910)
Epoch 85, accuracy: 0.3511
batch size: (899, 899)
Epoch 86, accuracy: 0.3542
Epoch 86, Train Loss: 1.1005, Val Loss: 1.0986
batch size: (885, 885)
Epoch 87, accuracy: 0.4055
batch size: (898, 898)
Epoch 88, accuracy: 0.3542
Epoch 88, Train Loss: 1.1072, Val Loss: 1.0986
batch size: (903, 903)
Epoch 89, accuracy: 0.3644
batch size: (898, 898)
Epoch 90, accuracy: 0.3537
Epoch 90, Train Loss: 1.0951, Val Loss: 1.0986
batch size: (902, 902)
Epoch 91, accuracy: 0.3978
batch size: (887, 887)
Epoch 92, accuracy: 0.3573
Epoch 92, Train Loss: 1.1293, Val Loss: 1.0986
batch size: (911, 911)
Epoch 93, accuracy: 0.3962
batch size: (905, 905)
Epoch 94, accuracy: 0.3593
Epoch 94, Train Loss: 1.1425, Val Loss: 1.0986
batch size: (899, 899)
Epoch 95, accuracy: 0.3683
batch size: (882, 882)
Epoch 96, accuracy: 0.3532
Epoch 96, Train Loss: 1.1236, Val Loss: 1.0986
batch size: (889, 889)
✅ Epoch 97: New best model saved with val_loss = 1.0972
Epoch 97, accuracy: 0.3995
batch size: (917, 917)
Epoch 98, accuracy: 0.3731
Epoch 98, Train Loss: 1.2377, Val Loss: 1.0986
batch size: (899, 899)
Epoch 99, accuracy: 0.3600
batch size: (886, 886)
Epoch 100, accuracy: 0.3653
Epoch 100, Train Loss: 1.1108, Val Loss: 1.0986
batch size: (900, 900)
Epoch 101, accuracy: 0.3547
batch size: (910, 910)
Epoch 102, accuracy: 0.3609
Epoch 102, Train Loss: 1.0901, Val Loss: 1.0986
batch size: (894, 894)
Epoch 103, accuracy: 0.3601
batch size: (896, 896)
Epoch 104, accuracy: 0.3684
Epoch 104, Train Loss: 1.0945, Val Loss: 1.0986
batch size: (888, 888)
Epoch 105, accuracy: 0.3906
batch size: (917, 917)
Epoch 106, accuracy: 0.3679
Epoch 106, Train Loss: 1.2006, Val Loss: 1.0986
batch size: (911, 911)
Epoch 107, accuracy: 0.3561
batch size: (879, 879)
Epoch 108, accuracy: 0.3529
Epoch 108, Train Loss: 1.1004, Val Loss: 1.0986
batch size: (891, 891)
Epoch 109, accuracy: 0.3563
batch size: (895, 895)
Epoch 110, accuracy: 0.3548
Epoch 110, Train Loss: 1.1113, Val Loss: 1.0986
batch size: (915, 915)
Epoch 111, accuracy: 0.3768
batch size: (907, 907)
Epoch 112, accuracy: 0.3649
Epoch 112, Train Loss: 1.1629, Val Loss: 1.0986
batch size: (914, 914)
Epoch 113, accuracy: 0.3564
batch size: (888, 888)
Epoch 114, accuracy: 0.3641
Epoch 114, Train Loss: 1.1209, Val Loss: 1.0986
batch size: (915, 915)
Epoch 115, accuracy: 0.3557
batch size: (885, 885)
Epoch 116, accuracy: 0.3577
Epoch 116, Train Loss: 1.1046, Val Loss: 1.0986
batch size: (913, 913)
Epoch 117, accuracy: 0.3748
batch size: (896, 896)
Epoch 118, accuracy: 0.3778
Epoch 118, Train Loss: 1.1817, Val Loss: 1.0986
batch size: (908, 908)
Epoch 119, accuracy: 0.3584
batch size: (901, 901)
Epoch 120, accuracy: 0.3839
Epoch 120, Train Loss: 1.0614, Val Loss: 1.0986
batch size: /root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
(915, 915)
Epoch 121, accuracy: 0.3950
batch size: (894, 894)
Epoch 122, accuracy: 0.3679
Epoch 122, Train Loss: 1.1048, Val Loss: 1.0986
batch size: (898, 898)
Epoch 123, accuracy: 0.3812
batch size: (884, 884)
Epoch 124, accuracy: 0.3971
Epoch 124, Train Loss: 1.1334, Val Loss: 1.0983
batch size: (896, 896)
Epoch 125, accuracy: 0.3695
batch size: (896, 896)
Epoch 126, accuracy: 0.3662
Epoch 126, Train Loss: 1.1272, Val Loss: 1.0986
batch size: (891, 891)
Epoch 127, accuracy: 0.3793
batch size: (898, 898)
Epoch 128, accuracy: 0.3674
Epoch 128, Train Loss: 1.1135, Val Loss: 1.0986
batch size: (896, 896)
Epoch 129, accuracy: 0.1659
batch size: (906, 906)
Epoch 130, accuracy: 0.1680
Epoch 130, Train Loss: 1.0828, Val Loss: 1.0986
batch size: (892, 892)
Epoch 131, accuracy: 0.1660
batch size: (910, 910)
Epoch 132, accuracy: 0.3552
Epoch 132, Train Loss: 1.1071, Val Loss: 1.0986
batch size: (892, 892)
Epoch 133, accuracy: 0.3597
batch size: (896, 896)
Epoch 134, accuracy: 0.3865
Epoch 134, Train Loss: 1.1014, Val Loss: 1.0986
batch size: (901, 901)
Epoch 135, accuracy: 0.3584
batch size: (891, 891)
Epoch 136, accuracy: 0.3598
Epoch 136, Train Loss: 1.0762, Val Loss: 1.0986
batch size: (904, 904)
Epoch 137, accuracy: 0.3634
batch size: (886, 886)
Epoch 138, accuracy: 0.3576
Epoch 138, Train Loss: 1.1337, Val Loss: 1.0986
batch size: (911, 911)
Epoch 139, accuracy: 0.3568
batch size: (888, 888)
Epoch 140, accuracy: 0.3715
Epoch 140, Train Loss: 1.1643, Val Loss: 1.0986
batch size: (882, 882)
Epoch 141, accuracy: 0.3970
batch size: (893, 893)
Epoch 142, accuracy: 0.3791
Epoch 142, Train Loss: 1.0982, Val Loss: 1.0986
batch size: (886, 886)
Epoch 143, accuracy: 0.3654
batch size: (891, 891)
Epoch 144, accuracy: 0.3536
Epoch 144, Train Loss: 1.1196, Val Loss: 1.0986
batch size: (906, 906)
Epoch 145, accuracy: 0.3785
batch size: (882, 882)
Epoch 146, accuracy: 0.3564
Epoch 146, Train Loss: 1.0982, Val Loss: 1.0986
batch size: (889, 889)
Epoch 147, accuracy: 0.3656
batch size: (908, 908)
Epoch 148, accuracy: 0.3707
Epoch 148, Train Loss: 1.0839, Val Loss: 1.0986
batch size: (897, 897)
Epoch 149, accuracy: 0.3660
batch size: (902, 902)
Epoch 150, accuracy: 0.3811
Epoch 150, Train Loss: 1.0890, Val Loss: 1.0986
batch size: (903, 903)
Epoch 151, accuracy: 0.3575
batch size: (897, 897)
Epoch 152, accuracy: 0.3557
Epoch 152, Train Loss: 1.0840, Val Loss: 1.0986
batch size: (907, 907)
Epoch 153, accuracy: 0.3578
batch size: (916, 916)
Epoch 154, accuracy: 0.3736
Epoch 154, Train Loss: 1.0997, Val Loss: 1.0986
batch size: (895, 895)
Epoch 155, accuracy: 0.3777
batch size: (904, 904)
Epoch 156, accuracy: 0.3543
Epoch 156, Train Loss: 1.1024, Val Loss: 1.0986
batch size: (907, 907)
Epoch 157, accuracy: 0.3637
batch size: (895, 895)
Epoch 158, accuracy: 0.3567
Epoch 158, Train Loss: 1.1262, Val Loss: 1.0986
batch size: (899, 899)
Epoch 159, accuracy: 0.3627
batch size: (904, 904)
Epoch 160, accuracy: 0.3548
Epoch 160, Train Loss: 1.1056, Val Loss: 1.0986
batch size: (913, 913)
Epoch 161, accuracy: 0.3801
batch size: (883, 883)
Epoch 162, accuracy: 0.3521
Epoch 162, Train Loss: 1.0945, Val Loss: 1.0986
batch size: (891, 891)
Epoch 163, accuracy: 0.3945
batch size: (903, 903)
Epoch 164, accuracy: 0.3646
Epoch 164, Train Loss: 1.1675, Val Loss: 1.0986
batch size: (910, 910)
Epoch 165, accuracy: 0.3740
batch size: (884, 884)
Epoch 166, accuracy: 0.3573
Epoch 166, Train Loss: 1.1480, Val Loss: 1.0986
batch size: (887, 887)
Epoch 167, accuracy: 0.3553
batch size: (917, 917)
Epoch 168, accuracy: 0.1666
Epoch 168, Train Loss: 1.1903, Val Loss: 1.0986
batch size: (901, 901)
Epoch 169, accuracy: 0.3599
batch size: (886, 886)
Epoch 170, accuracy: 0.3583
Epoch 170, Train Loss: 1.0965, Val Loss: 1.0986
batch size: (915, 915)
Epoch 171, accuracy: 0.3580
batch size: (917, 917)
Epoch 172, accuracy: 0.3600
Epoch 172, Train Loss: 1.0975, Val Loss: 1.0986
batch size: (905, 905)
Epoch 173, accuracy: 0.3943
batch size: (905, 905)
Epoch 174, accuracy: 0.3563
Epoch 174, Train Loss: 1.0627, Val Loss: 1.0986
batch size: (905, 905)
Epoch 175, accuracy: 0.3859
batch size: (880, 880)
Epoch 176, accuracy: 0.3732
Epoch 176, Train Loss: 1.1016, Val Loss: 1.0986
batch size: (904, 904)
Epoch 177, accuracy: 0.3543
batch size: (908, 908)
Epoch 178, accuracy: 0.3572
Epoch 178, Train Loss: 1.1089, Val Loss: 1.0986
batch size: (907, 907)
Epoch 179, accuracy: 0.3797
batch size: (881, 881)
Epoch 180, accuracy: 0.3593
Epoch 180, Train Loss: 1.1438, Val Loss: 1.0986
batch size: (902, 902)
Epoch 181, accuracy: 0.3568
batch size: (911, 911)
Epoch 182, accuracy: 0.3543
Epoch 182, Train Loss: 1.2767, Val Loss: 1.0986
batch size: (898, 898)
Epoch 183, accuracy: 0.3760
batch size: (900, 900)
Epoch 184, accuracy: 0.3779
Epoch 184, Train Loss: 1.0688, Val Loss: 1.0986
batch size: (900, 900)
Epoch 185, accuracy: 0.3620
batch size: (887, 887)
Epoch 186, accuracy: 0.3555
Epoch 186, Train Loss: 1.0965, Val Loss: 1.0986
batch size: (884, 884)
Epoch 187, accuracy: 0.3546
batch size: (904, 904)
Epoch 188, accuracy: 0.3616
Epoch 188, Train Loss: 1.0942, Val Loss: 1.0986
batch size: (908, 908)
Epoch 189, accuracy: 0.3928
batch size: (906, 906)
Epoch 190, accuracy: 0.3529
Epoch 190, Train Loss: 1.0704, Val Loss: 1.0986
batch size: (893, 893)
Epoch 191, accuracy: 0.3593
batch size: (911, 911)
Epoch 192, accuracy: 0.3532
Epoch 192, Train Loss: 1.1494, Val Loss: 1.0986
batch size: (886, 886)
Epoch 193, accuracy: 0.3714
batch size: (904, 904)
Epoch 194, accuracy: 0.3754
Epoch 194, Train Loss: 1.1348, Val Loss: 1.0986
batch size: (899, 899)
Epoch 195, accuracy: 0.3825
batch size: (903, 903)
Epoch 196, accuracy: 0.3527
Epoch 196, Train Loss: 1.1391, Val Loss: 1.0986
batch size: (891, 891)
Epoch 197, accuracy: 0.3557
batch size: (896, 896)
Epoch 198, accuracy: 0.3641
Epoch 198, Train Loss: 1.0833, Val Loss: 1.0986
batch size: (889, 889)
Epoch 199, accuracy: 0.3536
Loaded best model with val_loss = 1.0971583127975464
test :accuracy 0.3999, f1_macro: 0.1912, f1_micro: 0.3999, auc: 0.5022
Training GAT with 2 layers...
可训练参数: 196495_GAT
不可训练参数: 0
batch size: (901, 901)
✅ Epoch 0: New best model saved with val_loss = 1.0772
Epoch 0, accuracy: 0.6977
Epoch 0, Train Loss: 1.1005, Val Loss: 1.0772
batch size: (888, 888)
✅ Epoch 1: New best model saved with val_loss = 1.0561
Epoch 1, accuracy: 0.7209
batch size: (901, 901)
✅ Epoch 2: New best model saved with val_loss = 1.0293
Epoch 2, accuracy: 0.7282
Epoch 2, Train Loss: 1.0270, Val Loss: 1.0293
batch size: (892, 892)
✅ Epoch 3: New best model saved with val_loss = 0.9970
Epoch 3, accuracy: 0.7324
batch size: (891, 891)
✅ Epoch 4: New best model saved with val_loss = 0.9610
Epoch 4, accuracy: 0.7333
Epoch 4, Train Loss: 0.9335, Val Loss: 0.9610
batch size: (896, 896)
✅ Epoch 5: New best model saved with val_loss = 0.9225
Epoch 5, accuracy: 0.7348
batch size: (901, 901)
✅ Epoch 6: New best model saved with val_loss = 0.8867
Epoch 6, accuracy: 0.7352
Epoch 6, Train Loss: 0.8186, Val Loss: 0.8867
batch size: (906, 906)
✅ Epoch 7: New best model saved with val_loss = 0.8459
Epoch 7, accuracy: 0.7359
batch size: (901, 901)
✅ Epoch 8: New best model saved with val_loss = 0.8115
Epoch 8, accuracy: 0.7341
Epoch 8, Train Loss: 0.7085, Val Loss: 0.8115
batch size: (899, 899)
✅ Epoch 9: New best model saved with val_loss = 0.7779
Epoch 9, accuracy: 0.7339
batch size: (908, 908)
✅ Epoch 10: New best model saved with val_loss = 0.7420
Epoch 10, accuracy: 0.7326
Epoch 10, Train Loss: 0.5810, Val Loss: 0.7420
batch size: (886, 886)
✅ Epoch 11: New best model saved with val_loss = 0.7127
Epoch 11, accuracy: 0.7372
batch size: (914, 914)
✅ Epoch 12: New best model saved with val_loss = 0.6823
Epoch 12, accuracy: 0.7452
Epoch 12, Train Loss: 0.4801, Val Loss: 0.6823
batch size: (890, 890)
✅ Epoch 13: New best model saved with val_loss = 0.6523
Epoch 13, accuracy: 0.7431
batch size: (905, 905)
✅ Epoch 14: New best model saved with val_loss = 0.6284
Epoch 14, accuracy: 0.7434
Epoch 14, Train Loss: 0.3869, Val Loss: 0.6284
batch size: (897, 897)
✅ Epoch 15: New best model saved with val_loss = 0.6130
Epoch 15, accuracy: 0.7452
batch size: (908, 908)
✅ Epoch 16: New best model saved with val_loss = 0.5982
Epoch 16, accuracy: 0.7458
Epoch 16, Train Loss: 0.3202, Val Loss: 0.5982
batch size: (898, 898)
✅ Epoch 17: New best model saved with val_loss = 0.5870
Epoch 17, accuracy: 0.7456
batch size: (912, 912)
✅ Epoch 18: New best model saved with val_loss = 0.5780
Epoch 18, accuracy: 0.7433
Epoch 18, Train Loss: 0.2549, Val Loss: 0.5780
batch size: (902, 902)
✅ Epoch 19: New best model saved with val_loss = 0.5686
Epoch 19, accuracy: 0.7434
batch size: (894, 894)
✅ Epoch 20: New best model saved with val_loss = 0.5627
Epoch 20, accuracy: 0.7452
Epoch 20, Train Loss: 0.2041, Val Loss: 0.5627
batch size: (897, 897)
✅ Epoch 21: New best model saved with val_loss = 0.5625
Epoch 21, accuracy: 0.7434
batch size: (885, 885)
✅ Epoch 22: New best model saved with val_loss = 0.5588
Epoch 22, accuracy: 0.7442
Epoch 22, Train Loss: 0.1583, Val Loss: 0.5588
batch size: (901, 901)
✅ Epoch 23: New best model saved with val_loss = 0.5524
Epoch 23, accuracy: 0.7406
batch size: (907, 907)
Epoch 24, accuracy: 0.7363
Epoch 24, Train Loss: 0.1257, Val Loss: 0.5558
batch size: (895, 895)
✅ Epoch 25: New best model saved with val_loss = 0.5511
Epoch 25, accuracy: 0.7318
batch size: (911, 911)
Epoch 26, accuracy: 0.7281
Epoch 26, Train Loss: 0.1112, Val Loss: 0.5684
batch size: (903, 903)
Epoch 27, accuracy: 0.7229
batch size: (912, 912)
Epoch 28, accuracy: 0.7192
Epoch 28, Train Loss: 0.0952, Val Loss: 0.5650
batch size: (914, 914)
Epoch 29, accuracy: 0.7193
batch size: (899, 899)
Epoch 30, accuracy: 0.7165
Epoch 30, Train Loss: 0.0742, Val Loss: 0.5603
batch size: (909, 909)
Epoch 31, accuracy: 0.7178
batch size: (890, 890)
Epoch 32, accuracy: 0.7198
Epoch 32, Train Loss: 0.0577, Val Loss: 0.5670
batch size: (916, 916)
Epoch 33, accuracy: 0.7185
batch size: (908, 908)
Epoch 34, accuracy: 0.7189
Epoch 34, Train Loss: 0.0636, Val Loss: 0.5776
batch size: (910, 910)
Epoch 35, accuracy: 0.7199
batch size: (915, 915)
Epoch 36, accuracy: 0.7195
Epoch 36, Train Loss: 0.0793, Val Loss: 0.5662
batch size: (888, 888)
Epoch 37, accuracy: 0.7187
batch size: (886, 886)
Epoch 38, accuracy: 0.7206
Epoch 38, Train Loss: 0.0628, Val Loss: 0.5558
batch size: (908, 908)
Epoch 39, accuracy: 0.7201
batch size: (886, 886)
Epoch 40, accuracy: 0.7201
Epoch 40, Train Loss: 0.0678, Val Loss: 0.5653
batch size: (896, 896)
Epoch 41, accuracy: 0.7203
batch size: (901, 901)
Epoch 42, accuracy: 0.7214
Epoch 42, Train Loss: 0.0495, Val Loss: 0.5710
batch size: (916, 916)
Epoch 43, accuracy: 0.7217
batch size: (907, 907)
Epoch 44, accuracy: 0.7193
Epoch 44, Train Loss: 0.0652, Val Loss: 0.5725
batch size: (904, 904)
Epoch 45, accuracy: 0.7190
batch size: (882, 882)
Epoch 46, accuracy: 0.7210
Epoch 46, Train Loss: 0.0594, Val Loss: 0.5680
batch size: (897, 897)
Epoch 47, accuracy: 0.7203
batch size: (900, 900)
Epoch 48, accuracy: 0.7202
Epoch 48, Train Loss: 0.0622, Val Loss: 0.5788
batch size: (906, 906)
Epoch 49, accuracy: 0.7192
batch size: (913, 913)
Epoch 50, accuracy: 0.7200
Epoch 50, Train Loss: 0.0583, Val Loss: 0.5741
batch size: (900, 900)
Epoch 51, accuracy: 0.7196
batch size: (888, 888)
Epoch 52, accuracy: 0.7182
Epoch 52, Train Loss: 0.0759, Val Loss: 0.5597
batch size: (908, 908)
Epoch 53, accuracy: 0.7220
batch size: (882, 882)
Epoch 54, accuracy: 0.7204
Epoch 54, Train Loss: 0.0675, Val Loss: 0.5654
batch size: (905, 905)
Epoch 55, accuracy: 0.7184
batch size: (905, 905)
Epoch 56, accuracy: 0.7179
Epoch 56, Train Loss: 0.0660, Val Loss: 0.5710
batch size: (906, 906)
Epoch 57, accuracy: 0.7223
batch size: (918, 918)
Epoch 58, accuracy: 0.7212
Epoch 58, Train Loss: 0.0690, Val Loss: 0.5570
batch size: (898, 898)
Epoch 59, accuracy: 0.7202
batch size: (902, 902)
Epoch 60, accuracy: 0.7200
Epoch 60, Train Loss: 0.0601, Val Loss: 0.5620
batch size: (913, 913)
Epoch 61, accuracy: 0.7198
batch size: (895, 895)
Epoch 62, accuracy: 0.7204
Epoch 62, Train Loss: 0.0731, Val Loss: 0.5659
batch size: (915, 915)
Epoch 63, accuracy: 0.7206
batch size: (901, 901)
Epoch 64, accuracy: 0.7199
Epoch 64, Train Loss: 0.0655, Val Loss: 0.5721
batch size: (905, 905)
Epoch 65, accuracy: 0.7195
batch size: (878, 878)
Epoch 66, accuracy: 0.7209
Epoch 66, Train Loss: 0.0509, Val Loss: 0.5589
batch size: (903, 903)
Epoch 67, accuracy: 0.7203
batch size: (912, 912)
Epoch 68, accuracy: 0.7197
Epoch 68, Train Loss: 0.0725, Val Loss: 0.5699
batch size: (899, 899)
Epoch 69, accuracy: 0.7193
batch size: (898, 898)
Epoch 70, accuracy: 0.7209
Epoch 70, Train Loss: 0.0734, Val Loss: 0.5842
batch size: (899, 899)
Epoch 71, accuracy: 0.7200
batch size: (907, 907)
Epoch 72, accuracy: 0.7197
Epoch 72, Train Loss: 0.0781, Val Loss: 0.5752
batch size: (903, 903)
✅ Epoch 73: New best model saved with val_loss = 0.5447
Epoch 73, accuracy: 0.7204
batch size: (899, 899)
Epoch 74, accuracy: 0.7172
Epoch 74, Train Loss: 0.0612, Val Loss: 0.5727
batch size: (898, 898)
Epoch 75, accuracy: 0.7198
batch size: (878, 878)
Epoch 76, accuracy: 0.7198
Epoch 76, Train Loss: 0.0609, Val Loss: 0.5710
batch size: (897, 897)
Epoch 77, accuracy: 0.7209
batch size: (886, 886)
Epoch 78, accuracy: 0.7200
Epoch 78, Train Loss: 0.0556, Val Loss: 0.5712
batch size: (897, 897)
Epoch 79, accuracy: 0.7208
batch size: (891, 891)
Epoch 80, accuracy: 0.7201
Epoch 80, Train Loss: 0.0613, Val Loss: 0.5640
batch size: (923, 923)
Epoch 81, accuracy: 0.7189
batch size: (920, 920)
Epoch 82, accuracy: 0.7216
Epoch 82, Train Loss: 0.0630, Val Loss: 0.5670
batch size: (888, 888)
Epoch 83, accuracy: 0.7212
batch size: (908, 908)
Epoch 84, accuracy: 0.7206
Epoch 84, Train Loss: 0.0693, Val Loss: 0.5631
batch size: (910, 910)
Epoch 85, accuracy: 0.7203
batch size: (892, 892)
Epoch 86, accuracy: 0.7181
Epoch 86, Train Loss: 0.0651, Val Loss: 0.5790
batch size: (881, 881)
Epoch 87, accuracy: 0.7195
batch size: (913, 913)
Epoch 88, accuracy: 0.7197
Epoch 88, Train Loss: 0.0689, Val Loss: 0.5617
batch size: (902, 902)
Epoch 89, accuracy: 0.7220
batch size: (906, 906)
Epoch 90, accuracy: 0.7191
Epoch 90, Train Loss: 0.0739, Val Loss: 0.5556
batch size: (906, 906)
Epoch 91, accuracy: 0.7192
batch size: (912, 912)
Epoch 92, accuracy: 0.7206
Epoch 92, Train Loss: 0.0687, Val Loss: 0.5687
batch size: (917, 917)
Epoch 93, accuracy: 0.7204
batch size: (899, 899)
Epoch 94, accuracy: 0.7207
Epoch 94, Train Loss: 0.0628, Val Loss: 0.5757
batch size: (915, 915)
Epoch 95, accuracy: 0.7223
batch size: (917, 917)
Epoch 96, accuracy: 0.7192
Epoch 96, Train Loss: 0.0714, Val Loss: 0.5725
batch size: (891, 891)
Epoch 97, accuracy: 0.7213
batch size: (900, 900)
Epoch 98, accuracy: 0.7196
Epoch 98, Train Loss: 0.0725, Val Loss: 0.5726
batch size: (901, 901)
Epoch 99, accuracy: 0.7194
batch size: (915, 915)
Epoch 100, accuracy: 0.7227
Epoch 100, Train Loss: 0.0596, Val Loss: 0.5716
batch size: (873, 873)
Epoch 101, accuracy: 0.7210
batch size: (922, 922)
Epoch 102, accuracy: 0.7182
Epoch 102, Train Loss: 0.0696, Val Loss: 0.5639
batch size: (887, 887)
Epoch 103, accuracy: 0.7194
batch size: (894, 894)
Epoch 104, accuracy: 0.7223
Epoch 104, Train Loss: 0.0676, Val Loss: 0.5540
batch size: (889, 889)
Epoch 105, accuracy: 0.7204
batch size: (907, 907)
Epoch 106, accuracy: 0.7194
Epoch 106, Train Loss: 0.0669, Val Loss: 0.5652
batch size: (917, 917)
Epoch 107, accuracy: 0.7186
batch size: (907, 907)
Epoch 108, accuracy: 0.7185
Epoch 108, Train Loss: 0.0699, Val Loss: 0.5708
batch size: (904, 904)
Epoch 109, accuracy: 0.7211
batch size: (908, 908)
Epoch 110, accuracy: 0.7199
Epoch 110, Train Loss: 0.0636, Val Loss: 0.5525
batch size: (910, 910)
Epoch 111, accuracy: 0.7205
batch size: (906, 906)
Epoch 112, accuracy: 0.7205
Epoch 112, Train Loss: 0.0636, Val Loss: 0.5802
batch size: (890, 890)
Epoch 113, accuracy: 0.7213
batch size: (906, 906)
Epoch 114, accuracy: 0.7193
Epoch 114, Train Loss: 0.0609, Val Loss: 0.5658
batch size: (893, 893)
Epoch 115, accuracy: 0.7192
batch size: (907, 907)
Epoch 116, accuracy: 0.7194
Epoch 116, Train Loss: 0.0656, Val Loss: 0.5631
batch size: (874, 874)
Epoch 117, accuracy: 0.7212
batch size: (891, 891)
Epoch 118, accuracy: 0.7204
Epoch 118, Train Loss: 0.0634, Val Loss: 0.5676
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
batch size: (913, 913)
Epoch 119, accuracy: 0.7178
batch size: (873, 873)
Epoch 120, accuracy: 0.7195
Epoch 120, Train Loss: 0.0551, Val Loss: 0.5690
batch size: (881, 881)
Epoch 121, accuracy: 0.7196
batch size: (903, 903)
Epoch 122, accuracy: 0.7207
Epoch 122, Train Loss: 0.0704, Val Loss: 0.5606
batch size: (893, 893)
Epoch 123, accuracy: 0.7197
batch size: (900, 900)
Epoch 124, accuracy: 0.7197
Epoch 124, Train Loss: 0.0598, Val Loss: 0.5659
batch size: (887, 887)
Epoch 125, accuracy: 0.7209
batch size: (908, 908)
Epoch 126, accuracy: 0.7185
Epoch 126, Train Loss: 0.0646, Val Loss: 0.5765
batch size: (907, 907)
Epoch 127, accuracy: 0.7183
batch size: (889, 889)
Epoch 128, accuracy: 0.7200
Epoch 128, Train Loss: 0.0620, Val Loss: 0.5639
batch size: (887, 887)
Epoch 129, accuracy: 0.7218
batch size: (894, 894)
Epoch 130, accuracy: 0.7203
Epoch 130, Train Loss: 0.0614, Val Loss: 0.5708
batch size: (882, 882)
Epoch 131, accuracy: 0.7204
batch size: (891, 891)
Epoch 132, accuracy: 0.7179
Epoch 132, Train Loss: 0.0842, Val Loss: 0.5674
batch size: (905, 905)
Epoch 133, accuracy: 0.7198
batch size: (882, 882)
Epoch 134, accuracy: 0.7205
Epoch 134, Train Loss: 0.0636, Val Loss: 0.5762
batch size: (901, 901)
Epoch 135, accuracy: 0.7184
batch size: (902, 902)
Epoch 136, accuracy: 0.7212
Epoch 136, Train Loss: 0.0617, Val Loss: 0.5710
batch size: (913, 913)
Epoch 137, accuracy: 0.7218
batch size: (895, 895)
Epoch 138, accuracy: 0.7195
Epoch 138, Train Loss: 0.0597, Val Loss: 0.5639
batch size: (913, 913)
Epoch 139, accuracy: 0.7189
batch size: (888, 888)
Epoch 140, accuracy: 0.7208
Epoch 140, Train Loss: 0.0635, Val Loss: 0.5571
batch size: (903, 903)
Epoch 141, accuracy: 0.7193
batch size: (909, 909)
Epoch 142, accuracy: 0.7192
Epoch 142, Train Loss: 0.0660, Val Loss: 0.5647
batch size: (892, 892)
Epoch 143, accuracy: 0.7191
batch size: (898, 898)
Epoch 144, accuracy: 0.7198
Epoch 144, Train Loss: 0.0629, Val Loss: 0.5505
batch size: (904, 904)
Epoch 145, accuracy: 0.7187
batch size: (919, 919)
Epoch 146, accuracy: 0.7201
Epoch 146, Train Loss: 0.0585, Val Loss: 0.5741
batch size: (913, 913)
Epoch 147, accuracy: 0.7220
batch size: (896, 896)
Epoch 148, accuracy: 0.7195
Epoch 148, Train Loss: 0.0708, Val Loss: 0.5782
batch size: (909, 909)
Epoch 149, accuracy: 0.7215
batch size: (898, 898)
Epoch 150, accuracy: 0.7218
Epoch 150, Train Loss: 0.0682, Val Loss: 0.5721
batch size: (889, 889)
Epoch 151, accuracy: 0.7206
batch size: (917, 917)
Epoch 152, accuracy: 0.7206
Epoch 152, Train Loss: 0.0682, Val Loss: 0.5607
batch size: (902, 902)
Epoch 153, accuracy: 0.7193
batch size: (897, 897)
Epoch 154, accuracy: 0.7193
Epoch 154, Train Loss: 0.0625, Val Loss: 0.5721
batch size: (898, 898)
Epoch 155, accuracy: 0.7213
batch size: (914, 914)
Epoch 156, accuracy: 0.7221
Epoch 156, Train Loss: 0.0691, Val Loss: 0.5856
batch size: (909, 909)
Epoch 157, accuracy: 0.7195
batch size: (901, 901)
Epoch 158, accuracy: 0.7202
Epoch 158, Train Loss: 0.0679, Val Loss: 0.5806
batch size: (894, 894)
Epoch 159, accuracy: 0.7215
batch size: (899, 899)
Epoch 160, accuracy: 0.7196
Epoch 160, Train Loss: 0.0642, Val Loss: 0.5715
batch size: (901, 901)
Epoch 161, accuracy: 0.7213
batch size: (886, 886)
Epoch 162, accuracy: 0.7202
Epoch 162, Train Loss: 0.0651, Val Loss: 0.5563
batch size: (907, 907)
Epoch 163, accuracy: 0.7190
batch size: (890, 890)
Epoch 164, accuracy: 0.7213
Epoch 164, Train Loss: 0.0622, Val Loss: 0.5780
batch size: (895, 895)
Epoch 165, accuracy: 0.7190
batch size: (925, 925)
Epoch 166, accuracy: 0.7196
Epoch 166, Train Loss: 0.0700, Val Loss: 0.5737
batch size: (890, 890)
Epoch 167, accuracy: 0.7219
batch size: (891, 891)
Epoch 168, accuracy: 0.7193
Epoch 168, Train Loss: 0.0625, Val Loss: 0.5622
batch size: (885, 885)
Epoch 169, accuracy: 0.7188
batch size: (928, 928)
Epoch 170, accuracy: 0.7207
Epoch 170, Train Loss: 0.0694, Val Loss: 0.5669
batch size: (912, 912)
Epoch 171, accuracy: 0.7206
batch size: (889, 889)
Epoch 172, accuracy: 0.7230
Epoch 172, Train Loss: 0.0651, Val Loss: 0.5767
batch size: (885, 885)
Epoch 173, accuracy: 0.7174
batch size: (875, 875)
Epoch 174, accuracy: 0.7198
Epoch 174, Train Loss: 0.0544, Val Loss: 0.5661
batch size: (900, 900)
Epoch 175, accuracy: 0.7199
batch size: (895, 895)
Epoch 176, accuracy: 0.7190
Epoch 176, Train Loss: 0.0659, Val Loss: 0.5807
batch size: (904, 904)
Epoch 177, accuracy: 0.7209
batch size: (896, 896)
Epoch 178, accuracy: 0.7191
Epoch 178, Train Loss: 0.0662, Val Loss: 0.5742
batch size: (893, 893)
Epoch 179, accuracy: 0.7186
batch size: (910, 910)
Epoch 180, accuracy: 0.7228
Epoch 180, Train Loss: 0.0700, Val Loss: 0.5747
batch size: (902, 902)
Epoch 181, accuracy: 0.7206
batch size: (880, 880)
Epoch 182, accuracy: 0.7200
Epoch 182, Train Loss: 0.0613, Val Loss: 0.5607
batch size: (902, 902)
Epoch 183, accuracy: 0.7199
batch size: (881, 881)
Epoch 184, accuracy: 0.7190
Epoch 184, Train Loss: 0.0586, Val Loss: 0.5713
batch size: (916, 916)
Epoch 185, accuracy: 0.7212
batch size: (917, 917)
Epoch 186, accuracy: 0.7197
Epoch 186, Train Loss: 0.0593, Val Loss: 0.5785
batch size: (887, 887)
Epoch 187, accuracy: 0.7202
batch size: (900, 900)
Epoch 188, accuracy: 0.7189
Epoch 188, Train Loss: 0.0775, Val Loss: 0.5623
batch size: (899, 899)
Epoch 189, accuracy: 0.7220
batch size: (888, 888)
Epoch 190, accuracy: 0.7208
Epoch 190, Train Loss: 0.0578, Val Loss: 0.5779
batch size: (893, 893)
Epoch 191, accuracy: 0.7198
batch size: (904, 904)
Epoch 192, accuracy: 0.7194
Epoch 192, Train Loss: 0.0623, Val Loss: 0.5570
batch size: (918, 918)
Epoch 193, accuracy: 0.7207
batch size: (928, 928)
Epoch 194, accuracy: 0.7192
Epoch 194, Train Loss: 0.0777, Val Loss: 0.5664
batch size: (880, 880)
Epoch 195, accuracy: 0.7213
batch size: (905, 905)
Epoch 196, accuracy: 0.7203
Epoch 196, Train Loss: 0.0623, Val Loss: 0.5714
batch size: (887, 887)
Epoch 197, accuracy: 0.7185
batch size: (902, 902)
Epoch 198, accuracy: 0.7203
Epoch 198, Train Loss: 0.0733, Val Loss: 0.5655
batch size: (911, 911)
Epoch 199, accuracy: 0.7190
Loaded best model with val_loss = 0.5446962118148804
test :accuracy 0.7204, f1_macro: 0.7209, f1_micro: 0.7204, auc: 0.8836
Training GAT with 8 layers...
可训练参数: 1090447_GAT
不可训练参数: 0
batch size: (908, 908)
✅ Epoch 0: New best model saved with val_loss = 1.1003
Epoch 0, accuracy: 0.1679
Epoch 0, Train Loss: 1.0982, Val Loss: 1.1003
batch size: (905, 905)
✅ Epoch 1: New best model saved with val_loss = 1.0979
Epoch 1, accuracy: 0.4417
batch size: (892, 892)
✅ Epoch 2: New best model saved with val_loss = 1.0964
Epoch 2, accuracy: 0.4652
Epoch 2, Train Loss: 1.0989, Val Loss: 1.0964
batch size: (906, 906)
Epoch 3, accuracy: 0.1667
batch size: (898, 898)
✅ Epoch 4: New best model saved with val_loss = 1.0777
Epoch 4, accuracy: 0.4470
Epoch 4, Train Loss: 1.0941, Val Loss: 1.0777
batch size: (888, 888)
Epoch 5, accuracy: 0.1970
batch size: (898, 898)
✅ Epoch 6: New best model saved with val_loss = 0.9918
Epoch 6, accuracy: 0.6204
Epoch 6, Train Loss: 1.0515, Val Loss: 0.9918
batch size: (901, 901)
✅ Epoch 7: New best model saved with val_loss = 0.9035
Epoch 7, accuracy: 0.5337
batch size: (896, 896)
✅ Epoch 8: New best model saved with val_loss = 0.8463
Epoch 8, accuracy: 0.5278
Epoch 8, Train Loss: 0.7294, Val Loss: 0.8463
batch size: (910, 910)
Epoch 9, accuracy: 0.5237
batch size: (909, 909)
Epoch 10, accuracy: 0.5181
Epoch 10, Train Loss: 0.7598, Val Loss: 1.2164
batch size: (882, 882)
Epoch 11, accuracy: 0.6469
batch size: (919, 919)
Epoch 12, accuracy: 0.5442
Epoch 12, Train Loss: 0.6154, Val Loss: 0.8576
batch size: (897, 897)
Epoch 13, accuracy: 0.5604
batch size: (904, 904)
Epoch 14, accuracy: 0.6420
Epoch 14, Train Loss: 0.4637, Val Loss: 1.4844
batch size: (884, 884)
Epoch 15, accuracy: 0.6604
batch size: (891, 891)
Epoch 16, accuracy: 0.6726
Epoch 16, Train Loss: 0.5099, Val Loss: 1.2226
batch size: (904, 904)
Epoch 17, accuracy: 0.6807
batch size: (888, 888)
Epoch 18, accuracy: 0.6872
Epoch 18, Train Loss: 0.4801, Val Loss: 1.0367
batch size: (923, 923)
Epoch 19, accuracy: 0.6910
batch size: (906, 906)
Epoch 20, accuracy: 0.6934
Epoch 20, Train Loss: 0.3595, Val Loss: 0.9260
batch size: (901, 901)
Epoch 21, accuracy: 0.6927
batch size: (884, 884)
Epoch 22, accuracy: 0.6933
Epoch 22, Train Loss: 0.3528, Val Loss: 0.8973
batch size: (897, 897)
Epoch 23, accuracy: 0.6930
batch size: (883, 883)
Epoch 24, accuracy: 0.6903
Epoch 24, Train Loss: 0.3226, Val Loss: 0.8930
batch size: (919, 919)
Epoch 25, accuracy: 0.6951
batch size: (898, 898)
Epoch 26, accuracy: 0.6932
Epoch 26, Train Loss: 0.3234, Val Loss: 0.9261
batch size: (904, 904)
Epoch 27, accuracy: 0.6966
batch size: (921, 921)
Epoch 28, accuracy: 0.6930
Epoch 28, Train Loss: 0.3590, Val Loss: 0.8897
batch size: (900, 900)
Epoch 29, accuracy: 0.6937
batch size: (889, 889)
Epoch 30, accuracy: 0.6950
Epoch 30, Train Loss: 0.3125, Val Loss: 0.8510
batch size: (907, 907)
✅ Epoch 31: New best model saved with val_loss = 0.8272
Epoch 31, accuracy: 0.6954
batch size: (912, 912)
Epoch 32, accuracy: 0.6929
Epoch 32, Train Loss: 0.3215, Val Loss: 0.8535
batch size: (909, 909)
Epoch 33, accuracy: 0.6939
batch size: (915, 915)
Epoch 34, accuracy: 0.6945
Epoch 34, Train Loss: 0.3584, Val Loss: 0.8903
batch size: (878, 878)
Epoch 35, accuracy: 0.6945
batch size: (907, 907)
Epoch 36, accuracy: 0.6927
Epoch 36, Train Loss: 0.3029, Val Loss: 0.8723
batch size: (915, 915)
Epoch 37, accuracy: 0.6935
batch size: (905, 905)
Epoch 38, accuracy: 0.6954
Epoch 38, Train Loss: 0.3263, Val Loss: 0.8672
batch size: (898, 898)
Epoch 39, accuracy: 0.6949
batch size: (894, 894)
Epoch 40, accuracy: 0.6952
Epoch 40, Train Loss: 0.3840, Val Loss: 0.8324
batch size: (894, 894)
✅ Epoch 41: New best model saved with val_loss = 0.8053
Epoch 41, accuracy: 0.6974
batch size: (901, 901)
Epoch 42, accuracy: 0.6950
Epoch 42, Train Loss: 0.3059, Val Loss: 0.9259
batch size: (874, 874)
Epoch 43, accuracy: 0.6940
batch size: (899, 899)
Epoch 44, accuracy: 0.6952
Epoch 44, Train Loss: 0.3680, Val Loss: 0.9260
batch size: (914, 914)
Epoch 45, accuracy: 0.6940
batch size: (905, 905)
Epoch 46, accuracy: 0.6915
Epoch 46, Train Loss: 0.3164, Val Loss: 0.8684
batch size: (897, 897)
Epoch 47, accuracy: 0.6949
batch size: (904, 904)
Epoch 48, accuracy: 0.6940
Epoch 48, Train Loss: 0.3418, Val Loss: 0.9391
batch size: (890, 890)
Epoch 49, accuracy: 0.6961
batch size: (901, 901)
Epoch 50, accuracy: 0.6942
Epoch 50, Train Loss: 0.3563, Val Loss: 0.8661
batch size: (911, 911)
Epoch 51, accuracy: 0.6938
batch size: (900, 900)
Epoch 52, accuracy: 0.6955
Epoch 52, Train Loss: 0.3211, Val Loss: 0.8639
batch size: (913, 913)
Epoch 53, accuracy: 0.6920
batch size: (906, 906)
Epoch 54, accuracy: 0.6946
Epoch 54, Train Loss: 0.3302, Val Loss: 0.8658
batch size: (904, 904)
Epoch 55, accuracy: 0.6955
batch size: (916, 916)
Epoch 56, accuracy: 0.6943
Epoch 56, Train Loss: 0.3253, Val Loss: 0.8383
batch size: (894, 894)
Epoch 57, accuracy: 0.6946
batch size: (924, 924)
Epoch 58, accuracy: 0.6941
Epoch 58, Train Loss: 0.3253, Val Loss: 0.8680
batch size: (897, 897)
Epoch 59, accuracy: 0.6922
batch size: (885, 885)
Epoch 60, accuracy: 0.6958
Epoch 60, Train Loss: 0.4162, Val Loss: 0.8747
batch size: (894, 894)
Epoch 61, accuracy: 0.6929
batch size: (908, 908)
Epoch 62, accuracy: 0.6925
Epoch 62, Train Loss: 0.3194, Val Loss: 0.8653
batch size: (899, 899)
Epoch 63, accuracy: 0.6950
batch size: (901, 901)
Epoch 64, accuracy: 0.6944
Epoch 64, Train Loss: 0.3455, Val Loss: 0.8950
batch size: (881, 881)
Epoch 65, accuracy: 0.6931
batch size: (904, 904)
Epoch 66, accuracy: 0.6940
Epoch 66, Train Loss: 0.3132, Val Loss: 0.8697
batch size: (881, 881)
Epoch 67, accuracy: 0.6924
batch size: (895, 895)
Epoch 68, accuracy: 0.6950
Epoch 68, Train Loss: 0.3413, Val Loss: 0.8693
batch size: (907, 907)
Epoch 69, accuracy: 0.6948
batch size: (891, 891)
Epoch 70, accuracy: 0.6939
Epoch 70, Train Loss: 0.2896, Val Loss: 0.8264
batch size: (917, 917)
Epoch 71, accuracy: 0.6934
batch size: (888, 888)
Epoch 72, accuracy: 0.6950
Epoch 72, Train Loss: 0.3601, Val Loss: 0.8888
batch size: (900, 900)
Epoch 73, accuracy: 0.6975
batch size: (895, 895)
Epoch 74, accuracy: 0.6950
Epoch 74, Train Loss: 0.3131, Val Loss: 0.8561
batch size: (907, 907)
Epoch 75, accuracy: 0.6946
batch size: (900, 900)
Epoch 76, accuracy: 0.6941
Epoch 76, Train Loss: 0.3174, Val Loss: 0.8815
batch size: (894, 894)
Epoch 77, accuracy: 0.6932
batch size: (881, 881)
Epoch 78, accuracy: 0.6936
Epoch 78, Train Loss: 0.3033, Val Loss: 0.8918
batch size: (888, 888)
Epoch 79, accuracy: 0.6951
batch size: (918, 918)
Epoch 80, accuracy: 0.6918
Epoch 80, Train Loss: 0.3151, Val Loss: 0.8538
batch size: (894, 894)
Epoch 81, accuracy: 0.6950
batch size: (897, 897)
Epoch 82, accuracy: 0.6977
Epoch 82, Train Loss: 0.3224, Val Loss: 0.8480
batch size: (906, 906)
Epoch 83, accuracy: 0.6934
batch size: (902, 902)
Epoch 84, accuracy: 0.6943
Epoch 84, Train Loss: 0.3890, Val Loss: 0.8722
batch size: (882, 882)
Epoch 85, accuracy: 0.6950
batch size: (890, 890)
Epoch 86, accuracy: 0.6932
Epoch 86, Train Loss: 0.4412, Val Loss: 0.8941
batch size: (905, 905)
Epoch 87, accuracy: 0.6952
batch size: (905, 905)
Epoch 88, accuracy: 0.6944
Epoch 88, Train Loss: 0.2985, Val Loss: 0.9126
batch size: (901, 901)
Epoch 89, accuracy: 0.6941
batch size: (918, 918)
Epoch 90, accuracy: 0.6934
Epoch 90, Train Loss: 0.3533, Val Loss: 0.9060
batch size: (895, 895)
Epoch 91, accuracy: 0.6956
batch size: (898, 898)
Epoch 92, accuracy: 0.6918
Epoch 92, Train Loss: 0.3192, Val Loss: 0.8232
batch size: (897, 897)
Epoch 93, accuracy: 0.6951
batch size: (897, 897)
Epoch 94, accuracy: 0.6923
Epoch 94, Train Loss: 0.3090, Val Loss: 0.8677
batch size: (896, 896)
Epoch 95, accuracy: 0.6937
batch size: (911, 911)
Epoch 96, accuracy: 0.6927
Epoch 96, Train Loss: 0.3034, Val Loss: 0.8653
batch size: (902, 902)
Epoch 97, accuracy: 0.6941
batch size: (901, 901)
Epoch 98, accuracy: 0.6957
Epoch 98, Train Loss: 0.3287, Val Loss: 0.8960
batch size: (884, 884)
Epoch 99, accuracy: 0.6948
batch size: (908, 908)
Epoch 100, accuracy: 0.6933
Epoch 100, Train Loss: 0.3229, Val Loss: 0.8748
batch size: (882, 882)
Epoch 101, accuracy: 0.6942
batch size: (896, 896)
Epoch 102, accuracy: 0.6922
Epoch 102, Train Loss: 0.3005, Val Loss: 0.8322
batch size: (909, 909)
Epoch 103, accuracy: 0.6937
batch size: (899, 899)
Epoch 104, accuracy: 0.6951
Epoch 104, Train Loss: 0.3102, Val Loss: 0.8593
batch size: (910, 910)
Epoch 105, accuracy: 0.6942
batch size: (896, 896)
Epoch 106, accuracy: 0.6928
Epoch 106, Train Loss: 0.3693, Val Loss: 0.8508
batch size: (882, 882)
Epoch 107, accuracy: 0.6938
batch size: (910, 910)
Epoch 108, accuracy: 0.6947
Epoch 108, Train Loss: 0.3120, Val Loss: 0.9315
batch size: (899, 899)
Epoch 109, accuracy: 0.6956
batch size: (918, 918)
Epoch 110, accuracy: 0.6951
Epoch 110, Train Loss: 0.3764, Val Loss: 0.8908
batch size: (875, 875)
Epoch 111, accuracy: 0.6966
batch size: (907, 907)
Epoch 112, accuracy: 0.6940
Epoch 112, Train Loss: 0.3311, Val Loss: 0.8467
batch size: (902, 902)
Epoch 113, accuracy: 0.6953
batch size: (896, 896)
✅ Epoch 114: New best model saved with val_loss = 0.8021
Epoch 114, accuracy: 0.6959
Epoch 114, Train Loss: 0.3360, Val Loss: 0.8021
batch size: (887, 887)
Epoch 115, accuracy: 0.6939
batch size: (897, 897)
Epoch 116, accuracy: 0.6927
Epoch 116, Train Loss: 0.3294, Val Loss: 0.8671
batch size: (892, 892)
Epoch 117, accuracy: 0.6935
batch size: (900, 900)
Epoch 118, accuracy: 0.6957
Epoch 118, Train Loss: 0.3124, Val Loss: 0.8683
batch size: (904, 904)
Epoch 119, accuracy: 0.6956
batch size: (894, 894)
Epoch 120, accuracy: 0.6926
Epoch 120, Train Loss: 0.3329, Val Loss: 0.8866
batch size: (893, 893)
Epoch 121, accuracy: 0.6938
batch size: (889, 889)
Epoch 122, accuracy: 0.6959
Epoch 122, Train Loss: 0.3160, Val Loss: 0.8516
batch size: (905, 905)
Epoch 123, accuracy: 0.6943
batch size: (888, 888)
Epoch 124, accuracy: 0.6951
Epoch 124, Train Loss: 0.3496, Val Loss: 0.9594
batch size: (883, 883)
Epoch 125, accuracy: 0.6947
batch size: (921, 921)
Epoch 126, accuracy: 0.6945
Epoch 126, Train Loss: 0.3388, Val Loss: 0.9127
batch size: (893, 893)
Epoch 127, accuracy: 0.6928
batch size: (913, 913)
Epoch 128, accuracy: 0.6939
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 128, Train Loss: 0.3149, Val Loss: 0.8619
batch size: (913, 913)
Epoch 129, accuracy: 0.6941
batch size: (911, 911)
Epoch 130, accuracy: 0.6954
Epoch 130, Train Loss: 0.3547, Val Loss: 0.8958
batch size: (904, 904)
Epoch 131, accuracy: 0.6945
batch size: (905, 905)
Epoch 132, accuracy: 0.6914
Epoch 132, Train Loss: 0.3580, Val Loss: 0.8336
batch size: (879, 879)
Epoch 133, accuracy: 0.6930
batch size: (883, 883)
Epoch 134, accuracy: 0.6954
Epoch 134, Train Loss: 0.3093, Val Loss: 0.8516
batch size: (909, 909)
Epoch 135, accuracy: 0.6957
batch size: (901, 901)
Epoch 136, accuracy: 0.6946
Epoch 136, Train Loss: 0.3837, Val Loss: 0.8947
batch size: (899, 899)
Epoch 137, accuracy: 0.6949
batch size: (886, 886)
Epoch 138, accuracy: 0.6922
Epoch 138, Train Loss: 0.3080, Val Loss: 0.9020
batch size: (891, 891)
Epoch 139, accuracy: 0.6925
batch size: (893, 893)
Epoch 140, accuracy: 0.6968
Epoch 140, Train Loss: 0.3239, Val Loss: 0.8479
batch size: (912, 912)
Epoch 141, accuracy: 0.6947
batch size: (909, 909)
Epoch 142, accuracy: 0.6953
Epoch 142, Train Loss: 0.3011, Val Loss: 0.9124
batch size: (904, 904)
Epoch 143, accuracy: 0.6939
batch size: (900, 900)
Epoch 144, accuracy: 0.6981
Epoch 144, Train Loss: 0.2981, Val Loss: 0.8327
batch size: (887, 887)
Epoch 145, accuracy: 0.6923
batch size: (897, 897)
Epoch 146, accuracy: 0.6948
Epoch 146, Train Loss: 0.3331, Val Loss: 0.8780
batch size: (912, 912)
Epoch 147, accuracy: 0.6949
batch size: (890, 890)
Epoch 148, accuracy: 0.6941
Epoch 148, Train Loss: 0.3253, Val Loss: 0.8681
batch size: (906, 906)
Epoch 149, accuracy: 0.6940
batch size: (879, 879)
Epoch 150, accuracy: 0.6941
Epoch 150, Train Loss: 0.3030, Val Loss: 0.8874
batch size: (895, 895)
Epoch 151, accuracy: 0.6924
batch size: (909, 909)
Epoch 152, accuracy: 0.6942
Epoch 152, Train Loss: 0.3310, Val Loss: 0.8329
batch size: (894, 894)
Epoch 153, accuracy: 0.6931
batch size: (891, 891)
Epoch 154, accuracy: 0.6938
Epoch 154, Train Loss: 0.3853, Val Loss: 0.8966
batch size: (899, 899)
Epoch 155, accuracy: 0.6941
batch size: (892, 892)
Epoch 156, accuracy: 0.6946
Epoch 156, Train Loss: 0.3536, Val Loss: 0.8959
batch size: (904, 904)
Epoch 157, accuracy: 0.6931
batch size: (914, 914)
Epoch 158, accuracy: 0.6943
Epoch 158, Train Loss: 0.3197, Val Loss: 0.8780
batch size: (921, 921)
Epoch 159, accuracy: 0.6933
batch size: (896, 896)
Epoch 160, accuracy: 0.6961
Epoch 160, Train Loss: 0.3693, Val Loss: 0.8742
batch size: (896, 896)
Epoch 161, accuracy: 0.6948
batch size: (888, 888)
Epoch 162, accuracy: 0.6968
Epoch 162, Train Loss: 0.3366, Val Loss: 0.8907
batch size: (908, 908)
Epoch 163, accuracy: 0.6947
batch size: (908, 908)
Epoch 164, accuracy: 0.6931
Epoch 164, Train Loss: 0.3312, Val Loss: 0.8787
batch size: (892, 892)
Epoch 165, accuracy: 0.6921
batch size: (894, 894)
Epoch 166, accuracy: 0.6911
Epoch 166, Train Loss: 0.2898, Val Loss: 0.8508
batch size: (898, 898)
Epoch 167, accuracy: 0.6945
batch size: (888, 888)
Epoch 168, accuracy: 0.6945
Epoch 168, Train Loss: 0.3504, Val Loss: 0.8380
batch size: (889, 889)
Epoch 169, accuracy: 0.6926
batch size: (878, 878)
Epoch 170, accuracy: 0.6910
Epoch 170, Train Loss: 0.3446, Val Loss: 0.8688
batch size: (897, 897)
Epoch 171, accuracy: 0.6929
batch size: (906, 906)
Epoch 172, accuracy: 0.6927
Epoch 172, Train Loss: 0.3747, Val Loss: 0.8892
batch size: (893, 893)
Epoch 173, accuracy: 0.6954
batch size: (904, 904)
Epoch 174, accuracy: 0.6954
Epoch 174, Train Loss: 0.3760, Val Loss: 0.8773
batch size: (885, 885)
Epoch 175, accuracy: 0.6948
batch size: (911, 911)
Epoch 176, accuracy: 0.6945
Epoch 176, Train Loss: 0.3332, Val Loss: 0.9143
batch size: (882, 882)
Epoch 177, accuracy: 0.6941
batch size: (890, 890)
Epoch 178, accuracy: 0.6948
Epoch 178, Train Loss: 0.3186, Val Loss: 0.9180
batch size: (912, 912)
Epoch 179, accuracy: 0.6929
batch size: (912, 912)
Epoch 180, accuracy: 0.6952
Epoch 180, Train Loss: 0.2976, Val Loss: 0.8298
batch size: (889, 889)
Epoch 181, accuracy: 0.6949
batch size: (892, 892)
Epoch 182, accuracy: 0.6933
Epoch 182, Train Loss: 0.3241, Val Loss: 0.8817
batch size: (910, 910)
Epoch 183, accuracy: 0.6942
batch size: (905, 905)
Epoch 184, accuracy: 0.6942
Epoch 184, Train Loss: 0.3488, Val Loss: 0.8834
batch size: (895, 895)
Epoch 185, accuracy: 0.6960
batch size: (894, 894)
Epoch 186, accuracy: 0.6944
Epoch 186, Train Loss: 0.3148, Val Loss: 0.9093
batch size: (887, 887)
Epoch 187, accuracy: 0.6954
batch size: (886, 886)
Epoch 188, accuracy: 0.6901
Epoch 188, Train Loss: 0.3283, Val Loss: 0.8778
batch size: (898, 898)
Epoch 189, accuracy: 0.6964
batch size: (903, 903)
Epoch 190, accuracy: 0.6936
Epoch 190, Train Loss: 0.3258, Val Loss: 0.8861
batch size: (910, 910)
Epoch 191, accuracy: 0.6948
batch size: (905, 905)
Epoch 192, accuracy: 0.6921
Epoch 192, Train Loss: 0.3288, Val Loss: 0.8944
batch size: (888, 888)
Epoch 193, accuracy: 0.6940
batch size: (892, 892)
Epoch 194, accuracy: 0.6926
Epoch 194, Train Loss: 0.3207, Val Loss: 0.9108
batch size: (903, 903)
Epoch 195, accuracy: 0.6962
batch size: (920, 920)
Epoch 196, accuracy: 0.6937
Epoch 196, Train Loss: 0.3018, Val Loss: 0.8484
batch size: (888, 888)
Epoch 197, accuracy: 0.6950
batch size: (907, 907)
Epoch 198, accuracy: 0.6939
Epoch 198, Train Loss: 0.3402, Val Loss: 0.8755
batch size: (900, 900)
Epoch 199, accuracy: 0.6942
Loaded best model with val_loss = 0.8021381497383118
test :accuracy 0.6925, f1_macro: 0.6912, f1_micro: 0.6925, auc: 0.8299
Training GAT with 32 layers...
可训练参数: 4666255_GAT
不可训练参数: 0
batch size: (910, 910)
✅ Epoch 0: New best model saved with val_loss = 1.1036
Epoch 0, accuracy: 0.1654
Epoch 0, Train Loss: 1.0981, Val Loss: 1.1036
batch size: (872, 872)
✅ Epoch 1: New best model saved with val_loss = 1.0962
Epoch 1, accuracy: 0.4016
batch size: (894, 894)
Epoch 2, accuracy: 0.4309
Epoch 2, Train Loss: 1.0951, Val Loss: 1.0972
batch size: (905, 905)
Epoch 3, accuracy: 0.4013
batch size: (900, 900)
Epoch 4, accuracy: 0.4064
Epoch 4, Train Loss: 1.1613, Val Loss: 1.1026
batch size: (915, 915)
Epoch 5, accuracy: 0.4280
batch size: (888, 888)
Epoch 6, accuracy: 0.4294
Epoch 6, Train Loss: 1.1039, Val Loss: 1.0967
batch size: (890, 890)
Epoch 7, accuracy: 0.4264
batch size: (924, 924)
Epoch 8, accuracy: 0.4301
Epoch 8, Train Loss: 1.1008, Val Loss: 1.0992
batch size: (871, 871)
Epoch 9, accuracy: 0.4314
batch size: (889, 889)
Epoch 10, accuracy: 0.4271
Epoch 10, Train Loss: 1.0998, Val Loss: 1.0995
batch size: (902, 902)
Epoch 11, accuracy: 0.4284
batch size: (904, 904)
Epoch 12, accuracy: 0.4301
Epoch 12, Train Loss: 1.1006, Val Loss: 1.1000
batch size: (897, 897)
Epoch 13, accuracy: 0.4297
batch size: (877, 877)
Epoch 14, accuracy: 0.4315
Epoch 14, Train Loss: 1.1023, Val Loss: 1.1002
batch size: (887, 887)
Epoch 15, accuracy: 0.4311
batch size: (892, 892)
Epoch 16, accuracy: 0.4276
Epoch 16, Train Loss: 1.0929, Val Loss: 1.1002
batch size: (896, 896)
Epoch 17, accuracy: 0.4327
batch size: (915, 915)
Epoch 18, accuracy: 0.4310
Epoch 18, Train Loss: 1.1033, Val Loss: 1.1002
batch size: (902, 902)
Epoch 19, accuracy: 0.4263
batch size: (896, 896)
Epoch 20, accuracy: 0.4270
Epoch 20, Train Loss: 1.1030, Val Loss: 1.1002
batch size: (892, 892)
Epoch 21, accuracy: 0.4292
batch size: (894, 894)
Epoch 22, accuracy: 0.4297
Epoch 22, Train Loss: 1.1003, Val Loss: 1.1002
batch size: (898, 898)
Epoch 23, accuracy: 0.4296
batch size: (916, 916)
Epoch 24, accuracy: 0.4297
Epoch 24, Train Loss: 1.1072, Val Loss: 1.1002
batch size: (914, 914)
Epoch 25, accuracy: 0.4308
batch size: (902, 902)
Epoch 26, accuracy: 0.4315
Epoch 26, Train Loss: 1.1006, Val Loss: 1.1002
batch size: (907, 907)
Epoch 27, accuracy: 0.4337
batch size: (901, 901)
Epoch 28, accuracy: 0.4318
Epoch 28, Train Loss: 1.0981, Val Loss: 1.1002
batch size: (900, 900)
Epoch 29, accuracy: 0.4298
batch size: (896, 896)
Epoch 30, accuracy: 0.4306
Epoch 30, Train Loss: 1.1052, Val Loss: 1.1002
batch size: (907, 907)
Epoch 31, accuracy: 0.4279
batch size: (885, 885)
Epoch 32, accuracy: 0.4323
Epoch 32, Train Loss: 1.1007, Val Loss: 1.1002
batch size: (900, 900)
Epoch 33, accuracy: 0.4289
batch size: (900, 900)
Epoch 34, accuracy: 0.4281
Epoch 34, Train Loss: 1.0964, Val Loss: 1.1002
batch size: (907, 907)
Epoch 35, accuracy: 0.4272
batch size: (911, 911)
Epoch 36, accuracy: 0.4289
Epoch 36, Train Loss: 1.0977, Val Loss: 1.1002
batch size: (888, 888)
Epoch 37, accuracy: 0.4338
batch size: (905, 905)
Epoch 38, accuracy: 0.4288
Epoch 38, Train Loss: 1.1002, Val Loss: 1.1002
batch size: (896, 896)
Epoch 39, accuracy: 0.4326
batch size: (900, 900)
Epoch 40, accuracy: 0.4274
Epoch 40, Train Loss: 1.0947, Val Loss: 1.1002
batch size: (888, 888)
Epoch 41, accuracy: 0.4293
batch size: (897, 897)
Epoch 42, accuracy: 0.4300
Epoch 42, Train Loss: 1.1010, Val Loss: 1.1002
batch size: (894, 894)
Epoch 43, accuracy: 0.4311
batch size: (886, 886)
Epoch 44, accuracy: 0.4306
Epoch 44, Train Loss: 1.1016, Val Loss: 1.1002
batch size: (894, 894)
Epoch 45, accuracy: 0.4268
batch size: (897, 897)
Epoch 46, accuracy: 0.4312
Epoch 46, Train Loss: 1.1016, Val Loss: 1.1002
batch size: (897, 897)
Epoch 47, accuracy: 0.4331
batch size: (897, 897)
Epoch 48, accuracy: 0.4321
Epoch 48, Train Loss: 1.1034, Val Loss: 1.1002
batch size: (906, 906)
Epoch 49, accuracy: 0.4284
batch size: (916, 916)
Epoch 50, accuracy: 0.4326
Epoch 50, Train Loss: 1.0981, Val Loss: 1.1002
batch size: (902, 902)
Epoch 51, accuracy: 0.4306
batch size: (885, 885)
Epoch 52, accuracy: 0.4316
Epoch 52, Train Loss: 1.1032, Val Loss: 1.1002
batch size: (897, 897)
Epoch 53, accuracy: 0.4351
batch size: (889, 889)
Epoch 54, accuracy: 0.4292
Epoch 54, Train Loss: 1.1069, Val Loss: 1.1002
batch size: (910, 910)
Epoch 55, accuracy: 0.4296
batch size: (903, 903)
Epoch 56, accuracy: 0.4353
Epoch 56, Train Loss: 1.0997, Val Loss: 1.1002
batch size: (907, 907)
Epoch 57, accuracy: 0.4309
batch size: (907, 907)
Epoch 58, accuracy: 0.4337
Epoch 58, Train Loss: 1.0984, Val Loss: 1.1002
batch size: (904, 904)
Epoch 59, accuracy: 0.4303
batch size: (887, 887)
Epoch 60, accuracy: 0.4314
Epoch 60, Train Loss: 1.0981, Val Loss: 1.1002
batch size: (893, 893)
Epoch 61, accuracy: 0.4306
batch size: (893, 893)
Epoch 62, accuracy: 0.4291
Epoch 62, Train Loss: 1.1050, Val Loss: 1.1002
batch size: (909, 909)
Epoch 63, accuracy: 0.4285
batch size: (898, 898)
Epoch 64, accuracy: 0.4260
Epoch 64, Train Loss: 1.1006, Val Loss: 1.1002
batch size: (907, 907)
Epoch 65, accuracy: 0.4290
batch size: (895, 895)
Epoch 66, accuracy: 0.4298
Epoch 66, Train Loss: 1.1005, Val Loss: 1.1002
batch size: (919, 919)
Epoch 67, accuracy: 0.4292
batch size: (895, 895)
Epoch 68, accuracy: 0.4267
Epoch 68, Train Loss: 1.0961, Val Loss: 1.1002
batch size: (895, 895)
Epoch 69, accuracy: 0.4332
batch size: (907, 907)
Epoch 70, accuracy: 0.4282
Epoch 70, Train Loss: 1.1051, Val Loss: 1.1002
batch size: (900, 900)
Epoch 71, accuracy: 0.4311
batch size: (902, 902)
Epoch 72, accuracy: 0.4297
Epoch 72, Train Loss: 1.0981, Val Loss: 1.1002
batch size: (903, 903)
Epoch 73, accuracy: 0.4316
batch size: (905, 905)
Epoch 74, accuracy: 0.4338
Epoch 74, Train Loss: 1.1030, Val Loss: 1.1002
batch size: (903, 903)
Epoch 75, accuracy: 0.4272
batch size: (881, 881)
Epoch 76, accuracy: 0.4311
Epoch 76, Train Loss: 1.0950, Val Loss: 1.1002
batch size: (894, 894)
Epoch 77, accuracy: 0.4290
batch size: (867, 867)
Epoch 78, accuracy: 0.4311
Epoch 78, Train Loss: 1.0970, Val Loss: 1.1002
batch size: (899, 899)
Epoch 79, accuracy: 0.4292
batch size: (890, 890)
Epoch 80, accuracy: 0.4318
Epoch 80, Train Loss: 1.1041, Val Loss: 1.1002
batch size: (904, 904)
Epoch 81, accuracy: 0.4255
batch size: (899, 899)
Epoch 82, accuracy: 0.4272
Epoch 82, Train Loss: 1.1004, Val Loss: 1.1002
batch size: (894, 894)
Epoch 83, accuracy: 0.4291
batch size: (900, 900)
Epoch 84, accuracy: 0.4275
Epoch 84, Train Loss: 1.1050, Val Loss: 1.1002
batch size: (906, 906)
Epoch 85, accuracy: 0.4303
batch size: (899, 899)
Epoch 86, accuracy: 0.4294
Epoch 86, Train Loss: 1.0962, Val Loss: 1.1002
batch size: (900, 900)
Epoch 87, accuracy: 0.4329
batch size: (920, 920)
Epoch 88, accuracy: 0.4322
Epoch 88, Train Loss: 1.1012, Val Loss: 1.1002
batch size: (919, 919)
Epoch 89, accuracy: 0.4341
batch size: (905, 905)
Epoch 90, accuracy: 0.4312
Epoch 90, Train Loss: 1.1069, Val Loss: 1.1002
batch size: (889, 889)
Epoch 91, accuracy: 0.4303
batch size: (919, 919)
Epoch 92, accuracy: 0.4304
Epoch 92, Train Loss: 1.1036, Val Loss: 1.1002
batch size: (906, 906)
Epoch 93, accuracy: 0.4280
batch size: (915, 915)
Epoch 94, accuracy: 0.4310
Epoch 94, Train Loss: 1.0972, Val Loss: 1.1002
batch size: (887, 887)
Epoch 95, accuracy: 0.4329
batch size: (911, 911)
Epoch 96, accuracy: 0.4332
Epoch 96, Train Loss: 1.0984, Val Loss: 1.1002
batch size: (874, 874)
Epoch 97, accuracy: 0.4332
batch size: (908, 908)
Epoch 98, accuracy: 0.4303
Epoch 98, Train Loss: 1.1021, Val Loss: 1.1002
batch size: (894, 894)
Epoch 99, accuracy: 0.4292
batch size: (900, 900)
Epoch 100, accuracy: 0.4240
Epoch 100, Train Loss: 1.0971, Val Loss: 1.1002
batch size: (897, 897)
Epoch 101, accuracy: 0.4277
batch size: (900, 900)
Epoch 102, accuracy: 0.4327
Epoch 102, Train Loss: 1.0973, Val Loss: 1.1002
batch size: (901, 901)
Epoch 103, accuracy: 0.4314
batch size: (902, 902)
Epoch 104, accuracy: 0.4313
Epoch 104, Train Loss: 1.0999, Val Loss: 1.1002
batch size: (925, 925)
Epoch 105, accuracy: 0.4329
batch size: (892, 892)
Epoch 106, accuracy: 0.4283
Epoch 106, Train Loss: 1.0963, Val Loss: 1.1002
batch size: (907, 907)
Epoch 107, accuracy: 0.4294
batch size: (890, 890)
Epoch 108, accuracy: 0.4306
Epoch 108, Train Loss: 1.0998, Val Loss: 1.1002
batch size: (889, 889)
Epoch 109, accuracy: 0.4310
batch size: (910, 910)
Epoch 110, accuracy: 0.4304
Epoch 110, Train Loss: 1.1015, Val Loss: 1.1002
batch size: (902, 902)
Epoch 111, accuracy: 0.4292
batch size: (883, 883)
Epoch 112, accuracy: 0.4305
Epoch 112, Train Loss: 1.0968, Val Loss: 1.1002
batch size: (913, 913)
Epoch 113, accuracy: 0.4305
batch size: (899, 899)
Epoch 114, accuracy: 0.4314
Epoch 114, Train Loss: 1.1038, Val Loss: 1.1002
batch size: (889, 889)
Epoch 115, accuracy: 0.4317
batch size: (897, 897)
Epoch 116, accuracy: 0.4300
Epoch 116, Train Loss: 1.0960, Val Loss: 1.1002
batch size: (888, 888)
Epoch 117, accuracy: 0.4288
batch size: (902, 902)
Epoch 118, accuracy: 0.4318
Epoch 118, Train Loss: 1.0982, Val Loss: 1.1002
batch size: (885, 885)
Epoch 119, accuracy: 0.4308
batch size: (893, 893)
Epoch 120, accuracy: 0.4305
Epoch 120, Train Loss: 1.1016, Val Loss: 1.1002
batch size: (909, 909)
Epoch 121, accuracy: 0.4320
batch size: (886, 886)
Epoch 122, accuracy: 0.4316
Epoch 122, Train Loss: 1.0977, Val Loss: 1.1002
batch size: (908, 908)
Epoch 123, accuracy: 0.4279
batch size: (879, 879)
Epoch 124, accuracy: 0.4314
Epoch 124, Train Loss: 1.0996, Val Loss: 1.1002
batch size: (896, 896)
Epoch 125, accuracy: 0.4308
batch size: (911, 911)
Epoch 126, accuracy: 0.4320
Epoch 126, Train Loss: 1.0960, Val Loss: 1.1002
batch size: (886, 886)
Epoch 127, accuracy: 0.4315
batch size: (888, 888)
Epoch 128, accuracy: 0.4285
Epoch 128, Train Loss: 1.0973, Val Loss: 1.1002
batch size: (896, 896)
Epoch 129, accuracy: 0.4284
batch size: (891, 891)
Epoch 130, accuracy: 0.4301
Epoch 130, Train Loss: 1.0972, Val Loss: 1.1002
batch size: (900, 900)
Epoch 131, accuracy: 0.4291
batch size: (904, 904)
Epoch 132, accuracy: 0.4263
Epoch 132, Train Loss: 1.1001, Val Loss: 1.1002
batch size: (894, 894)
Epoch 133, accuracy: 0.4311
batch size: (907, 907)
Epoch 134, accuracy: 0.4286
Epoch 134, Train Loss: 1.0961, Val Loss: 1.1002
batch size: (899, 899)
Epoch 135, accuracy: 0.4288
batch size: (884, 884)
Epoch 136, accuracy: 0.4296
Epoch 136, Train Loss: 1.0966, Val Loss: 1.1002
batch size: (913, 913)
Epoch 137, accuracy: 0.4326
batch size: (897, 897)
Epoch 138, accuracy: 0.4305
Epoch 138, Train Loss: 1.1002, Val Loss: 1.1002
batch size: (911, 911)
Epoch 139, accuracy: 0.4314
batch size: (898, 898)
Epoch 140, accuracy: 0.4279
Epoch 140, Train Loss: 1.0976, Val Loss: 1.1002
batch size: (876, 876)
Epoch 141, accuracy: 0.4288
batch size: (900, 900)
Epoch 142, accuracy: 0.4336
Epoch 142, Train Loss: 1.0974, Val Loss: 1.1002
batch size: (898, 898)
Epoch 143, accuracy: 0.4307
batch size: (913, 913)
Epoch 144, accuracy: 0.4316
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 144, Train Loss: 1.0949, Val Loss: 1.1002
batch size: (920, 920)
Epoch 145, accuracy: 0.4270
batch size: (891, 891)
Epoch 146, accuracy: 0.4295
Epoch 146, Train Loss: 1.0974, Val Loss: 1.1002
batch size: (901, 901)
Epoch 147, accuracy: 0.4269
batch size: (899, 899)
Epoch 148, accuracy: 0.4347
Epoch 148, Train Loss: 1.1024, Val Loss: 1.1002
batch size: (904, 904)
Epoch 149, accuracy: 0.4279
batch size: (895, 895)
Epoch 150, accuracy: 0.4320
Epoch 150, Train Loss: 1.0954, Val Loss: 1.1002
batch size: (891, 891)
Epoch 151, accuracy: 0.4280
batch size: (905, 905)
Epoch 152, accuracy: 0.4308
Epoch 152, Train Loss: 1.1010, Val Loss: 1.1002
batch size: (889, 889)
Epoch 153, accuracy: 0.4273
batch size: (896, 896)
Epoch 154, accuracy: 0.4296
Epoch 154, Train Loss: 1.0977, Val Loss: 1.1002
batch size: (917, 917)
Epoch 155, accuracy: 0.4320
batch size: (903, 903)
Epoch 156, accuracy: 0.4312
Epoch 156, Train Loss: 1.1004, Val Loss: 1.1002
batch size: (899, 899)
Epoch 157, accuracy: 0.4346
batch size: (900, 900)
Epoch 158, accuracy: 0.4335
Epoch 158, Train Loss: 1.0989, Val Loss: 1.1002
batch size: (887, 887)
Epoch 159, accuracy: 0.4292
batch size: (921, 921)
Epoch 160, accuracy: 0.4322
Epoch 160, Train Loss: 1.1021, Val Loss: 1.1002
batch size: (893, 893)
Epoch 161, accuracy: 0.4304
batch size: (900, 900)
Epoch 162, accuracy: 0.4331
Epoch 162, Train Loss: 1.1058, Val Loss: 1.1002
batch size: (912, 912)
Epoch 163, accuracy: 0.4324
batch size: (912, 912)
Epoch 164, accuracy: 0.4320
Epoch 164, Train Loss: 1.1026, Val Loss: 1.1002
batch size: (909, 909)
Epoch 165, accuracy: 0.4348
batch size: (898, 898)
Epoch 166, accuracy: 0.4272
Epoch 166, Train Loss: 1.0987, Val Loss: 1.1002
batch size: (892, 892)
Epoch 167, accuracy: 0.4282
batch size: (912, 912)
Epoch 168, accuracy: 0.4347
Epoch 168, Train Loss: 1.0933, Val Loss: 1.1002
batch size: (881, 881)
Epoch 169, accuracy: 0.4320
batch size: (904, 904)
Epoch 170, accuracy: 0.4282
Epoch 170, Train Loss: 1.1042, Val Loss: 1.1002
batch size: (907, 907)
Epoch 171, accuracy: 0.4318
batch size: (914, 914)
Epoch 172, accuracy: 0.4284
Epoch 172, Train Loss: 1.1025, Val Loss: 1.1002
batch size: (880, 880)
Epoch 173, accuracy: 0.4319
batch size: (896, 896)
Epoch 174, accuracy: 0.4346
Epoch 174, Train Loss: 1.0994, Val Loss: 1.1002
batch size: (887, 887)
Epoch 175, accuracy: 0.4321
batch size: (912, 912)
Epoch 176, accuracy: 0.4332
Epoch 176, Train Loss: 1.1016, Val Loss: 1.1002
batch size: (903, 903)
Epoch 177, accuracy: 0.4340
batch size: (917, 917)
Epoch 178, accuracy: 0.4303
Epoch 178, Train Loss: 1.1014, Val Loss: 1.1002
batch size: (902, 902)
Epoch 179, accuracy: 0.4319
batch size: (906, 906)
Epoch 180, accuracy: 0.4266
Epoch 180, Train Loss: 1.0998, Val Loss: 1.1002
batch size: (914, 914)
Epoch 181, accuracy: 0.4283
batch size: (896, 896)
Epoch 182, accuracy: 0.4313
Epoch 182, Train Loss: 1.1001, Val Loss: 1.1002
batch size: (900, 900)
Epoch 183, accuracy: 0.4304
batch size: (911, 911)
Epoch 184, accuracy: 0.4310
Epoch 184, Train Loss: 1.1015, Val Loss: 1.1002
batch size: (904, 904)
Epoch 185, accuracy: 0.4337
batch size: (916, 916)
Epoch 186, accuracy: 0.4283
Epoch 186, Train Loss: 1.0962, Val Loss: 1.1002
batch size: (914, 914)
Epoch 187, accuracy: 0.4317
batch size: (896, 896)
Epoch 188, accuracy: 0.4296
Epoch 188, Train Loss: 1.1040, Val Loss: 1.1002
batch size: (892, 892)
Epoch 189, accuracy: 0.4329
batch size: (910, 910)
Epoch 190, accuracy: 0.4328
Epoch 190, Train Loss: 1.0988, Val Loss: 1.1002
batch size: (898, 898)
Epoch 191, accuracy: 0.4291
batch size: (900, 900)
Epoch 192, accuracy: 0.4323
Epoch 192, Train Loss: 1.0955, Val Loss: 1.1002
batch size: (901, 901)
Epoch 193, accuracy: 0.4278
batch size: (884, 884)
Epoch 194, accuracy: 0.4338
Epoch 194, Train Loss: 1.0998, Val Loss: 1.1002
batch size: (903, 903)
Epoch 195, accuracy: 0.4331
batch size: (904, 904)
Epoch 196, accuracy: 0.4291
Epoch 196, Train Loss: 1.1009, Val Loss: 1.1002
batch size: (907, 907)
Epoch 197, accuracy: 0.4275
batch size: (887, 887)
Epoch 198, accuracy: 0.4302
Epoch 198, Train Loss: 1.0954, Val Loss: 1.1002
batch size: (901, 901)
Epoch 199, accuracy: 0.4276
Loaded best model with val_loss = 1.0962094068527222
test :accuracy 0.4008, f1_macro: 0.1908, f1_micro: 0.4008, auc: 0.2901
Training JKNet with 2 layers...
可训练参数: 391942_JKNet
不可训练参数: 0
batch size: (901, 901)
✅ Epoch 0: New best model saved with val_loss = 1.0956
Epoch 0, accuracy: 0.2920
Epoch 0, Train Loss: 1.3579, Val Loss: 1.0956
batch size: (909, 909)
✅ Epoch 1: New best model saved with val_loss = 1.0937
Epoch 1, accuracy: 0.2236
batch size: (897, 897)
✅ Epoch 2: New best model saved with val_loss = 1.0908
Epoch 2, accuracy: 0.2296
Epoch 2, Train Loss: 0.1246, Val Loss: 1.0908
batch size: (918, 918)
✅ Epoch 3: New best model saved with val_loss = 1.0887
Epoch 3, accuracy: 0.1961
batch size: (891, 891)
✅ Epoch 4: New best model saved with val_loss = 1.0848
Epoch 4, accuracy: 0.1884
Epoch 4, Train Loss: 0.0095, Val Loss: 1.0848
batch size: (916, 916)
✅ Epoch 5: New best model saved with val_loss = 1.0785
Epoch 5, accuracy: 0.2001
batch size: (894, 894)
✅ Epoch 6: New best model saved with val_loss = 1.0735
Epoch 6, accuracy: 0.2125
Epoch 6, Train Loss: 0.0036, Val Loss: 1.0735
batch size: (893, 893)
✅ Epoch 7: New best model saved with val_loss = 1.0660
Epoch 7, accuracy: 0.2273
batch size: (903, 903)
✅ Epoch 8: New best model saved with val_loss = 1.0618
Epoch 8, accuracy: 0.2335
Epoch 8, Train Loss: 0.0013, Val Loss: 1.0618
batch size: (904, 904)
✅ Epoch 9: New best model saved with val_loss = 1.0585
Epoch 9, accuracy: 0.2369
batch size: (899, 899)
✅ Epoch 10: New best model saved with val_loss = 1.0554
Epoch 10, accuracy: 0.2403
Epoch 10, Train Loss: 0.0003, Val Loss: 1.0554
batch size: (907, 907)
✅ Epoch 11: New best model saved with val_loss = 1.0537
Epoch 11, accuracy: 0.2421
batch size: (898, 898)
✅ Epoch 12: New best model saved with val_loss = 1.0532
Epoch 12, accuracy: 0.2439
Epoch 12, Train Loss: 0.0003, Val Loss: 1.0532
batch size: (894, 894)
✅ Epoch 13: New best model saved with val_loss = 1.0515
Epoch 13, accuracy: 0.2443
batch size: (881, 881)
✅ Epoch 14: New best model saved with val_loss = 1.0488
Epoch 14, accuracy: 0.2436
Epoch 14, Train Loss: 0.0001, Val Loss: 1.0488
batch size: (910, 910)
✅ Epoch 15: New best model saved with val_loss = 1.0437
Epoch 15, accuracy: 0.2519
batch size: (909, 909)
✅ Epoch 16: New best model saved with val_loss = 1.0424
Epoch 16, accuracy: 0.2500
Epoch 16, Train Loss: 0.0008, Val Loss: 1.0424
batch size: (908, 908)
✅ Epoch 17: New best model saved with val_loss = 1.0400
Epoch 17, accuracy: 0.2493
batch size: (888, 888)
Epoch 18, accuracy: 0.2480
Epoch 18, Train Loss: 0.0000, Val Loss: 1.0414
batch size: (912, 912)
✅ Epoch 19: New best model saved with val_loss = 1.0397
Epoch 19, accuracy: 0.2508
batch size: (901, 901)
✅ Epoch 20: New best model saved with val_loss = 1.0388
Epoch 20, accuracy: 0.2527
Epoch 20, Train Loss: 0.0001, Val Loss: 1.0388
batch size: (875, 875)
Epoch 21, accuracy: 0.2495
batch size: (906, 906)
✅ Epoch 22: New best model saved with val_loss = 1.0380
Epoch 22, accuracy: 0.2549
Epoch 22, Train Loss: 0.0000, Val Loss: 1.0380
batch size: (898, 898)
✅ Epoch 23: New best model saved with val_loss = 1.0351
Epoch 23, accuracy: 0.2563
batch size: (883, 883)
✅ Epoch 24: New best model saved with val_loss = 1.0347
Epoch 24, accuracy: 0.2539
Epoch 24, Train Loss: 0.0000, Val Loss: 1.0347
batch size: (898, 898)
Epoch 25, accuracy: 0.2528
batch size: (891, 891)
Epoch 26, accuracy: 0.2499
Epoch 26, Train Loss: 0.0000, Val Loss: 1.0367
batch size: (905, 905)
Epoch 27, accuracy: 0.2542
batch size: (892, 892)
✅ Epoch 28: New best model saved with val_loss = 1.0338
Epoch 28, accuracy: 0.2632
Epoch 28, Train Loss: 0.0000, Val Loss: 1.0338
batch size: (903, 903)
✅ Epoch 29: New best model saved with val_loss = 1.0334
Epoch 29, accuracy: 0.2696
batch size: (905, 905)
Epoch 30, accuracy: 0.2676
Epoch 30, Train Loss: 0.0001, Val Loss: 1.0338
batch size: (883, 883)
✅ Epoch 31: New best model saved with val_loss = 1.0329
Epoch 31, accuracy: 0.2734
batch size: (875, 875)
Epoch 32, accuracy: 0.2832
Epoch 32, Train Loss: 0.0000, Val Loss: 1.0342
batch size: (902, 902)
Epoch 33, accuracy: 0.2788
batch size: (891, 891)
Epoch 34, accuracy: 0.2950
Epoch 34, Train Loss: 0.0000, Val Loss: 1.0350
batch size: (903, 903)
Epoch 35, accuracy: 0.2892
batch size: (872, 872)
Epoch 36, accuracy: 0.2892
Epoch 36, Train Loss: 0.0001, Val Loss: 1.0367
batch size: (902, 902)
Epoch 37, accuracy: 0.2933
batch size: (886, 886)
Epoch 38, accuracy: 0.2939
Epoch 38, Train Loss: 0.0001, Val Loss: 1.0379
batch size: (878, 878)
Epoch 39, accuracy: 0.2897
batch size: (912, 912)
Epoch 40, accuracy: 0.2817
Epoch 40, Train Loss: 0.0000, Val Loss: 1.0400
batch size: (886, 886)
Epoch 41, accuracy: 0.2973
batch size: (893, 893)
Epoch 42, accuracy: 0.2926
Epoch 42, Train Loss: 0.0001, Val Loss: 1.0412
batch size: (910, 910)
Epoch 43, accuracy: 0.2977
batch size: (887, 887)
Epoch 44, accuracy: 0.3084
Epoch 44, Train Loss: 0.0000, Val Loss: 1.0376
batch size: (908, 908)
Epoch 45, accuracy: 0.3087
batch size: (896, 896)
Epoch 46, accuracy: 0.3004
Epoch 46, Train Loss: 0.0000, Val Loss: 1.0377
batch size: (886, 886)
Epoch 47, accuracy: 0.3092
batch size: (893, 893)
Epoch 48, accuracy: 0.3096
Epoch 48, Train Loss: 0.0000, Val Loss: 1.0382
batch size: (894, 894)
Epoch 49, accuracy: 0.3027
batch size: (895, 895)
Epoch 50, accuracy: 0.3020
Epoch 50, Train Loss: 0.0000, Val Loss: 1.0384
batch size: (927, 927)
Epoch 51, accuracy: 0.3052
batch size: (891, 891)
Epoch 52, accuracy: 0.3042
Epoch 52, Train Loss: 0.0000, Val Loss: 1.0361
batch size: (895, 895)
Epoch 53, accuracy: 0.3018
batch size: (902, 902)
Epoch 54, accuracy: 0.3092
Epoch 54, Train Loss: 0.0000, Val Loss: 1.0351
batch size: (896, 896)
Epoch 55, accuracy: 0.3143
batch size: (910, 910)
Epoch 56, accuracy: 0.3100
Epoch 56, Train Loss: 0.0000, Val Loss: 1.0368
batch size: (897, 897)
Epoch 57, accuracy: 0.3023
batch size: (905, 905)
Epoch 58, accuracy: 0.2987
Epoch 58, Train Loss: 0.0001, Val Loss: 1.0367
batch size: (881, 881)
Epoch 59, accuracy: 0.3049
batch size: (881, 881)
Epoch 60, accuracy: 0.3091
Epoch 60, Train Loss: 0.0001, Val Loss: 1.0363
batch size: (901, 901)
Epoch 61, accuracy: 0.3178
batch size: (906, 906)
Epoch 62, accuracy: 0.3175
Epoch 62, Train Loss: 0.0000, Val Loss: 1.0339
batch size: (900, 900)
Epoch 63, accuracy: 0.3164
batch size: (914, 914)
Epoch 64, accuracy: 0.3092
Epoch 64, Train Loss: 0.0000, Val Loss: 1.0370
batch size: (873, 873)
Epoch 65, accuracy: 0.3099
batch size: (884, 884)
Epoch 66, accuracy: 0.3179
Epoch 66, Train Loss: 0.0000, Val Loss: 1.0364
batch size: (884, 884)
Epoch 67, accuracy: 0.3241
batch size: (905, 905)
Epoch 68, accuracy: 0.3166
Epoch 68, Train Loss: 0.0000, Val Loss: 1.0356
batch size: (885, 885)
Epoch 69, accuracy: 0.3237
batch size: (905, 905)
Epoch 70, accuracy: 0.3258
Epoch 70, Train Loss: 0.0001, Val Loss: 1.0364
batch size: (892, 892)
Epoch 71, accuracy: 0.3382
batch size: (893, 893)
Epoch 72, accuracy: 0.3452
Epoch 72, Train Loss: 0.0001, Val Loss: 1.0358
batch size: (879, 879)
Epoch 73, accuracy: 0.3299
batch size: (905, 905)
Epoch 74, accuracy: 0.3224
Epoch 74, Train Loss: 0.0001, Val Loss: 1.0371
batch size: (888, 888)
Epoch 75, accuracy: 0.3267
batch size: (901, 901)
Epoch 76, accuracy: 0.3317
Epoch 76, Train Loss: 0.0008, Val Loss: 1.0396
batch size: (900, 900)
Epoch 77, accuracy: 0.3269
batch size: (901, 901)
Epoch 78, accuracy: 0.3223
Epoch 78, Train Loss: 0.0001, Val Loss: 1.0382
batch size: (906, 906)
Epoch 79, accuracy: 0.3244
batch size: (888, 888)
Epoch 80, accuracy: 0.3217
Epoch 80, Train Loss: 0.0000, Val Loss: 1.0381
batch size: (910, 910)
Epoch 81, accuracy: 0.3080
batch size: (908, 908)
Epoch 82, accuracy: 0.2993
Epoch 82, Train Loss: 0.0003, Val Loss: 1.0387
batch size: (922, 922)
Epoch 83, accuracy: 0.3067
batch size: (896, 896)
Epoch 84, accuracy: 0.3093
Epoch 84, Train Loss: 0.0001, Val Loss: 1.0371
batch size: (903, 903)
Epoch 85, accuracy: 0.3162
batch size: (897, 897)
Epoch 86, accuracy: 0.3161
Epoch 86, Train Loss: 0.0000, Val Loss: 1.0357
batch size: (904, 904)
Epoch 87, accuracy: 0.3234
batch size: (910, 910)
Epoch 88, accuracy: 0.3314
Epoch 88, Train Loss: 0.0000, Val Loss: 1.0364
batch size: (899, 899)
Epoch 89, accuracy: 0.3443
batch size: (895, 895)
Epoch 90, accuracy: 0.3391
Epoch 90, Train Loss: 0.0000, Val Loss: 1.0374
batch size: (891, 891)
Epoch 91, accuracy: 0.3511
batch size: (898, 898)
Epoch 92, accuracy: 0.3538
Epoch 92, Train Loss: 0.0000, Val Loss: 1.0395
batch size: (914, 914)
Epoch 93, accuracy: 0.3441
batch size: (893, 893)
Epoch 94, accuracy: 0.3426
Epoch 94, Train Loss: 0.0000, Val Loss: 1.0394
batch size: (898, 898)
Epoch 95, accuracy: 0.3323
batch size: (903, 903)
Epoch 96, accuracy: 0.3170
Epoch 96, Train Loss: 0.0000, Val Loss: 1.0398
batch size: (904, 904)
Epoch 97, accuracy: 0.3131
batch size: (883, 883)
Epoch 98, accuracy: 0.3242
Epoch 98, Train Loss: 0.0000, Val Loss: 1.0382
batch size: (901, 901)
Epoch 99, accuracy: 0.3183
batch size: (900, 900)
Epoch 100, accuracy: 0.3353
Epoch 100, Train Loss: 0.0001, Val Loss: 1.0376
batch size: (886, 886)
Epoch 101, accuracy: 0.3383
batch size: (890, 890)
Epoch 102, accuracy: 0.3360
Epoch 102, Train Loss: 0.0000, Val Loss: 1.0379
batch size: (905, 905)
Epoch 103, accuracy: 0.3351
batch size: (904, 904)
Epoch 104, accuracy: 0.3331
Epoch 104, Train Loss: 0.0001, Val Loss: 1.0386
batch size: (900, 900)
Epoch 105, accuracy: 0.3210
batch size: (886, 886)
Epoch 106, accuracy: 0.3217
Epoch 106, Train Loss: 0.0000, Val Loss: 1.0387
batch size: (887, 887)
Epoch 107, accuracy: 0.3119
batch size: (884, 884)
Epoch 108, accuracy: 0.3189
Epoch 108, Train Loss: 0.0000, Val Loss: 1.0385
batch size: (903, 903)
Epoch 109, accuracy: 0.3250
batch size: (907, 907)
Epoch 110, accuracy: 0.3278
Epoch 110, Train Loss: 0.0000, Val Loss: 1.0379
batch size: (907, 907)
Epoch 111, accuracy: 0.3379
batch size: (884, 884)
Epoch 112, accuracy: 0.3338
Epoch 112, Train Loss: 0.0000, Val Loss: 1.0370
batch size: (906, 906)
Epoch 113, accuracy: 0.3341
batch size: (902, 902)
Epoch 114, accuracy: 0.3408
Epoch 114, Train Loss: 0.0000, Val Loss: 1.0388
batch size: (916, 916)
Epoch 115, accuracy: 0.3321
batch size: (912, 912)
Epoch 116, accuracy: 0.3212
Epoch 116, Train Loss: 0.0000, Val Loss: 1.0386
batch size: (881, 881)
Epoch 117, accuracy: 0.3126
batch size: (910, 910)
Epoch 118, accuracy: 0.3146
Epoch 118, Train Loss: 0.0004, Val Loss: 1.0365
batch size: (907, 907)
Epoch 119, accuracy: 0.3080
batch size: (892, 892)
Epoch 120, accuracy: 0.3100
Epoch 120, Train Loss: 0.0000, Val Loss: 1.0391
batch size: (902, 902)
Epoch 121, accuracy: 0.3147
batch size: (897, 897)
Epoch 122, accuracy: 0.3342
Epoch 122, Train Loss: 0.0000, Val Loss: 1.0371
batch size: (895, 895)
Epoch 123, accuracy: 0.3318
batch size: (910, 910)
Epoch 124, accuracy: 0.3344
Epoch 124, Train Loss: 0.0000, Val Loss: 1.0377
batch size: (888, 888)
Epoch 125, accuracy: 0.3433
batch size: (922, 922)
Epoch 126, accuracy: 0.3352
Epoch 126, Train Loss: 0.0001, Val Loss: 1.0371
batch size: (930, 930)
Epoch 127, accuracy: 0.3211
batch size: (898, 898)
Epoch 128, accuracy: 0.3010
Epoch 128, Train Loss: 0.0000, Val Loss: 1.0358
batch size: (905, 905)
Epoch 129, accuracy: 0.3008
batch size: (899, 899)
Epoch 130, accuracy: 0.3122
Epoch 130, Train Loss: 0.0000, Val Loss: 1.0345
batch size: (876, 876)
Epoch 131, accuracy: 0.3004
batch size: (903, 903)
Epoch 132, accuracy: 0.3086
Epoch 132, Train Loss: 0.0000, Val Loss: 1.0380
batch size: (904, 904)
Epoch 133, accuracy: 0.3186
batch size: (890, 890)
Epoch 134, accuracy: 0.3101
Epoch 134, Train Loss: 0.0000, Val Loss: 1.0367
batch size: (903, 903)
Epoch 135, accuracy: 0.3112
batch size: (893, 893)
Epoch 136, accuracy: 0.3060
Epoch 136, Train Loss: 0.0000, Val Loss: 1.0383
batch size: (890, 890)
Epoch 137, accuracy: 0.3054
batch size: (899, 899)
Epoch 138, accuracy: 0.2991
Epoch 138, Train Loss: 0.0000, Val Loss: 1.0398
batch size: (911, 911)
Epoch 139, accuracy: 0.3080
batch size: (891, 891)
Epoch 140, accuracy: 0.3068
Epoch 140, Train Loss: 0.0004, Val Loss: 1.0413
batch size: (902, 902)
Epoch 141, accuracy: 0.3103
batch size: (907, 907)
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 142, accuracy: 0.3096
Epoch 142, Train Loss: 0.0000, Val Loss: 1.0391
batch size: (894, 894)
Epoch 143, accuracy: 0.3242
batch size: (915, 915)
Epoch 144, accuracy: 0.3231
Epoch 144, Train Loss: 0.0000, Val Loss: 1.0360
batch size: (889, 889)
Epoch 145, accuracy: 0.3150
batch size: (925, 925)
Epoch 146, accuracy: 0.3161
Epoch 146, Train Loss: 0.0000, Val Loss: 1.0364
batch size: (888, 888)
Epoch 147, accuracy: 0.3192
batch size: (897, 897)
Epoch 148, accuracy: 0.3056
Epoch 148, Train Loss: 0.0000, Val Loss: 1.0358
batch size: (912, 912)
Epoch 149, accuracy: 0.3023
batch size: (920, 920)
Epoch 150, accuracy: 0.2985
Epoch 150, Train Loss: 0.0000, Val Loss: 1.0355
batch size: (895, 895)
Epoch 151, accuracy: 0.3131
batch size: (898, 898)
Epoch 152, accuracy: 0.3253
Epoch 152, Train Loss: 0.0003, Val Loss: 1.0354
batch size: (909, 909)
Epoch 153, accuracy: 0.3315
batch size: (908, 908)
Epoch 154, accuracy: 0.3253
Epoch 154, Train Loss: 0.0000, Val Loss: 1.0380
batch size: (908, 908)
Epoch 155, accuracy: 0.3253
batch size: (885, 885)
Epoch 156, accuracy: 0.3103
Epoch 156, Train Loss: 0.0003, Val Loss: 1.0380
batch size: (902, 902)
Epoch 157, accuracy: 0.3218
batch size: (910, 910)
Epoch 158, accuracy: 0.3125
Epoch 158, Train Loss: 0.0000, Val Loss: 1.0366
batch size: (906, 906)
Epoch 159, accuracy: 0.3153
batch size: (893, 893)
Epoch 160, accuracy: 0.3137
Epoch 160, Train Loss: 0.0000, Val Loss: 1.0383
batch size: (889, 889)
Epoch 161, accuracy: 0.3183
batch size: (907, 907)
Epoch 162, accuracy: 0.3230
Epoch 162, Train Loss: 0.0001, Val Loss: 1.0386
batch size: (892, 892)
Epoch 163, accuracy: 0.3193
batch size: (874, 874)
Epoch 164, accuracy: 0.3273
Epoch 164, Train Loss: 0.0001, Val Loss: 1.0381
batch size: (908, 908)
Epoch 165, accuracy: 0.3177
batch size: (900, 900)
Epoch 166, accuracy: 0.3134
Epoch 166, Train Loss: 0.0000, Val Loss: 1.0379
batch size: (885, 885)
Epoch 167, accuracy: 0.3042
batch size: (917, 917)
Epoch 168, accuracy: 0.3215
Epoch 168, Train Loss: 0.0000, Val Loss: 1.0370
batch size: (898, 898)
Epoch 169, accuracy: 0.3236
batch size: (905, 905)
Epoch 170, accuracy: 0.3274
Epoch 170, Train Loss: 0.0001, Val Loss: 1.0369
batch size: (894, 894)
Epoch 171, accuracy: 0.3144
batch size: (895, 895)
Epoch 172, accuracy: 0.3174
Epoch 172, Train Loss: 0.0000, Val Loss: 1.0360
batch size: (897, 897)
Epoch 173, accuracy: 0.3299
batch size: (897, 897)
Epoch 174, accuracy: 0.3351
Epoch 174, Train Loss: 0.0000, Val Loss: 1.0350
batch size: (914, 914)
Epoch 175, accuracy: 0.3328
batch size: (918, 918)
Epoch 176, accuracy: 0.3112
Epoch 176, Train Loss: 0.0000, Val Loss: 1.0369
batch size: (913, 913)
Epoch 177, accuracy: 0.3122
batch size: (901, 901)
Epoch 178, accuracy: 0.3257
Epoch 178, Train Loss: 0.0003, Val Loss: 1.0347
batch size: (893, 893)
Epoch 179, accuracy: 0.3324
batch size: (907, 907)
Epoch 180, accuracy: 0.3330
Epoch 180, Train Loss: 0.0003, Val Loss: 1.0357
batch size: (909, 909)
Epoch 181, accuracy: 0.3218
batch size: (926, 926)
Epoch 182, accuracy: 0.3202
Epoch 182, Train Loss: 0.0000, Val Loss: 1.0354
batch size: (899, 899)
Epoch 183, accuracy: 0.3285
batch size: (877, 877)
Epoch 184, accuracy: 0.3321
Epoch 184, Train Loss: 0.0000, Val Loss: 1.0357
batch size: (897, 897)
Epoch 185, accuracy: 0.3142
batch size: (897, 897)
Epoch 186, accuracy: 0.3189
Epoch 186, Train Loss: 0.0000, Val Loss: 1.0381
batch size: (901, 901)
Epoch 187, accuracy: 0.3124
batch size: (896, 896)
Epoch 188, accuracy: 0.3056
Epoch 188, Train Loss: 0.0000, Val Loss: 1.0375
batch size: (906, 906)
Epoch 189, accuracy: 0.3095
batch size: (892, 892)
Epoch 190, accuracy: 0.3126
Epoch 190, Train Loss: 0.0001, Val Loss: 1.0385
batch size: (901, 901)
Epoch 191, accuracy: 0.3185
batch size: (894, 894)
Epoch 192, accuracy: 0.3211
Epoch 192, Train Loss: 0.0002, Val Loss: 1.0372
batch size: (893, 893)
Epoch 193, accuracy: 0.3125
batch size: (904, 904)
Epoch 194, accuracy: 0.3159
Epoch 194, Train Loss: 0.0000, Val Loss: 1.0360
batch size: (912, 912)
Epoch 195, accuracy: 0.3152
batch size: (897, 897)
Epoch 196, accuracy: 0.3283
Epoch 196, Train Loss: 0.0000, Val Loss: 1.0371
batch size: (901, 901)
Epoch 197, accuracy: 0.3172
batch size: (881, 881)
Epoch 198, accuracy: 0.3279
Epoch 198, Train Loss: 0.0004, Val Loss: 1.0373
batch size: (917, 917)
Epoch 199, accuracy: 0.3153
Loaded best model with val_loss = 1.032896876335144
test :accuracy 0.2747, f1_macro: 0.2400, f1_micro: 0.2747, auc: 0.8079
Training JKNet with 8 layers...
可训练参数: 1184518_JKNet
不可训练参数: 0
batch size: (880, 880)
✅ Epoch 0: New best model saved with val_loss = 1.0797
Epoch 0, accuracy: 0.4051
Epoch 0, Train Loss: 1.4275, Val Loss: 1.0797
batch size: (893, 893)
Epoch 1, accuracy: 0.4258
batch size: (903, 903)
Epoch 2, accuracy: 0.3724
Epoch 2, Train Loss: 2.1989, Val Loss: 1.6412
batch size: (907, 907)
Epoch 3, accuracy: 0.3979
batch size: (907, 907)
Epoch 4, accuracy: 0.4268
Epoch 4, Train Loss: 0.9737, Val Loss: 1.2847
batch size: (932, 932)
Epoch 5, accuracy: 0.4333
batch size: (898, 898)
✅ Epoch 6: New best model saved with val_loss = 1.0582
Epoch 6, accuracy: 0.4303
Epoch 6, Train Loss: 0.3167, Val Loss: 1.0582
batch size: (909, 909)
✅ Epoch 7: New best model saved with val_loss = 1.0499
Epoch 7, accuracy: 0.4862
batch size: (883, 883)
Epoch 8, accuracy: 0.4407
Epoch 8, Train Loss: 0.1579, Val Loss: 1.0572
batch size: (898, 898)
Epoch 9, accuracy: 0.5058
batch size: (897, 897)
Epoch 10, accuracy: 0.4671
Epoch 10, Train Loss: 0.0350, Val Loss: 1.0792
batch size: (898, 898)
Epoch 11, accuracy: 0.4489
batch size: (917, 917)
Epoch 12, accuracy: 0.4142
Epoch 12, Train Loss: 0.0175, Val Loss: 1.0847
batch size: (922, 922)
Epoch 13, accuracy: 0.3907
batch size: (896, 896)
Epoch 14, accuracy: 0.4094
Epoch 14, Train Loss: 0.0141, Val Loss: 1.0855
batch size: (905, 905)
Epoch 15, accuracy: 0.4380
batch size: (893, 893)
Epoch 16, accuracy: 0.4537
Epoch 16, Train Loss: 0.0081, Val Loss: 1.0843
batch size: (903, 903)
Epoch 17, accuracy: 0.4568
batch size: (900, 900)
Epoch 18, accuracy: 0.4545
Epoch 18, Train Loss: 0.0252, Val Loss: 1.0831
batch size: (902, 902)
Epoch 19, accuracy: 0.4510
batch size: (892, 892)
Epoch 20, accuracy: 0.4507
Epoch 20, Train Loss: 0.0090, Val Loss: 1.0836
batch size: (883, 883)
Epoch 21, accuracy: 0.4543
batch size: (915, 915)
Epoch 22, accuracy: 0.4534
Epoch 22, Train Loss: 0.0089, Val Loss: 1.0837
batch size: (898, 898)
Epoch 23, accuracy: 0.4527
batch size: (894, 894)
Epoch 24, accuracy: 0.4531
Epoch 24, Train Loss: 0.0075, Val Loss: 1.0830
batch size: (893, 893)
Epoch 25, accuracy: 0.4532
batch size: (899, 899)
Epoch 26, accuracy: 0.4503
Epoch 26, Train Loss: 0.0077, Val Loss: 1.0834
batch size: (892, 892)
Epoch 27, accuracy: 0.4556
batch size: (901, 901)
Epoch 28, accuracy: 0.4510
Epoch 28, Train Loss: 0.0129, Val Loss: 1.0824
batch size: (881, 881)
Epoch 29, accuracy: 0.4568
batch size: (875, 875)
Epoch 30, accuracy: 0.4591
Epoch 30, Train Loss: 0.0200, Val Loss: 1.0833
batch size: (914, 914)
Epoch 31, accuracy: 0.4547
batch size: (880, 880)
Epoch 32, accuracy: 0.4580
Epoch 32, Train Loss: 0.0110, Val Loss: 1.0826
batch size: (908, 908)
Epoch 33, accuracy: 0.4536
batch size: (880, 880)
Epoch 34, accuracy: 0.4594
Epoch 34, Train Loss: 0.0134, Val Loss: 1.0806
batch size: (896, 896)
Epoch 35, accuracy: 0.4551
batch size: (910, 910)
Epoch 36, accuracy: 0.4597
Epoch 36, Train Loss: 0.0117, Val Loss: 1.0830
batch size: (903, 903)
Epoch 37, accuracy: 0.4568
batch size: (894, 894)
Epoch 38, accuracy: 0.4590
Epoch 38, Train Loss: 0.0120, Val Loss: 1.0826
batch size: (900, 900)
Epoch 39, accuracy: 0.4510
batch size: (894, 894)
Epoch 40, accuracy: 0.4497
Epoch 40, Train Loss: 0.0091, Val Loss: 1.0839
batch size: (900, 900)
Epoch 41, accuracy: 0.4485
batch size: (895, 895)
Epoch 42, accuracy: 0.4507
Epoch 42, Train Loss: 0.0165, Val Loss: 1.0833
batch size: (895, 895)
Epoch 43, accuracy: 0.4451
batch size: (889, 889)
Epoch 44, accuracy: 0.4452
Epoch 44, Train Loss: 0.0065, Val Loss: 1.0849
batch size: (895, 895)
Epoch 45, accuracy: 0.4301
batch size: (888, 888)
Epoch 46, accuracy: 0.4481
Epoch 46, Train Loss: 0.0127, Val Loss: 1.0833
batch size: (890, 890)
Epoch 47, accuracy: 0.4509
batch size: (908, 908)
Epoch 48, accuracy: 0.4559
Epoch 48, Train Loss: 0.0145, Val Loss: 1.0829
batch size: (895, 895)
Epoch 49, accuracy: 0.4525
batch size: (886, 886)
Epoch 50, accuracy: 0.4586
Epoch 50, Train Loss: 0.0143, Val Loss: 1.0815
batch size: (886, 886)
Epoch 51, accuracy: 0.4572
batch size: (901, 901)
Epoch 52, accuracy: 0.4518
Epoch 52, Train Loss: 0.0163, Val Loss: 1.0820
batch size: (910, 910)
Epoch 53, accuracy: 0.4567
batch size: (892, 892)
Epoch 54, accuracy: 0.4584
Epoch 54, Train Loss: 0.0104, Val Loss: 1.0830
batch size: (884, 884)
Epoch 55, accuracy: 0.4538
batch size: (904, 904)
Epoch 56, accuracy: 0.4521
Epoch 56, Train Loss: 0.0265, Val Loss: 1.0838
batch size: (898, 898)
Epoch 57, accuracy: 0.4464
batch size: (898, 898)
Epoch 58, accuracy: 0.4546
Epoch 58, Train Loss: 0.0151, Val Loss: 1.0830
batch size: (896, 896)
Epoch 59, accuracy: 0.4442
batch size: (903, 903)
Epoch 60, accuracy: 0.4497
Epoch 60, Train Loss: 0.0118, Val Loss: 1.0830
batch size: (894, 894)
Epoch 61, accuracy: 0.4534
batch size: (895, 895)
Epoch 62, accuracy: 0.4524
Epoch 62, Train Loss: 0.0308, Val Loss: 1.0834
batch size: (893, 893)
Epoch 63, accuracy: 0.4580
batch size: (888, 888)
Epoch 64, accuracy: 0.4484
Epoch 64, Train Loss: 0.0288, Val Loss: 1.0844
batch size: (894, 894)
Epoch 65, accuracy: 0.4468
batch size: (881, 881)
Epoch 66, accuracy: 0.4466
Epoch 66, Train Loss: 0.0148, Val Loss: 1.0840
batch size: (904, 904)
Epoch 67, accuracy: 0.4605
batch size: (899, 899)
Epoch 68, accuracy: 0.4578
Epoch 68, Train Loss: 0.0137, Val Loss: 1.0837
batch size: (914, 914)
Epoch 69, accuracy: 0.4569
batch size: (885, 885)
Epoch 70, accuracy: 0.4596
Epoch 70, Train Loss: 0.0223, Val Loss: 1.0818
batch size: (880, 880)
Epoch 71, accuracy: 0.4497
batch size: (908, 908)
Epoch 72, accuracy: 0.4523
Epoch 72, Train Loss: 0.0140, Val Loss: 1.0826
batch size: (906, 906)
Epoch 73, accuracy: 0.4554
batch size: (899, 899)
Epoch 74, accuracy: 0.4477
Epoch 74, Train Loss: 0.0086, Val Loss: 1.0842
batch size: (905, 905)
Epoch 75, accuracy: 0.4457
batch size: (911, 911)
Epoch 76, accuracy: 0.4537
Epoch 76, Train Loss: 0.0105, Val Loss: 1.0825
batch size: (883, 883)
Epoch 77, accuracy: 0.4546
batch size: (911, 911)
Epoch 78, accuracy: 0.4557
Epoch 78, Train Loss: 0.0049, Val Loss: 1.0827
batch size: (906, 906)
Epoch 79, accuracy: 0.4533
batch size: (905, 905)
Epoch 80, accuracy: 0.4548
Epoch 80, Train Loss: 0.0226, Val Loss: 1.0833
batch size: (901, 901)
Epoch 81, accuracy: 0.4540
batch size: (887, 887)
Epoch 82, accuracy: 0.4571
Epoch 82, Train Loss: 0.0117, Val Loss: 1.0830
batch size: (907, 907)
Epoch 83, accuracy: 0.4483
batch size: (883, 883)
Epoch 84, accuracy: 0.4523
Epoch 84, Train Loss: 0.0152, Val Loss: 1.0826
batch size: (912, 912)
Epoch 85, accuracy: 0.4518
batch size: (900, 900)
Epoch 86, accuracy: 0.4522
Epoch 86, Train Loss: 0.0091, Val Loss: 1.0830
batch size: (893, 893)
Epoch 87, accuracy: 0.4506
batch size: (901, 901)
Epoch 88, accuracy: 0.4535
Epoch 88, Train Loss: 0.0100, Val Loss: 1.0823
batch size: (899, 899)
Epoch 89, accuracy: 0.4554
batch size: (910, 910)
Epoch 90, accuracy: 0.4526
Epoch 90, Train Loss: 0.0230, Val Loss: 1.0837
batch size: (891, 891)
Epoch 91, accuracy: 0.4573
batch size: (885, 885)
Epoch 92, accuracy: 0.4500
Epoch 92, Train Loss: 0.0101, Val Loss: 1.0823
batch size: (915, 915)
Epoch 93, accuracy: 0.4506
batch size: (906, 906)
Epoch 94, accuracy: 0.4560
Epoch 94, Train Loss: 0.0087, Val Loss: 1.0824
batch size: (907, 907)
Epoch 95, accuracy: 0.4582
batch size: (897, 897)
Epoch 96, accuracy: 0.4524
Epoch 96, Train Loss: 0.0055, Val Loss: 1.0820
batch size: (917, 917)
Epoch 97, accuracy: 0.4509
batch size: (915, 915)
Epoch 98, accuracy: 0.4495
Epoch 98, Train Loss: 0.0130, Val Loss: 1.0832
batch size: (898, 898)
Epoch 99, accuracy: 0.4507
batch size: (899, 899)
Epoch 100, accuracy: 0.4531
Epoch 100, Train Loss: 0.0091, Val Loss: 1.0831
batch size: (903, 903)
Epoch 101, accuracy: 0.4516
batch size: (900, 900)
Epoch 102, accuracy: 0.4567
Epoch 102, Train Loss: 0.0086, Val Loss: 1.0821
batch size: (904, 904)
Epoch 103, accuracy: 0.4554
batch size: (896, 896)
Epoch 104, accuracy: 0.4556
Epoch 104, Train Loss: 0.0065, Val Loss: 1.0842
batch size: (895, 895)
Epoch 105, accuracy: 0.4545
batch size: (905, 905)
Epoch 106, accuracy: 0.4614
Epoch 106, Train Loss: 0.0137, Val Loss: 1.0826
batch size: (905, 905)
Epoch 107, accuracy: 0.4584
batch size: (911, 911)
Epoch 108, accuracy: 0.4577
Epoch 108, Train Loss: 0.0367, Val Loss: 1.0824
batch size: (893, 893)
Epoch 109, accuracy: 0.4566
batch size: (908, 908)
Epoch 110, accuracy: 0.4483
Epoch 110, Train Loss: 0.0166, Val Loss: 1.0844
batch size: (898, 898)
Epoch 111, accuracy: 0.4541
batch size: (913, 913)
Epoch 112, accuracy: 0.4535
Epoch 112, Train Loss: 0.0122, Val Loss: 1.0828
batch size: (900, 900)
Epoch 113, accuracy: 0.4548
batch size: (884, 884)
Epoch 114, accuracy: 0.4510
Epoch 114, Train Loss: 0.0085, Val Loss: 1.0832
batch size: (900, 900)
Epoch 115, accuracy: 0.4549
batch size: (894, 894)
Epoch 116, accuracy: 0.4517
Epoch 116, Train Loss: 0.0165, Val Loss: 1.0835
batch size: (890, 890)
Epoch 117, accuracy: 0.4530
batch size: (878, 878)
Epoch 118, accuracy: 0.4442
Epoch 118, Train Loss: 0.0119, Val Loss: 1.0842
batch size: (890, 890)
Epoch 119, accuracy: 0.4310
batch size: (896, 896)
Epoch 120, accuracy: 0.4543
Epoch 120, Train Loss: 0.0099, Val Loss: 1.0838
batch size: (884, 884)
Epoch 121, accuracy: 0.4493
batch size: (906, 906)
Epoch 122, accuracy: 0.4470
Epoch 122, Train Loss: 0.0131, Val Loss: 1.0834
batch size: (885, 885)
Epoch 123, accuracy: 0.4486
batch size: (890, 890)
Epoch 124, accuracy: 0.4528
Epoch 124, Train Loss: 0.0126, Val Loss: 1.0835
batch size: (898, 898)
Epoch 125, accuracy: 0.4508
batch size: (893, 893)
Epoch 126, accuracy: 0.4486
Epoch 126, Train Loss: 0.0101, Val Loss: 1.0853
batch size: (883, 883)
Epoch 127, accuracy: 0.4494
batch size: (897, 897)
Epoch 128, accuracy: 0.4409
Epoch 128, Train Loss: 0.0057, Val Loss: 1.0848
batch size: (896, 896)
Epoch 129, accuracy: 0.4543
batch size: (879, 879)
Epoch 130, accuracy: 0.4511
Epoch 130, Train Loss: 0.0081, Val Loss: 1.0829
batch size: (895, 895)
Epoch 131, accuracy: 0.4468
batch size: (901, 901)
Epoch 132, accuracy: 0.4385
Epoch 132, Train Loss: 0.0063, Val Loss: 1.0852
batch size: (907, 907)
Epoch 133, accuracy: 0.4459
batch size: (895, 895)
Epoch 134, accuracy: 0.4495
Epoch 134, Train Loss: 0.0134, Val Loss: 1.0832
batch size: (894, 894)
Epoch 135, accuracy: 0.4514
batch size: (907, 907)
Epoch 136, accuracy: 0.4497
Epoch 136, Train Loss: 0.0058, Val Loss: 1.0838
batch size: (903, 903)
Epoch 137, accuracy: 0.4554
batch size: (920, 920)
Epoch 138, accuracy: 0.4483
Epoch 138, Train Loss: 0.0086, Val Loss: 1.0831
batch size: (912, 912)
Epoch 139, accuracy: 0.4434
batch size: (889, 889)
Epoch 140, accuracy: 0.4509
Epoch 140, Train Loss: 0.0094, Val Loss: 1.0842
batch size: (902, 902)
Epoch 141, accuracy: 0.4549
batch size: (903, 903)
Epoch 142, accuracy: 0.4553
Epoch 142, Train Loss: 0.0101, Val Loss: 1.0826
batch size: (901, 901)
Epoch 143, accuracy: 0.4558
batch size: (909, 909)
Epoch 144, accuracy: 0.4478
Epoch 144, Train Loss: 0.0101, Val Loss: 1.0836
batch size: (900, 900)
Epoch 145, accuracy: 0.4548
batch size: (893, 893)
Epoch 146, accuracy: 0.4544
Epoch 146, Train Loss: 0.0076, Val Loss: 1.0838
batch size: (899, 899)
Epoch 147, accuracy: 0.4589
batch size: (898, 898)
Epoch 148, accuracy: 0.4549
Epoch 148, Train Loss: 0.0070, Val Loss: 1.0830
batch size: (908, 908)
Epoch 149, accuracy: 0.4556
batch size: (881, 881)
Epoch 150, accuracy: 0.4575
Epoch 150, Train Loss: 0.0210, Val Loss: 1.0814
batch size: (877, 877)
Epoch 151, accuracy: 0.4616
batch size: (910, 910)
Epoch 152, accuracy: 0.4558
Epoch 152, Train Loss: 0.0109, Val Loss: 1.0828
batch size: (900, 900)
Epoch 153, accuracy: 0.4505
batch size: (898, 898)
Epoch 154, accuracy: 0.4520
Epoch 154, Train Loss: 0.0103, Val Loss: 1.0840
batch size: (901, 901)
Epoch 155, accuracy: 0.4489
batch size: (882, 882)
Epoch 156, accuracy: 0.4451
Epoch 156, Train Loss: 0.0065, Val Loss: 1.0833
batch size: (915, 915)
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 157, accuracy: 0.4555
batch size: (898, 898)
Epoch 158, accuracy: 0.4453
Epoch 158, Train Loss: 0.0239, Val Loss: 1.0840
batch size: (900, 900)
Epoch 159, accuracy: 0.4538
batch size: (909, 909)
Epoch 160, accuracy: 0.4563
Epoch 160, Train Loss: 0.0170, Val Loss: 1.0824
batch size: (903, 903)
Epoch 161, accuracy: 0.4589
batch size: (880, 880)
Epoch 162, accuracy: 0.4426
Epoch 162, Train Loss: 0.0104, Val Loss: 1.0847
batch size: (908, 908)
Epoch 163, accuracy: 0.4440
batch size: (892, 892)
Epoch 164, accuracy: 0.4422
Epoch 164, Train Loss: 0.0067, Val Loss: 1.0840
batch size: (892, 892)
Epoch 165, accuracy: 0.4489
batch size: (894, 894)
Epoch 166, accuracy: 0.4445
Epoch 166, Train Loss: 0.0185, Val Loss: 1.0842
batch size: (900, 900)
Epoch 167, accuracy: 0.4432
batch size: (920, 920)
Epoch 168, accuracy: 0.4437
Epoch 168, Train Loss: 0.0207, Val Loss: 1.0844
batch size: (914, 914)
Epoch 169, accuracy: 0.4485
batch size: (897, 897)
Epoch 170, accuracy: 0.4455
Epoch 170, Train Loss: 0.0067, Val Loss: 1.0847
batch size: (901, 901)
Epoch 171, accuracy: 0.4523
batch size: (895, 895)
Epoch 172, accuracy: 0.4506
Epoch 172, Train Loss: 0.0110, Val Loss: 1.0850
batch size: (899, 899)
Epoch 173, accuracy: 0.4517
batch size: (884, 884)
Epoch 174, accuracy: 0.4501
Epoch 174, Train Loss: 0.0097, Val Loss: 1.0842
batch size: (898, 898)
Epoch 175, accuracy: 0.4461
batch size: (886, 886)
Epoch 176, accuracy: 0.4513
Epoch 176, Train Loss: 0.0107, Val Loss: 1.0842
batch size: (893, 893)
Epoch 177, accuracy: 0.4579
batch size: (901, 901)
Epoch 178, accuracy: 0.4524
Epoch 178, Train Loss: 0.0161, Val Loss: 1.0844
batch size: (913, 913)
Epoch 179, accuracy: 0.4553
batch size: (896, 896)
Epoch 180, accuracy: 0.4576
Epoch 180, Train Loss: 0.0093, Val Loss: 1.0833
batch size: (884, 884)
Epoch 181, accuracy: 0.4517
batch size: (917, 917)
Epoch 182, accuracy: 0.4611
Epoch 182, Train Loss: 0.0125, Val Loss: 1.0823
batch size: (908, 908)
Epoch 183, accuracy: 0.4433
batch size: (896, 896)
Epoch 184, accuracy: 0.4498
Epoch 184, Train Loss: 0.0149, Val Loss: 1.0833
batch size: (917, 917)
Epoch 185, accuracy: 0.4515
batch size: (908, 908)
Epoch 186, accuracy: 0.4461
Epoch 186, Train Loss: 0.0151, Val Loss: 1.0845
batch size: (889, 889)
Epoch 187, accuracy: 0.4512
batch size: (903, 903)
Epoch 188, accuracy: 0.4563
Epoch 188, Train Loss: 0.0138, Val Loss: 1.0831
batch size: (887, 887)
Epoch 189, accuracy: 0.4500
batch size: (894, 894)
Epoch 190, accuracy: 0.4535
Epoch 190, Train Loss: 0.0152, Val Loss: 1.0832
batch size: (910, 910)
Epoch 191, accuracy: 0.4574
batch size: (900, 900)
Epoch 192, accuracy: 0.4580
Epoch 192, Train Loss: 0.0138, Val Loss: 1.0826
batch size: (895, 895)
Epoch 193, accuracy: 0.4464
batch size: (905, 905)
Epoch 194, accuracy: 0.4297
Epoch 194, Train Loss: 0.0138, Val Loss: 1.0857
batch size: (892, 892)
Epoch 195, accuracy: 0.4330
batch size: (921, 921)
Epoch 196, accuracy: 0.4481
Epoch 196, Train Loss: 0.0113, Val Loss: 1.0840
batch size: (897, 897)
Epoch 197, accuracy: 0.4418
batch size: (898, 898)
Epoch 198, accuracy: 0.4470
Epoch 198, Train Loss: 0.0065, Val Loss: 1.0844
batch size: (900, 900)
Epoch 199, accuracy: 0.4588
Loaded best model with val_loss = 1.049939751625061
test :accuracy 0.4840, f1_macro: 0.3239, f1_micro: 0.4840, auc: 0.7482
Training JKNet with 32 layers...
可训练参数: 4354822_JKNet
不可训练参数: 0
batch size: (908, 908)
✅ Epoch 0: New best model saved with val_loss = 1.9011
Epoch 0, accuracy: 0.4297
Epoch 0, Train Loss: 1.2493, Val Loss: 1.9011
batch size: (898, 898)
✅ Epoch 1: New best model saved with val_loss = 1.1633
Epoch 1, accuracy: 0.4036
batch size: (893, 893)
✅ Epoch 2: New best model saved with val_loss = 1.1045
Epoch 2, accuracy: 0.4029
Epoch 2, Train Loss: 3.1331, Val Loss: 1.1045
batch size: (901, 901)
Epoch 3, accuracy: 0.1647
batch size: (905, 905)
✅ Epoch 4: New best model saved with val_loss = 1.0869
Epoch 4, accuracy: 0.4309
Epoch 4, Train Loss: 2.1997, Val Loss: 1.0869
batch size: (898, 898)
✅ Epoch 5: New best model saved with val_loss = 1.0729
Epoch 5, accuracy: 0.4278
batch size: (915, 915)
✅ Epoch 6: New best model saved with val_loss = 1.0620
Epoch 6, accuracy: 0.4293
Epoch 6, Train Loss: 2.6870, Val Loss: 1.0620
batch size: (897, 897)
✅ Epoch 7: New best model saved with val_loss = 1.0567
Epoch 7, accuracy: 0.4159
batch size: (894, 894)
Epoch 8, accuracy: 0.4532
Epoch 8, Train Loss: 0.7690, Val Loss: 1.0610
batch size: (908, 908)
Epoch 9, accuracy: 0.4035
batch size: (905, 905)
Epoch 10, accuracy: 0.4018
Epoch 10, Train Loss: 0.1664, Val Loss: 1.0711
batch size: (903, 903)
Epoch 11, accuracy: 0.4051
batch size: (916, 916)
Epoch 12, accuracy: 0.4020
Epoch 12, Train Loss: 0.8349, Val Loss: 1.0755
batch size: (884, 884)
Epoch 13, accuracy: 0.4077
batch size: (913, 913)
Epoch 14, accuracy: 0.4051
Epoch 14, Train Loss: 0.1107, Val Loss: 1.0749
batch size: (875, 875)
Epoch 15, accuracy: 0.4018
batch size: (903, 903)
Epoch 16, accuracy: 0.4045
Epoch 16, Train Loss: 0.0991, Val Loss: 1.0744
batch size: (892, 892)
Epoch 17, accuracy: 0.4051
batch size: (891, 891)
Epoch 18, accuracy: 0.4008
Epoch 18, Train Loss: 0.0659, Val Loss: 1.0777
batch size: (909, 909)
Epoch 19, accuracy: 0.4018
batch size: (890, 890)
Epoch 20, accuracy: 0.4029
Epoch 20, Train Loss: 0.0497, Val Loss: 1.0761
batch size: (901, 901)
Epoch 21, accuracy: 0.4054
batch size: (869, 869)
Epoch 22, accuracy: 0.4066
Epoch 22, Train Loss: 0.0785, Val Loss: 1.0769
batch size: (890, 890)
Epoch 23, accuracy: 0.4002
batch size: (894, 894)
Epoch 24, accuracy: 0.4016
Epoch 24, Train Loss: 0.0896, Val Loss: 1.0738
batch size: (896, 896)
Epoch 25, accuracy: 0.4016
batch size: (885, 885)
Epoch 26, accuracy: 0.4011
Epoch 26, Train Loss: 0.0592, Val Loss: 1.0751
batch size: (901, 901)
Epoch 27, accuracy: 0.4001
batch size: (890, 890)
Epoch 28, accuracy: 0.4035
Epoch 28, Train Loss: 0.0880, Val Loss: 1.0747
batch size: (902, 902)
Epoch 29, accuracy: 0.4064
batch size: (896, 896)
Epoch 30, accuracy: 0.4002
Epoch 30, Train Loss: 0.0692, Val Loss: 1.0736
batch size: (889, 889)
Epoch 31, accuracy: 0.4030
batch size: (890, 890)
Epoch 32, accuracy: 0.4013
Epoch 32, Train Loss: 0.0662, Val Loss: 1.0756
batch size: (918, 918)
Epoch 33, accuracy: 0.3993
batch size: (903, 903)
Epoch 34, accuracy: 0.4024
Epoch 34, Train Loss: 0.0552, Val Loss: 1.0759
batch size: (912, 912)
Epoch 35, accuracy: 0.4057
batch size: (902, 902)
Epoch 36, accuracy: 0.4016
Epoch 36, Train Loss: 0.0718, Val Loss: 1.0743
batch size: (893, 893)
Epoch 37, accuracy: 0.3985
batch size: (898, 898)
Epoch 38, accuracy: 0.4017
Epoch 38, Train Loss: 0.0811, Val Loss: 1.0752
batch size: (905, 905)
Epoch 39, accuracy: 0.4021
batch size: (910, 910)
Epoch 40, accuracy: 0.3998
Epoch 40, Train Loss: 0.0723, Val Loss: 1.0759
batch size: (895, 895)
Epoch 41, accuracy: 0.4025
batch size: (899, 899)
Epoch 42, accuracy: 0.4036
Epoch 42, Train Loss: 0.0598, Val Loss: 1.0733
batch size: (894, 894)
Epoch 43, accuracy: 0.4051
batch size: (897, 897)
Epoch 44, accuracy: 0.3971
Epoch 44, Train Loss: 0.0659, Val Loss: 1.0744
batch size: (906, 906)
Epoch 45, accuracy: 0.4046
batch size: (878, 878)
Epoch 46, accuracy: 0.4018
Epoch 46, Train Loss: 0.0904, Val Loss: 1.0737
batch size: (897, 897)
Epoch 47, accuracy: 0.4016
batch size: (887, 887)
Epoch 48, accuracy: 0.4017
Epoch 48, Train Loss: 0.0628, Val Loss: 1.0741
batch size: (900, 900)
Epoch 49, accuracy: 0.4056
batch size: (919, 919)
Epoch 50, accuracy: 0.4039
Epoch 50, Train Loss: 0.0825, Val Loss: 1.0810
batch size: (892, 892)
Epoch 51, accuracy: 0.4047
batch size: (919, 919)
Epoch 52, accuracy: 0.3993
Epoch 52, Train Loss: 0.0749, Val Loss: 1.0751
batch size: (891, 891)
Epoch 53, accuracy: 0.4056
batch size: (911, 911)
Epoch 54, accuracy: 0.4054
Epoch 54, Train Loss: 0.0703, Val Loss: 1.0757
batch size: (910, 910)
Epoch 55, accuracy: 0.4017
batch size: (899, 899)
Epoch 56, accuracy: 0.4056
Epoch 56, Train Loss: 0.0777, Val Loss: 1.0735
batch size: (905, 905)
Epoch 57, accuracy: 0.4051
batch size: (912, 912)
Epoch 58, accuracy: 0.4044
Epoch 58, Train Loss: 0.0569, Val Loss: 1.0777
batch size: (904, 904)
Epoch 59, accuracy: 0.3958
batch size: (885, 885)
Epoch 60, accuracy: 0.4064
Epoch 60, Train Loss: 0.0756, Val Loss: 1.0745
batch size: (903, 903)
Epoch 61, accuracy: 0.4042
batch size: (912, 912)
Epoch 62, accuracy: 0.4071
Epoch 62, Train Loss: 0.0794, Val Loss: 1.0763
batch size: (896, 896)
Epoch 63, accuracy: 0.4044
batch size: (910, 910)
Epoch 64, accuracy: 0.4038
Epoch 64, Train Loss: 0.0781, Val Loss: 1.0753
batch size: (895, 895)
Epoch 65, accuracy: 0.3991
batch size: (898, 898)
Epoch 66, accuracy: 0.4023
Epoch 66, Train Loss: 0.0639, Val Loss: 1.0745
batch size: (915, 915)
Epoch 67, accuracy: 0.4010
batch size: (897, 897)
Epoch 68, accuracy: 0.4028
Epoch 68, Train Loss: 0.0667, Val Loss: 1.0754
batch size: (899, 899)
Epoch 69, accuracy: 0.4017
batch size: (884, 884)
Epoch 70, accuracy: 0.4030
Epoch 70, Train Loss: 0.0564, Val Loss: 1.0814
batch size: (901, 901)
Epoch 71, accuracy: 0.4027
batch size: (884, 884)
Epoch 72, accuracy: 0.4037
Epoch 72, Train Loss: 0.0606, Val Loss: 1.0742
batch size: (914, 914)
Epoch 73, accuracy: 0.4017
batch size: (905, 905)
Epoch 74, accuracy: 0.4029
Epoch 74, Train Loss: 0.0916, Val Loss: 1.0743
batch size: (891, 891)
Epoch 75, accuracy: 0.3948
batch size: (887, 887)
Epoch 76, accuracy: 0.4039
Epoch 76, Train Loss: 0.0703, Val Loss: 1.0730
batch size: (899, 899)
Epoch 77, accuracy: 0.4024
batch size: (912, 912)
Epoch 78, accuracy: 0.4043
Epoch 78, Train Loss: 0.0574, Val Loss: 1.0758
batch size: (905, 905)
Epoch 79, accuracy: 0.4021
batch size: (895, 895)
Epoch 80, accuracy: 0.4052
Epoch 80, Train Loss: 0.0587, Val Loss: 1.0776
batch size: (895, 895)
Epoch 81, accuracy: 0.4013
batch size: (919, 919)
Epoch 82, accuracy: 0.4045
Epoch 82, Train Loss: 0.0944, Val Loss: 1.0739
batch size: (887, 887)
Epoch 83, accuracy: 0.4035
batch size: (900, 900)
Epoch 84, accuracy: 0.4048
Epoch 84, Train Loss: 0.0693, Val Loss: 1.0739
batch size: (906, 906)
Epoch 85, accuracy: 0.4058
batch size: (896, 896)
Epoch 86, accuracy: 0.4020
Epoch 86, Train Loss: 0.0650, Val Loss: 1.0743
batch size: (895, 895)
Epoch 87, accuracy: 0.4034
batch size: (917, 917)
Epoch 88, accuracy: 0.4078
Epoch 88, Train Loss: 0.0697, Val Loss: 1.0747
batch size: (904, 904)
Epoch 89, accuracy: 0.4041
batch size: (896, 896)
Epoch 90, accuracy: 0.4015
Epoch 90, Train Loss: 0.0742, Val Loss: 1.0746
batch size: (898, 898)
Epoch 91, accuracy: 0.4002
batch size: (890, 890)
Epoch 92, accuracy: 0.4050
Epoch 92, Train Loss: 0.0580, Val Loss: 1.0732
batch size: (912, 912)
Epoch 93, accuracy: 0.4046
batch size: (898, 898)
Epoch 94, accuracy: 0.4062
Epoch 94, Train Loss: 0.1060, Val Loss: 1.0776
batch size: (892, 892)
Epoch 95, accuracy: 0.4011
batch size: (882, 882)
Epoch 96, accuracy: 0.4001
Epoch 96, Train Loss: 0.0694, Val Loss: 1.0761
batch size: (885, 885)
Epoch 97, accuracy: 0.3990
batch size: (895, 895)
Epoch 98, accuracy: 0.4005
Epoch 98, Train Loss: 0.0607, Val Loss: 1.0750
batch size: (898, 898)
Epoch 99, accuracy: 0.4041
batch size: (903, 903)
Epoch 100, accuracy: 0.4004
Epoch 100, Train Loss: 0.0626, Val Loss: 1.0754
batch size: (896, 896)
Epoch 101, accuracy: 0.4013
batch size: (889, 889)
Epoch 102, accuracy: 0.4043
Epoch 102, Train Loss: 0.0820, Val Loss: 1.0756
batch size: (907, 907)
Epoch 103, accuracy: 0.4012
batch size: (915, 915)
Epoch 104, accuracy: 0.4005
Epoch 104, Train Loss: 0.0865, Val Loss: 1.0770
batch size: (875, 875)
Epoch 105, accuracy: 0.4057
batch size: (894, 894)
Epoch 106, accuracy: 0.4045
Epoch 106, Train Loss: 0.0538, Val Loss: 1.0738
batch size: (884, 884)
Epoch 107, accuracy: 0.4009
batch size: (881, 881)
Epoch 108, accuracy: 0.4026
Epoch 108, Train Loss: 0.0692, Val Loss: 1.0735
batch size: (906, 906)
Epoch 109, accuracy: 0.3993
batch size: (914, 914)
Epoch 110, accuracy: 0.4047
Epoch 110, Train Loss: 0.0952, Val Loss: 1.0742
batch size: (912, 912)
Epoch 111, accuracy: 0.4022
batch size: (895, 895)
Epoch 112, accuracy: 0.4032
Epoch 112, Train Loss: 0.0948, Val Loss: 1.0734
batch size: (884, 884)
Epoch 113, accuracy: 0.4016
batch size: (910, 910)
Epoch 114, accuracy: 0.4018
Epoch 114, Train Loss: 0.0513, Val Loss: 1.0754
batch size: (913, 913)
Epoch 115, accuracy: 0.4050
batch size: (897, 897)
Epoch 116, accuracy: 0.4002
Epoch 116, Train Loss: 0.0755, Val Loss: 1.0790
batch size: (906, 906)
Epoch 117, accuracy: 0.3995
batch size: (889, 889)
Epoch 118, accuracy: 0.4017
Epoch 118, Train Loss: 0.0701, Val Loss: 1.0786
batch size: (903, 903)
Epoch 119, accuracy: 0.4029
batch size: (906, 906)
Epoch 120, accuracy: 0.3998
Epoch 120, Train Loss: 0.0732, Val Loss: 1.0776
batch size: (911, 911)
Epoch 121, accuracy: 0.4007
batch size: (898, 898)
Epoch 122, accuracy: 0.4022
Epoch 122, Train Loss: 0.0773, Val Loss: 1.0742
batch size: (912, 912)
Epoch 123, accuracy: 0.4028
batch size: (895, 895)
Epoch 124, accuracy: 0.4085
Epoch 124, Train Loss: 0.0693, Val Loss: 1.0753
batch size: (894, 894)
Epoch 125, accuracy: 0.4038
batch size: (887, 887)
Epoch 126, accuracy: 0.4043
Epoch 126, Train Loss: 0.0788, Val Loss: 1.0771
batch size: (899, 899)
Epoch 127, accuracy: 0.4050
batch size: (905, 905)
Epoch 128, accuracy: 0.4055
Epoch 128, Train Loss: 0.0736, Val Loss: 1.0752
batch size: (898, 898)
Epoch 129, accuracy: 0.3972
batch size: (913, 913)
Epoch 130, accuracy: 0.4026
Epoch 130, Train Loss: 0.0629, Val Loss: 1.0779
batch size: (895, 895)
Epoch 131, accuracy: 0.4019
batch size: (907, 907)
Epoch 132, accuracy: 0.3993
Epoch 132, Train Loss: 0.0557, Val Loss: 1.0745
batch size: (899, 899)
Epoch 133, accuracy: 0.4039
batch size: (898, 898)
Epoch 134, accuracy: 0.4048
Epoch 134, Train Loss: 0.0820, Val Loss: 1.0736
batch size: (898, 898)
Epoch 135, accuracy: 0.4035
batch size: (906, 906)
Epoch 136, accuracy: 0.4049
Epoch 136, Train Loss: 0.0456, Val Loss: 1.0749
batch size: (928, 928)
Epoch 137, accuracy: 0.3989
batch size: (893, 893)
Epoch 138, accuracy: 0.4077
Epoch 138, Train Loss: 0.0792, Val Loss: 1.0742
batch size: (895, 895)
Epoch 139, accuracy: 0.4032
batch size: (885, 885)
Epoch 140, accuracy: 0.4033
Epoch 140, Train Loss: 0.0899, Val Loss: 1.0738
batch size: (893, 893)
Epoch 141, accuracy: 0.4047
batch size: (910, 910)
Epoch 142, accuracy: 0.4031
Epoch 142, Train Loss: 0.0475, Val Loss: 1.0763
batch size: (894, 894)
Epoch 143, accuracy: 0.3994
batch size: (902, 902)
Epoch 144, accuracy: 0.4020
Epoch 144, Train Loss: 0.0788, Val Loss: 1.0746
batch size: (900, 900)
Epoch 145, accuracy: 0.4037
batch size: (893, 893)
Epoch 146, accuracy: 0.4045
Epoch 146, Train Loss: 0.0636, Val Loss: 1.0803
batch size: (894, 894)
Epoch 147, accuracy: 0.4013
batch size: (898, 898)
Epoch 148, accuracy: 0.4038
Epoch 148, Train Loss: 0.0554, Val Loss: 1.0764
batch size: (888, 888)
Epoch 149, accuracy: 0.4042
batch size: (896, 896)
Epoch 150, accuracy: 0.4051
Epoch 150, Train Loss: 0.0562, Val Loss: 1.0738
batch size: (907, 907)
Epoch 151, accuracy: 0.4037
batch size: (924, 924)
Epoch 152, accuracy: 0.4025
Epoch 152, Train Loss: 0.0734, Val Loss: 1.0754
batch size: (889, 889)
Epoch 153, accuracy: 0.4043
batch size: (909, 909)
Epoch 154, accuracy: 0.4011
Epoch 154, Train Loss: 0.0559, Val Loss: 1.0732
batch size: (898, 898)
Epoch 155, accuracy: 0.4024
batch size: (880, 880)
Epoch 156, accuracy: 0.4071
Epoch 156, Train Loss: 0.0542, Val Loss: 1.0732
batch size: (880, 880)
Epoch 157, accuracy: 0.4013
batch size: (886, 886)
Epoch 158, accuracy: 0.4017
Epoch 158, Train Loss: 0.0895, Val Loss: 1.0749
batch size: (881, 881)
Epoch 159, accuracy: 0.4018
batch size: (887, 887)
Epoch 160, accuracy: 0.4043
Epoch 160, Train Loss: 0.0716, Val Loss: 1.0743
batch size: (880, 880)
Epoch 161, accuracy: 0.4020
batch size: (911, 911)
Epoch 162, accuracy: 0.4007
Epoch 162, Train Loss: 0.0788, Val Loss: 1.0795
batch size: (916, 916)
Epoch 163, accuracy: 0.4020
batch size: (907, 907)
Epoch 164, accuracy: 0.4004
Epoch 164, Train Loss: 0.0602, Val Loss: 1.0745
batch size: (896, 896)
Epoch 165, accuracy: 0.3989
batch size: (910, 910)
Epoch 166, accuracy: 0.3995
Epoch 166, Train Loss: 0.0659, Val Loss: 1.0774
batch size: (911, 911)
Epoch 167, accuracy: 0.4018
batch size: (888, 888)
Epoch 168, accuracy: 0.4008
Epoch 168, Train Loss: 0.0554, Val Loss: 1.0745
batch size: (901, 901)
Epoch 169, accuracy: 0.4011
batch size: /root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
(917, 917)
Epoch 170, accuracy: 0.4033
Epoch 170, Train Loss: 0.0626, Val Loss: 1.0750
batch size: (898, 898)
Epoch 171, accuracy: 0.4037
batch size: (889, 889)
Epoch 172, accuracy: 0.4018
Epoch 172, Train Loss: 0.0967, Val Loss: 1.0775
batch size: (902, 902)
Epoch 173, accuracy: 0.4016
batch size: (917, 917)
Epoch 174, accuracy: 0.4032
Epoch 174, Train Loss: 0.0592, Val Loss: 1.0755
batch size: (902, 902)
Epoch 175, accuracy: 0.4006
batch size: (909, 909)
Epoch 176, accuracy: 0.4036
Epoch 176, Train Loss: 0.0920, Val Loss: 1.0744
batch size: (904, 904)
Epoch 177, accuracy: 0.4000
batch size: (893, 893)
Epoch 178, accuracy: 0.4080
Epoch 178, Train Loss: 0.0840, Val Loss: 1.0756
batch size: (919, 919)
Epoch 179, accuracy: 0.4056
batch size: (913, 913)
Epoch 180, accuracy: 0.4027
Epoch 180, Train Loss: 0.0524, Val Loss: 1.0762
batch size: (895, 895)
Epoch 181, accuracy: 0.4045
batch size: (900, 900)
Epoch 182, accuracy: 0.4034
Epoch 182, Train Loss: 0.0476, Val Loss: 1.0771
batch size: (895, 895)
Epoch 183, accuracy: 0.4065
batch size: (895, 895)
Epoch 184, accuracy: 0.4017
Epoch 184, Train Loss: 0.0763, Val Loss: 1.0749
batch size: (900, 900)
Epoch 185, accuracy: 0.4028
batch size: (886, 886)
Epoch 186, accuracy: 0.4022
Epoch 186, Train Loss: 0.0750, Val Loss: 1.0767
batch size: (905, 905)
Epoch 187, accuracy: 0.4022
batch size: (904, 904)
Epoch 188, accuracy: 0.4063
Epoch 188, Train Loss: 0.0794, Val Loss: 1.0761
batch size: (890, 890)
Epoch 189, accuracy: 0.4030
batch size: (897, 897)
Epoch 190, accuracy: 0.4027
Epoch 190, Train Loss: 0.0734, Val Loss: 1.0745
batch size: (890, 890)
Epoch 191, accuracy: 0.4082
batch size: (908, 908)
Epoch 192, accuracy: 0.3991
Epoch 192, Train Loss: 0.0836, Val Loss: 1.0745
batch size: (896, 896)
Epoch 193, accuracy: 0.3981
batch size: (911, 911)
Epoch 194, accuracy: 0.4050
Epoch 194, Train Loss: 0.0732, Val Loss: 1.0754
batch size: (879, 879)
Epoch 195, accuracy: 0.4042
batch size: (921, 921)
Epoch 196, accuracy: 0.3990
Epoch 196, Train Loss: 0.0811, Val Loss: 1.0774
batch size: (912, 912)
Epoch 197, accuracy: 0.4030
batch size: (891, 891)
Epoch 198, accuracy: 0.4035
Epoch 198, Train Loss: 0.1051, Val Loss: 1.0739
batch size: (890, 890)
Epoch 199, accuracy: 0.4009
Loaded best model with val_loss = 1.056741714477539
test :accuracy 0.4139, f1_macro: 0.2764, f1_micro: 0.4139, auc: 0.6991
Training resGCN with 2 layers...
可训练参数: 132626_resGCN
不可训练参数: 0
batch size: (884, 884)
✅ Epoch 0: New best model saved with val_loss = 1.0960
Epoch 0, accuracy: 0.4485
Epoch 0, Train Loss: 1.0986, Val Loss: 1.0960
batch size: (908, 908)
✅ Epoch 1: New best model saved with val_loss = 1.0765
Epoch 1, accuracy: 0.4683
batch size: (905, 905)
✅ Epoch 2: New best model saved with val_loss = 1.0566
Epoch 2, accuracy: 0.4153
Epoch 2, Train Loss: 1.0749, Val Loss: 1.0566
batch size: (895, 895)
✅ Epoch 3: New best model saved with val_loss = 1.0331
Epoch 3, accuracy: 0.4093
batch size: (913, 913)
✅ Epoch 4: New best model saved with val_loss = 1.0145
Epoch 4, accuracy: 0.4141
Epoch 4, Train Loss: 1.0440, Val Loss: 1.0145
batch size: (897, 897)
✅ Epoch 5: New best model saved with val_loss = 0.9944
Epoch 5, accuracy: 0.4233
batch size: (904, 904)
✅ Epoch 6: New best model saved with val_loss = 0.9746
Epoch 6, accuracy: 0.4342
Epoch 6, Train Loss: 1.0083, Val Loss: 0.9746
batch size: (907, 907)
✅ Epoch 7: New best model saved with val_loss = 0.9538
Epoch 7, accuracy: 0.4504
batch size: (895, 895)
✅ Epoch 8: New best model saved with val_loss = 0.9374
Epoch 8, accuracy: 0.4771
Epoch 8, Train Loss: 0.9796, Val Loss: 0.9374
batch size: (895, 895)
✅ Epoch 9: New best model saved with val_loss = 0.9208
Epoch 9, accuracy: 0.4880
batch size: (889, 889)
✅ Epoch 10: New best model saved with val_loss = 0.9077
Epoch 10, accuracy: 0.4995
Epoch 10, Train Loss: 0.9118, Val Loss: 0.9077
batch size: (904, 904)
✅ Epoch 11: New best model saved with val_loss = 0.8948
Epoch 11, accuracy: 0.5170
batch size: (923, 923)
✅ Epoch 12: New best model saved with val_loss = 0.8832
Epoch 12, accuracy: 0.5192
Epoch 12, Train Loss: 0.9000, Val Loss: 0.8832
batch size: (896, 896)
✅ Epoch 13: New best model saved with val_loss = 0.8751
Epoch 13, accuracy: 0.5334
batch size: (911, 911)
✅ Epoch 14: New best model saved with val_loss = 0.8681
Epoch 14, accuracy: 0.5390
Epoch 14, Train Loss: 0.8898, Val Loss: 0.8681
batch size: (878, 878)
✅ Epoch 15: New best model saved with val_loss = 0.8591
Epoch 15, accuracy: 0.5477
batch size: (903, 903)
✅ Epoch 16: New best model saved with val_loss = 0.8500
Epoch 16, accuracy: 0.5540
Epoch 16, Train Loss: 0.8834, Val Loss: 0.8500
batch size: (904, 904)
✅ Epoch 17: New best model saved with val_loss = 0.8440
Epoch 17, accuracy: 0.5522
batch size: (904, 904)
✅ Epoch 18: New best model saved with val_loss = 0.8364
Epoch 18, accuracy: 0.5418
Epoch 18, Train Loss: 0.8065, Val Loss: 0.8364
batch size: (879, 879)
✅ Epoch 19: New best model saved with val_loss = 0.8343
Epoch 19, accuracy: 0.5329
batch size: (900, 900)
✅ Epoch 20: New best model saved with val_loss = 0.8315
Epoch 20, accuracy: 0.5376
Epoch 20, Train Loss: 0.7953, Val Loss: 0.8315
batch size: (926, 926)
✅ Epoch 21: New best model saved with val_loss = 0.8245
Epoch 21, accuracy: 0.5456
batch size: (889, 889)
✅ Epoch 22: New best model saved with val_loss = 0.8200
Epoch 22, accuracy: 0.5750
Epoch 22, Train Loss: 0.7581, Val Loss: 0.8200
batch size: (898, 898)
✅ Epoch 23: New best model saved with val_loss = 0.8153
Epoch 23, accuracy: 0.6062
batch size: (908, 908)
Epoch 24, accuracy: 0.6353
Epoch 24, Train Loss: 0.7580, Val Loss: 0.8164
batch size: (874, 874)
✅ Epoch 25: New best model saved with val_loss = 0.8110
Epoch 25, accuracy: 0.6412
batch size: (908, 908)
Epoch 26, accuracy: 0.6347
Epoch 26, Train Loss: 0.7868, Val Loss: 0.8126
batch size: (901, 901)
✅ Epoch 27: New best model saved with val_loss = 0.8095
Epoch 27, accuracy: 0.6258
batch size: (880, 880)
✅ Epoch 28: New best model saved with val_loss = 0.8064
Epoch 28, accuracy: 0.6249
Epoch 28, Train Loss: 0.7532, Val Loss: 0.8064
batch size: (908, 908)
Epoch 29, accuracy: 0.6363
batch size: (922, 922)
✅ Epoch 30: New best model saved with val_loss = 0.7955
Epoch 30, accuracy: 0.6479
Epoch 30, Train Loss: 0.7477, Val Loss: 0.7955
batch size: (904, 904)
Epoch 31, accuracy: 0.6563
batch size: (887, 887)
✅ Epoch 32: New best model saved with val_loss = 0.7915
Epoch 32, accuracy: 0.6580
Epoch 32, Train Loss: 0.6990, Val Loss: 0.7915
batch size: (887, 887)
✅ Epoch 33: New best model saved with val_loss = 0.7911
Epoch 33, accuracy: 0.6645
batch size: (892, 892)
✅ Epoch 34: New best model saved with val_loss = 0.7910
Epoch 34, accuracy: 0.6621
Epoch 34, Train Loss: 0.6884, Val Loss: 0.7910
batch size: (906, 906)
✅ Epoch 35: New best model saved with val_loss = 0.7875
Epoch 35, accuracy: 0.6513
batch size: (931, 931)
Epoch 36, accuracy: 0.6466
Epoch 36, Train Loss: 0.6768, Val Loss: 0.7908
batch size: (892, 892)
Epoch 37, accuracy: 0.6408
batch size: (901, 901)
Epoch 38, accuracy: 0.6537
Epoch 38, Train Loss: 0.6786, Val Loss: 0.7977
batch size: (900, 900)
✅ Epoch 39: New best model saved with val_loss = 0.7859
Epoch 39, accuracy: 0.6644
batch size: (902, 902)
✅ Epoch 40: New best model saved with val_loss = 0.7683
Epoch 40, accuracy: 0.6789
Epoch 40, Train Loss: 0.6433, Val Loss: 0.7683
batch size: (912, 912)
✅ Epoch 41: New best model saved with val_loss = 0.7630
Epoch 41, accuracy: 0.6827
batch size: (895, 895)
✅ Epoch 42: New best model saved with val_loss = 0.7564
Epoch 42, accuracy: 0.6817
Epoch 42, Train Loss: 0.6804, Val Loss: 0.7564
batch size: (914, 914)
Epoch 43, accuracy: 0.6728
batch size: (903, 903)
Epoch 44, accuracy: 0.6551
Epoch 44, Train Loss: 0.6149, Val Loss: 0.7751
batch size: (896, 896)
Epoch 45, accuracy: 0.6531
batch size: (890, 890)
Epoch 46, accuracy: 0.6697
Epoch 46, Train Loss: 0.6179, Val Loss: 0.7589
batch size: (888, 888)
✅ Epoch 47: New best model saved with val_loss = 0.7481
Epoch 47, accuracy: 0.6860
batch size: (899, 899)
✅ Epoch 48: New best model saved with val_loss = 0.7385
Epoch 48, accuracy: 0.6969
Epoch 48, Train Loss: 0.6568, Val Loss: 0.7385
batch size: (897, 897)
✅ Epoch 49: New best model saved with val_loss = 0.7372
Epoch 49, accuracy: 0.6988
batch size: (876, 876)
✅ Epoch 50: New best model saved with val_loss = 0.7358
Epoch 50, accuracy: 0.6995
Epoch 50, Train Loss: 0.5620, Val Loss: 0.7358
batch size: (882, 882)
✅ Epoch 51: New best model saved with val_loss = 0.7336
Epoch 51, accuracy: 0.6918
batch size: (902, 902)
Epoch 52, accuracy: 0.6673
Epoch 52, Train Loss: 0.5667, Val Loss: 0.7557
batch size: (895, 895)
Epoch 53, accuracy: 0.6612
batch size: (910, 910)
Epoch 54, accuracy: 0.6713
Epoch 54, Train Loss: 0.5907, Val Loss: 0.7449
batch size: (895, 895)
✅ Epoch 55: New best model saved with val_loss = 0.7248
Epoch 55, accuracy: 0.6968
batch size: (904, 904)
✅ Epoch 56: New best model saved with val_loss = 0.7090
Epoch 56, accuracy: 0.7030
Epoch 56, Train Loss: 0.5198, Val Loss: 0.7090
batch size: (899, 899)
✅ Epoch 57: New best model saved with val_loss = 0.7030
Epoch 57, accuracy: 0.7012
batch size: (921, 921)
Epoch 58, accuracy: 0.6961
Epoch 58, Train Loss: 0.6138, Val Loss: 0.7142
batch size: (924, 924)
Epoch 59, accuracy: 0.6854
batch size: (889, 889)
Epoch 60, accuracy: 0.6743
Epoch 60, Train Loss: 0.5700, Val Loss: 0.7372
batch size: (891, 891)
Epoch 61, accuracy: 0.6796
batch size: (895, 895)
Epoch 62, accuracy: 0.6959
Epoch 62, Train Loss: 0.5382, Val Loss: 0.7072
batch size: (899, 899)
Epoch 63, accuracy: 0.6972
batch size: (884, 884)
✅ Epoch 64: New best model saved with val_loss = 0.6888
Epoch 64, accuracy: 0.6965
Epoch 64, Train Loss: 0.5609, Val Loss: 0.6888
batch size: (906, 906)
Epoch 65, accuracy: 0.6975
batch size: (909, 909)
Epoch 66, accuracy: 0.6989
Epoch 66, Train Loss: 0.4679, Val Loss: 0.6919
batch size: (897, 897)
Epoch 67, accuracy: 0.6968
batch size: (907, 907)
Epoch 68, accuracy: 0.7002
Epoch 68, Train Loss: 0.4628, Val Loss: 0.6893
batch size: (898, 898)
Epoch 69, accuracy: 0.6979
batch size: (890, 890)
Epoch 70, accuracy: 0.7007
Epoch 70, Train Loss: 0.5362, Val Loss: 0.6984
batch size: (873, 873)
Epoch 71, accuracy: 0.6974
batch size: (873, 873)
Epoch 72, accuracy: 0.7004
Epoch 72, Train Loss: 0.5045, Val Loss: 0.7010
batch size: (883, 883)
Epoch 73, accuracy: 0.6982
batch size: (892, 892)
Epoch 74, accuracy: 0.6998
Epoch 74, Train Loss: 0.5204, Val Loss: 0.6997
batch size: (902, 902)
Epoch 75, accuracy: 0.7004
batch size: (891, 891)
Epoch 76, accuracy: 0.7005
Epoch 76, Train Loss: 0.5255, Val Loss: 0.6918
batch size: (922, 922)
Epoch 77, accuracy: 0.7008
batch size: (894, 894)
Epoch 78, accuracy: 0.7004
Epoch 78, Train Loss: 0.4875, Val Loss: 0.6897
batch size: (905, 905)
Epoch 79, accuracy: 0.6995
batch size: (905, 905)
Epoch 80, accuracy: 0.6986
Epoch 80, Train Loss: 0.5183, Val Loss: 0.6924
batch size: (895, 895)
Epoch 81, accuracy: 0.6972
batch size: (907, 907)
Epoch 82, accuracy: 0.7005
Epoch 82, Train Loss: 0.4874, Val Loss: 0.7004
batch size: (911, 911)
✅ Epoch 83: New best model saved with val_loss = 0.6867
Epoch 83, accuracy: 0.7007
batch size: (896, 896)
Epoch 84, accuracy: 0.7028
Epoch 84, Train Loss: 0.4554, Val Loss: 0.7024
batch size: (872, 872)
Epoch 85, accuracy: 0.7007
batch size: (888, 888)
Epoch 86, accuracy: 0.7007
Epoch 86, Train Loss: 0.5520, Val Loss: 0.6917
batch size: (914, 914)
Epoch 87, accuracy: 0.7007
batch size: (894, 894)
Epoch 88, accuracy: 0.7015
Epoch 88, Train Loss: 0.4722, Val Loss: 0.7114
batch size: (908, 908)
Epoch 89, accuracy: 0.6992
batch size: (910, 910)
Epoch 90, accuracy: 0.6985
Epoch 90, Train Loss: 0.4506, Val Loss: 0.6987
batch size: (876, 876)
Epoch 91, accuracy: 0.7004
batch size: (896, 896)
Epoch 92, accuracy: 0.6972
Epoch 92, Train Loss: 0.5805, Val Loss: 0.7024
batch size: (899, 899)
Epoch 93, accuracy: 0.7008
batch size: (908, 908)
Epoch 94, accuracy: 0.6982
Epoch 94, Train Loss: 0.5367, Val Loss: 0.7066
batch size: (905, 905)
Epoch 95, accuracy: 0.7019
batch size: (908, 908)
Epoch 96, accuracy: 0.6985
Epoch 96, Train Loss: 0.5989, Val Loss: 0.6939
batch size: (916, 916)
Epoch 97, accuracy: 0.7008
batch size: (900, 900)
Epoch 98, accuracy: 0.7018
Epoch 98, Train Loss: 0.5396, Val Loss: 0.7027
batch size: (899, 899)
Epoch 99, accuracy: 0.7006
batch size: (906, 906)
Epoch 100, accuracy: 0.7013
Epoch 100, Train Loss: 0.5517, Val Loss: 0.6961
batch size: (902, 902)
Epoch 101, accuracy: 0.6993
batch size: (927, 927)
Epoch 102, accuracy: 0.6992
Epoch 102, Train Loss: 0.5486, Val Loss: 0.6973
batch size: (907, 907)
Epoch 103, accuracy: 0.6995
batch size: (894, 894)
Epoch 104, accuracy: 0.7001
Epoch 104, Train Loss: 0.5405, Val Loss: 0.7015
batch size: (912, 912)
Epoch 105, accuracy: 0.7004
batch size: (902, 902)
Epoch 106, accuracy: 0.7014
Epoch 106, Train Loss: 0.4941, Val Loss: 0.6930
batch size: (904, 904)
Epoch 107, accuracy: 0.6986
batch size: (921, 921)
Epoch 108, accuracy: 0.6992
Epoch 108, Train Loss: 0.4913, Val Loss: 0.6963
batch size: (905, 905)
Epoch 109, accuracy: 0.6971
batch size: (913, 913)
Epoch 110, accuracy: 0.7011
Epoch 110, Train Loss: 0.5489, Val Loss: 0.7097
batch size: (910, 910)
✅ Epoch 111: New best model saved with val_loss = 0.6854
Epoch 111, accuracy: 0.7001
batch size: (896, 896)
Epoch 112, accuracy: 0.6992
Epoch 112, Train Loss: 0.5152, Val Loss: 0.6965
batch size: (904, 904)
Epoch 113, accuracy: 0.6981
batch size: (902, 902)
Epoch 114, accuracy: 0.6997
Epoch 114, Train Loss: 0.5604, Val Loss: 0.7032
batch size: (891, 891)
Epoch 115, accuracy: 0.7001
batch size: (879, 879)
Epoch 116, accuracy: 0.6993
Epoch 116, Train Loss: 0.6007, Val Loss: 0.7102
batch size: (906, 906)
Epoch 117, accuracy: 0.7004
batch size: (889, 889)
Epoch 118, accuracy: 0.6996
Epoch 118, Train Loss: 0.4751, Val Loss: 0.6952
batch size: (886, 886)
Epoch 119, accuracy: 0.7014
batch size: (904, 904)
Epoch 120, accuracy: 0.6998
Epoch 120, Train Loss: 0.6028, Val Loss: 0.6879
batch size: (915, 915)
Epoch 121, accuracy: 0.7002
batch size: (901, 901)
Epoch 122, accuracy: 0.7004
Epoch 122, Train Loss: 0.4734, Val Loss: 0.6863
batch size: (897, 897)
Epoch 123, accuracy: 0.7002
batch size: (907, 907)
Epoch 124, accuracy: 0.7002
Epoch 124, Train Loss: 0.5204, Val Loss: 0.7031
batch size: (880, 880)
Epoch 125, accuracy: 0.7016
batch size: (899, 899)
Epoch 126, accuracy: 0.6997
Epoch 126, Train Loss: 0.5215, Val Loss: 0.6949
batch size: (903, 903)
Epoch 127, accuracy: 0.6986
batch size: (903, 903)
Epoch 128, accuracy: 0.6988
Epoch 128, Train Loss: 0.5084, Val Loss: 0.6913
batch size: (911, 911)
Epoch 129, accuracy: 0.6999
batch size: (885, 885)
Epoch 130, accuracy: 0.7008
Epoch 130, Train Loss: 0.4851, Val Loss: 0.6955
batch size: (909, 909)
Epoch 131, accuracy: 0.7010
batch size: (906, 906)
Epoch 132, accuracy: 0.6987
Epoch 132, Train Loss: 0.5189, Val Loss: 0.6953
batch size: (889, 889)
Epoch 133, accuracy: 0.7001
batch size: (925, 925)
Epoch 134, accuracy: 0.7003
Epoch 134, Train Loss: 0.5451, Val Loss: 0.6888
batch size: (919, 919)
Epoch 135, accuracy: 0.6980
batch size: (889, 889)
Epoch 136, accuracy: 0.7005
Epoch 136, Train Loss: 0.5012, Val Loss: 0.6967
batch size: (905, 905)
Epoch 137, accuracy: 0.6987
batch size: (900, 900)
Epoch 138, accuracy: 0.7013
Epoch 138, Train Loss: 0.5835, Val Loss: 0.6900
batch size: (901, 901)
Epoch 139, accuracy: 0.6980
batch size: (907, 907)
Epoch 140, accuracy: 0.7011
Epoch 140, Train Loss: 0.4986, Val Loss: 0.7014
batch size: (898, 898)
Epoch 141, accuracy: 0.6991
batch size: (904, 904)
Epoch 142, accuracy: 0.6999
Epoch 142, Train Loss: 0.4700, Val Loss: 0.6892
batch size: (900, 900)
Epoch 143, accuracy: 0.6991
batch size: (894, 894)
Epoch 144, accuracy: 0.7009
Epoch 144, Train Loss: 0.5290, Val Loss: 0.6976
batch size: (902, 902)
Epoch 145, accuracy: 0.6998
batch size: (882, 882)
Epoch 146, accuracy: 0.6995
Epoch 146, Train Loss: 0.4682, Val Loss: 0.6944
batch size: (897, 897)
Epoch 147, accuracy: 0.6984
batch size: (894, 894)
Epoch 148, accuracy: 0.6986
Epoch 148, Train Loss: 0.4568, Val Loss: 0.6965
batch size: (905, 905)
Epoch 149, accuracy: 0.7018
batch size: (886, 886)
Epoch 150, accuracy: 0.7015
Epoch 150, Train Loss: 0.5877, Val Loss: 0.6923
batch size: /root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
(905, 905)
Epoch 151, accuracy: 0.7004
batch size: (902, 902)
Epoch 152, accuracy: 0.7024
Epoch 152, Train Loss: 0.4829, Val Loss: 0.6955
batch size: (905, 905)
Epoch 153, accuracy: 0.6990
batch size: (906, 906)
Epoch 154, accuracy: 0.7006
Epoch 154, Train Loss: 0.4651, Val Loss: 0.6947
batch size: (899, 899)
Epoch 155, accuracy: 0.7014
batch size: (909, 909)
Epoch 156, accuracy: 0.6992
Epoch 156, Train Loss: 0.5593, Val Loss: 0.6921
batch size: (906, 906)
Epoch 157, accuracy: 0.7003
batch size: (908, 908)
Epoch 158, accuracy: 0.6998
Epoch 158, Train Loss: 0.4992, Val Loss: 0.7041
batch size: (897, 897)
Epoch 159, accuracy: 0.6991
batch size: (906, 906)
Epoch 160, accuracy: 0.7016
Epoch 160, Train Loss: 0.4368, Val Loss: 0.6973
batch size: (889, 889)
Epoch 161, accuracy: 0.7011
batch size: (903, 903)
Epoch 162, accuracy: 0.7010
Epoch 162, Train Loss: 0.4660, Val Loss: 0.6900
batch size: (882, 882)
Epoch 163, accuracy: 0.6987
batch size: (914, 914)
Epoch 164, accuracy: 0.7008
Epoch 164, Train Loss: 0.5224, Val Loss: 0.6973
batch size: (893, 893)
Epoch 165, accuracy: 0.7000
batch size: (905, 905)
Epoch 166, accuracy: 0.7002
Epoch 166, Train Loss: 0.4946, Val Loss: 0.6998
batch size: (912, 912)
Epoch 167, accuracy: 0.6982
batch size: (896, 896)
Epoch 168, accuracy: 0.6996
Epoch 168, Train Loss: 0.4824, Val Loss: 0.7123
batch size: (884, 884)
Epoch 169, accuracy: 0.6993
batch size: (885, 885)
Epoch 170, accuracy: 0.6984
Epoch 170, Train Loss: 0.4693, Val Loss: 0.7032
batch size: (912, 912)
Epoch 171, accuracy: 0.6991
batch size: (900, 900)
Epoch 172, accuracy: 0.7019
Epoch 172, Train Loss: 0.5323, Val Loss: 0.7015
batch size: (910, 910)
Epoch 173, accuracy: 0.6989
batch size: (897, 897)
Epoch 174, accuracy: 0.6992
Epoch 174, Train Loss: 0.5137, Val Loss: 0.6952
batch size: (916, 916)
Epoch 175, accuracy: 0.7007
batch size: (887, 887)
Epoch 176, accuracy: 0.7013
Epoch 176, Train Loss: 0.5213, Val Loss: 0.6930
batch size: (903, 903)
Epoch 177, accuracy: 0.7005
batch size: (882, 882)
Epoch 178, accuracy: 0.7003
Epoch 178, Train Loss: 0.5214, Val Loss: 0.6936
batch size: (899, 899)
Epoch 179, accuracy: 0.6994
batch size: (913, 913)
Epoch 180, accuracy: 0.6986
Epoch 180, Train Loss: 0.5352, Val Loss: 0.7047
batch size: (885, 885)
Epoch 181, accuracy: 0.7006
batch size: (898, 898)
Epoch 182, accuracy: 0.6996
Epoch 182, Train Loss: 0.5396, Val Loss: 0.6956
batch size: (895, 895)
Epoch 183, accuracy: 0.6980
batch size: (896, 896)
Epoch 184, accuracy: 0.6983
Epoch 184, Train Loss: 0.5055, Val Loss: 0.6964
batch size: (895, 895)
Epoch 185, accuracy: 0.7038
batch size: (908, 908)
Epoch 186, accuracy: 0.6975
Epoch 186, Train Loss: 0.5160, Val Loss: 0.6962
batch size: (917, 917)
Epoch 187, accuracy: 0.7002
batch size: (906, 906)
Epoch 188, accuracy: 0.6985
Epoch 188, Train Loss: 0.5014, Val Loss: 0.6958
batch size: (910, 910)
Epoch 189, accuracy: 0.7001
batch size: (903, 903)
Epoch 190, accuracy: 0.6989
Epoch 190, Train Loss: 0.4962, Val Loss: 0.6978
batch size: (898, 898)
Epoch 191, accuracy: 0.6989
batch size: (904, 904)
Epoch 192, accuracy: 0.6999
Epoch 192, Train Loss: 0.5098, Val Loss: 0.7006
batch size: (895, 895)
Epoch 193, accuracy: 0.7020
batch size: (912, 912)
Epoch 194, accuracy: 0.7008
Epoch 194, Train Loss: 0.5056, Val Loss: 0.6933
batch size: (904, 904)
Epoch 195, accuracy: 0.6988
batch size: (914, 914)
Epoch 196, accuracy: 0.6984
Epoch 196, Train Loss: 0.5045, Val Loss: 0.6873
batch size: (915, 915)
Epoch 197, accuracy: 0.6992
batch size: (894, 894)
Epoch 198, accuracy: 0.7001
Epoch 198, Train Loss: 0.5414, Val Loss: 0.6977
batch size: (900, 900)
Epoch 199, accuracy: 0.6994
Loaded best model with val_loss = 0.6853950023651123
test :accuracy 0.7004, f1_macro: 0.6784, f1_micro: 0.7004, auc: 0.8439
Training resGCN with 8 layers...
可训练参数: 1850386_resGCN
不可训练参数: 0
batch size: (902, 902)
✅ Epoch 0: New best model saved with val_loss = 1.0994
Epoch 0, accuracy: 0.1674
Epoch 0, Train Loss: 1.1099, Val Loss: 1.0994
batch size: (912, 912)
Epoch 1, accuracy: 0.4023
batch size: (906, 906)
Epoch 2, accuracy: 0.4049
Epoch 2, Train Loss: 1.0986, Val Loss: 1.1004
batch size: (895, 895)
Epoch 3, accuracy: 0.4076
batch size: (900, 900)
Epoch 4, accuracy: 0.4043
Epoch 4, Train Loss: 1.0987, Val Loss: 1.1008
batch size: (899, 899)
Epoch 5, accuracy: 0.4043
batch size: (909, 909)
Epoch 6, accuracy: 0.4002
Epoch 6, Train Loss: 1.0987, Val Loss: 1.1010
batch size: (893, 893)
Epoch 7, accuracy: 0.4050
batch size: (887, 887)
Epoch 8, accuracy: 0.4037
Epoch 8, Train Loss: 1.0987, Val Loss: 1.1010
batch size: (918, 918)
Epoch 9, accuracy: 0.4039
batch size: (894, 894)
Epoch 10, accuracy: 0.4024
Epoch 10, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (894, 894)
Epoch 11, accuracy: 0.4014
batch size: (891, 891)
Epoch 12, accuracy: 0.4060
Epoch 12, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (897, 897)
Epoch 13, accuracy: 0.4068
batch size: (906, 906)
Epoch 14, accuracy: 0.4029
Epoch 14, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (895, 895)
Epoch 15, accuracy: 0.4058
batch size: (905, 905)
Epoch 16, accuracy: 0.4052
Epoch 16, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (877, 877)
Epoch 17, accuracy: 0.4012
batch size: (920, 920)
Epoch 18, accuracy: 0.4010
Epoch 18, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (902, 902)
Epoch 19, accuracy: 0.4064
batch size: (899, 899)
Epoch 20, accuracy: 0.4022
Epoch 20, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (898, 898)
Epoch 21, accuracy: 0.4010
batch size: (918, 918)
Epoch 22, accuracy: 0.3981
Epoch 22, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (916, 916)
Epoch 23, accuracy: 0.4003
batch size: (892, 892)
Epoch 24, accuracy: 0.4008
Epoch 24, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (900, 900)
Epoch 25, accuracy: 0.4034
batch size: (889, 889)
Epoch 26, accuracy: 0.4044
Epoch 26, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (915, 915)
Epoch 27, accuracy: 0.4010
batch size: (906, 906)
Epoch 28, accuracy: 0.4016
Epoch 28, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (909, 909)
Epoch 29, accuracy: 0.3985
batch size: (911, 911)
Epoch 30, accuracy: 0.4001
Epoch 30, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (917, 917)
Epoch 31, accuracy: 0.4061
batch size: (880, 880)
Epoch 32, accuracy: 0.4040
Epoch 32, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (895, 895)
Epoch 33, accuracy: 0.4047
batch size: (904, 904)
Epoch 34, accuracy: 0.3993
Epoch 34, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (910, 910)
Epoch 35, accuracy: 0.4063
batch size: (896, 896)
Epoch 36, accuracy: 0.4042
Epoch 36, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (894, 894)
Epoch 37, accuracy: 0.4006
batch size: (896, 896)
Epoch 38, accuracy: 0.4032
Epoch 38, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (915, 915)
Epoch 39, accuracy: 0.4011
batch size: (901, 901)
Epoch 40, accuracy: 0.3968
Epoch 40, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (889, 889)
Epoch 41, accuracy: 0.4013
batch size: (891, 891)
Epoch 42, accuracy: 0.4022
Epoch 42, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (906, 906)
Epoch 43, accuracy: 0.4004
batch size: (905, 905)
Epoch 44, accuracy: 0.4007
Epoch 44, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (899, 899)
Epoch 45, accuracy: 0.3973
batch size: (885, 885)
Epoch 46, accuracy: 0.4053
Epoch 46, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (915, 915)
Epoch 47, accuracy: 0.4026
batch size: (900, 900)
Epoch 48, accuracy: 0.4042
Epoch 48, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (898, 898)
Epoch 49, accuracy: 0.3967
batch size: (894, 894)
Epoch 50, accuracy: 0.4060
Epoch 50, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (894, 894)
Epoch 51, accuracy: 0.4063
batch size: (902, 902)
Epoch 52, accuracy: 0.4065
Epoch 52, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (893, 893)
Epoch 53, accuracy: 0.4000
batch size: (903, 903)
Epoch 54, accuracy: 0.4006
Epoch 54, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (911, 911)
Epoch 55, accuracy: 0.3981
batch size: (918, 918)
Epoch 56, accuracy: 0.4033
Epoch 56, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (902, 902)
Epoch 57, accuracy: 0.4007
batch size: (927, 927)
Epoch 58, accuracy: 0.4072
Epoch 58, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (881, 881)
Epoch 59, accuracy: 0.4018
batch size: (896, 896)
Epoch 60, accuracy: 0.4067
Epoch 60, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (900, 900)
Epoch 61, accuracy: 0.4033
batch size: (904, 904)
Epoch 62, accuracy: 0.4009
Epoch 62, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (888, 888)
Epoch 63, accuracy: 0.4004
batch size: (889, 889)
Epoch 64, accuracy: 0.4003
Epoch 64, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (874, 874)
Epoch 65, accuracy: 0.4013
batch size: (879, 879)
Epoch 66, accuracy: 0.4023
Epoch 66, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (902, 902)
Epoch 67, accuracy: 0.4044
batch size: (894, 894)
Epoch 68, accuracy: 0.4018
Epoch 68, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (911, 911)
Epoch 69, accuracy: 0.4041
batch size: (904, 904)
Epoch 70, accuracy: 0.3991
Epoch 70, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (876, 876)
Epoch 71, accuracy: 0.4039
batch size: (897, 897)
Epoch 72, accuracy: 0.4063
Epoch 72, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (914, 914)
Epoch 73, accuracy: 0.4006
batch size: (898, 898)
Epoch 74, accuracy: 0.4025
Epoch 74, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (901, 901)
Epoch 75, accuracy: 0.4014
batch size: (898, 898)
Epoch 76, accuracy: 0.4013
Epoch 76, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (892, 892)
Epoch 77, accuracy: 0.4013
batch size: (907, 907)
Epoch 78, accuracy: 0.4041
Epoch 78, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (915, 915)
Epoch 79, accuracy: 0.4017
batch size: (908, 908)
Epoch 80, accuracy: 0.4038
Epoch 80, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (898, 898)
Epoch 81, accuracy: 0.4007
batch size: (907, 907)
Epoch 82, accuracy: 0.4045
Epoch 82, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (912, 912)
Epoch 83, accuracy: 0.4029
batch size: (923, 923)
Epoch 84, accuracy: 0.4067
Epoch 84, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (900, 900)
Epoch 85, accuracy: 0.3997
batch size: (904, 904)
Epoch 86, accuracy: 0.4072
Epoch 86, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (910, 910)
Epoch 87, accuracy: 0.4030
batch size: (896, 896)
Epoch 88, accuracy: 0.3999
Epoch 88, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (912, 912)
Epoch 89, accuracy: 0.4026
batch size: (882, 882)
Epoch 90, accuracy: 0.4005
Epoch 90, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (918, 918)
Epoch 91, accuracy: 0.4020
batch size: (914, 914)
Epoch 92, accuracy: 0.3995
Epoch 92, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (891, 891)
Epoch 93, accuracy: 0.3991
batch size: (890, 890)
Epoch 94, accuracy: 0.4049
Epoch 94, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (918, 918)
Epoch 95, accuracy: 0.3974
batch size: (908, 908)
Epoch 96, accuracy: 0.4031
Epoch 96, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (904, 904)
Epoch 97, accuracy: 0.4020
batch size: (910, 910)
Epoch 98, accuracy: 0.4033
Epoch 98, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (911, 911)
Epoch 99, accuracy: 0.4034
batch size: (907, 907)
Epoch 100, accuracy: 0.4056
Epoch 100, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (892, 892)
Epoch 101, accuracy: 0.3990
batch size: (903, 903)
Epoch 102, accuracy: 0.4016
Epoch 102, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (899, 899)
Epoch 103, accuracy: 0.3998
batch size: (899, 899)
Epoch 104, accuracy: 0.4011
Epoch 104, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (899, 899)
Epoch 105, accuracy: 0.4016
batch size: (887, 887)
Epoch 106, accuracy: 0.4065
Epoch 106, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (902, 902)
Epoch 107, accuracy: 0.4018
batch size: (909, 909)
Epoch 108, accuracy: 0.4028
Epoch 108, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (917, 917)
Epoch 109, accuracy: 0.4041
batch size: (895, 895)
Epoch 110, accuracy: 0.4023
Epoch 110, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (902, 902)
Epoch 111, accuracy: 0.4046
batch size: (903, 903)
Epoch 112, accuracy: 0.4023
Epoch 112, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (902, 902)
Epoch 113, accuracy: 0.4018
batch size: (900, 900)
Epoch 114, accuracy: 0.3994
Epoch 114, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (901, 901)
Epoch 115, accuracy: 0.4040
batch size: (907, 907)
Epoch 116, accuracy: 0.4043
Epoch 116, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (883, 883)
Epoch 117, accuracy: 0.3970
batch size: (932, 932)
Epoch 118, accuracy: 0.4029
Epoch 118, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (902, 902)
Epoch 119, accuracy: 0.4045
batch size: (898, 898)
Epoch 120, accuracy: 0.3983
Epoch 120, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (916, 916)
Epoch 121, accuracy: 0.4045
batch size: (888, 888)
Epoch 122, accuracy: 0.4005
Epoch 122, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (897, 897)
Epoch 123, accuracy: 0.4006
batch size: (890, 890)
Epoch 124, accuracy: 0.4015
Epoch 124, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (899, 899)
Epoch 125, accuracy: 0.4017
batch size: (897, 897)
Epoch 126, accuracy: 0.4055
Epoch 126, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (899, 899)
Epoch 127, accuracy: 0.3975
batch size: (880, 880)
Epoch 128, accuracy: 0.4024
Epoch 128, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (902, 902)
Epoch 129, accuracy: 0.4008
batch size: (906, 906)
Epoch 130, accuracy: 0.4001
Epoch 130, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (895, 895)
Epoch 131, accuracy: 0.4039
batch size: (911, 911)
Epoch 132, accuracy: 0.4044
Epoch 132, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (897, 897)
Epoch 133, accuracy: 0.4008
batch size: (898, 898)
Epoch 134, accuracy: 0.4025
Epoch 134, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (892, 892)
Epoch 135, accuracy: 0.4034
batch size: (913, 913)
Epoch 136, accuracy: 0.4001
Epoch 136, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (888, 888)
Epoch 137, accuracy: 0.4005
batch size: (886, 886)
Epoch 138, accuracy: 0.4030
Epoch 138, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (908, 908)
Epoch 139, accuracy: 0.4042
batch size: (929, 929)
Epoch 140, accuracy: 0.4024
Epoch 140, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (883, 883)
Epoch 141, accuracy: 0.4041
batch size: (904, 904)
Epoch 142, accuracy: 0.4000
Epoch 142, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (901, 901)
Epoch 143, accuracy: 0.4036
batch size: (898, 898)
Epoch 144, accuracy: 0.4006
Epoch 144, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (902, 902)
Epoch 145, accuracy: 0.4049
batch size: (900, 900)
Epoch 146, accuracy: 0.3977
Epoch 146, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (904, 904)
Epoch 147, accuracy: 0.4034
batch size: (871, 871)
Epoch 148, accuracy: 0.4027
Epoch 148, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (884, 884)
Epoch 149, accuracy: 0.3987
batch size: (892, 892)
Epoch 150, accuracy: 0.4061
Epoch 150, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (885, 885)
Epoch 151, accuracy: 0.4000
batch size: (906, 906)
Epoch 152, accuracy: 0.4055
Epoch 152, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (904, 904)
Epoch 153, accuracy: 0.4035
batch size: (922, 922)
Epoch 154, accuracy: 0.4021
Epoch 154, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (907, 907)
Epoch 155, accuracy: 0.4021
batch size: (900, 900)
Epoch 156, accuracy: 0.4048
Epoch 156, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (910, 910)
Epoch 157, accuracy: 0.4035
batch size: (901, 901)
Epoch 158, accuracy: 0.3938
Epoch 158, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (915, 915)
Epoch 159, accuracy: 0.3996
batch size: (900, 900)
Epoch 160, accuracy: 0.4040
Epoch 160, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (899, 899)
Epoch 161, accuracy: 0.4021
batch size: (910, 910)
Epoch 162, accuracy: 0.4054
Epoch 162, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (903, 903)
Epoch 163, accuracy: 0.4017
batch size: (891, 891)
Epoch 164, accuracy: 0.4049
Epoch 164, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (904, 904)
Epoch 165, accuracy: 0.4025
batch size: (892, 892)
Epoch 166, accuracy: 0.3984
Epoch 166, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (916, 916)
Epoch 167, accuracy: 0.4082
batch size: (877, 877)
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 168, accuracy: 0.4044
Epoch 168, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (895, 895)
Epoch 169, accuracy: 0.4026
batch size: (888, 888)
Epoch 170, accuracy: 0.4017
Epoch 170, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (906, 906)
Epoch 171, accuracy: 0.4032
batch size: (894, 894)
Epoch 172, accuracy: 0.4031
Epoch 172, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (912, 912)
Epoch 173, accuracy: 0.4014
batch size: (902, 902)
Epoch 174, accuracy: 0.4065
Epoch 174, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (908, 908)
Epoch 175, accuracy: 0.4059
batch size: (897, 897)
Epoch 176, accuracy: 0.4024
Epoch 176, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (887, 887)
Epoch 177, accuracy: 0.4031
batch size: (907, 907)
Epoch 178, accuracy: 0.4015
Epoch 178, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (887, 887)
Epoch 179, accuracy: 0.3991
batch size: (908, 908)
Epoch 180, accuracy: 0.4013
Epoch 180, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (897, 897)
Epoch 181, accuracy: 0.3988
batch size: (893, 893)
Epoch 182, accuracy: 0.4069
Epoch 182, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (903, 903)
Epoch 183, accuracy: 0.4005
batch size: (907, 907)
Epoch 184, accuracy: 0.4027
Epoch 184, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (908, 908)
Epoch 185, accuracy: 0.4015
batch size: (907, 907)
Epoch 186, accuracy: 0.4018
Epoch 186, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (882, 882)
Epoch 187, accuracy: 0.4032
batch size: (907, 907)
Epoch 188, accuracy: 0.4018
Epoch 188, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (910, 910)
Epoch 189, accuracy: 0.4027
batch size: (891, 891)
Epoch 190, accuracy: 0.4024
Epoch 190, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (899, 899)
Epoch 191, accuracy: 0.4017
batch size: (885, 885)
Epoch 192, accuracy: 0.4012
Epoch 192, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (903, 903)
Epoch 193, accuracy: 0.3978
batch size: (895, 895)
Epoch 194, accuracy: 0.4026
Epoch 194, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (899, 899)
Epoch 195, accuracy: 0.3981
batch size: (893, 893)
Epoch 196, accuracy: 0.4038
Epoch 196, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (903, 903)
Epoch 197, accuracy: 0.4040
batch size: (910, 910)
Epoch 198, accuracy: 0.4034
Epoch 198, Train Loss: 1.0987, Val Loss: 1.1009
batch size: (918, 918)
Epoch 199, accuracy: 0.4004
Loaded best model with val_loss = 1.0994499921798706
test :accuracy 0.1665, f1_macro: 0.0952, f1_micro: 0.1665, auc: 0.5000
Training resGCN with 32 layers...
可训练参数: 573202_resGCN
不可训练参数: 0
batch size: (873, 873)
✅ Epoch 0: New best model saved with val_loss = 1.1000
Epoch 0, accuracy: 0.2234
Epoch 0, Train Loss: 185766.4844, Val Loss: 1.1000
batch size: (897, 897)
Epoch 1, accuracy: 0.1664
batch size: (916, 916)
Epoch 2, accuracy: 0.1683
Epoch 2, Train Loss: 155.4529, Val Loss: 1.1012
batch size: (919, 919)
Epoch 3, accuracy: 0.1646
batch size: (893, 893)
Epoch 4, accuracy: 0.1673
Epoch 4, Train Loss: 1.0987, Val Loss: 1.1023
batch size: (882, 882)
Epoch 5, accuracy: 0.1686
batch size: (900, 900)
Epoch 6, accuracy: 0.1663
Epoch 6, Train Loss: 1.0987, Val Loss: 1.1032
batch size: (880, 880)
Epoch 7, accuracy: 0.1659
batch size: (879, 879)
Epoch 8, accuracy: 0.1680
Epoch 8, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (910, 910)
Epoch 9, accuracy: 0.1661
batch size: (891, 891)
Epoch 10, accuracy: 0.1671
Epoch 10, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (907, 907)
Epoch 11, accuracy: 0.1676
batch size: (901, 901)
Epoch 12, accuracy: 0.1671
Epoch 12, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (891, 891)
Epoch 13, accuracy: 0.1685
batch size: (916, 916)
Epoch 14, accuracy: 0.1696
Epoch 14, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (895, 895)
Epoch 15, accuracy: 0.1673
batch size: (901, 901)
Epoch 16, accuracy: 0.1667
Epoch 16, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (916, 916)
Epoch 17, accuracy: 0.1690
batch size: (883, 883)
Epoch 18, accuracy: 0.1674
Epoch 18, Train Loss: 1.0987, Val Loss: 1.1034
batch size: (899, 899)
Epoch 19, accuracy: 0.1682
batch size: (879, 879)
Epoch 20, accuracy: 0.1674
Epoch 20, Train Loss: 1.0987, Val Loss: 1.1034
batch size: (881, 881)
Epoch 21, accuracy: 0.1671
batch size: (908, 908)
Epoch 22, accuracy: 0.1681
Epoch 22, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (892, 892)
Epoch 23, accuracy: 0.1671
batch size: (907, 907)
Epoch 24, accuracy: 0.1664
Epoch 24, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (913, 913)
Epoch 25, accuracy: 0.1670
batch size: (884, 884)
Epoch 26, accuracy: 0.1626
Epoch 26, Train Loss: 1.0987, Val Loss: 1.1034
batch size: (916, 916)
Epoch 27, accuracy: 0.1674
batch size: (895, 895)
Epoch 28, accuracy: 0.1659
Epoch 28, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (902, 902)
Epoch 29, accuracy: 0.1663
batch size: (900, 900)
Epoch 30, accuracy: 0.1667
Epoch 30, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (905, 905)
Epoch 31, accuracy: 0.1701
batch size: (903, 903)
Epoch 32, accuracy: 0.1650
Epoch 32, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (885, 885)
Epoch 33, accuracy: 0.1684
batch size: (866, 866)
Epoch 34, accuracy: 0.1692
Epoch 34, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (906, 906)
Epoch 35, accuracy: 0.1669
batch size: (877, 877)
Epoch 36, accuracy: 0.1690
Epoch 36, Train Loss: 1.0987, Val Loss: 1.1034
batch size: (900, 900)
Epoch 37, accuracy: 0.1682
batch size: (890, 890)
Epoch 38, accuracy: 0.1685
Epoch 38, Train Loss: 1.0987, Val Loss: 1.1034
batch size: (904, 904)
Epoch 39, accuracy: 0.1682
batch size: (898, 898)
Epoch 40, accuracy: 0.1669
Epoch 40, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (888, 888)
Epoch 41, accuracy: 0.1660
batch size: (892, 892)
Epoch 42, accuracy: 0.1691
Epoch 42, Train Loss: 1.0987, Val Loss: 1.1034
batch size: (907, 907)
Epoch 43, accuracy: 0.1676
batch size: (884, 884)
Epoch 44, accuracy: 0.1674
Epoch 44, Train Loss: 1.0987, Val Loss: 1.1034
batch size: (903, 903)
Epoch 45, accuracy: 0.1685
batch size: (897, 897)
Epoch 46, accuracy: 0.1696
Epoch 46, Train Loss: 1.0987, Val Loss: 1.1034
batch size: (907, 907)
Epoch 47, accuracy: 0.1699
batch size: (896, 896)
Epoch 48, accuracy: 0.1653
Epoch 48, Train Loss: 1.0987, Val Loss: 1.1034
batch size: (901, 901)
Epoch 49, accuracy: 0.1669
batch size: (886, 886)
Epoch 50, accuracy: 0.1674
Epoch 50, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (902, 902)
Epoch 51, accuracy: 0.1681
batch size: (897, 897)
Epoch 52, accuracy: 0.1681
Epoch 52, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (882, 882)
Epoch 53, accuracy: 0.1675
batch size: (897, 897)
Epoch 54, accuracy: 0.1688
Epoch 54, Train Loss: 1.0987, Val Loss: 1.1034
batch size: (882, 882)
Epoch 55, accuracy: 0.1696
batch size: (900, 900)
Epoch 56, accuracy: 0.1676
Epoch 56, Train Loss: 1.0987, Val Loss: 1.1034
batch size: (907, 907)
Epoch 57, accuracy: 0.1676
batch size: (910, 910)
Epoch 58, accuracy: 0.1663
Epoch 58, Train Loss: 1.0987, Val Loss: 1.1034
batch size: (885, 885)
Epoch 59, accuracy: 0.1647
batch size: (916, 916)
Epoch 60, accuracy: 0.1701
Epoch 60, Train Loss: 1.0987, Val Loss: 1.1034
batch size: (889, 889)
Epoch 61, accuracy: 0.1678
batch size: (919, 919)
Epoch 62, accuracy: 0.1713
Epoch 62, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (880, 880)
Epoch 63, accuracy: 0.1660
batch size: (875, 875)
Epoch 64, accuracy: 0.1644
Epoch 64, Train Loss: 1.0987, Val Loss: 1.1034
batch size: (887, 887)
Epoch 65, accuracy: 0.1660
batch size: (901, 901)
Epoch 66, accuracy: 0.1688
Epoch 66, Train Loss: 1.0987, Val Loss: 1.1034
batch size: (879, 879)
Epoch 67, accuracy: 0.1669
batch size: (900, 900)
Epoch 68, accuracy: 0.1685
Epoch 68, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (904, 904)
Epoch 69, accuracy: 0.1690
batch size: (897, 897)
Epoch 70, accuracy: 0.1697
Epoch 70, Train Loss: 1.0987, Val Loss: 1.1034
batch size: (890, 890)
Epoch 71, accuracy: 0.1695
batch size: (907, 907)
Epoch 72, accuracy: 0.1685
Epoch 72, Train Loss: 1.0987, Val Loss: 1.1034
batch size: (910, 910)
Epoch 73, accuracy: 0.1640
batch size: (894, 894)
Epoch 74, accuracy: 0.1688
Epoch 74, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (909, 909)
Epoch 75, accuracy: 0.1677
batch size: (904, 904)
Epoch 76, accuracy: 0.1675
Epoch 76, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (900, 900)
Epoch 77, accuracy: 0.1677
batch size: (885, 885)
Epoch 78, accuracy: 0.1642
Epoch 78, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (904, 904)
Epoch 79, accuracy: 0.1716
batch size: (908, 908)
Epoch 80, accuracy: 0.1701
Epoch 80, Train Loss: 1.0987, Val Loss: 1.1034
batch size: (913, 913)
Epoch 81, accuracy: 0.1697
batch size: (900, 900)
Epoch 82, accuracy: 0.1661
Epoch 82, Train Loss: 1.0987, Val Loss: 1.1034
batch size: (903, 903)
Epoch 83, accuracy: 0.1695
batch size: (874, 874)
Epoch 84, accuracy: 0.1649
Epoch 84, Train Loss: 1.0987, Val Loss: 1.1034
batch size: (896, 896)
Epoch 85, accuracy: 0.1694
batch size: (912, 912)
Epoch 86, accuracy: 0.1652
Epoch 86, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (892, 892)
Epoch 87, accuracy: 0.1681
batch size: (916, 916)
Epoch 88, accuracy: 0.1685
Epoch 88, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (901, 901)
Epoch 89, accuracy: 0.1674
batch size: (912, 912)
Epoch 90, accuracy: 0.1680
Epoch 90, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (911, 911)
Epoch 91, accuracy: 0.1673
batch size: (909, 909)
Epoch 92, accuracy: 0.1684
Epoch 92, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (893, 893)
Epoch 93, accuracy: 0.1673
batch size: (899, 899)
Epoch 94, accuracy: 0.1673
Epoch 94, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (910, 910)
Epoch 95, accuracy: 0.1688
batch size: (892, 892)
Epoch 96, accuracy: 0.1699
Epoch 96, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (902, 902)
Epoch 97, accuracy: 0.1668
batch size: (893, 893)
Epoch 98, accuracy: 0.1658
Epoch 98, Train Loss: 1.0987, Val Loss: 1.1034
batch size: (897, 897)
Epoch 99, accuracy: 0.1669
batch size: (906, 906)
Epoch 100, accuracy: 0.1654
Epoch 100, Train Loss: 1.0987, Val Loss: 1.1034
batch size: (894, 894)
Epoch 101, accuracy: 0.1663
batch size: (909, 909)
Epoch 102, accuracy: 0.1647
Epoch 102, Train Loss: 1.0987, Val Loss: 1.1034
batch size: (882, 882)
Epoch 103, accuracy: 0.1678
batch size: (915, 915)
Epoch 104, accuracy: 0.1661
Epoch 104, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (914, 914)
Epoch 105, accuracy: 0.1684
batch size: (910, 910)
Epoch 106, accuracy: 0.1667
Epoch 106, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (923, 923)
Epoch 107, accuracy: 0.1656
batch size: (901, 901)
Epoch 108, accuracy: 0.1671
Epoch 108, Train Loss: 1.0987, Val Loss: 1.1034
batch size: (887, 887)
Epoch 109, accuracy: 0.1699
batch size: (891, 891)
Epoch 110, accuracy: 0.1706
Epoch 110, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (908, 908)
Epoch 111, accuracy: 0.1669
batch size: (912, 912)
Epoch 112, accuracy: 0.1687
Epoch 112, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (891, 891)
Epoch 113, accuracy: 0.1643
batch size: (896, 896)
Epoch 114, accuracy: 0.1668
Epoch 114, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (892, 892)
Epoch 115, accuracy: 0.1650
batch size: (888, 888)
Epoch 116, accuracy: 0.1671
Epoch 116, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (886, 886)
Epoch 117, accuracy: 0.1703
batch size: (905, 905)
Epoch 118, accuracy: 0.1662
Epoch 118, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (891, 891)
Epoch 119, accuracy: 0.1691
batch size: (897, 897)
Epoch 120, accuracy: 0.1675
Epoch 120, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (907, 907)
Epoch 121, accuracy: 0.1663
batch size: (895, 895)
Epoch 122, accuracy: 0.1663
Epoch 122, Train Loss: 1.0987, Val Loss: 1.1034
batch size: (893, 893)
Epoch 123, accuracy: 0.1721
batch size: (904, 904)
Epoch 124, accuracy: 0.1670
Epoch 124, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (886, 886)
Epoch 125, accuracy: 0.1677
batch size: (903, 903)
Epoch 126, accuracy: 0.1684
Epoch 126, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (905, 905)
Epoch 127, accuracy: 0.1685
batch size: (894, 894)
Epoch 128, accuracy: 0.1693
Epoch 128, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (902, 902)
Epoch 129, accuracy: 0.1682
batch size: (892, 892)
Epoch 130, accuracy: 0.1662
Epoch 130, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (901, 901)
Epoch 131, accuracy: 0.1693
batch size: (893, 893)
Epoch 132, accuracy: 0.1674
Epoch 132, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (880, 880)
Epoch 133, accuracy: 0.1684
batch size: (903, 903)
Epoch 134, accuracy: 0.1667
Epoch 134, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (890, 890)
Epoch 135, accuracy: 0.1685
batch size: (904, 904)
Epoch 136, accuracy: 0.1698
Epoch 136, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (924, 924)
Epoch 137, accuracy: 0.1648
batch size: (911, 911)
Epoch 138, accuracy: 0.1676
Epoch 138, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (890, 890)
Epoch 139, accuracy: 0.1684
batch size: (896, 896)
Epoch 140, accuracy: 0.1642
Epoch 140, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (888, 888)
Epoch 141, accuracy: 0.1682
batch size: (914, 914)
Epoch 142, accuracy: 0.1659
Epoch 142, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (906, 906)
Epoch 143, accuracy: 0.1683
batch size: (906, 906)
Epoch 144, accuracy: 0.1680
Epoch 144, Train Loss: 1.0987, Val Loss: 1.1034
batch size: (904, 904)
Epoch 145, accuracy: 0.1681
batch size: (905, 905)
Epoch 146, accuracy: 0.1684
Epoch 146, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (902, 902)
Epoch 147, accuracy: 0.1662
batch size: (883, 883)
Epoch 148, accuracy: 0.1685
Epoch 148, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (901, 901)
Epoch 149, accuracy: 0.1671
batch size: (910, 910)
Epoch 150, accuracy: 0.1665
Epoch 150, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (902, 902)
Epoch 151, accuracy: 0.1698
batch size: (909, 909)
Epoch 152, accuracy: 0.1681
Epoch 152, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (919, 919)
Epoch 153, accuracy: 0.1675
batch size: (884, 884)
Epoch 154, accuracy: 0.1644
Epoch 154, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (888, 888)
Epoch 155, accuracy: 0.1685
batch size: (916, 916)
Epoch 156, accuracy: 0.1692
Epoch 156, Train Loss: 1.0987, Val Loss: 1.1034
batch size: (904, 904)
Epoch 157, accuracy: 0.1668
batch size: (900, 900)
Epoch 158, accuracy: 0.1663
Epoch 158, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (895, 895)
Epoch 159, accuracy: 0.1677
batch size: (885, 885)
Epoch 160, accuracy: 0.1643
Epoch 160, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (903, 903)
Epoch 161, accuracy: 0.1667
batch size: (895, 895)
Epoch 162, accuracy: 0.1691
Epoch 162, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (900, 900)
Epoch 163, accuracy: 0.1684
batch size: (885, 885)
Epoch 164, accuracy: 0.1681
Epoch 164, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (893, 893)
Epoch 165, accuracy: 0.1668
batch size: (899, 899)
Epoch 166, accuracy: 0.1657
Epoch 166, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (880, 880)
Epoch 167, accuracy: 0.1666
batch size: (898, 898)
Epoch 168, accuracy: 0.1665
Epoch 168, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (909, 909)
Epoch 169, accuracy: 0.1684
batch size: (907, 907)
Epoch 170, accuracy: 0.1655
Epoch 170, Train Loss: 1.0987, Val Loss: 1.1034
batch size: (903, 903)
Epoch 171, accuracy: 0.1675
batch size: (922, 922)
Epoch 172, accuracy: 0.1670
Epoch 172, Train Loss: 1.0987, Val Loss: 1.1034
batch size: (908, 908)
Epoch 173, accuracy: 0.1684
batch size: (906, 906)
Epoch 174, accuracy: 0.1681
Epoch 174, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (878, 878)
Epoch 175, accuracy: 0.1678
batch size: (902, 902)
Epoch 176, accuracy: 0.1727
Epoch 176, Train Loss: 1.0987, Val Loss: 1.1034
batch size: (892, 892)
Epoch 177, accuracy: 0.1671
batch size: (897, 897)
Epoch 178, accuracy: 0.1657
Epoch 178, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (912, 912)
Epoch 179, accuracy: 0.1696
batch size: (894, 894)
Epoch 180, accuracy: 0.1661
Epoch 180, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (895, 895)
Epoch 181, accuracy: 0.1691
batch size: (901, 901)
Epoch 182, accuracy: 0.1645
Epoch 182, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (892, 892)
Epoch 183, accuracy: 0.1706
batch size: (919, 919)
Epoch 184, accuracy: 0.1674
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 184, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (905, 905)
Epoch 185, accuracy: 0.1668
batch size: (911, 911)
Epoch 186, accuracy: 0.1686
Epoch 186, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (910, 910)
Epoch 187, accuracy: 0.1654
batch size: (912, 912)
Epoch 188, accuracy: 0.1662
Epoch 188, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (906, 906)
Epoch 189, accuracy: 0.1703
batch size: (892, 892)
Epoch 190, accuracy: 0.1681
Epoch 190, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (914, 914)
Epoch 191, accuracy: 0.1698
batch size: (904, 904)
Epoch 192, accuracy: 0.1687
Epoch 192, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (884, 884)
Epoch 193, accuracy: 0.1651
batch size: (900, 900)
Epoch 194, accuracy: 0.1682
Epoch 194, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (892, 892)
Epoch 195, accuracy: 0.1685
batch size: (911, 911)
Epoch 196, accuracy: 0.1644
Epoch 196, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (895, 895)
Epoch 197, accuracy: 0.1655
batch size: (898, 898)
Epoch 198, accuracy: 0.1690
Epoch 198, Train Loss: 1.0987, Val Loss: 1.1033
batch size: (893, 893)
Epoch 199, accuracy: 0.1662
Loaded best model with val_loss = 1.1000123023986816
test :accuracy 0.2226, f1_macro: 0.1896, f1_micro: 0.2226, auc: 0.4718
Training GINConv with 2 layers...
可训练参数: 265483_GINConv
不可训练参数: 0
batch size: (900, 900)
✅ Epoch 0: New best model saved with val_loss = 1.1257
Epoch 0, accuracy: 0.3554
Epoch 0, Train Loss: 1.2268, Val Loss: 1.1257
batch size: (914, 914)
✅ Epoch 1: New best model saved with val_loss = 1.0297
Epoch 1, accuracy: 0.4610
batch size: (907, 907)
✅ Epoch 2: New best model saved with val_loss = 1.0091
Epoch 2, accuracy: 0.4230
Epoch 2, Train Loss: 1.5774, Val Loss: 1.0091
batch size: (897, 897)
✅ Epoch 3: New best model saved with val_loss = 0.9828
Epoch 3, accuracy: 0.4483
batch size: (902, 902)
✅ Epoch 4: New best model saved with val_loss = 0.9583
Epoch 4, accuracy: 0.4844
Epoch 4, Train Loss: 0.2641, Val Loss: 0.9583
batch size: (914, 914)
✅ Epoch 5: New best model saved with val_loss = 0.9445
Epoch 5, accuracy: 0.5023
batch size: (898, 898)
✅ Epoch 6: New best model saved with val_loss = 0.9227
Epoch 6, accuracy: 0.5551
Epoch 6, Train Loss: 0.2145, Val Loss: 0.9227
batch size: (913, 913)
✅ Epoch 7: New best model saved with val_loss = 0.8959
Epoch 7, accuracy: 0.6177
batch size: (886, 886)
✅ Epoch 8: New best model saved with val_loss = 0.8756
Epoch 8, accuracy: 0.6605
Epoch 8, Train Loss: 0.0729, Val Loss: 0.8756
batch size: (886, 886)
✅ Epoch 9: New best model saved with val_loss = 0.8528
Epoch 9, accuracy: 0.6793
batch size: (911, 911)
✅ Epoch 10: New best model saved with val_loss = 0.8235
Epoch 10, accuracy: 0.6928
Epoch 10, Train Loss: 0.0306, Val Loss: 0.8235
batch size: (887, 887)
✅ Epoch 11: New best model saved with val_loss = 0.8005
Epoch 11, accuracy: 0.7031
batch size: (892, 892)
✅ Epoch 12: New best model saved with val_loss = 0.7683
Epoch 12, accuracy: 0.7100
Epoch 12, Train Loss: 0.0564, Val Loss: 0.7683
batch size: (905, 905)
✅ Epoch 13: New best model saved with val_loss = 0.7374
Epoch 13, accuracy: 0.7169
batch size: (893, 893)
✅ Epoch 14: New best model saved with val_loss = 0.7360
Epoch 14, accuracy: 0.7167
Epoch 14, Train Loss: 0.0164, Val Loss: 0.7360
batch size: (888, 888)
✅ Epoch 15: New best model saved with val_loss = 0.7281
Epoch 15, accuracy: 0.7158
batch size: (896, 896)
Epoch 16, accuracy: 0.7130
Epoch 16, Train Loss: 0.0068, Val Loss: 0.7379
batch size: (893, 893)
Epoch 17, accuracy: 0.7093
batch size: (899, 899)
Epoch 18, accuracy: 0.7119
Epoch 18, Train Loss: 0.0091, Val Loss: 0.7486
batch size: (892, 892)
Epoch 19, accuracy: 0.7127
batch size: (900, 900)
Epoch 20, accuracy: 0.7121
Epoch 20, Train Loss: 0.0017, Val Loss: 0.7932
batch size: (892, 892)
Epoch 21, accuracy: 0.7141
batch size: (918, 918)
Epoch 22, accuracy: 0.7175
Epoch 22, Train Loss: 0.0008, Val Loss: 0.8311
batch size: (890, 890)
Epoch 23, accuracy: 0.7153
batch size: (905, 905)
Epoch 24, accuracy: 0.7171
Epoch 24, Train Loss: 0.0047, Val Loss: 0.8534
batch size: (911, 911)
Epoch 25, accuracy: 0.7200
batch size: (906, 906)
Epoch 26, accuracy: 0.7211
Epoch 26, Train Loss: 0.0015, Val Loss: 0.8947
batch size: (919, 919)
Epoch 27, accuracy: 0.7217
batch size: (915, 915)
Epoch 28, accuracy: 0.7240
Epoch 28, Train Loss: 0.0022, Val Loss: 0.9442
batch size: (904, 904)
Epoch 29, accuracy: 0.7218
batch size: (901, 901)
Epoch 30, accuracy: 0.7209
Epoch 30, Train Loss: 0.0016, Val Loss: 1.0037
batch size: (904, 904)
Epoch 31, accuracy: 0.7222
batch size: (905, 905)
Epoch 32, accuracy: 0.7221
Epoch 32, Train Loss: 0.0024, Val Loss: 1.0355
batch size: (884, 884)
Epoch 33, accuracy: 0.7232
batch size: (861, 861)
Epoch 34, accuracy: 0.7247
Epoch 34, Train Loss: 0.0009, Val Loss: 1.1850
batch size: (871, 871)
Epoch 35, accuracy: 0.7222
batch size: (895, 895)
Epoch 36, accuracy: 0.7228
Epoch 36, Train Loss: 0.0012, Val Loss: 1.1597
batch size: (901, 901)
Epoch 37, accuracy: 0.7231
batch size: (918, 918)
Epoch 38, accuracy: 0.7240
Epoch 38, Train Loss: 0.0004, Val Loss: 1.2473
batch size: (908, 908)
Epoch 39, accuracy: 0.7244
batch size: (887, 887)
Epoch 40, accuracy: 0.7233
Epoch 40, Train Loss: 0.0007, Val Loss: 1.3026
batch size: (904, 904)
Epoch 41, accuracy: 0.7230
batch size: (900, 900)
Epoch 42, accuracy: 0.7254
Epoch 42, Train Loss: 0.0010, Val Loss: 1.3858
batch size: (893, 893)
Epoch 43, accuracy: 0.7235
batch size: (886, 886)
Epoch 44, accuracy: 0.7251
Epoch 44, Train Loss: 0.0006, Val Loss: 1.4777
batch size: (889, 889)
Epoch 45, accuracy: 0.7246
batch size: (905, 905)
Epoch 46, accuracy: 0.7233
Epoch 46, Train Loss: 0.0016, Val Loss: 1.6168
batch size: (909, 909)
Epoch 47, accuracy: 0.7258
batch size: (900, 900)
Epoch 48, accuracy: 0.7247
Epoch 48, Train Loss: 0.0006, Val Loss: 1.6750
batch size: (898, 898)
Epoch 49, accuracy: 0.7267
batch size: (881, 881)
Epoch 50, accuracy: 0.7245
Epoch 50, Train Loss: 0.0008, Val Loss: 1.6964
batch size: (889, 889)
Epoch 51, accuracy: 0.7234
batch size: (903, 903)
Epoch 52, accuracy: 0.7238
Epoch 52, Train Loss: 0.0003, Val Loss: 1.7501
batch size: (898, 898)
Epoch 53, accuracy: 0.7261
batch size: (902, 902)
Epoch 54, accuracy: 0.7243
Epoch 54, Train Loss: 0.0009, Val Loss: 1.8947
batch size: (896, 896)
Epoch 55, accuracy: 0.7250
batch size: (884, 884)
Epoch 56, accuracy: 0.7253
Epoch 56, Train Loss: 0.0005, Val Loss: 1.8123
batch size: (901, 901)
Epoch 57, accuracy: 0.7258
batch size: (895, 895)
Epoch 58, accuracy: 0.7281
Epoch 58, Train Loss: 0.0010, Val Loss: 1.9306
batch size: (900, 900)
Epoch 59, accuracy: 0.7245
batch size: (900, 900)
Epoch 60, accuracy: 0.7261
Epoch 60, Train Loss: 0.0009, Val Loss: 1.9570
batch size: (912, 912)
Epoch 61, accuracy: 0.7265
batch size: (895, 895)
Epoch 62, accuracy: 0.7258
Epoch 62, Train Loss: 0.0006, Val Loss: 2.0362
batch size: (916, 916)
Epoch 63, accuracy: 0.7279
batch size: (913, 913)
Epoch 64, accuracy: 0.7264
Epoch 64, Train Loss: 0.0006, Val Loss: 2.1297
batch size: (905, 905)
Epoch 65, accuracy: 0.7275
batch size: (901, 901)
Epoch 66, accuracy: 0.7241
Epoch 66, Train Loss: 0.0027, Val Loss: 2.0955
batch size: (905, 905)
Epoch 67, accuracy: 0.7263
batch size: (894, 894)
Epoch 68, accuracy: 0.7286
Epoch 68, Train Loss: 0.0014, Val Loss: 2.1271
batch size: (904, 904)
Epoch 69, accuracy: 0.7275
batch size: (890, 890)
Epoch 70, accuracy: 0.7280
Epoch 70, Train Loss: 0.0042, Val Loss: 2.2454
batch size: (894, 894)
Epoch 71, accuracy: 0.7275
batch size: (903, 903)
Epoch 72, accuracy: 0.7250
Epoch 72, Train Loss: 0.0045, Val Loss: 2.0804
batch size: (884, 884)
Epoch 73, accuracy: 0.7284
batch size: (899, 899)
Epoch 74, accuracy: 0.7277
Epoch 74, Train Loss: 0.0006, Val Loss: 2.1564
batch size: (910, 910)
Epoch 75, accuracy: 0.7274
batch size: (896, 896)
Epoch 76, accuracy: 0.7233
Epoch 76, Train Loss: 0.0006, Val Loss: 2.1537
batch size: (908, 908)
Epoch 77, accuracy: 0.7266
batch size: (906, 906)
Epoch 78, accuracy: 0.7258
Epoch 78, Train Loss: 0.0015, Val Loss: 2.2475
batch size: (888, 888)
Epoch 79, accuracy: 0.7255
batch size: (895, 895)
Epoch 80, accuracy: 0.7265
Epoch 80, Train Loss: 0.0010, Val Loss: 2.2050
batch size: (890, 890)
Epoch 81, accuracy: 0.7256
batch size: (897, 897)
Epoch 82, accuracy: 0.7261
Epoch 82, Train Loss: 0.0053, Val Loss: 2.2774
batch size: (907, 907)
Epoch 83, accuracy: 0.7273
batch size: (906, 906)
Epoch 84, accuracy: 0.7286
Epoch 84, Train Loss: 0.0006, Val Loss: 2.3062
batch size: (890, 890)
Epoch 85, accuracy: 0.7258
batch size: (900, 900)
Epoch 86, accuracy: 0.7255
Epoch 86, Train Loss: 0.0013, Val Loss: 2.2117
batch size: (908, 908)
Epoch 87, accuracy: 0.7255
batch size: (906, 906)
Epoch 88, accuracy: 0.7279
Epoch 88, Train Loss: 0.0021, Val Loss: 2.3347
batch size: (887, 887)
Epoch 89, accuracy: 0.7260
batch size: (921, 921)
Epoch 90, accuracy: 0.7249
Epoch 90, Train Loss: 0.0014, Val Loss: 2.1725
batch size: (896, 896)
Epoch 91, accuracy: 0.7275
batch size: (906, 906)
Epoch 92, accuracy: 0.7256
Epoch 92, Train Loss: 0.0007, Val Loss: 2.2199
batch size: (903, 903)
Epoch 93, accuracy: 0.7244
batch size: (875, 875)
Epoch 94, accuracy: 0.7250
Epoch 94, Train Loss: 0.0010, Val Loss: 2.2074
batch size: (906, 906)
Epoch 95, accuracy: 0.7242
batch size: (910, 910)
Epoch 96, accuracy: 0.7264
Epoch 96, Train Loss: 0.0007, Val Loss: 2.2732
batch size: (914, 914)
Epoch 97, accuracy: 0.7228
batch size: (882, 882)
Epoch 98, accuracy: 0.7265
Epoch 98, Train Loss: 0.0008, Val Loss: 2.1856
batch size: (911, 911)
Epoch 99, accuracy: 0.7243
batch size: (886, 886)
Epoch 100, accuracy: 0.7270
Epoch 100, Train Loss: 0.0011, Val Loss: 2.2588
batch size: (904, 904)
Epoch 101, accuracy: 0.7246
batch size: (893, 893)
Epoch 102, accuracy: 0.7282
Epoch 102, Train Loss: 0.0007, Val Loss: 2.2741
batch size: (908, 908)
Epoch 103, accuracy: 0.7256
batch size: (899, 899)
Epoch 104, accuracy: 0.7266
Epoch 104, Train Loss: 0.0015, Val Loss: 2.2704
batch size: (885, 885)
Epoch 105, accuracy: 0.7255
batch size: (916, 916)
Epoch 106, accuracy: 0.7267
Epoch 106, Train Loss: 0.0017, Val Loss: 2.1354
batch size: (900, 900)
Epoch 107, accuracy: 0.7247
batch size: (909, 909)
Epoch 108, accuracy: 0.7244
Epoch 108, Train Loss: 0.0005, Val Loss: 2.2583
batch size: (909, 909)
Epoch 109, accuracy: 0.7254
batch size: (920, 920)
Epoch 110, accuracy: 0.7263
Epoch 110, Train Loss: 0.0010, Val Loss: 2.2875
batch size: (897, 897)
Epoch 111, accuracy: 0.7266
batch size: (902, 902)
Epoch 112, accuracy: 0.7269
Epoch 112, Train Loss: 0.0005, Val Loss: 2.3373
batch size: (890, 890)
Epoch 113, accuracy: 0.7249
batch size: (896, 896)
Epoch 114, accuracy: 0.7248
Epoch 114, Train Loss: 0.0006, Val Loss: 2.3254
batch size: (901, 901)
Epoch 115, accuracy: 0.7287
batch size: (903, 903)
Epoch 116, accuracy: 0.7247
Epoch 116, Train Loss: 0.0017, Val Loss: 2.1726
batch size: (893, 893)
Epoch 117, accuracy: 0.7268
batch size: (885, 885)
Epoch 118, accuracy: 0.7271
Epoch 118, Train Loss: 0.0010, Val Loss: 2.2522
batch size: (883, 883)
Epoch 119, accuracy: 0.7245
batch size: (920, 920)
Epoch 120, accuracy: 0.7253
Epoch 120, Train Loss: 0.0003, Val Loss: 2.2492
batch size: (887, 887)
Epoch 121, accuracy: 0.7259
batch size: (908, 908)
Epoch 122, accuracy: 0.7246
Epoch 122, Train Loss: 0.0009, Val Loss: 2.1778
batch size: (881, 881)
Epoch 123, accuracy: 0.7263
batch size: (904, 904)
Epoch 124, accuracy: 0.7251
Epoch 124, Train Loss: 0.0005, Val Loss: 2.1831
batch size: (884, 884)
Epoch 125, accuracy: 0.7278
batch size: (899, 899)
Epoch 126, accuracy: 0.7265
Epoch 126, Train Loss: 0.0015, Val Loss: 2.2466
batch size: (924, 924)
Epoch 127, accuracy: 0.7282
batch size: (901, 901)
Epoch 128, accuracy: 0.7250
Epoch 128, Train Loss: 0.0011, Val Loss: 2.3585
batch size: (892, 892)
Epoch 129, accuracy: 0.7251
batch size: (901, 901)
Epoch 130, accuracy: 0.7264
Epoch 130, Train Loss: 0.0008, Val Loss: 2.2880
batch size: (902, 902)
Epoch 131, accuracy: 0.7276
batch size: (904, 904)
Epoch 132, accuracy: 0.7249
Epoch 132, Train Loss: 0.0007, Val Loss: 2.1888
batch size: (888, 888)
Epoch 133, accuracy: 0.7262
batch size: (900, 900)
Epoch 134, accuracy: 0.7275
Epoch 134, Train Loss: 0.0031, Val Loss: 2.2237
batch size: (886, 886)
Epoch 135, accuracy: 0.7292
batch size: (915, 915)
Epoch 136, accuracy: 0.7251
Epoch 136, Train Loss: 0.0012, Val Loss: 2.2239
batch size: (893, 893)
Epoch 137, accuracy: 0.7272
batch size: (886, 886)
Epoch 138, accuracy: 0.7265
Epoch 138, Train Loss: 0.0004, Val Loss: 2.1821
batch size: (907, 907)
Epoch 139, accuracy: 0.7265
batch size: (930, 930)
Epoch 140, accuracy: 0.7255
Epoch 140, Train Loss: 0.0009, Val Loss: 2.2950
batch size: (906, 906)
Epoch 141, accuracy: 0.7241
batch size: (890, 890)
Epoch 142, accuracy: 0.7251
Epoch 142, Train Loss: 0.0011, Val Loss: 2.2220
batch size: (896, 896)
Epoch 143, accuracy: 0.7258
batch size: (911, 911)
Epoch 144, accuracy: 0.7255
Epoch 144, Train Loss: 0.0011, Val Loss: 2.4173
batch size: (868, 868)
Epoch 145, accuracy: 0.7250
batch size: (897, 897)
Epoch 146, accuracy: 0.7245
Epoch 146, Train Loss: 0.0007, Val Loss: 2.2926
batch size: (896, 896)
Epoch 147, accuracy: 0.7260
batch size: (900, 900)
Epoch 148, accuracy: 0.7254
Epoch 148, Train Loss: 0.0012, Val Loss: 2.1995
batch size: (908, 908)
Epoch 149, accuracy: 0.7276
batch size: (915, 915)
Epoch 150, accuracy: 0.7251
Epoch 150, Train Loss: 0.0012, Val Loss: 2.3028
batch size: (895, 895)
Epoch 151, accuracy: 0.7271
batch size: (910, 910)
Epoch 152, accuracy: 0.7255
Epoch 152, Train Loss: 0.0041, Val Loss: 2.1521
batch size: (897, 897)
Epoch 153, accuracy: 0.7265
batch size: (900, 900)
Epoch 154, accuracy: 0.7277
Epoch 154, Train Loss: 0.0033, Val Loss: 2.3029
batch size: (897, 897)
Epoch 155, accuracy: 0.7286
batch size: (887, 887)
Epoch 156, accuracy: 0.7250
Epoch 156, Train Loss: 0.0012, Val Loss: 2.2996
batch size: (908, 908)
Epoch 157, accuracy: 0.7262
batch size: (903, 903)
Epoch 158, accuracy: 0.7244
Epoch 158, Train Loss: 0.0013, Val Loss: 2.3512
batch size: (889, 889)
Epoch 159, accuracy: 0.7275
batch size: (900, 900)
Epoch 160, accuracy: 0.7251
Epoch 160, Train Loss: 0.0023, Val Loss: 2.2159
batch size: (900, 900)
Epoch 161, accuracy: 0.7285
batch size: (911, 911)
Epoch 162, accuracy: 0.7253
Epoch 162, Train Loss: 0.0010, Val Loss: 2.2855
batch size: (895, 895)
Epoch 163, accuracy: 0.7274
batch size: (885, 885)
Epoch 164, accuracy: 0.7271
Epoch 164, Train Loss: 0.0006, Val Loss: 2.2205
batch size: (905, 905)
Epoch 165, accuracy: 0.7267
batch size: (911, 911)
Epoch 166, accuracy: 0.7246
Epoch 166, Train Loss: 0.0020, Val Loss: 2.1928
batch size: (889, 889)
Epoch 167, accuracy: 0.7268
batch size: (893, 893)
Epoch 168, accuracy: 0.7269
Epoch 168, Train Loss: 0.0009, Val Loss: 2.2618
batch size: (899, 899)
Epoch 169, accuracy: 0.7269
batch size: (902, 902)
Epoch 170, accuracy: 0.7271
Epoch 170, Train Loss: 0.0008, Val Loss: 2.2608
batch size: (901, 901)
Epoch 171, accuracy: 0.7263
batch size: (908, 908)
Epoch 172, accuracy: 0.7251
Epoch 172, Train Loss: 0.0007, Val Loss: 2.2293
batch size: (883, 883)
Epoch 173, accuracy: 0.7246
batch size: (913, 913)
Epoch 174, accuracy: 0.7259
Epoch 174, Train Loss: 0.0007, Val Loss: 2.1758
batch size: (890, 890)
Epoch 175, accuracy: 0.7270
batch size: (904, 904)
Epoch 176, accuracy: 0.7252
Epoch 176, Train Loss: 0.0008, Val Loss: 2.2865
batch size: (893, 893)
Epoch 177, accuracy: 0.7267
batch size: (900, 900)
Epoch 178, accuracy: 0.7245
Epoch 178, Train Loss: 0.0007, Val Loss: 2.2711
batch size: (897, 897)
Epoch 179, accuracy: 0.7268
batch size: (915, 915)
Epoch 180, accuracy: 0.7276
Epoch 180, Train Loss: 0.0011, Val Loss: 2.2005
batch size: (902, 902)
Epoch 181, accuracy: 0.7245
batch size: (871, 871)
Epoch 182, accuracy: 0.7250
Epoch 182, Train Loss: 0.0006, Val Loss: 2.3094
batch size: (877, 877)
Epoch 183, accuracy: 0.7251
batch size: (902, 902)
Epoch 184, accuracy: 0.7255
Epoch 184, Train Loss: 0.0010, Val Loss: 2.2291
batch size: (898, 898)
Epoch 185, accuracy: 0.7268
batch size: (905, 905)
Epoch 186, accuracy: 0.7276
Epoch 186, Train Loss: 0.0017, Val Loss: 2.1314
batch size: (908, 908)
Epoch 187, accuracy: 0.7250
batch size: (927, 927)
Epoch 188, accuracy: 0.7262
Epoch 188, Train Loss: 0.0012, Val Loss: 2.2105
batch size: (898, 898)
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 189, accuracy: 0.7293
batch size: (890, 890)
Epoch 190, accuracy: 0.7250
Epoch 190, Train Loss: 0.0015, Val Loss: 2.3302
batch size: (924, 924)
Epoch 191, accuracy: 0.7296
batch size: (909, 909)
Epoch 192, accuracy: 0.7252
Epoch 192, Train Loss: 0.0008, Val Loss: 2.2774
batch size: (901, 901)
Epoch 193, accuracy: 0.7274
batch size: (896, 896)
Epoch 194, accuracy: 0.7259
Epoch 194, Train Loss: 0.0041, Val Loss: 2.1850
batch size: (893, 893)
Epoch 195, accuracy: 0.7259
batch size: (914, 914)
Epoch 196, accuracy: 0.7253
Epoch 196, Train Loss: 0.0008, Val Loss: 2.1978
batch size: (908, 908)
Epoch 197, accuracy: 0.7265
batch size: (916, 916)
Epoch 198, accuracy: 0.7251
Epoch 198, Train Loss: 0.0013, Val Loss: 2.1195
batch size: (896, 896)
Epoch 199, accuracy: 0.7245
Loaded best model with val_loss = 0.7280523180961609
test :accuracy 0.7182, f1_macro: 0.7091, f1_micro: 0.7182, auc: 0.8566
Training GINConv with 8 layers...
可训练参数: 1059595_GINConv
不可训练参数: 0
batch size: (911, 911)
✅ Epoch 0: New best model saved with val_loss = 4.4913
Epoch 0, accuracy: 0.4019
Epoch 0, Train Loss: 1.4424, Val Loss: 4.4913
batch size: (900, 900)
Epoch 1, accuracy: 0.1680
batch size: (906, 906)
✅ Epoch 2: New best model saved with val_loss = 4.0550
Epoch 2, accuracy: 0.4339
Epoch 2, Train Loss: 3.3957, Val Loss: 4.0550
batch size: (897, 897)
✅ Epoch 3: New best model saved with val_loss = 1.1511
Epoch 3, accuracy: 0.4298
batch size: (901, 901)
Epoch 4, accuracy: 0.4209
Epoch 4, Train Loss: 1.1604, Val Loss: 1.5968
batch size: (905, 905)
Epoch 5, accuracy: 0.2148
batch size: (893, 893)
Epoch 6, accuracy: 0.4013
Epoch 6, Train Loss: 1.1134, Val Loss: 1.3750
batch size: (917, 917)
Epoch 7, accuracy: 0.4011
batch size: (898, 898)
Epoch 8, accuracy: 0.4014
Epoch 8, Train Loss: 1.0907, Val Loss: 1.3001
batch size: (909, 909)
Epoch 9, accuracy: 0.4053
batch size: (891, 891)
Epoch 10, accuracy: 0.3976
Epoch 10, Train Loss: 1.0757, Val Loss: 1.2724
batch size: (922, 922)
Epoch 11, accuracy: 0.3973
batch size: (901, 901)
Epoch 12, accuracy: 0.4009
Epoch 12, Train Loss: 1.0768, Val Loss: 1.2064
batch size: (908, 908)
Epoch 13, accuracy: 0.3962
batch size: (900, 900)
Epoch 14, accuracy: 0.4041
Epoch 14, Train Loss: 1.0623, Val Loss: 1.2247
batch size: (911, 911)
Epoch 15, accuracy: 0.4042
batch size: (892, 892)
Epoch 16, accuracy: 0.4043
Epoch 16, Train Loss: 1.0755, Val Loss: 1.2012
batch size: (905, 905)
Epoch 17, accuracy: 0.4006
batch size: (911, 911)
Epoch 18, accuracy: 0.4048
Epoch 18, Train Loss: 1.1634, Val Loss: 1.2009
batch size: (907, 907)
Epoch 19, accuracy: 0.4021
batch size: (913, 913)
Epoch 20, accuracy: 0.4034
Epoch 20, Train Loss: 1.0731, Val Loss: 1.2067
batch size: (900, 900)
Epoch 21, accuracy: 0.4012
batch size: (916, 916)
Epoch 22, accuracy: 0.4048
Epoch 22, Train Loss: 1.0735, Val Loss: 1.1991
batch size: (913, 913)
Epoch 23, accuracy: 0.4041
batch size: (893, 893)
Epoch 24, accuracy: 0.4038
Epoch 24, Train Loss: 1.0738, Val Loss: 1.1730
batch size: (896, 896)
Epoch 25, accuracy: 0.4009
batch size: (901, 901)
Epoch 26, accuracy: 0.4055
Epoch 26, Train Loss: 1.1240, Val Loss: 1.1863
batch size: (899, 899)
Epoch 27, accuracy: 0.4001
batch size: (896, 896)
Epoch 28, accuracy: 0.4028
Epoch 28, Train Loss: 1.0576, Val Loss: 1.1813
batch size: (898, 898)
Epoch 29, accuracy: 0.4065
batch size: (899, 899)
Epoch 30, accuracy: 0.4041
Epoch 30, Train Loss: 1.0916, Val Loss: 1.1922
batch size: (890, 890)
Epoch 31, accuracy: 0.4018
batch size: (913, 913)
Epoch 32, accuracy: 0.4028
Epoch 32, Train Loss: 1.1429, Val Loss: 1.1850
batch size: (905, 905)
Epoch 33, accuracy: 0.4027
batch size: (902, 902)
Epoch 34, accuracy: 0.4001
Epoch 34, Train Loss: 1.0812, Val Loss: 1.1850
batch size: (901, 901)
Epoch 35, accuracy: 0.4022
batch size: (893, 893)
Epoch 36, accuracy: 0.4003
Epoch 36, Train Loss: 1.0580, Val Loss: 1.1826
batch size: (909, 909)
Epoch 37, accuracy: 0.4035
batch size: (906, 906)
Epoch 38, accuracy: 0.3964
Epoch 38, Train Loss: 1.0759, Val Loss: 1.1919
batch size: (903, 903)
Epoch 39, accuracy: 0.4019
batch size: (876, 876)
Epoch 40, accuracy: 0.4027
Epoch 40, Train Loss: 1.0591, Val Loss: 1.1783
batch size: (897, 897)
Epoch 41, accuracy: 0.4053
batch size: (912, 912)
Epoch 42, accuracy: 0.4040
Epoch 42, Train Loss: 1.1494, Val Loss: 1.1930
batch size: (891, 891)
Epoch 43, accuracy: 0.4000
batch size: (897, 897)
Epoch 44, accuracy: 0.4039
Epoch 44, Train Loss: 1.0548, Val Loss: 1.1866
batch size: (891, 891)
Epoch 45, accuracy: 0.4007
batch size: (900, 900)
Epoch 46, accuracy: 0.4041
Epoch 46, Train Loss: 1.0774, Val Loss: 1.1993
batch size: (894, 894)
Epoch 47, accuracy: 0.4021
batch size: (889, 889)
Epoch 48, accuracy: 0.4004
Epoch 48, Train Loss: 1.1286, Val Loss: 1.2058
batch size: (894, 894)
Epoch 49, accuracy: 0.4016
batch size: (894, 894)
Epoch 50, accuracy: 0.4018
Epoch 50, Train Loss: 1.0724, Val Loss: 1.2086
batch size: (901, 901)
Epoch 51, accuracy: 0.4009
batch size: (915, 915)
Epoch 52, accuracy: 0.4055
Epoch 52, Train Loss: 1.0762, Val Loss: 1.2259
batch size: (908, 908)
Epoch 53, accuracy: 0.4033
batch size: (885, 885)
Epoch 54, accuracy: 0.4051
Epoch 54, Train Loss: 1.0768, Val Loss: 1.2056
batch size: (908, 908)
Epoch 55, accuracy: 0.4057
batch size: (913, 913)
Epoch 56, accuracy: 0.4027
Epoch 56, Train Loss: 1.0600, Val Loss: 1.1812
batch size: (902, 902)
Epoch 57, accuracy: 0.4030
batch size: (919, 919)
Epoch 58, accuracy: 0.3990
Epoch 58, Train Loss: 1.0720, Val Loss: 1.1981
batch size: (881, 881)
Epoch 59, accuracy: 0.4024
batch size: (912, 912)
Epoch 60, accuracy: 0.4044
Epoch 60, Train Loss: 1.0589, Val Loss: 1.1971
batch size: (890, 890)
Epoch 61, accuracy: 0.4037
batch size: (912, 912)
Epoch 62, accuracy: 0.4033
Epoch 62, Train Loss: 1.0746, Val Loss: 1.1776
batch size: (876, 876)
Epoch 63, accuracy: 0.4025
batch size: (914, 914)
Epoch 64, accuracy: 0.4008
Epoch 64, Train Loss: 1.0749, Val Loss: 1.1804
batch size: (912, 912)
Epoch 65, accuracy: 0.4016
batch size: (899, 899)
Epoch 66, accuracy: 0.4025
Epoch 66, Train Loss: 1.0783, Val Loss: 1.1961
batch size: (916, 916)
Epoch 67, accuracy: 0.3974
batch size: (931, 931)
Epoch 68, accuracy: 0.4021
Epoch 68, Train Loss: 1.0762, Val Loss: 1.1926
batch size: (899, 899)
Epoch 69, accuracy: 0.4040
batch size: (893, 893)
Epoch 70, accuracy: 0.4043
Epoch 70, Train Loss: 1.0731, Val Loss: 1.1819
batch size: (897, 897)
Epoch 71, accuracy: 0.4013
batch size: (912, 912)
Epoch 72, accuracy: 0.4011
Epoch 72, Train Loss: 1.0734, Val Loss: 1.1892
batch size: (908, 908)
Epoch 73, accuracy: 0.4032
batch size: (889, 889)
Epoch 74, accuracy: 0.3991
Epoch 74, Train Loss: 1.0905, Val Loss: 1.2068
batch size: (886, 886)
Epoch 75, accuracy: 0.3998
batch size: (888, 888)
Epoch 76, accuracy: 0.4011
Epoch 76, Train Loss: 1.0759, Val Loss: 1.1718
batch size: (892, 892)
Epoch 77, accuracy: 0.4041
batch size: (889, 889)
Epoch 78, accuracy: 0.4025
Epoch 78, Train Loss: 1.0744, Val Loss: 1.2044
batch size: (904, 904)
Epoch 79, accuracy: 0.3974
batch size: (912, 912)
Epoch 80, accuracy: 0.4055
Epoch 80, Train Loss: 1.0780, Val Loss: 1.1826
batch size: (909, 909)
Epoch 81, accuracy: 0.4018
batch size: (907, 907)
Epoch 82, accuracy: 0.4023
Epoch 82, Train Loss: 1.0565, Val Loss: 1.1845
batch size: (911, 911)
Epoch 83, accuracy: 0.4008
batch size: (901, 901)
Epoch 84, accuracy: 0.4030
Epoch 84, Train Loss: 1.0716, Val Loss: 1.2018
batch size: (899, 899)
Epoch 85, accuracy: 0.4058
batch size: (910, 910)
Epoch 86, accuracy: 0.4057
Epoch 86, Train Loss: 1.0781, Val Loss: 1.1919
batch size: (882, 882)
Epoch 87, accuracy: 0.4043
batch size: (905, 905)
Epoch 88, accuracy: 0.3983
Epoch 88, Train Loss: 1.0854, Val Loss: 1.1872
batch size: (916, 916)
Epoch 89, accuracy: 0.4001
batch size: (891, 891)
Epoch 90, accuracy: 0.4007
Epoch 90, Train Loss: 1.0744, Val Loss: 1.1968
batch size: (893, 893)
Epoch 91, accuracy: 0.4035
batch size: (890, 890)
Epoch 92, accuracy: 0.3970
Epoch 92, Train Loss: 1.0662, Val Loss: 1.1814
batch size: (893, 893)
Epoch 93, accuracy: 0.4008
batch size: (895, 895)
Epoch 94, accuracy: 0.4025
Epoch 94, Train Loss: 1.0768, Val Loss: 1.1860
batch size: (913, 913)
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 95, accuracy: 0.3999
batch size: (907, 907)
Epoch 96, accuracy: 0.4033
Epoch 96, Train Loss: 1.0728, Val Loss: 1.1823
batch size: (883, 883)
Epoch 97, accuracy: 0.4037
batch size: (918, 918)
Epoch 98, accuracy: 0.4002
Epoch 98, Train Loss: 1.0856, Val Loss: 1.2029
batch size: (910, 910)
Epoch 99, accuracy: 0.4035
batch size: (901, 901)
Epoch 100, accuracy: 0.4080
Epoch 100, Train Loss: 1.0793, Val Loss: 1.1874
batch size: (905, 905)
Epoch 101, accuracy: 0.4026
batch size: (899, 899)
Epoch 102, accuracy: 0.4006
Epoch 102, Train Loss: 1.0776, Val Loss: 1.1871
batch size: (913, 913)
Epoch 103, accuracy: 0.3998
batch size: (911, 911)
Epoch 104, accuracy: 0.4010
Epoch 104, Train Loss: 1.0798, Val Loss: 1.2057
batch size: (910, 910)
Epoch 105, accuracy: 0.4054
batch size: (923, 923)
Epoch 106, accuracy: 0.4079
Epoch 106, Train Loss: 1.0566, Val Loss: 1.1779
batch size: (904, 904)
Epoch 107, accuracy: 0.4032
batch size: (901, 901)
Epoch 108, accuracy: 0.4019
Epoch 108, Train Loss: 1.0567, Val Loss: 1.1828
batch size: (894, 894)
Epoch 109, accuracy: 0.4023
batch size: (894, 894)
Epoch 110, accuracy: 0.3999
Epoch 110, Train Loss: 1.0741, Val Loss: 1.1967
batch size: (888, 888)
Epoch 111, accuracy: 0.4040
batch size: (878, 878)
Epoch 112, accuracy: 0.4040
Epoch 112, Train Loss: 1.0773, Val Loss: 1.1935
batch size: (907, 907)
Epoch 113, accuracy: 0.4042
batch size: (914, 914)
Epoch 114, accuracy: 0.4042
Epoch 114, Train Loss: 1.0757, Val Loss: 1.1836
batch size: (878, 878)
Epoch 115, accuracy: 0.4008
batch size: (907, 907)
Epoch 116, accuracy: 0.4060
Epoch 116, Train Loss: 1.0790, Val Loss: 1.1748
batch size: (903, 903)
Epoch 117, accuracy: 0.4047
batch size: (900, 900)
Epoch 118, accuracy: 0.4033
Epoch 118, Train Loss: 1.0799, Val Loss: 1.1877
batch size: (909, 909)
Epoch 119, accuracy: 0.4032
batch size: (908, 908)
Epoch 120, accuracy: 0.3981
Epoch 120, Train Loss: 1.0754, Val Loss: 1.1911
batch size: (888, 888)
Epoch 121, accuracy: 0.4039
batch size: (897, 897)
Epoch 122, accuracy: 0.4040
Epoch 122, Train Loss: 1.0777, Val Loss: 1.1926
batch size: (880, 880)
Epoch 123, accuracy: 0.4083
batch size: (886, 886)
Epoch 124, accuracy: 0.4001
Epoch 124, Train Loss: 1.0797, Val Loss: 1.1933
batch size: (893, 893)
Epoch 125, accuracy: 0.4045
batch size: (885, 885)
Epoch 126, accuracy: 0.3999
Epoch 126, Train Loss: 1.0670, Val Loss: 1.1752
batch size: (896, 896)
Epoch 127, accuracy: 0.4004
batch size: (900, 900)
Epoch 128, accuracy: 0.4014
Epoch 128, Train Loss: 1.0746, Val Loss: 1.1784
batch size: (903, 903)
Epoch 129, accuracy: 0.4085
batch size: (901, 901)
Epoch 130, accuracy: 0.4002
Epoch 130, Train Loss: 1.0583, Val Loss: 1.1840
batch size: (881, 881)
Epoch 131, accuracy: 0.4033
batch size: (897, 897)
Epoch 132, accuracy: 0.3986
Epoch 132, Train Loss: 1.0582, Val Loss: 1.2064
batch size: (898, 898)
Epoch 133, accuracy: 0.4021
batch size: (903, 903)
Epoch 134, accuracy: 0.4026
Epoch 134, Train Loss: 1.0755, Val Loss: 1.1850
batch size: (903, 903)
Epoch 135, accuracy: 0.4007
batch size: (886, 886)
Epoch 136, accuracy: 0.4040
Epoch 136, Train Loss: 1.1325, Val Loss: 1.2175
batch size: (886, 886)
Epoch 137, accuracy: 0.4013
batch size: (897, 897)
Epoch 138, accuracy: 0.4011
Epoch 138, Train Loss: 1.0768, Val Loss: 1.1934
batch size: (894, 894)
Epoch 139, accuracy: 0.4032
batch size: (907, 907)
Epoch 140, accuracy: 0.4052
Epoch 140, Train Loss: 1.0969, Val Loss: 1.1954
batch size: (914, 914)
Epoch 141, accuracy: 0.3999
batch size: (892, 892)
Epoch 142, accuracy: 0.4010
Epoch 142, Train Loss: 1.0815, Val Loss: 1.1984
batch size: (906, 906)
Epoch 143, accuracy: 0.3989
batch size: (909, 909)
Epoch 144, accuracy: 0.4006
Epoch 144, Train Loss: 1.0762, Val Loss: 1.2091
batch size: (904, 904)
Epoch 145, accuracy: 0.4045
batch size: (886, 886)
Epoch 146, accuracy: 0.4004
Epoch 146, Train Loss: 1.0766, Val Loss: 1.1885
batch size: (897, 897)
Epoch 147, accuracy: 0.3977
batch size: (893, 893)
Epoch 148, accuracy: 0.4049
Epoch 148, Train Loss: 1.0735, Val Loss: 1.2060
batch size: (890, 890)
Epoch 149, accuracy: 0.4026
batch size: (901, 901)
Epoch 150, accuracy: 0.4037
Epoch 150, Train Loss: 1.0576, Val Loss: 1.1906
batch size: (885, 885)
Epoch 151, accuracy: 0.4040
batch size: (883, 883)
Epoch 152, accuracy: 0.4055
Epoch 152, Train Loss: 1.0578, Val Loss: 1.1887
batch size: (900, 900)
Epoch 153, accuracy: 0.4020
batch size: (897, 897)
Epoch 154, accuracy: 0.4011
Epoch 154, Train Loss: 1.1071, Val Loss: 1.2194
batch size: (923, 923)
Epoch 155, accuracy: 0.4034
batch size: (921, 921)
Epoch 156, accuracy: 0.4027
Epoch 156, Train Loss: 1.1173, Val Loss: 1.1898
batch size: (901, 901)
Epoch 157, accuracy: 0.4049
batch size: (907, 907)
Epoch 158, accuracy: 0.4035
Epoch 158, Train Loss: 1.0619, Val Loss: 1.2001
batch size: (911, 911)
Epoch 159, accuracy: 0.4010
batch size: (905, 905)
Epoch 160, accuracy: 0.4017
Epoch 160, Train Loss: 1.0937, Val Loss: 1.1755
batch size: (888, 888)
Epoch 161, accuracy: 0.4000
batch size: (905, 905)
Epoch 162, accuracy: 0.4004
Epoch 162, Train Loss: 1.0715, Val Loss: 1.1942
batch size: (889, 889)
Epoch 163, accuracy: 0.4024
batch size: (898, 898)
Epoch 164, accuracy: 0.4063
Epoch 164, Train Loss: 1.0746, Val Loss: 1.2123
batch size: (901, 901)
Epoch 165, accuracy: 0.4029
batch size: (909, 909)
Epoch 166, accuracy: 0.4048
Epoch 166, Train Loss: 1.0618, Val Loss: 1.2184
batch size: (875, 875)
Epoch 167, accuracy: 0.4037
batch size: (884, 884)
Epoch 168, accuracy: 0.4025
Epoch 168, Train Loss: 1.0737, Val Loss: 1.2325
batch size: (905, 905)
Epoch 169, accuracy: 0.4030
batch size: (889, 889)
Epoch 170, accuracy: 0.4039
Epoch 170, Train Loss: 1.0586, Val Loss: 1.1925
batch size: (898, 898)
Epoch 171, accuracy: 0.3980
batch size: (905, 905)
Epoch 172, accuracy: 0.4050
Epoch 172, Train Loss: 1.1052, Val Loss: 1.1883
batch size: (894, 894)
Epoch 173, accuracy: 0.4031
batch size: (885, 885)
Epoch 174, accuracy: 0.3995
Epoch 174, Train Loss: 1.0573, Val Loss: 1.2127
batch size: (899, 899)
Epoch 175, accuracy: 0.4034
batch size: (883, 883)
Epoch 176, accuracy: 0.4065
Epoch 176, Train Loss: 1.0722, Val Loss: 1.1928
batch size: (879, 879)
Epoch 177, accuracy: 0.4062
batch size: (889, 889)
Epoch 178, accuracy: 0.4014
Epoch 178, Train Loss: 1.0744, Val Loss: 1.1739
batch size: (896, 896)
Epoch 179, accuracy: 0.4007
batch size: (925, 925)
Epoch 180, accuracy: 0.4033
Epoch 180, Train Loss: 1.0775, Val Loss: 1.1707
batch size: (905, 905)
Epoch 181, accuracy: 0.4007
batch size: (898, 898)
Epoch 182, accuracy: 0.4027
Epoch 182, Train Loss: 1.0627, Val Loss: 1.1824
batch size: (919, 919)
Epoch 183, accuracy: 0.4002
batch size: (898, 898)
Epoch 184, accuracy: 0.4000
Epoch 184, Train Loss: 1.0741, Val Loss: 1.1999
batch size: (906, 906)
Epoch 185, accuracy: 0.4011
batch size: (896, 896)
Epoch 186, accuracy: 0.4036
Epoch 186, Train Loss: 1.0788, Val Loss: 1.1973
batch size: (899, 899)
Epoch 187, accuracy: 0.4016
batch size: (918, 918)
Epoch 188, accuracy: 0.4023
Epoch 188, Train Loss: 1.0747, Val Loss: 1.1793
batch size: (898, 898)
Epoch 189, accuracy: 0.4013
batch size: (889, 889)
Epoch 190, accuracy: 0.4033
Epoch 190, Train Loss: 1.0602, Val Loss: 1.1784
batch size: (905, 905)
Epoch 191, accuracy: 0.4043
batch size: (914, 914)
Epoch 192, accuracy: 0.4039
Epoch 192, Train Loss: 1.0762, Val Loss: 1.1856
batch size: (904, 904)
Epoch 193, accuracy: 0.4026
batch size: (894, 894)
Epoch 194, accuracy: 0.3994
Epoch 194, Train Loss: 1.1032, Val Loss: 1.1962
batch size: (890, 890)
Epoch 195, accuracy: 0.4013
batch size: (897, 897)
Epoch 196, accuracy: 0.4054
Epoch 196, Train Loss: 1.0958, Val Loss: 1.2049
batch size: (886, 886)
Epoch 197, accuracy: 0.4023
batch size: (895, 895)
Epoch 198, accuracy: 0.4022
Epoch 198, Train Loss: 1.0544, Val Loss: 1.1889
batch size: (877, 877)
Epoch 199, accuracy: 0.4047
Loaded best model with val_loss = 1.1510554552078247
test :accuracy 0.4275, f1_macro: 0.1996, f1_micro: 0.4275, auc: 0.5047
Training GINConv with 32 layers...
可训练参数: 4236043_GINConv
不可训练参数: 0
batch size: (901, 901)
✅ Epoch 0: New best model saved with val_loss = 20024918.0000
Epoch 0, accuracy: 0.4116
Epoch 0, Train Loss: 1.2245, Val Loss: 20024918.0000
batch size: (910, 910)
✅ Epoch 1: New best model saved with val_loss = 172458.1719
Epoch 1, accuracy: 0.2136
batch size: (891, 891)
✅ Epoch 2: New best model saved with val_loss = 2859.7595
Epoch 2, accuracy: 0.2133
Epoch 2, Train Loss: 2.3297, Val Loss: 2859.7595
batch size: (889, 889)
✅ Epoch 3: New best model saved with val_loss = 6.4528
Epoch 3, accuracy: 0.2116
batch size: (904, 904)
✅ Epoch 4: New best model saved with val_loss = 4.3543
Epoch 4, accuracy: 0.2135
Epoch 4, Train Loss: 1.4715, Val Loss: 4.3543
batch size: (895, 895)
✅ Epoch 5: New best model saved with val_loss = 1.8204
Epoch 5, accuracy: 0.2120
batch size: (899, 899)
Epoch 6, accuracy: 0.2169
Epoch 6, Train Loss: 1.0763, Val Loss: 2.4559
batch size: (880, 880)
✅ Epoch 7: New best model saved with val_loss = 1.6709
Epoch 7, accuracy: 0.4018
batch size: (877, 877)
Epoch 8, accuracy: 0.4001
Epoch 8, Train Loss: 1.0754, Val Loss: 2.5729
batch size: (893, 893)
✅ Epoch 9: New best model saved with val_loss = 1.5736
Epoch 9, accuracy: 0.3554
batch size: (921, 921)
Epoch 10, accuracy: 0.3558
Epoch 10, Train Loss: 1.0679, Val Loss: 2.3283
batch size: (902, 902)
Epoch 11, accuracy: 0.4004
batch size: (908, 908)
Epoch 12, accuracy: 0.1693
Epoch 12, Train Loss: 1.1106, Val Loss: 2.3533
batch size: (893, 893)
Epoch 13, accuracy: 0.2094
batch size: (912, 912)
Epoch 14, accuracy: 0.2146
Epoch 14, Train Loss: 1.0604, Val Loss: 1.8983
batch size: (900, 900)
Epoch 15, accuracy: 0.2156
batch size: (878, 878)
Epoch 16, accuracy: 0.2159
Epoch 16, Train Loss: 4.1362, Val Loss: 2.0070
batch size: (887, 887)
Epoch 17, accuracy: 0.2138
batch size: (896, 896)
Epoch 18, accuracy: 0.2139
Epoch 18, Train Loss: 1.0564, Val Loss: 1.7933
batch size: (897, 897)
Epoch 19, accuracy: 0.2139
batch size: (903, 903)
Epoch 20, accuracy: 0.4084
Epoch 20, Train Loss: 1.2039, Val Loss: 1.8164
batch size: (897, 897)
Epoch 21, accuracy: 0.4035
batch size: (899, 899)
Epoch 22, accuracy: 0.4020
Epoch 22, Train Loss: 1.0748, Val Loss: 1.8033
batch size: (900, 900)
Epoch 23, accuracy: 0.2139
batch size: (894, 894)
Epoch 24, accuracy: 0.2153
Epoch 24, Train Loss: 1.3843, Val Loss: 1.8931
batch size: (904, 904)
Epoch 25, accuracy: 0.2117
batch size: (895, 895)
Epoch 26, accuracy: 0.3998
Epoch 26, Train Loss: 1.2442, Val Loss: 2.1336
batch size: (913, 913)
Epoch 27, accuracy: 0.4036
batch size: (904, 904)
Epoch 28, accuracy: 0.3976
Epoch 28, Train Loss: 1.0599, Val Loss: 1.6574
batch size: (920, 920)
Epoch 29, accuracy: 0.4012
batch size: (898, 898)
Epoch 30, accuracy: 0.4000
Epoch 30, Train Loss: 1.1038, Val Loss: 1.6279
batch size: (910, 910)
Epoch 31, accuracy: 0.4017
batch size: (890, 890)
Epoch 32, accuracy: 0.4021
Epoch 32, Train Loss: 1.0821, Val Loss: 1.6565
batch size: (897, 897)
Epoch 33, accuracy: 0.4049
batch size: (903, 903)
Epoch 34, accuracy: 0.2141
Epoch 34, Train Loss: 1.7582, Val Loss: 1.9243
batch size: (884, 884)
Epoch 35, accuracy: 0.4019
batch size: (908, 908)
Epoch 36, accuracy: 0.4029
Epoch 36, Train Loss: 1.0778, Val Loss: 1.8898
batch size: (912, 912)
Epoch 37, accuracy: 0.4015
batch size: (903, 903)
Epoch 38, accuracy: 0.4064
Epoch 38, Train Loss: 1.1393, Val Loss: 1.7391
batch size: (888, 888)
Epoch 39, accuracy: 0.2106
batch size: (895, 895)
Epoch 40, accuracy: 0.3979
Epoch 40, Train Loss: 1.0734, Val Loss: 1.7463
batch size: (885, 885)
Epoch 41, accuracy: 0.4012
batch size: (891, 891)
Epoch 42, accuracy: 0.3999
Epoch 42, Train Loss: 1.2669, Val Loss: 1.7647
batch size: (885, 885)
Epoch 43, accuracy: 0.4002
batch size: (907, 907)
Epoch 44, accuracy: 0.4020
Epoch 44, Train Loss: 1.0569, Val Loss: 1.7496
batch size: (898, 898)
Epoch 45, accuracy: 0.4016
batch size: (884, 884)
Epoch 46, accuracy: 0.2160
Epoch 46, Train Loss: 3.0194, Val Loss: 2.3268
batch size: (918, 918)
Epoch 47, accuracy: 0.4020
batch size: (915, 915)
Epoch 48, accuracy: 0.3995
Epoch 48, Train Loss: 1.3620, Val Loss: 1.8420
batch size: (904, 904)
Epoch 49, accuracy: 0.4007
batch size: (901, 901)
Epoch 50, accuracy: 0.4019
Epoch 50, Train Loss: 1.0762, Val Loss: 1.7773
batch size: (891, 891)
Epoch 51, accuracy: 0.4053
batch size: (909, 909)
Epoch 52, accuracy: 0.4027
Epoch 52, Train Loss: 1.0560, Val Loss: 1.7062
batch size: (907, 907)
Epoch 53, accuracy: 0.4013
batch size: (894, 894)
Epoch 54, accuracy: 0.4013
Epoch 54, Train Loss: 1.0582, Val Loss: 1.7733
batch size: (888, 888)
Epoch 55, accuracy: 0.4040
batch size: (913, 913)
Epoch 56, accuracy: 0.4044
Epoch 56, Train Loss: 1.0824, Val Loss: 1.7732
batch size: (897, 897)
Epoch 57, accuracy: 0.4034
batch size: (892, 892)
Epoch 58, accuracy: 0.4022
Epoch 58, Train Loss: 1.0560, Val Loss: 1.7292
batch size: (895, 895)
Epoch 59, accuracy: 0.4029
batch size: (910, 910)
Epoch 60, accuracy: 0.3994
Epoch 60, Train Loss: 1.1109, Val Loss: 1.8761
batch size: (884, 884)
Epoch 61, accuracy: 0.2122
batch size: (913, 913)
Epoch 62, accuracy: 0.4032
Epoch 62, Train Loss: 1.0767, Val Loss: 1.6606
batch size: (889, 889)
Epoch 63, accuracy: 0.4026
batch size: (889, 889)
Epoch 64, accuracy: 0.3994
Epoch 64, Train Loss: 1.3602, Val Loss: 1.8968
batch size: (895, 895)
Epoch 65, accuracy: 0.4000
batch size: (898, 898)
Epoch 66, accuracy: 0.4025
Epoch 66, Train Loss: 1.0580, Val Loss: 1.6952
batch size: (900, 900)
Epoch 67, accuracy: 0.2154
batch size: (905, 905)
Epoch 68, accuracy: 0.4049
Epoch 68, Train Loss: 1.0767, Val Loss: 1.6574
batch size: (905, 905)
Epoch 69, accuracy: 0.3971
batch size: (886, 886)
Epoch 70, accuracy: 0.2143
Epoch 70, Train Loss: 2.1563, Val Loss: 2.1547
batch size: (898, 898)
Epoch 71, accuracy: 0.4101
batch size: (905, 905)
Epoch 72, accuracy: 0.4046
Epoch 72, Train Loss: 1.0803, Val Loss: 1.6776
batch size: (886, 886)
✅ Epoch 73: New best model saved with val_loss = 1.5218
Epoch 73, accuracy: 0.4046
batch size: (896, 896)
Epoch 74, accuracy: 0.4020
Epoch 74, Train Loss: 1.2603, Val Loss: 1.8422
batch size: (900, 900)
Epoch 75, accuracy: 0.4012
batch size: (890, 890)
Epoch 76, accuracy: 0.4055
Epoch 76, Train Loss: 1.0770, Val Loss: 1.6108
batch size: (904, 904)
Epoch 77, accuracy: 0.3990
batch size: (906, 906)
Epoch 78, accuracy: 0.2127
Epoch 78, Train Loss: 2.8539, Val Loss: 1.9996
batch size: (898, 898)
Epoch 79, accuracy: 0.4021
batch size: (895, 895)
Epoch 80, accuracy: 0.4059
Epoch 80, Train Loss: 1.0586, Val Loss: 1.7239
batch size: (899, 899)
Epoch 81, accuracy: 0.2134
batch size: (894, 894)
Epoch 82, accuracy: 0.4020
Epoch 82, Train Loss: 1.0600, Val Loss: 1.6832
batch size: (901, 901)
Epoch 83, accuracy: 0.2153
batch size: (910, 910)
Epoch 84, accuracy: 0.4034
Epoch 84, Train Loss: 1.0842, Val Loss: 1.7883
batch size: (902, 902)
Epoch 85, accuracy: 0.4042
batch size: (910, 910)
Epoch 86, accuracy: 0.4041
Epoch 86, Train Loss: 1.0764, Val Loss: 1.6690
batch size: (903, 903)
Epoch 87, accuracy: 0.4009
batch size: (914, 914)
Epoch 88, accuracy: 0.3996
Epoch 88, Train Loss: 1.0641, Val Loss: 1.6482
batch size: (906, 906)
Epoch 89, accuracy: 0.2139
batch size: (903, 903)
Epoch 90, accuracy: 0.4033
Epoch 90, Train Loss: 1.0900, Val Loss: 1.7444
batch size: (913, 913)
Epoch 91, accuracy: 0.3994
batch size: (915, 915)
Epoch 92, accuracy: 0.4027
Epoch 92, Train Loss: 1.1097, Val Loss: 1.7531
batch size: (880, 880)
Epoch 93, accuracy: 0.4011
batch size: (894, 894)
Epoch 94, accuracy: 0.4004
Epoch 94, Train Loss: 1.1932, Val Loss: 1.7516
batch size: (914, 914)
Epoch 95, accuracy: 0.4058
batch size: (908, 908)
Epoch 96, accuracy: 0.4036
Epoch 96, Train Loss: 1.0808, Val Loss: 1.7379
batch size: (888, 888)
Epoch 97, accuracy: 0.4020
batch size: (897, 897)
Epoch 98, accuracy: 0.4054
Epoch 98, Train Loss: 3.3290, Val Loss: 1.8427
batch size: (923, 923)
Epoch 99, accuracy: 0.4021
batch size: (919, 919)
Epoch 100, accuracy: 0.2139
Epoch 100, Train Loss: 2.8680, Val Loss: 2.3427
batch size: (903, 903)
Epoch 101, accuracy: 0.4007
batch size: (902, 902)
Epoch 102, accuracy: 0.2127
Epoch 102, Train Loss: 2.2706, Val Loss: 2.1143
batch size: (883, 883)
Epoch 103, accuracy: 0.2149
batch size: (915, 915)
Epoch 104, accuracy: 0.4017
Epoch 104, Train Loss: 1.0775, Val Loss: 1.6712
batch size: (884, 884)
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 105, accuracy: 0.3999
batch size: (909, 909)
Epoch 106, accuracy: 0.4030
Epoch 106, Train Loss: 1.0777, Val Loss: 1.6235
batch size: (906, 906)
Epoch 107, accuracy: 0.4005
batch size: (894, 894)
Epoch 108, accuracy: 0.4009
Epoch 108, Train Loss: 1.0627, Val Loss: 1.6579
batch size: (924, 924)
Epoch 109, accuracy: 0.4037
batch size: (909, 909)
Epoch 110, accuracy: 0.4017
Epoch 110, Train Loss: 1.0598, Val Loss: 1.7367
batch size: (893, 893)
Epoch 111, accuracy: 0.4029
batch size: (896, 896)
Epoch 112, accuracy: 0.4031
Epoch 112, Train Loss: 1.0542, Val Loss: 1.6707
batch size: (905, 905)
Epoch 113, accuracy: 0.2118
batch size: (904, 904)
Epoch 114, accuracy: 0.2138
Epoch 114, Train Loss: 1.9123, Val Loss: 2.4781
batch size: (895, 895)
Epoch 115, accuracy: 0.2120
batch size: (911, 911)
Epoch 116, accuracy: 0.4035
Epoch 116, Train Loss: 1.0775, Val Loss: 1.8748
batch size: (905, 905)
Epoch 117, accuracy: 0.2135
batch size: (902, 902)
Epoch 118, accuracy: 0.4025
Epoch 118, Train Loss: 1.0594, Val Loss: 1.6667
batch size: (906, 906)
Epoch 119, accuracy: 0.4047
batch size: (917, 917)
Epoch 120, accuracy: 0.4031
Epoch 120, Train Loss: 1.1934, Val Loss: 1.8937
batch size: (919, 919)
Epoch 121, accuracy: 0.3973
batch size: (914, 914)
Epoch 122, accuracy: 0.2150
Epoch 122, Train Loss: 3.0279, Val Loss: 2.4045
batch size: (889, 889)
Epoch 123, accuracy: 0.4008
batch size: (878, 878)
Epoch 124, accuracy: 0.4011
Epoch 124, Train Loss: 3.3033, Val Loss: 2.0824
batch size: (896, 896)
Epoch 125, accuracy: 0.3966
batch size: (906, 906)
Epoch 126, accuracy: 0.2151
Epoch 126, Train Loss: 2.9965, Val Loss: 2.2600
batch size: (896, 896)
Epoch 127, accuracy: 0.4026
batch size: (897, 897)
Epoch 128, accuracy: 0.4039
Epoch 128, Train Loss: 1.0817, Val Loss: 1.5752
batch size: (893, 893)
Epoch 129, accuracy: 0.2142
batch size: (903, 903)
Epoch 130, accuracy: 0.2144
Epoch 130, Train Loss: 1.7383, Val Loss: 1.9835
batch size: (895, 895)
Epoch 131, accuracy: 0.2144
batch size: (904, 904)
Epoch 132, accuracy: 0.4062
Epoch 132, Train Loss: 1.1020, Val Loss: 1.6934
batch size: (896, 896)
Epoch 133, accuracy: 0.4045
batch size: (907, 907)
Epoch 134, accuracy: 0.4033
Epoch 134, Train Loss: 1.0791, Val Loss: 1.5809
batch size: (913, 913)
Epoch 135, accuracy: 0.4008
batch size: (899, 899)
Epoch 136, accuracy: 0.2149
Epoch 136, Train Loss: 2.2490, Val Loss: 1.9893
batch size: (893, 893)
Epoch 137, accuracy: 0.2153
batch size: (891, 891)
Epoch 138, accuracy: 0.4033
Epoch 138, Train Loss: 1.0771, Val Loss: 1.5454
batch size: (903, 903)
Epoch 139, accuracy: 0.4024
batch size: (900, 900)
Epoch 140, accuracy: 0.4033
Epoch 140, Train Loss: 1.0757, Val Loss: 1.6473
batch size: (899, 899)
Epoch 141, accuracy: 0.3980
batch size: (891, 891)
Epoch 142, accuracy: 0.2145
Epoch 142, Train Loss: 2.9609, Val Loss: 2.1395
batch size: (899, 899)
Epoch 143, accuracy: 0.4015
batch size: (898, 898)
Epoch 144, accuracy: 0.3999
Epoch 144, Train Loss: 1.1246, Val Loss: 1.8947
batch size: (908, 908)
Epoch 145, accuracy: 0.2142
batch size: (892, 892)
Epoch 146, accuracy: 0.4023
Epoch 146, Train Loss: 1.0745, Val Loss: 1.7302
batch size: (909, 909)
Epoch 147, accuracy: 0.2134
batch size: (906, 906)
Epoch 148, accuracy: 0.4032
Epoch 148, Train Loss: 3.3410, Val Loss: 1.9217
batch size: (901, 901)
Epoch 149, accuracy: 0.4002
batch size: (898, 898)
Epoch 150, accuracy: 0.4019
Epoch 150, Train Loss: 3.2294, Val Loss: 1.8693
batch size: (918, 918)
Epoch 151, accuracy: 0.2161
batch size: (899, 899)
Epoch 152, accuracy: 0.4035
Epoch 152, Train Loss: 3.2160, Val Loss: 1.8751
batch size: (881, 881)
Epoch 153, accuracy: 0.2134
batch size: (875, 875)
Epoch 154, accuracy: 0.2137
Epoch 154, Train Loss: 2.3444, Val Loss: 2.0411
batch size: (898, 898)
Epoch 155, accuracy: 0.4020
batch size: (874, 874)
Epoch 156, accuracy: 0.4027
Epoch 156, Train Loss: 1.0764, Val Loss: 1.7387
batch size: (891, 891)
Epoch 157, accuracy: 0.4031
batch size: (899, 899)
Epoch 158, accuracy: 0.4006
Epoch 158, Train Loss: 1.0908, Val Loss: 1.5435
batch size: (887, 887)
Epoch 159, accuracy: 0.4015
batch size: (902, 902)
Epoch 160, accuracy: 0.4016
Epoch 160, Train Loss: 1.0565, Val Loss: 1.5667
batch size: (903, 903)
Epoch 161, accuracy: 0.4026
batch size: (913, 913)
Epoch 162, accuracy: 0.4034
Epoch 162, Train Loss: 1.6020, Val Loss: 2.1508
batch size: (887, 887)
Epoch 163, accuracy: 0.2158
batch size: (923, 923)
Epoch 164, accuracy: 0.2128
Epoch 164, Train Loss: 3.1098, Val Loss: 1.9585
batch size: (915, 915)
Epoch 165, accuracy: 0.4047
batch size: (899, 899)
Epoch 166, accuracy: 0.4053
Epoch 166, Train Loss: 1.0634, Val Loss: 1.5766
batch size: (905, 905)
Epoch 167, accuracy: 0.2131
batch size: (893, 893)
Epoch 168, accuracy: 0.2146
Epoch 168, Train Loss: 2.7054, Val Loss: 2.0926
batch size: (917, 917)
Epoch 169, accuracy: 0.4063
batch size: (885, 885)
Epoch 170, accuracy: 0.2157
Epoch 170, Train Loss: 2.5565, Val Loss: 2.4395
batch size: (902, 902)
Epoch 171, accuracy: 0.2131
batch size: (901, 901)
Epoch 172, accuracy: 0.4047
Epoch 172, Train Loss: 1.0555, Val Loss: 1.6820
batch size: (906, 906)
Epoch 173, accuracy: 0.2147
batch size: (887, 887)
Epoch 174, accuracy: 0.2139
Epoch 174, Train Loss: 2.2092, Val Loss: 2.1154
batch size: (907, 907)
Epoch 175, accuracy: 0.4037
batch size: (906, 906)
Epoch 176, accuracy: 0.4015
Epoch 176, Train Loss: 1.1372, Val Loss: 1.5882
batch size: (892, 892)
Epoch 177, accuracy: 0.4019
batch size: (909, 909)
Epoch 178, accuracy: 0.2166
Epoch 178, Train Loss: 2.3451, Val Loss: 2.2851
batch size: (911, 911)
Epoch 179, accuracy: 0.4036
batch size: (897, 897)
Epoch 180, accuracy: 0.4053
Epoch 180, Train Loss: 1.0629, Val Loss: 1.8066
batch size: (907, 907)
Epoch 181, accuracy: 0.4044
batch size: (914, 914)
Epoch 182, accuracy: 0.4026
Epoch 182, Train Loss: 3.1170, Val Loss: 1.7863
batch size: (895, 895)
Epoch 183, accuracy: 0.4053
batch size: (919, 919)
Epoch 184, accuracy: 0.4036
Epoch 184, Train Loss: 1.2632, Val Loss: 2.0804
batch size: (915, 915)
Epoch 185, accuracy: 0.4072
batch size: (918, 918)
Epoch 186, accuracy: 0.3987
Epoch 186, Train Loss: 1.2452, Val Loss: 1.9323
batch size: (899, 899)
Epoch 187, accuracy: 0.3990
batch size: (906, 906)
Epoch 188, accuracy: 0.4069
Epoch 188, Train Loss: 1.0787, Val Loss: 1.8783
batch size: (892, 892)
Epoch 189, accuracy: 0.4046
batch size: (906, 906)
Epoch 190, accuracy: 0.2131
Epoch 190, Train Loss: 2.6473, Val Loss: 2.1109
batch size: (915, 915)
Epoch 191, accuracy: 0.4051
batch size: (879, 879)
Epoch 192, accuracy: 0.3997
Epoch 192, Train Loss: 1.0757, Val Loss: 1.8146
batch size: (923, 923)
Epoch 193, accuracy: 0.4041
batch size: (904, 904)
Epoch 194, accuracy: 0.4038
Epoch 194, Train Loss: 1.0548, Val Loss: 1.7117
batch size: (903, 903)
Epoch 195, accuracy: 0.4054
batch size: (899, 899)
Epoch 196, accuracy: 0.4013
Epoch 196, Train Loss: 1.2761, Val Loss: 1.8908
batch size: (913, 913)
Epoch 197, accuracy: 0.4010
batch size: (919, 919)
Epoch 198, accuracy: 0.4055
Epoch 198, Train Loss: 1.0774, Val Loss: 1.8274
batch size: (883, 883)
Epoch 199, accuracy: 0.4027
Loaded best model with val_loss = 1.521826982498169
test :accuracy 0.4018, f1_macro: 0.1911, f1_micro: 0.4018, auc: 0.5030
Training mlp with 2 layers...
可训练参数: 83465_mlp
不可训练参数: 0
batch size: (896, 896)
✅ Epoch 0: New best model saved with val_loss = 1.0810
Epoch 0, accuracy: 0.4133
Epoch 0, Train Loss: 1.1646, Val Loss: 1.0810
batch size: (899, 899)
✅ Epoch 1: New best model saved with val_loss = 1.0772
Epoch 1, accuracy: 0.4507
batch size: (908, 908)
✅ Epoch 2: New best model saved with val_loss = 1.0729
Epoch 2, accuracy: 0.4698
Epoch 2, Train Loss: 0.2035, Val Loss: 1.0729
batch size: (899, 899)
✅ Epoch 3: New best model saved with val_loss = 1.0674
Epoch 3, accuracy: 0.4958
batch size: (903, 903)
✅ Epoch 4: New best model saved with val_loss = 1.0611
Epoch 4, accuracy: 0.5108
Epoch 4, Train Loss: 0.0256, Val Loss: 1.0611
batch size: (882, 882)
✅ Epoch 5: New best model saved with val_loss = 1.0541
Epoch 5, accuracy: 0.5173
batch size: (880, 880)
✅ Epoch 6: New best model saved with val_loss = 1.0474
Epoch 6, accuracy: 0.5280
Epoch 6, Train Loss: 0.0043, Val Loss: 1.0474
batch size: (888, 888)
✅ Epoch 7: New best model saved with val_loss = 1.0412
Epoch 7, accuracy: 0.5297
batch size: (881, 881)
✅ Epoch 8: New best model saved with val_loss = 1.0357
Epoch 8, accuracy: 0.5371
Epoch 8, Train Loss: 0.0012, Val Loss: 1.0357
batch size: (897, 897)
✅ Epoch 9: New best model saved with val_loss = 1.0306
Epoch 9, accuracy: 0.5416
batch size: (892, 892)
✅ Epoch 10: New best model saved with val_loss = 1.0256
Epoch 10, accuracy: 0.5386
Epoch 10, Train Loss: 0.0006, Val Loss: 1.0256
batch size: (898, 898)
✅ Epoch 11: New best model saved with val_loss = 1.0207
Epoch 11, accuracy: 0.5447
batch size: (894, 894)
✅ Epoch 12: New best model saved with val_loss = 1.0157
Epoch 12, accuracy: 0.5506
Epoch 12, Train Loss: 0.0003, Val Loss: 1.0157
batch size: (889, 889)
✅ Epoch 13: New best model saved with val_loss = 1.0105
Epoch 13, accuracy: 0.5589
batch size: (886, 886)
✅ Epoch 14: New best model saved with val_loss = 1.0050
Epoch 14, accuracy: 0.5635
Epoch 14, Train Loss: 0.0002, Val Loss: 1.0050
batch size: (925, 925)
✅ Epoch 15: New best model saved with val_loss = 0.9993
Epoch 15, accuracy: 0.5699
batch size: (923, 923)
✅ Epoch 16: New best model saved with val_loss = 0.9933
Epoch 16, accuracy: 0.5821
Epoch 16, Train Loss: 0.0001, Val Loss: 0.9933
batch size: (926, 926)
✅ Epoch 17: New best model saved with val_loss = 0.9870
Epoch 17, accuracy: 0.5900
batch size: (893, 893)
✅ Epoch 18: New best model saved with val_loss = 0.9803
Epoch 18, accuracy: 0.5998
Epoch 18, Train Loss: 0.0001, Val Loss: 0.9803
batch size: (910, 910)
✅ Epoch 19: New best model saved with val_loss = 0.9732
Epoch 19, accuracy: 0.6136
batch size: (889, 889)
✅ Epoch 20: New best model saved with val_loss = 0.9658
Epoch 20, accuracy: 0.6212
Epoch 20, Train Loss: 0.0001, Val Loss: 0.9658
batch size: (916, 916)
✅ Epoch 21: New best model saved with val_loss = 0.9581
Epoch 21, accuracy: 0.6261
batch size: (917, 917)
✅ Epoch 22: New best model saved with val_loss = 0.9500
Epoch 22, accuracy: 0.6357
Epoch 22, Train Loss: 0.0000, Val Loss: 0.9500
batch size: (884, 884)
✅ Epoch 23: New best model saved with val_loss = 0.9417
Epoch 23, accuracy: 0.6408
batch size: (913, 913)
✅ Epoch 24: New best model saved with val_loss = 0.9331
Epoch 24, accuracy: 0.6476
Epoch 24, Train Loss: 0.0000, Val Loss: 0.9331
batch size: (903, 903)
✅ Epoch 25: New best model saved with val_loss = 0.9244
Epoch 25, accuracy: 0.6534
batch size: (887, 887)
✅ Epoch 26: New best model saved with val_loss = 0.9157
Epoch 26, accuracy: 0.6632
Epoch 26, Train Loss: 0.0000, Val Loss: 0.9157
batch size: (906, 906)
✅ Epoch 27: New best model saved with val_loss = 0.9068
Epoch 27, accuracy: 0.6662
batch size: (876, 876)
✅ Epoch 28: New best model saved with val_loss = 0.8980
Epoch 28, accuracy: 0.6697
Epoch 28, Train Loss: 0.0000, Val Loss: 0.8980
batch size: (897, 897)
✅ Epoch 29: New best model saved with val_loss = 0.8893
Epoch 29, accuracy: 0.6734
batch size: (894, 894)
✅ Epoch 30: New best model saved with val_loss = 0.8808
Epoch 30, accuracy: 0.6733
Epoch 30, Train Loss: 0.0000, Val Loss: 0.8808
batch size: (910, 910)
✅ Epoch 31: New best model saved with val_loss = 0.8725
Epoch 31, accuracy: 0.6758
batch size: (907, 907)
✅ Epoch 32: New best model saved with val_loss = 0.8644
Epoch 32, accuracy: 0.6761
Epoch 32, Train Loss: 0.0000, Val Loss: 0.8644
batch size: (916, 916)
✅ Epoch 33: New best model saved with val_loss = 0.8566
Epoch 33, accuracy: 0.6791
batch size: (891, 891)
✅ Epoch 34: New best model saved with val_loss = 0.8489
Epoch 34, accuracy: 0.6803
Epoch 34, Train Loss: 0.0000, Val Loss: 0.8489
batch size: (903, 903)
✅ Epoch 35: New best model saved with val_loss = 0.8414
Epoch 35, accuracy: 0.6778
batch size: (901, 901)
✅ Epoch 36: New best model saved with val_loss = 0.8340
Epoch 36, accuracy: 0.6767
Epoch 36, Train Loss: 0.0000, Val Loss: 0.8340
batch size: (906, 906)
✅ Epoch 37: New best model saved with val_loss = 0.8270
Epoch 37, accuracy: 0.6793
batch size: (912, 912)
✅ Epoch 38: New best model saved with val_loss = 0.8202
Epoch 38, accuracy: 0.6794
Epoch 38, Train Loss: 0.0000, Val Loss: 0.8202
batch size: (897, 897)
✅ Epoch 39: New best model saved with val_loss = 0.8137
Epoch 39, accuracy: 0.6807
batch size: (917, 917)
✅ Epoch 40: New best model saved with val_loss = 0.8076
Epoch 40, accuracy: 0.6814
Epoch 40, Train Loss: 0.0000, Val Loss: 0.8076
batch size: (910, 910)
✅ Epoch 41: New best model saved with val_loss = 0.8017
Epoch 41, accuracy: 0.6801
batch size: (901, 901)
✅ Epoch 42: New best model saved with val_loss = 0.7964
Epoch 42, accuracy: 0.6829
Epoch 42, Train Loss: 0.0000, Val Loss: 0.7964
batch size: (898, 898)
✅ Epoch 43: New best model saved with val_loss = 0.7916
Epoch 43, accuracy: 0.6818
batch size: (910, 910)
✅ Epoch 44: New best model saved with val_loss = 0.7870
Epoch 44, accuracy: 0.6821
Epoch 44, Train Loss: 0.0000, Val Loss: 0.7870
batch size: (903, 903)
✅ Epoch 45: New best model saved with val_loss = 0.7828
Epoch 45, accuracy: 0.6832
batch size: (897, 897)
✅ Epoch 46: New best model saved with val_loss = 0.7792
Epoch 46, accuracy: 0.6847
Epoch 46, Train Loss: 0.0001, Val Loss: 0.7792
batch size: (890, 890)
✅ Epoch 47: New best model saved with val_loss = 0.7760
Epoch 47, accuracy: 0.6857
batch size: (892, 892)
✅ Epoch 48: New best model saved with val_loss = 0.7733
Epoch 48, accuracy: 0.6862
Epoch 48, Train Loss: 0.0001, Val Loss: 0.7733
batch size: (896, 896)
✅ Epoch 49: New best model saved with val_loss = 0.7712
Epoch 49, accuracy: 0.6830
batch size: (904, 904)
✅ Epoch 50: New best model saved with val_loss = 0.7695
Epoch 50, accuracy: 0.6841
Epoch 50, Train Loss: 0.0001, Val Loss: 0.7695
batch size: (899, 899)
✅ Epoch 51: New best model saved with val_loss = 0.7683
Epoch 51, accuracy: 0.6851
batch size: (890, 890)
✅ Epoch 52: New best model saved with val_loss = 0.7675
Epoch 52, accuracy: 0.6836
Epoch 52, Train Loss: 0.0001, Val Loss: 0.7675
batch size: (904, 904)
✅ Epoch 53: New best model saved with val_loss = 0.7675
Epoch 53, accuracy: 0.6833
batch size: (897, 897)
Epoch 54, accuracy: 0.6836
Epoch 54, Train Loss: 0.0001, Val Loss: 0.7679
batch size: (895, 895)
Epoch 55, accuracy: 0.6872
batch size: (875, 875)
Epoch 56, accuracy: 0.6864
Epoch 56, Train Loss: 0.0001, Val Loss: 0.7702
batch size: (896, 896)
Epoch 57, accuracy: 0.6846
batch size: (884, 884)
Epoch 58, accuracy: 0.6864
Epoch 58, Train Loss: 0.0001, Val Loss: 0.7744
batch size: (899, 899)
Epoch 59, accuracy: 0.6844
batch size: (902, 902)
Epoch 60, accuracy: 0.6872
Epoch 60, Train Loss: 0.0001, Val Loss: 0.7833
batch size: (914, 914)
Epoch 61, accuracy: 0.6854
batch size: (887, 887)
Epoch 62, accuracy: 0.6862
Epoch 62, Train Loss: 0.0001, Val Loss: 0.7950
batch size: (909, 909)
Epoch 63, accuracy: 0.6873
batch size: (891, 891)
Epoch 64, accuracy: 0.6863
Epoch 64, Train Loss: 0.0001, Val Loss: 0.8091
batch size: (915, 915)
Epoch 65, accuracy: 0.6865
batch size: (897, 897)
Epoch 66, accuracy: 0.6857
Epoch 66, Train Loss: 0.0001, Val Loss: 0.8263
batch size: (881, 881)
Epoch 67, accuracy: 0.6868
batch size: (897, 897)
Epoch 68, accuracy: 0.6858
Epoch 68, Train Loss: 0.0001, Val Loss: 0.8455
batch size: (912, 912)
Epoch 69, accuracy: 0.6865
batch size: (893, 893)
Epoch 70, accuracy: 0.6852
Epoch 70, Train Loss: 0.0001, Val Loss: 0.8662
batch size: (911, 911)
Epoch 71, accuracy: 0.6851
batch size: (898, 898)
Epoch 72, accuracy: 0.6861
Epoch 72, Train Loss: 0.0001, Val Loss: 0.8884
batch size: (887, 887)
Epoch 73, accuracy: 0.6864
batch size: (903, 903)
Epoch 74, accuracy: 0.6855
Epoch 74, Train Loss: 0.0001, Val Loss: 0.9106
batch size: (888, 888)
Epoch 75, accuracy: 0.6860
batch size: (909, 909)
Epoch 76, accuracy: 0.6863
Epoch 76, Train Loss: 0.0001, Val Loss: 0.9323
batch size: (875, 875)
Epoch 77, accuracy: 0.6856
batch size: (905, 905)
Epoch 78, accuracy: 0.6844
Epoch 78, Train Loss: 0.0001, Val Loss: 0.9533
batch size: (907, 907)
Epoch 79, accuracy: 0.6868
batch size: (901, 901)
Epoch 80, accuracy: 0.6856
Epoch 80, Train Loss: 0.0001, Val Loss: 0.9737
batch size: (888, 888)
Epoch 81, accuracy: 0.6866
batch size: (890, 890)
Epoch 82, accuracy: 0.6877
Epoch 82, Train Loss: 0.0001, Val Loss: 0.9927
batch size: (908, 908)
Epoch 83, accuracy: 0.6872
batch size: (898, 898)
Epoch 84, accuracy: 0.6877
Epoch 84, Train Loss: 0.0001, Val Loss: 1.0114
batch size: (886, 886)
Epoch 85, accuracy: 0.6865
batch size: (898, 898)
Epoch 86, accuracy: 0.6866
Epoch 86, Train Loss: 0.0001, Val Loss: 1.0281
batch size: (901, 901)
Epoch 87, accuracy: 0.6851
batch size: (898, 898)
Epoch 88, accuracy: 0.6847
Epoch 88, Train Loss: 0.0001, Val Loss: 1.0413
batch size: (904, 904)
Epoch 89, accuracy: 0.6856
batch size: (918, 918)
Epoch 90, accuracy: 0.6874
Epoch 90, Train Loss: 0.0001, Val Loss: 1.0545
batch size: (905, 905)
Epoch 91, accuracy: 0.6869
batch size: (903, 903)
Epoch 92, accuracy: 0.6849
Epoch 92, Train Loss: 0.0001, Val Loss: 1.0649
batch size: (902, 902)
Epoch 93, accuracy: 0.6892
batch size: (902, 902)
Epoch 94, accuracy: 0.6840
Epoch 94, Train Loss: 0.0001, Val Loss: 1.0744
batch size: (907, 907)
Epoch 95, accuracy: 0.6869
batch size: (910, 910)
Epoch 96, accuracy: 0.6872
Epoch 96, Train Loss: 0.0001, Val Loss: 1.0819
batch size: (884, 884)
Epoch 97, accuracy: 0.6870
batch size: (894, 894)
Epoch 98, accuracy: 0.6860
Epoch 98, Train Loss: 0.0001, Val Loss: 1.0863
batch size: (893, 893)
Epoch 99, accuracy: 0.6874
batch size: (910, 910)
Epoch 100, accuracy: 0.6897
Epoch 100, Train Loss: 0.0001, Val Loss: 1.0922
batch size: (891, 891)
Epoch 101, accuracy: 0.6870
batch size: (895, 895)
Epoch 102, accuracy: 0.6876
Epoch 102, Train Loss: 0.0001, Val Loss: 1.0968
batch size: (914, 914)
Epoch 103, accuracy: 0.6865
batch size: (904, 904)
Epoch 104, accuracy: 0.6850
Epoch 104, Train Loss: 0.0001, Val Loss: 1.1017
batch size: (902, 902)
Epoch 105, accuracy: 0.6854
batch size: (893, 893)
Epoch 106, accuracy: 0.6875
Epoch 106, Train Loss: 0.0001, Val Loss: 1.1057
batch size: (892, 892)
Epoch 107, accuracy: 0.6872
batch size: (896, 896)
Epoch 108, accuracy: 0.6855
Epoch 108, Train Loss: 0.0001, Val Loss: 1.1077
batch size: (894, 894)
Epoch 109, accuracy: 0.6870
batch size: (905, 905)
Epoch 110, accuracy: 0.6865
Epoch 110, Train Loss: 0.0001, Val Loss: 1.1080
batch size: (901, 901)
Epoch 111, accuracy: 0.6860
batch size: (905, 905)
Epoch 112, accuracy: 0.6859
Epoch 112, Train Loss: 0.0001, Val Loss: 1.1102
batch size: (891, 891)
Epoch 113, accuracy: 0.6877
batch size: (895, 895)
Epoch 114, accuracy: 0.6875
Epoch 114, Train Loss: 0.0001, Val Loss: 1.1117
batch size: (903, 903)
Epoch 115, accuracy: 0.6867
batch size: (925, 925)
Epoch 116, accuracy: 0.6871
Epoch 116, Train Loss: 0.0001, Val Loss: 1.1130
batch size: (900, 900)
Epoch 117, accuracy: 0.6874
batch size: (923, 923)
Epoch 118, accuracy: 0.6859
Epoch 118, Train Loss: 0.0001, Val Loss: 1.1141
batch size: (914, 914)
Epoch 119, accuracy: 0.6844
batch size: (902, 902)
Epoch 120, accuracy: 0.6872
Epoch 120, Train Loss: 0.0001, Val Loss: 1.1146
batch size: (913, 913)
Epoch 121, accuracy: 0.6867
batch size: (895, 895)
Epoch 122, accuracy: 0.6862
Epoch 122, Train Loss: 0.0001, Val Loss: 1.1144
batch size: (912, 912)
Epoch 123, accuracy: 0.6866
batch size: (901, 901)
Epoch 124, accuracy: 0.6867
Epoch 124, Train Loss: 0.0001, Val Loss: 1.1149
batch size: (899, 899)
Epoch 125, accuracy: 0.6852
batch size: (903, 903)
Epoch 126, accuracy: 0.6865
Epoch 126, Train Loss: 0.0001, Val Loss: 1.1135
batch size: (911, 911)
Epoch 127, accuracy: 0.6860
batch size: (904, 904)
Epoch 128, accuracy: 0.6882
Epoch 128, Train Loss: 0.0001, Val Loss: 1.1139
batch size: (901, 901)
Epoch 129, accuracy: 0.6877
batch size: (902, 902)
Epoch 130, accuracy: 0.6859
Epoch 130, Train Loss: 0.0001, Val Loss: 1.1147
batch size: (904, 904)
Epoch 131, accuracy: 0.6874
batch size: (897, 897)
Epoch 132, accuracy: 0.6856
Epoch 132, Train Loss: 0.0001, Val Loss: 1.1152
batch size: (880, 880)
Epoch 133, accuracy: 0.6854
batch size: (889, 889)
Epoch 134, accuracy: 0.6858
Epoch 134, Train Loss: 0.0001, Val Loss: 1.1158
batch size: (904, 904)
Epoch 135, accuracy: 0.6851
batch size: (892, 892)
Epoch 136, accuracy: 0.6857
Epoch 136, Train Loss: 0.0001, Val Loss: 1.1157
batch size: (898, 898)
Epoch 137, accuracy: 0.6855
batch size: (907, 907)
Epoch 138, accuracy: 0.6879
Epoch 138, Train Loss: 0.0001, Val Loss: 1.1165
batch size: (912, 912)
Epoch 139, accuracy: 0.6868
batch size: (905, 905)
Epoch 140, accuracy: 0.6865
Epoch 140, Train Loss: 0.0001, Val Loss: 1.1177
batch size: (895, 895)
Epoch 141, accuracy: 0.6874
batch size: (903, 903)
Epoch 142, accuracy: 0.6868
Epoch 142, Train Loss: 0.0001, Val Loss: 1.1184
batch size: (893, 893)
Epoch 143, accuracy: 0.6881
batch size: (899, 899)
Epoch 144, accuracy: 0.6865
Epoch 144, Train Loss: 0.0001, Val Loss: 1.1186
batch size: (894, 894)
Epoch 145, accuracy: 0.6839
batch size: (887, 887)
Epoch 146, accuracy: 0.6871
Epoch 146, Train Loss: 0.0001, Val Loss: 1.1187
batch size: (889, 889)
Epoch 147, accuracy: 0.6877
batch size: (911, 911)
Epoch 148, accuracy: 0.6870
Epoch 148, Train Loss: 0.0001, Val Loss: 1.1185
batch size: (875, 875)
Epoch 149, accuracy: 0.6878
batch size: (902, 902)
Epoch 150, accuracy: 0.6867
Epoch 150, Train Loss: 0.0001, Val Loss: 1.1184
batch size: (894, 894)
Epoch 151, accuracy: 0.6860
batch size: (889, 889)
Epoch 152, accuracy: 0.6847
Epoch 152, Train Loss: 0.0001, Val Loss: 1.1193
batch size: (907, 907)
Epoch 153, accuracy: 0.6872
batch size: (901, 901)
Epoch 154, accuracy: 0.6861
Epoch 154, Train Loss: 0.0001, Val Loss: 1.1200
batch size: (899, 899)
Epoch 155, accuracy: 0.6873
batch size: (883, 883)
Epoch 156, accuracy: 0.6852
Epoch 156, Train Loss: 0.0001, Val Loss: 1.1208
batch size: (898, 898)
Epoch 157, accuracy: 0.6860
batch size: (885, 885)
Epoch 158, accuracy: 0.6858
Epoch 158, Train Loss: 0.0001, Val Loss: 1.1194
batch size: (897, 897)
Epoch 159, accuracy: 0.6848
batch size: (903, 903)
Epoch 160, accuracy: 0.6861
Epoch 160, Train Loss: 0.0001, Val Loss: 1.1197
batch size: (862, 862)
Epoch 161, accuracy: 0.6862
batch size: (915, 915)
Epoch 162, accuracy: 0.6871
Epoch 162, Train Loss: 0.0001, Val Loss: 1.1202
batch size: (886, 886)
Epoch 163, accuracy: 0.6873
batch size: (901, 901)
Epoch 164, accuracy: 0.6856
Epoch 164, Train Loss: 0.0001, Val Loss: 1.1196
batch size: (898, 898)
Epoch 165, accuracy: 0.6874
batch size: (884, 884)
Epoch 166, accuracy: 0.6878
Epoch 166, Train Loss: 0.0001, Val Loss: 1.1184
batch size: (869, 869)
Epoch 167, accuracy: 0.6849
batch size: (913, 913)
Epoch 168, accuracy: 0.6860
Epoch 168, Train Loss: 0.0001, Val Loss: 1.1174
batch size: (900, 900)
Epoch 169, accuracy: 0.6857
batch size: (876, 876)
Epoch 170, accuracy: 0.6854
Epoch 170, Train Loss: 0.0001, Val Loss: 1.1174
batch size: (902, 902)
Epoch 171, accuracy: 0.6870
batch size: (907, 907)
Epoch 172, accuracy: 0.6856
Epoch 172, Train Loss: 0.0001, Val Loss: 1.1196
batch size: (888, 888)
Epoch 173, accuracy: 0.6853
batch size: (902, 902)
Epoch 174, accuracy: 0.6885
Epoch 174, Train Loss: 0.0001, Val Loss: 1.1186
batch size: (892, 892)
Epoch 175, accuracy: 0.6857
batch size: (903, 903)
Epoch 176, accuracy: 0.6858
Epoch 176, Train Loss: 0.0001, Val Loss: 1.1194
batch size: (904, 904)
Epoch 177, accuracy: 0.6865
batch size: (890, 890)
Epoch 178, accuracy: 0.6859
Epoch 178, Train Loss: 0.0001, Val Loss: 1.1189
batch size: (897, 897)
Epoch 179, accuracy: 0.6857
batch size: (884, 884)
Epoch 180, accuracy: 0.6861
Epoch 180, Train Loss: 0.0001, Val Loss: 1.1197
batch size: (899, 899)
Epoch 181, accuracy: 0.6865
batch size: (899, 899)
Epoch 182, accuracy: 0.6871
Epoch 182, Train Loss: 0.0001, Val Loss: 1.1197
batch size: (906, 906)
Epoch 183, accuracy: 0.6882
batch size: (897, 897)
Epoch 184, accuracy: 0.6866
Epoch 184, Train Loss: 0.0001, Val Loss: 1.1186
batch size: (901, 901)
Epoch 185, accuracy: 0.6846
batch size: (886, 886)
Epoch 186, accuracy: 0.6863
Epoch 186, Train Loss: 0.0001, Val Loss: 1.1173
batch size: (890, 890)
Epoch 187, accuracy: 0.6870
batch size: (894, 894)
Epoch 188, accuracy: 0.6857
Epoch 188, Train Loss: 0.0001, Val Loss: 1.1177
batch size: (890, 890)
Epoch 189, accuracy: 0.6857
batch size: (903, 903)/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))

Epoch 190, accuracy: 0.6853
Epoch 190, Train Loss: 0.0001, Val Loss: 1.1176
batch size: (914, 914)
Epoch 191, accuracy: 0.6867
batch size: (888, 888)
Epoch 192, accuracy: 0.6863
Epoch 192, Train Loss: 0.0001, Val Loss: 1.1171
batch size: (902, 902)
Epoch 193, accuracy: 0.6846
batch size: (907, 907)
Epoch 194, accuracy: 0.6850
Epoch 194, Train Loss: 0.0001, Val Loss: 1.1169
batch size: (902, 902)
Epoch 195, accuracy: 0.6858
batch size: (924, 924)
Epoch 196, accuracy: 0.6862
Epoch 196, Train Loss: 0.0001, Val Loss: 1.1165
batch size: (892, 892)
Epoch 197, accuracy: 0.6869
batch size: (894, 894)
Epoch 198, accuracy: 0.6862
Epoch 198, Train Loss: 0.0001, Val Loss: 1.1164
batch size: (904, 904)
Epoch 199, accuracy: 0.6866
Loaded best model with val_loss = 0.767487108707428
test :accuracy 0.6846, f1_macro: 0.6819, f1_micro: 0.6846, auc: 0.8369
Training mlp with 8 layers...
可训练参数: 186377_mlp
不可训练参数: 0
batch size: (904, 904)
✅ Epoch 0: New best model saved with val_loss = 1.1133
Epoch 0, accuracy: 0.1695
Epoch 0, Train Loss: 1.1419, Val Loss: 1.1133
batch size: (904, 904)
Epoch 1, accuracy: 0.1631
batch size: (903, 903)
Epoch 2, accuracy: 0.1662
Epoch 2, Train Loss: 0.3747, Val Loss: 1.1204
batch size: (904, 904)
Epoch 3, accuracy: 0.1645
batch size: (887, 887)
Epoch 4, accuracy: 0.1682
Epoch 4, Train Loss: 0.0270, Val Loss: 1.1224
batch size: (885, 885)
Epoch 5, accuracy: 0.1669
batch size: (885, 885)
Epoch 6, accuracy: 0.1649
Epoch 6, Train Loss: 0.0043, Val Loss: 1.1254
batch size: (892, 892)
Epoch 7, accuracy: 0.1668
batch size: (915, 915)
Epoch 8, accuracy: 0.1702
Epoch 8, Train Loss: 0.0022, Val Loss: 1.1288
batch size: (899, 899)
Epoch 9, accuracy: 0.1706
batch size: (911, 911)
Epoch 10, accuracy: 0.1702
Epoch 10, Train Loss: 0.0017, Val Loss: 1.1316
batch size: (892, 892)
Epoch 11, accuracy: 0.1670
batch size: (896, 896)
Epoch 12, accuracy: 0.1660
Epoch 12, Train Loss: 0.0013, Val Loss: 1.1339
batch size: (891, 891)
Epoch 13, accuracy: 0.1682
batch size: (893, 893)
Epoch 14, accuracy: 0.1660
Epoch 14, Train Loss: 0.0012, Val Loss: 1.1341
batch size: (905, 905)
Epoch 15, accuracy: 0.1682
batch size: (923, 923)
Epoch 16, accuracy: 0.1666
Epoch 16, Train Loss: 0.0011, Val Loss: 1.1348
batch size: (890, 890)
Epoch 17, accuracy: 0.1678
batch size: (900, 900)
Epoch 18, accuracy: 0.1666
Epoch 18, Train Loss: 0.0013, Val Loss: 1.1352
batch size: (906, 906)
Epoch 19, accuracy: 0.1682
batch size: (902, 902)
Epoch 20, accuracy: 0.1678
Epoch 20, Train Loss: 0.0013, Val Loss: 1.1350
batch size: (878, 878)
Epoch 21, accuracy: 0.1702
batch size: (913, 913)
Epoch 22, accuracy: 0.1684
Epoch 22, Train Loss: 0.0012, Val Loss: 1.1358
batch size: (907, 907)
Epoch 23, accuracy: 0.1643
batch size: (888, 888)
Epoch 24, accuracy: 0.1759
Epoch 24, Train Loss: 0.0013, Val Loss: 1.1345
batch size: (896, 896)
Epoch 25, accuracy: 0.2378
batch size: (904, 904)
Epoch 26, accuracy: 0.3500
Epoch 26, Train Loss: 0.0011, Val Loss: 1.1344
batch size: (904, 904)
Epoch 27, accuracy: 0.4028
batch size: (897, 897)
Epoch 28, accuracy: 0.4159
Epoch 28, Train Loss: 0.0011, Val Loss: 1.1363
batch size: (902, 902)
Epoch 29, accuracy: 0.4245
batch size: (895, 895)
Epoch 30, accuracy: 0.4283
Epoch 30, Train Loss: 0.0011, Val Loss: 1.1385
batch size: (895, 895)
Epoch 31, accuracy: 0.4302
batch size: (915, 915)
Epoch 32, accuracy: 0.4306
Epoch 32, Train Loss: 0.0012, Val Loss: 1.1419
batch size: (903, 903)
Epoch 33, accuracy: 0.4296
batch size: (894, 894)
Epoch 34, accuracy: 0.4310
Epoch 34, Train Loss: 0.0012, Val Loss: 1.1449
batch size: (897, 897)
Epoch 35, accuracy: 0.4271
batch size: (916, 916)
Epoch 36, accuracy: 0.4296
Epoch 36, Train Loss: 0.0012, Val Loss: 1.1478
batch size: (896, 896)
Epoch 37, accuracy: 0.4307
batch size: (890, 890)
Epoch 38, accuracy: 0.4308
Epoch 38, Train Loss: 0.0012, Val Loss: 1.1502
batch size: (890, 890)
Epoch 39, accuracy: 0.4307
batch size: (888, 888)
Epoch 40, accuracy: 0.4292
Epoch 40, Train Loss: 0.0014, Val Loss: 1.1516
batch size: (901, 901)
Epoch 41, accuracy: 0.4347
batch size: (902, 902)
Epoch 42, accuracy: 0.4314
Epoch 42, Train Loss: 0.0013, Val Loss: 1.1522
batch size: (917, 917)
Epoch 43, accuracy: 0.4348
batch size: (909, 909)
Epoch 44, accuracy: 0.4326
Epoch 44, Train Loss: 0.0013, Val Loss: 1.1522
batch size: (909, 909)
Epoch 45, accuracy: 0.4316
batch size: (899, 899)
Epoch 46, accuracy: 0.4300
Epoch 46, Train Loss: 0.0013, Val Loss: 1.1518
batch size: (918, 918)
Epoch 47, accuracy: 0.4288
batch size: (900, 900)
Epoch 48, accuracy: 0.4307
Epoch 48, Train Loss: 0.0013, Val Loss: 1.1507
batch size: (906, 906)
Epoch 49, accuracy: 0.4278
batch size: (915, 915)
Epoch 50, accuracy: 0.4243
Epoch 50, Train Loss: 0.0011, Val Loss: 1.1482
batch size: (886, 886)
Epoch 51, accuracy: 0.4291
batch size: (901, 901)
Epoch 52, accuracy: 0.4268
Epoch 52, Train Loss: 0.0013, Val Loss: 1.1452
batch size: (897, 897)
Epoch 53, accuracy: 0.4279
batch size: (910, 910)
Epoch 54, accuracy: 0.4237
Epoch 54, Train Loss: 0.0012, Val Loss: 1.1416
batch size: (912, 912)
Epoch 55, accuracy: 0.4200
batch size: (884, 884)
Epoch 56, accuracy: 0.4225
Epoch 56, Train Loss: 0.0012, Val Loss: 1.1382
batch size: (912, 912)
Epoch 57, accuracy: 0.4127
batch size: (907, 907)
Epoch 58, accuracy: 0.4174
Epoch 58, Train Loss: 0.0013, Val Loss: 1.1334
batch size: (910, 910)
Epoch 59, accuracy: 0.4072
batch size: (913, 913)
Epoch 60, accuracy: 0.4074
Epoch 60, Train Loss: 0.0012, Val Loss: 1.1289
batch size: (906, 906)
Epoch 61, accuracy: 0.3985
batch size: (899, 899)
Epoch 62, accuracy: 0.4009
Epoch 62, Train Loss: 0.0013, Val Loss: 1.1238
batch size: (895, 895)
Epoch 63, accuracy: 0.3920
batch size: (892, 892)
Epoch 64, accuracy: 0.3893
Epoch 64, Train Loss: 0.0012, Val Loss: 1.1182
batch size: (889, 889)
Epoch 65, accuracy: 0.3906
batch size: (893, 893)
✅ Epoch 66: New best model saved with val_loss = 1.1131
Epoch 66, accuracy: 0.3840
Epoch 66, Train Loss: 0.0012, Val Loss: 1.1131
batch size: (875, 875)
✅ Epoch 67: New best model saved with val_loss = 1.1113
Epoch 67, accuracy: 0.3809
batch size: (907, 907)
✅ Epoch 68: New best model saved with val_loss = 1.1089
Epoch 68, accuracy: 0.3793
Epoch 68, Train Loss: 0.0012, Val Loss: 1.1089
batch size: (887, 887)
✅ Epoch 69: New best model saved with val_loss = 1.1067
Epoch 69, accuracy: 0.3733
batch size: (890, 890)
✅ Epoch 70: New best model saved with val_loss = 1.1050
Epoch 70, accuracy: 0.3730
Epoch 70, Train Loss: 0.0012, Val Loss: 1.1050
batch size: (905, 905)
✅ Epoch 71: New best model saved with val_loss = 1.1033
Epoch 71, accuracy: 0.3724
batch size: (925, 925)
✅ Epoch 72: New best model saved with val_loss = 1.1015
Epoch 72, accuracy: 0.3681
Epoch 72, Train Loss: 0.0011, Val Loss: 1.1015
batch size: (909, 909)
✅ Epoch 73: New best model saved with val_loss = 1.0996
Epoch 73, accuracy: 0.3687
batch size: (893, 893)
✅ Epoch 74: New best model saved with val_loss = 1.0983
Epoch 74, accuracy: 0.3684
Epoch 74, Train Loss: 0.0012, Val Loss: 1.0983
batch size: (892, 892)
✅ Epoch 75: New best model saved with val_loss = 1.0979
Epoch 75, accuracy: 0.3715
batch size: (907, 907)
✅ Epoch 76: New best model saved with val_loss = 1.0969
Epoch 76, accuracy: 0.3713
Epoch 76, Train Loss: 0.0012, Val Loss: 1.0969
batch size: (921, 921)
✅ Epoch 77: New best model saved with val_loss = 1.0964
Epoch 77, accuracy: 0.3715
batch size: (889, 889)
✅ Epoch 78: New best model saved with val_loss = 1.0961
Epoch 78, accuracy: 0.3726
Epoch 78, Train Loss: 0.0013, Val Loss: 1.0961
batch size: (894, 894)
✅ Epoch 79: New best model saved with val_loss = 1.0955
Epoch 79, accuracy: 0.3755
batch size: (885, 885)
✅ Epoch 80: New best model saved with val_loss = 1.0947
Epoch 80, accuracy: 0.3818
Epoch 80, Train Loss: 0.0012, Val Loss: 1.0947
batch size: (885, 885)
✅ Epoch 81: New best model saved with val_loss = 1.0944
Epoch 81, accuracy: 0.3847
batch size: (905, 905)
✅ Epoch 82: New best model saved with val_loss = 1.0939
Epoch 82, accuracy: 0.3881
Epoch 82, Train Loss: 0.0011, Val Loss: 1.0939
batch size: (895, 895)
✅ Epoch 83: New best model saved with val_loss = 1.0935
Epoch 83, accuracy: 0.3963
batch size: (877, 877)
✅ Epoch 84: New best model saved with val_loss = 1.0929
Epoch 84, accuracy: 0.3957
Epoch 84, Train Loss: 0.0013, Val Loss: 1.0929
batch size: (902, 902)
Epoch 85, accuracy: 0.3995
batch size: (912, 912)
Epoch 86, accuracy: 0.4031
Epoch 86, Train Loss: 0.0012, Val Loss: 1.0942
batch size: (921, 921)
Epoch 87, accuracy: 0.4091
batch size: (897, 897)
Epoch 88, accuracy: 0.4139
Epoch 88, Train Loss: 0.0013, Val Loss: 1.0951
batch size: (925, 925)
Epoch 89, accuracy: 0.4186
batch size: (913, 913)
Epoch 90, accuracy: 0.4234
Epoch 90, Train Loss: 0.0013, Val Loss: 1.0957
batch size: (900, 900)
Epoch 91, accuracy: 0.4274
batch size: (889, 889)
Epoch 92, accuracy: 0.4312
Epoch 92, Train Loss: 0.0012, Val Loss: 1.0965
batch size: (897, 897)
Epoch 93, accuracy: 0.4354
batch size: (896, 896)
Epoch 94, accuracy: 0.4409
Epoch 94, Train Loss: 0.0012, Val Loss: 1.0977
batch size: (908, 908)
Epoch 95, accuracy: 0.4443
batch size: (905, 905)
Epoch 96, accuracy: 0.4461
Epoch 96, Train Loss: 0.0012, Val Loss: 1.1011
batch size: (909, 909)
Epoch 97, accuracy: 0.4519
batch size: (891, 891)
Epoch 98, accuracy: 0.4527
Epoch 98, Train Loss: 0.0013, Val Loss: 1.1036
batch size: (893, 893)
Epoch 99, accuracy: 0.4552
batch size: (906, 906)
Epoch 100, accuracy: 0.4592
Epoch 100, Train Loss: 0.0013, Val Loss: 1.1069
batch size: (915, 915)
Epoch 101, accuracy: 0.4621
batch size: (887, 887)
Epoch 102, accuracy: 0.4636
Epoch 102, Train Loss: 0.0012, Val Loss: 1.1106
batch size: (893, 893)
Epoch 103, accuracy: 0.4665
batch size: (909, 909)
Epoch 104, accuracy: 0.4703
Epoch 104, Train Loss: 0.0012, Val Loss: 1.1152
batch size: (896, 896)
Epoch 105, accuracy: 0.4661
batch size: (910, 910)
Epoch 106, accuracy: 0.4708
Epoch 106, Train Loss: 0.0011, Val Loss: 1.1184
batch size: (907, 907)
Epoch 107, accuracy: 0.4719
batch size: (920, 920)
Epoch 108, accuracy: 0.4733
Epoch 108, Train Loss: 0.0011, Val Loss: 1.1228
batch size: (896, 896)
Epoch 109, accuracy: 0.4758
batch size: (900, 900)
Epoch 110, accuracy: 0.4739
Epoch 110, Train Loss: 0.0012, Val Loss: 1.1255
batch size: (912, 912)
Epoch 111, accuracy: 0.4769
batch size: (889, 889)
Epoch 112, accuracy: 0.4777
Epoch 112, Train Loss: 0.0013, Val Loss: 1.1269
batch size: (893, 893)
Epoch 113, accuracy: 0.4796
batch size: (909, 909)
Epoch 114, accuracy: 0.4814
Epoch 114, Train Loss: 0.0011, Val Loss: 1.1297
batch size: (899, 899)
Epoch 115, accuracy: 0.4810
batch size: (902, 902)
Epoch 116, accuracy: 0.4827
Epoch 116, Train Loss: 0.0012, Val Loss: 1.1317
batch size: (891, 891)
Epoch 117, accuracy: 0.4799
batch size: (917, 917)
Epoch 118, accuracy: 0.4808
Epoch 118, Train Loss: 0.0012, Val Loss: 1.1333
batch size: (882, 882)
Epoch 119, accuracy: 0.4844
batch size: (896, 896)
Epoch 120, accuracy: 0.4819
Epoch 120, Train Loss: 0.0013, Val Loss: 1.1361
batch size: (903, 903)
Epoch 121, accuracy: 0.4854
batch size: (894, 894)
Epoch 122, accuracy: 0.4851
Epoch 122, Train Loss: 0.0013, Val Loss: 1.1359
batch size: (909, 909)
Epoch 123, accuracy: 0.4856
batch size: (892, 892)
Epoch 124, accuracy: 0.4865
Epoch 124, Train Loss: 0.0013, Val Loss: 1.1360
batch size: (914, 914)
Epoch 125, accuracy: 0.4869
batch size: (892, 892)
Epoch 126, accuracy: 0.4872
Epoch 126, Train Loss: 0.0012, Val Loss: 1.1369
batch size: (882, 882)
Epoch 127, accuracy: 0.4860
batch size: (887, 887)
Epoch 128, accuracy: 0.4853
Epoch 128, Train Loss: 0.0012, Val Loss: 1.1390
batch size: (897, 897)
Epoch 129, accuracy: 0.4865
batch size: (914, 914)
Epoch 130, accuracy: 0.4862
Epoch 130, Train Loss: 0.0012, Val Loss: 1.1396
batch size: (891, 891)
Epoch 131, accuracy: 0.4873
batch size: (891, 891)
Epoch 132, accuracy: 0.4871
Epoch 132, Train Loss: 0.0011, Val Loss: 1.1401
batch size: (897, 897)
Epoch 133, accuracy: 0.4881
batch size: (870, 870)
Epoch 134, accuracy: 0.4871
Epoch 134, Train Loss: 0.0013, Val Loss: 1.1389
batch size: (896, 896)
Epoch 135, accuracy: 0.4867
batch size: (898, 898)
Epoch 136, accuracy: 0.4862
Epoch 136, Train Loss: 0.0011, Val Loss: 1.1395
batch size: (902, 902)
Epoch 137, accuracy: 0.4861
batch size: (902, 902)
Epoch 138, accuracy: 0.4870
Epoch 138, Train Loss: 0.0012, Val Loss: 1.1403
batch size: (896, 896)
Epoch 139, accuracy: 0.4871
batch size: (910, 910)
Epoch 140, accuracy: 0.4884
Epoch 140, Train Loss: 0.0012, Val Loss: 1.1401
batch size: (919, 919)
Epoch 141, accuracy: 0.4912
batch size: (915, 915)
Epoch 142, accuracy: 0.4892
Epoch 142, Train Loss: 0.0012, Val Loss: 1.1401
batch size: (909, 909)
Epoch 143, accuracy: 0.4909
batch size: (916, 916)
Epoch 144, accuracy: 0.4897
Epoch 144, Train Loss: 0.0011, Val Loss: 1.1400
batch size: (900, 900)
Epoch 145, accuracy: 0.4871
batch size: (901, 901)
Epoch 146, accuracy: 0.4862
Epoch 146, Train Loss: 0.0011, Val Loss: 1.1409
batch size: (890, 890)
Epoch 147, accuracy: 0.4903
batch size: (879, 879)
Epoch 148, accuracy: 0.4900
Epoch 148, Train Loss: 0.0012, Val Loss: 1.1413
batch size: (901, 901)
Epoch 149, accuracy: 0.4907
batch size: (923, 923)
Epoch 150, accuracy: 0.4915
Epoch 150, Train Loss: 0.0011, Val Loss: 1.1415
batch size: (899, 899)
Epoch 151, accuracy: 0.4879
batch size: (898, 898)
Epoch 152, accuracy: 0.4886
Epoch 152, Train Loss: 0.0013, Val Loss: 1.1405
batch size: (915, 915)
Epoch 153, accuracy: 0.4918
batch size: (908, 908)
Epoch 154, accuracy: 0.4897
Epoch 154, Train Loss: 0.0012, Val Loss: 1.1416
batch size: (914, 914)
Epoch 155, accuracy: 0.4896
batch size: (894, 894)
Epoch 156, accuracy: 0.4890
Epoch 156, Train Loss: 0.0012, Val Loss: 1.1414
batch size: (908, 908)
Epoch 157, accuracy: 0.4900
batch size: (876, 876)
Epoch 158, accuracy: 0.4904
Epoch 158, Train Loss: 0.0012, Val Loss: 1.1430
batch size: (885, 885)
Epoch 159, accuracy: 0.4888
batch size: (904, 904)
Epoch 160, accuracy: 0.4900
Epoch 160, Train Loss: 0.0013, Val Loss: 1.1432
batch size: (909, 909)
Epoch 161, accuracy: 0.4882
batch size: (892, 892)
Epoch 162, accuracy: 0.4899
Epoch 162, Train Loss: 0.0013, Val Loss: 1.1432
batch size: (893, 893)
Epoch 163, accuracy: 0.4881
batch size: (898, 898)
Epoch 164, accuracy: 0.4900
Epoch 164, Train Loss: 0.0012, Val Loss: 1.1428
batch size: (896, 896)
Epoch 165, accuracy: 0.4920
batch size: (912, 912)
Epoch 166, accuracy: 0.4882
Epoch 166, Train Loss: 0.0011, Val Loss: 1.1433
batch size: (911, 911)
Epoch 167, accuracy: 0.4877
batch size: (886, 886)
Epoch 168, accuracy: 0.4869
Epoch 168, Train Loss: 0.0012, Val Loss: 1.1428
batch size: (901, 901)
Epoch 169, accuracy: 0.4888
batch size: (904, 904)
Epoch 170, accuracy: 0.4890
Epoch 170, Train Loss: 0.0013, Val Loss: 1.1418
batch size: (896, 896)
Epoch 171, accuracy: 0.4898
batch size: (891, 891)
Epoch 172, accuracy: 0.4899
Epoch 172, Train Loss: 0.0011, Val Loss: 1.1435
batch size: (909, 909)
Epoch 173, accuracy: 0.4873
batch size: (899, 899)
Epoch 174, accuracy: 0.4886
Epoch 174, Train Loss: 0.0012, Val Loss: 1.1440
batch size: (898, 898)
Epoch 175, accuracy: 0.4887
batch size: (895, 895)
Epoch 176, accuracy: 0.4882
Epoch 176, Train Loss: 0.0012, Val Loss: 1.1434
batch size: (921, 921)
Epoch 177, accuracy: 0.4867
batch size: (906, 906)
Epoch 178, accuracy: 0.4885
Epoch 178, Train Loss: 0.0011, Val Loss: 1.1423
batch size: (915, 915)
Epoch 179, accuracy: 0.4890
batch size: (906, 906)
Epoch 180, accuracy: 0.4893
Epoch 180, Train Loss: 0.0012, Val Loss: 1.1418
batch size: (906, 906)
Epoch 181, accuracy: 0.4886
batch size: (891, 891)
Epoch 182, accuracy: 0.4887
Epoch 182, Train Loss: 0.0012, Val Loss: 1.1418
batch size: (892, 892)
Epoch 183, accuracy: 0.4853
batch size: (912, 912)
Epoch 184, accuracy: 0.4892
Epoch 184, Train Loss: 0.0011, Val Loss: 1.1421
batch size: (889, 889)
Epoch 185, accuracy: 0.4877
batch size: (895, 895)
Epoch 186, accuracy: 0.4924
Epoch 186, Train Loss: 0.0012, Val Loss: 1.1414
batch size: (882, 882)
Epoch 187, accuracy: 0.4928
batch size: (882, 882)
Epoch 188, accuracy: 0.4882
Epoch 188, Train Loss: 0.0011, Val Loss: 1.1432
batch size: (901, 901)
Epoch 189, accuracy: 0.4893
batch size: (899, 899)
Epoch 190, accuracy: 0.4865
Epoch 190, Train Loss: 0.0013, Val Loss: 1.1430
batch size: (894, 894)
Epoch 191, accuracy: 0.4891
batch size: (895, 895)
Epoch 192, accuracy: 0.4885
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 192, Train Loss: 0.0012, Val Loss: 1.1432
batch size: (894, 894)
Epoch 193, accuracy: 0.4863
batch size: (889, 889)
Epoch 194, accuracy: 0.4909
Epoch 194, Train Loss: 0.0013, Val Loss: 1.1435
batch size: (880, 880)
Epoch 195, accuracy: 0.4890
batch size: (905, 905)
Epoch 196, accuracy: 0.4899
Epoch 196, Train Loss: 0.0011, Val Loss: 1.1428
batch size: (906, 906)
Epoch 197, accuracy: 0.4875
batch size: (929, 929)
Epoch 198, accuracy: 0.4875
Epoch 198, Train Loss: 0.0011, Val Loss: 1.1416
batch size: (896, 896)
Epoch 199, accuracy: 0.4897
Loaded best model with val_loss = 1.0929324626922607
test :accuracy 0.3935, f1_macro: 0.3750, f1_micro: 0.3935, auc: 0.6333
Training mlp with 32 layers...
可训练参数: 598025_mlp
不可训练参数: 0
batch size: (918, 918)
✅ Epoch 0: New best model saved with val_loss = 1.0992
Epoch 0, accuracy: 0.4304
Epoch 0, Train Loss: 1.1595, Val Loss: 1.0992
batch size: (906, 906)
✅ Epoch 1: New best model saved with val_loss = 1.0967
Epoch 1, accuracy: 0.4290
batch size: (890, 890)
Epoch 2, accuracy: 0.4313
Epoch 2, Train Loss: 1.8428, Val Loss: 1.0988
batch size: (882, 882)
Epoch 3, accuracy: 0.4319
batch size: (914, 914)
Epoch 4, accuracy: 0.1656
Epoch 4, Train Loss: 1.1573, Val Loss: 1.1029
batch size: (895, 895)
Epoch 5, accuracy: 0.1693
batch size: (910, 910)
Epoch 6, accuracy: 0.1683
Epoch 6, Train Loss: 0.9786, Val Loss: 1.1063
batch size: (889, 889)
Epoch 7, accuracy: 0.1691
batch size: (901, 901)
Epoch 8, accuracy: 0.1672
Epoch 8, Train Loss: 0.8157, Val Loss: 1.1081
batch size: (879, 879)
Epoch 9, accuracy: 0.1698
batch size: (890, 890)
Epoch 10, accuracy: 0.4271
Epoch 10, Train Loss: 0.7698, Val Loss: 1.1084
batch size: (891, 891)
Epoch 11, accuracy: 0.4294
batch size: (901, 901)
Epoch 12, accuracy: 0.4299
Epoch 12, Train Loss: 0.7438, Val Loss: 1.1081
batch size: (903, 903)
Epoch 13, accuracy: 0.4308
batch size: (893, 893)
Epoch 14, accuracy: 0.4284
Epoch 14, Train Loss: 0.7255, Val Loss: 1.1078
batch size: (894, 894)
Epoch 15, accuracy: 0.4293
batch size: (897, 897)
Epoch 16, accuracy: 0.4303
Epoch 16, Train Loss: 0.7212, Val Loss: 1.1077
batch size: (895, 895)
Epoch 17, accuracy: 0.4293
batch size: (907, 907)
Epoch 18, accuracy: 0.4310
Epoch 18, Train Loss: 0.7210, Val Loss: 1.1076
batch size: (901, 901)
Epoch 19, accuracy: 0.4333
batch size: (904, 904)
Epoch 20, accuracy: 0.4324
Epoch 20, Train Loss: 0.7193, Val Loss: 1.1074
batch size: (895, 895)
Epoch 21, accuracy: 0.4313
batch size: (907, 907)
Epoch 22, accuracy: 0.4312
Epoch 22, Train Loss: 0.7188, Val Loss: 1.1072
batch size: (903, 903)
Epoch 23, accuracy: 0.4265
batch size: (909, 909)
Epoch 24, accuracy: 0.4316
Epoch 24, Train Loss: 0.7178, Val Loss: 1.1072
batch size: (890, 890)
Epoch 25, accuracy: 0.4276
batch size: (882, 882)
Epoch 26, accuracy: 0.4317
Epoch 26, Train Loss: 0.7176, Val Loss: 1.1071
batch size: (895, 895)
Epoch 27, accuracy: 0.4323
batch size: (914, 914)
Epoch 28, accuracy: 0.4308
Epoch 28, Train Loss: 0.7201, Val Loss: 1.1071
batch size: (869, 869)
Epoch 29, accuracy: 0.4263
batch size: (896, 896)
Epoch 30, accuracy: 0.4264
Epoch 30, Train Loss: 0.7188, Val Loss: 1.1071
batch size: (906, 906)
Epoch 31, accuracy: 0.4264
batch size: (899, 899)
Epoch 32, accuracy: 0.4307
Epoch 32, Train Loss: 0.7177, Val Loss: 1.1071
batch size: (892, 892)
Epoch 33, accuracy: 0.4285
batch size: (908, 908)
Epoch 34, accuracy: 0.4304
Epoch 34, Train Loss: 0.7167, Val Loss: 1.1071
batch size: (879, 879)
Epoch 35, accuracy: 0.4285
batch size: (885, 885)
Epoch 36, accuracy: 0.4301
Epoch 36, Train Loss: 0.7185, Val Loss: 1.1071
batch size: (904, 904)
Epoch 37, accuracy: 0.4317
batch size: (892, 892)
Epoch 38, accuracy: 0.4259
Epoch 38, Train Loss: 0.7177, Val Loss: 1.1072
batch size: (916, 916)
Epoch 39, accuracy: 0.4286
batch size: (901, 901)
Epoch 40, accuracy: 0.4278
Epoch 40, Train Loss: 0.7169, Val Loss: 1.1075
batch size: (889, 889)
Epoch 41, accuracy: 0.4294
batch size: (894, 894)
Epoch 42, accuracy: 0.4338
Epoch 42, Train Loss: 0.7180, Val Loss: 1.1076
batch size: (886, 886)
Epoch 43, accuracy: 0.4340
batch size: (890, 890)
Epoch 44, accuracy: 0.4308
Epoch 44, Train Loss: 0.7178, Val Loss: 1.1078
batch size: (883, 883)
Epoch 45, accuracy: 0.4308
batch size: (907, 907)
Epoch 46, accuracy: 0.4298
Epoch 46, Train Loss: 0.7190, Val Loss: 1.1080
batch size: (902, 902)
Epoch 47, accuracy: 0.4281
batch size: (895, 895)
Epoch 48, accuracy: 0.4319
Epoch 48, Train Loss: 0.7189, Val Loss: 1.1081
batch size: (895, 895)
Epoch 49, accuracy: 0.4269
batch size: (901, 901)
Epoch 50, accuracy: 0.4310
Epoch 50, Train Loss: 0.7162, Val Loss: 1.1080
batch size: (904, 904)
Epoch 51, accuracy: 0.4341
batch size: (891, 891)
Epoch 52, accuracy: 0.4300
Epoch 52, Train Loss: 0.7187, Val Loss: 1.1081
batch size: (915, 915)
Epoch 53, accuracy: 0.4330
batch size: (907, 907)
Epoch 54, accuracy: 0.4285
Epoch 54, Train Loss: 0.7170, Val Loss: 1.1082
batch size: (901, 901)
Epoch 55, accuracy: 0.4320
batch size: (910, 910)
Epoch 56, accuracy: 0.4277
Epoch 56, Train Loss: 0.7157, Val Loss: 1.1083
batch size: (915, 915)
Epoch 57, accuracy: 0.4277
batch size: (902, 902)
Epoch 58, accuracy: 0.4242
Epoch 58, Train Loss: 0.7180, Val Loss: 1.1083
batch size: (881, 881)
Epoch 59, accuracy: 0.4303
batch size: (912, 912)
Epoch 60, accuracy: 0.4329
Epoch 60, Train Loss: 0.7168, Val Loss: 1.1084
batch size: (888, 888)
Epoch 61, accuracy: 0.4363
batch size: (901, 901)
Epoch 62, accuracy: 0.4284
Epoch 62, Train Loss: 0.7162, Val Loss: 1.1084
batch size: (879, 879)
Epoch 63, accuracy: 0.4304
batch size: (882, 882)
Epoch 64, accuracy: 0.4345
Epoch 64, Train Loss: 0.7161, Val Loss: 1.1086
batch size: (909, 909)
Epoch 65, accuracy: 0.4340
batch size: (911, 911)
Epoch 66, accuracy: 0.4316
Epoch 66, Train Loss: 0.7179, Val Loss: 1.1088
batch size: (908, 908)
Epoch 67, accuracy: 0.4286
batch size: (907, 907)
Epoch 68, accuracy: 0.4305
Epoch 68, Train Loss: 0.7159, Val Loss: 1.1089
batch size: (916, 916)
Epoch 69, accuracy: 0.4347
batch size: (915, 915)
Epoch 70, accuracy: 0.4330
Epoch 70, Train Loss: 0.7181, Val Loss: 1.1091
batch size: (908, 908)
Epoch 71, accuracy: 0.4308
batch size: (896, 896)
Epoch 72, accuracy: 0.4310
Epoch 72, Train Loss: 0.7188, Val Loss: 1.1092
batch size: (901, 901)
Epoch 73, accuracy: 0.4292
batch size: (897, 897)
Epoch 74, accuracy: 0.4293
Epoch 74, Train Loss: 0.7168, Val Loss: 1.1092
batch size: (887, 887)
Epoch 75, accuracy: 0.4301
batch size: (914, 914)
Epoch 76, accuracy: 0.4267
Epoch 76, Train Loss: 0.7185, Val Loss: 1.1095
batch size: (890, 890)
Epoch 77, accuracy: 0.4277
batch size: (880, 880)
Epoch 78, accuracy: 0.4313
Epoch 78, Train Loss: 0.7198, Val Loss: 1.1094
batch size: (882, 882)
Epoch 79, accuracy: 0.4292
batch size: (896, 896)
Epoch 80, accuracy: 0.4290
Epoch 80, Train Loss: 0.7181, Val Loss: 1.1094
batch size: (892, 892)
Epoch 81, accuracy: 0.4298
batch size: (898, 898)
Epoch 82, accuracy: 0.4302
Epoch 82, Train Loss: 0.7187, Val Loss: 1.1092
batch size: (899, 899)
Epoch 83, accuracy: 0.4268
batch size: (904, 904)
Epoch 84, accuracy: 0.4314
Epoch 84, Train Loss: 0.7184, Val Loss: 1.1091
batch size: (908, 908)
Epoch 85, accuracy: 0.4349
batch size: (884, 884)
Epoch 86, accuracy: 0.4294
Epoch 86, Train Loss: 0.7179, Val Loss: 1.1091
batch size: (882, 882)
Epoch 87, accuracy: 0.4304
batch size: (907, 907)
Epoch 88, accuracy: 0.4285
Epoch 88, Train Loss: 0.7200, Val Loss: 1.1089
batch size: (899, 899)
Epoch 89, accuracy: 0.4303
batch size: (920, 920)
Epoch 90, accuracy: 0.4296
Epoch 90, Train Loss: 0.7166, Val Loss: 1.1088
batch size: (901, 901)
Epoch 91, accuracy: 0.4261
batch size: (914, 914)
Epoch 92, accuracy: 0.4301
Epoch 92, Train Loss: 0.7167, Val Loss: 1.1087
batch size: (905, 905)
Epoch 93, accuracy: 0.4304
batch size: (906, 906)
Epoch 94, accuracy: 0.4280
Epoch 94, Train Loss: 0.7173, Val Loss: 1.1087
batch size: (886, 886)
Epoch 95, accuracy: 0.4365
batch size: (892, 892)
Epoch 96, accuracy: 0.4313
Epoch 96, Train Loss: 0.7195, Val Loss: 1.1088
batch size: (902, 902)
Epoch 97, accuracy: 0.4319
batch size: (883, 883)
Epoch 98, accuracy: 0.4311
Epoch 98, Train Loss: 0.7193, Val Loss: 1.1091
batch size: (917, 917)
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 99, accuracy: 0.4267
batch size: (899, 899)
Epoch 100, accuracy: 0.4256
Epoch 100, Train Loss: 0.7172, Val Loss: 1.1097
batch size: (905, 905)
Epoch 101, accuracy: 0.4310
batch size: (907, 907)
Epoch 102, accuracy: 0.4272
Epoch 102, Train Loss: 0.7164, Val Loss: 1.1106
batch size: (886, 886)
Epoch 103, accuracy: 0.4287
batch size: (914, 914)
Epoch 104, accuracy: 0.4306
Epoch 104, Train Loss: 0.7193, Val Loss: 1.1120
batch size: (898, 898)
Epoch 105, accuracy: 0.4310
batch size: (909, 909)
Epoch 106, accuracy: 0.4304
Epoch 106, Train Loss: 0.7170, Val Loss: 1.1125
batch size: (915, 915)
Epoch 107, accuracy: 0.4296
batch size: (895, 895)
Epoch 108, accuracy: 0.4271
Epoch 108, Train Loss: 0.7190, Val Loss: 1.1131
batch size: (900, 900)
Epoch 109, accuracy: 0.4280
batch size: (900, 900)
Epoch 110, accuracy: 0.4299
Epoch 110, Train Loss: 0.7164, Val Loss: 1.1137
batch size: (918, 918)
Epoch 111, accuracy: 0.4315
batch size: (892, 892)
Epoch 112, accuracy: 0.4280
Epoch 112, Train Loss: 0.7178, Val Loss: 1.1143
batch size: (904, 904)
Epoch 113, accuracy: 0.4288
batch size: (894, 894)
Epoch 114, accuracy: 0.4259
Epoch 114, Train Loss: 0.7201, Val Loss: 1.1151
batch size: (890, 890)
Epoch 115, accuracy: 0.4323
batch size: (909, 909)
Epoch 116, accuracy: 0.4286
Epoch 116, Train Loss: 0.7182, Val Loss: 1.1157
batch size: (887, 887)
Epoch 117, accuracy: 0.4287
batch size: (891, 891)
Epoch 118, accuracy: 0.4342
Epoch 118, Train Loss: 0.7209, Val Loss: 1.1162
batch size: (898, 898)
Epoch 119, accuracy: 0.4264
batch size: (902, 902)
Epoch 120, accuracy: 0.4282
Epoch 120, Train Loss: 0.7168, Val Loss: 1.1168
batch size: (901, 901)
Epoch 121, accuracy: 0.4324
batch size: (892, 892)
Epoch 122, accuracy: 0.4297
Epoch 122, Train Loss: 0.7192, Val Loss: 1.1177
batch size: (889, 889)
Epoch 123, accuracy: 0.4304
batch size: (898, 898)
Epoch 124, accuracy: 0.4312
Epoch 124, Train Loss: 0.7181, Val Loss: 1.1180
batch size: (902, 902)
Epoch 125, accuracy: 0.4310
batch size: (902, 902)
Epoch 126, accuracy: 0.4269
Epoch 126, Train Loss: 0.7158, Val Loss: 1.1182
batch size: (903, 903)
Epoch 127, accuracy: 0.4289
batch size: (902, 902)
Epoch 128, accuracy: 0.4309
Epoch 128, Train Loss: 0.7189, Val Loss: 1.1183
batch size: (921, 921)
Epoch 129, accuracy: 0.4300
batch size: (890, 890)
Epoch 130, accuracy: 0.4290
Epoch 130, Train Loss: 0.7180, Val Loss: 1.1186
batch size: (898, 898)
Epoch 131, accuracy: 0.4283
batch size: (901, 901)
Epoch 132, accuracy: 0.4312
Epoch 132, Train Loss: 0.7187, Val Loss: 1.1187
batch size: (891, 891)
Epoch 133, accuracy: 0.4304
batch size: (906, 906)
Epoch 134, accuracy: 0.4270
Epoch 134, Train Loss: 0.7199, Val Loss: 1.1187
batch size: (876, 876)
Epoch 135, accuracy: 0.4265
batch size: (893, 893)
Epoch 136, accuracy: 0.4262
Epoch 136, Train Loss: 0.7179, Val Loss: 1.1189
batch size: (904, 904)
Epoch 137, accuracy: 0.4290
batch size: (904, 904)
Epoch 138, accuracy: 0.4309
Epoch 138, Train Loss: 0.7166, Val Loss: 1.1191
batch size: (907, 907)
Epoch 139, accuracy: 0.4283
batch size: (900, 900)
Epoch 140, accuracy: 0.4271
Epoch 140, Train Loss: 0.7177, Val Loss: 1.1194
batch size: (905, 905)
Epoch 141, accuracy: 0.4309
batch size: (895, 895)
Epoch 142, accuracy: 0.4283
Epoch 142, Train Loss: 0.7196, Val Loss: 1.1192
batch size: (888, 888)
Epoch 143, accuracy: 0.4295
batch size: (880, 880)
Epoch 144, accuracy: 0.4295
Epoch 144, Train Loss: 0.7173, Val Loss: 1.1193
batch size: (888, 888)
Epoch 145, accuracy: 0.4321
batch size: (898, 898)
Epoch 146, accuracy: 0.4288
Epoch 146, Train Loss: 0.7170, Val Loss: 1.1197
batch size: (898, 898)
Epoch 147, accuracy: 0.4298
batch size: (910, 910)
Epoch 148, accuracy: 0.4290
Epoch 148, Train Loss: 0.7189, Val Loss: 1.1198
batch size: (902, 902)
Epoch 149, accuracy: 0.4295
batch size: (887, 887)
Epoch 150, accuracy: 0.4313
Epoch 150, Train Loss: 0.7169, Val Loss: 1.1198
batch size: (895, 895)
Epoch 151, accuracy: 0.4273
batch size: (907, 907)
Epoch 152, accuracy: 0.4293
Epoch 152, Train Loss: 0.7211, Val Loss: 1.1197
batch size: (903, 903)
Epoch 153, accuracy: 0.4303
batch size: (900, 900)
Epoch 154, accuracy: 0.4280
Epoch 154, Train Loss: 0.7184, Val Loss: 1.1195
batch size: (900, 900)
Epoch 155, accuracy: 0.4308
batch size: (879, 879)
Epoch 156, accuracy: 0.4265
Epoch 156, Train Loss: 0.7173, Val Loss: 1.1194
batch size: (912, 912)
Epoch 157, accuracy: 0.4255
batch size: (882, 882)
Epoch 158, accuracy: 0.4283
Epoch 158, Train Loss: 0.7184, Val Loss: 1.1195
batch size: (902, 902)
Epoch 159, accuracy: 0.4302
batch size: (902, 902)
Epoch 160, accuracy: 0.4291
Epoch 160, Train Loss: 0.7179, Val Loss: 1.1194
batch size: (878, 878)
Epoch 161, accuracy: 0.4264
batch size: (900, 900)
Epoch 162, accuracy: 0.4320
Epoch 162, Train Loss: 0.7182, Val Loss: 1.1195
batch size: (926, 926)
Epoch 163, accuracy: 0.4302
batch size: (911, 911)
Epoch 164, accuracy: 0.4313
Epoch 164, Train Loss: 0.7178, Val Loss: 1.1193
batch size: (899, 899)
Epoch 165, accuracy: 0.4259
batch size: (920, 920)
Epoch 166, accuracy: 0.4284
Epoch 166, Train Loss: 0.7177, Val Loss: 1.1199
batch size: (892, 892)
Epoch 167, accuracy: 0.4261
batch size: (913, 913)
Epoch 168, accuracy: 0.4288
Epoch 168, Train Loss: 0.7180, Val Loss: 1.1197
batch size: (898, 898)
Epoch 169, accuracy: 0.4278
batch size: (902, 902)
Epoch 170, accuracy: 0.4264
Epoch 170, Train Loss: 0.7181, Val Loss: 1.1199
batch size: (908, 908)
Epoch 171, accuracy: 0.4314
batch size: (906, 906)
Epoch 172, accuracy: 0.4262
Epoch 172, Train Loss: 0.7164, Val Loss: 1.1199
batch size: (882, 882)
Epoch 173, accuracy: 0.4277
batch size: (902, 902)
Epoch 174, accuracy: 0.4306
Epoch 174, Train Loss: 0.7175, Val Loss: 1.1196
batch size: (917, 917)
Epoch 175, accuracy: 0.4309
batch size: (908, 908)
Epoch 176, accuracy: 0.4257
Epoch 176, Train Loss: 0.7152, Val Loss: 1.1197
batch size: (900, 900)
Epoch 177, accuracy: 0.4276
batch size: (909, 909)
Epoch 178, accuracy: 0.4284
Epoch 178, Train Loss: 0.7186, Val Loss: 1.1198
batch size: (886, 886)
Epoch 179, accuracy: 0.4292
batch size: (898, 898)
Epoch 180, accuracy: 0.4290
Epoch 180, Train Loss: 0.7181, Val Loss: 1.1198
batch size: (907, 907)
Epoch 181, accuracy: 0.4246
batch size: (920, 920)
Epoch 182, accuracy: 0.4317
Epoch 182, Train Loss: 0.7171, Val Loss: 1.1198
batch size: (910, 910)
Epoch 183, accuracy: 0.4270
batch size: (894, 894)
Epoch 184, accuracy: 0.4301
Epoch 184, Train Loss: 0.7213, Val Loss: 1.1195
batch size: (906, 906)
Epoch 185, accuracy: 0.4281
batch size: (910, 910)
Epoch 186, accuracy: 0.4289
Epoch 186, Train Loss: 0.7173, Val Loss: 1.1195
batch size: (868, 868)
Epoch 187, accuracy: 0.4284
batch size: (914, 914)
Epoch 188, accuracy: 0.4283
Epoch 188, Train Loss: 0.7181, Val Loss: 1.1195
batch size: (893, 893)
Epoch 189, accuracy: 0.4277
batch size: (913, 913)
Epoch 190, accuracy: 0.4297
Epoch 190, Train Loss: 0.7179, Val Loss: 1.1195
batch size: (897, 897)
Epoch 191, accuracy: 0.4250
batch size: (892, 892)
Epoch 192, accuracy: 0.4276
Epoch 192, Train Loss: 0.7190, Val Loss: 1.1194
batch size: (901, 901)
Epoch 193, accuracy: 0.4333
batch size: (897, 897)
Epoch 194, accuracy: 0.4260
Epoch 194, Train Loss: 0.7160, Val Loss: 1.1191
batch size: (909, 909)
Epoch 195, accuracy: 0.4283
batch size: (896, 896)
Epoch 196, accuracy: 0.4325
Epoch 196, Train Loss: 0.7175, Val Loss: 1.1194
batch size: (906, 906)
Epoch 197, accuracy: 0.4257
batch size: (897, 897)
Epoch 198, accuracy: 0.4296
Epoch 198, Train Loss: 0.7205, Val Loss: 1.1190
batch size: (895, 895)
Epoch 199, accuracy: 0.4298
Loaded best model with val_loss = 1.0967036485671997
test :accuracy 0.4297, f1_macro: 0.2004, f1_micro: 0.4297, auc: 0.5000
Final Results: /root/miniconda3/lib/python3.12/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling
  warnings.warn(f"Using '{self.__class__.__name__}' without a "
{'GCN_2_Pubmed': np.float64(0.7246753246753247), 'GCN_8_Pubmed': np.float64(0.4326114989550447), 'GCN_32_Pubmed': np.float64(0.4135053453019983), 'GraphSAGE_2_Pubmed': np.float64(0.7082339429278205), 'GraphSAGE_8_Pubmed': np.float64(0.4036799928369969), 'GraphSAGE_32_Pubmed': np.float64(0.3998848487532663), 'GAT_2_Pubmed': np.float64(0.7203770397047708), 'GAT_8_Pubmed': np.float64(0.6924926529521774), 'GAT_32_Pubmed': np.float64(0.4008487826669645), 'JKNet_2_Pubmed': np.float64(0.27465257736535986), 'JKNet_8_Pubmed': np.float64(0.4839707662646281), 'JKNet_32_Pubmed': np.float64(0.4138859463276836), 'resGCN_2_Pubmed': np.float64(0.7004183727968667), 'resGCN_8_Pubmed': np.float64(0.16653358175849525), 'resGCN_32_Pubmed': np.float64(0.22256617317182592), 'GINConv_2_Pubmed': np.float64(0.718193950177936), 'GINConv_8_Pubmed': np.float64(0.4274660653490737), 'GINConv_32_Pubmed': np.float64(0.40184788430637386), 'mlp_2_Pubmed': np.float64(0.6845970096434575), 'mlp_8_Pubmed': np.float64(0.3934961531579889), 'mlp_32_Pubmed': np.float64(0.429695885509839)} ['133385_GCN_0', '532745_GCN_0', '2130185_GCN_0', '262153_GraphSAGE_0', '45833_GraphSAGE_0', '98057_GraphSAGE_0', '196495_GAT_0', '1090447_GAT_0', '4666255_GAT_0', '391942_JKNet_0', '1184518_JKNet_0', '4354822_JKNet_0', '132626_resGCN_0', '1850386_resGCN_0', '573202_resGCN_0', '265483_GINConv_0', '1059595_GINConv_0', '4236043_GINConv_0', '83465_mlp_0', '186377_mlp_0', '598025_mlp_0']
========== Running baseline 2/3 ==========
Training GCN with 2 layers...
可训练参数: 133385_GCN
不可训练参数: 0
batch size: (892, 892)
✅ Epoch 0: New best model saved with val_loss = 1.0744
Epoch 0, accuracy: 0.5666
Epoch 0, Train Loss: 1.2749, Val Loss: 1.0744
batch size: (906, 906)
✅ Epoch 1: New best model saved with val_loss = 1.0555
Epoch 1, accuracy: 0.5568
batch size: (901, 901)
✅ Epoch 2: New best model saved with val_loss = 1.0407
Epoch 2, accuracy: 0.5815
Epoch 2, Train Loss: 0.2428, Val Loss: 1.0407
batch size: (901, 901)
✅ Epoch 3: New best model saved with val_loss = 1.0278
Epoch 3, accuracy: 0.6344
batch size: (903, 903)
✅ Epoch 4: New best model saved with val_loss = 1.0155
Epoch 4, accuracy: 0.6785
Epoch 4, Train Loss: 0.0688, Val Loss: 1.0155
batch size: (894, 894)
✅ Epoch 5: New best model saved with val_loss = 1.0025
Epoch 5, accuracy: 0.7074
batch size: (910, 910)
✅ Epoch 6: New best model saved with val_loss = 0.9898
Epoch 6, accuracy: 0.7227
Epoch 6, Train Loss: 0.0321, Val Loss: 0.9898
batch size: (923, 923)
✅ Epoch 7: New best model saved with val_loss = 0.9745
Epoch 7, accuracy: 0.7337
batch size: (904, 904)
✅ Epoch 8: New best model saved with val_loss = 0.9618
Epoch 8, accuracy: 0.7340
Epoch 8, Train Loss: 0.0147, Val Loss: 0.9618
batch size: (894, 894)
✅ Epoch 9: New best model saved with val_loss = 0.9463
Epoch 9, accuracy: 0.7364
batch size: (896, 896)
✅ Epoch 10: New best model saved with val_loss = 0.9319
Epoch 10, accuracy: 0.7380
Epoch 10, Train Loss: 0.0147, Val Loss: 0.9319
batch size: (887, 887)
✅ Epoch 11: New best model saved with val_loss = 0.9191
Epoch 11, accuracy: 0.7409
batch size: (884, 884)
✅ Epoch 12: New best model saved with val_loss = 0.9054
Epoch 12, accuracy: 0.7408
Epoch 12, Train Loss: 0.0040, Val Loss: 0.9054
batch size: (896, 896)
✅ Epoch 13: New best model saved with val_loss = 0.8897
Epoch 13, accuracy: 0.7437
batch size: (893, 893)
✅ Epoch 14: New best model saved with val_loss = 0.8799
Epoch 14, accuracy: 0.7416
Epoch 14, Train Loss: 0.0036, Val Loss: 0.8799
batch size: (909, 909)
✅ Epoch 15: New best model saved with val_loss = 0.8636
Epoch 15, accuracy: 0.7432
batch size: (903, 903)
✅ Epoch 16: New best model saved with val_loss = 0.8538
Epoch 16, accuracy: 0.7455
Epoch 16, Train Loss: 0.0049, Val Loss: 0.8538
batch size: (894, 894)
✅ Epoch 17: New best model saved with val_loss = 0.8393
Epoch 17, accuracy: 0.7418
batch size: (912, 912)
✅ Epoch 18: New best model saved with val_loss = 0.8202
Epoch 18, accuracy: 0.7440
Epoch 18, Train Loss: 0.0019, Val Loss: 0.8202
batch size: (890, 890)
✅ Epoch 19: New best model saved with val_loss = 0.8121
Epoch 19, accuracy: 0.7437
batch size: (905, 905)
✅ Epoch 20: New best model saved with val_loss = 0.7985
Epoch 20, accuracy: 0.7430
Epoch 20, Train Loss: 0.0008, Val Loss: 0.7985
batch size: (898, 898)
✅ Epoch 21: New best model saved with val_loss = 0.7854
Epoch 21, accuracy: 0.7425
batch size: (911, 911)
✅ Epoch 22: New best model saved with val_loss = 0.7792
Epoch 22, accuracy: 0.7427
Epoch 22, Train Loss: 0.0011, Val Loss: 0.7792
batch size: (890, 890)
✅ Epoch 23: New best model saved with val_loss = 0.7658
Epoch 23, accuracy: 0.7404
batch size: (888, 888)
✅ Epoch 24: New best model saved with val_loss = 0.7483
Epoch 24, accuracy: 0.7405
Epoch 24, Train Loss: 0.0024, Val Loss: 0.7483
batch size: (887, 887)
✅ Epoch 25: New best model saved with val_loss = 0.7442
Epoch 25, accuracy: 0.7381
batch size: (906, 906)
✅ Epoch 26: New best model saved with val_loss = 0.7218
Epoch 26, accuracy: 0.7352
Epoch 26, Train Loss: 0.0014, Val Loss: 0.7218
batch size: (900, 900)
✅ Epoch 27: New best model saved with val_loss = 0.7186
Epoch 27, accuracy: 0.7350
batch size: (901, 901)
✅ Epoch 28: New best model saved with val_loss = 0.7083
Epoch 28, accuracy: 0.7338
Epoch 28, Train Loss: 0.0005, Val Loss: 0.7083
batch size: (896, 896)
✅ Epoch 29: New best model saved with val_loss = 0.6947
Epoch 29, accuracy: 0.7311
batch size: (917, 917)
✅ Epoch 30: New best model saved with val_loss = 0.6810
Epoch 30, accuracy: 0.7311
Epoch 30, Train Loss: 0.0014, Val Loss: 0.6810
batch size: (895, 895)
✅ Epoch 31: New best model saved with val_loss = 0.6777
Epoch 31, accuracy: 0.7295
batch size: (904, 904)
✅ Epoch 32: New best model saved with val_loss = 0.6675
Epoch 32, accuracy: 0.7309
Epoch 32, Train Loss: 0.0005, Val Loss: 0.6675
batch size: (890, 890)
✅ Epoch 33: New best model saved with val_loss = 0.6583
Epoch 33, accuracy: 0.7284
batch size: (906, 906)
✅ Epoch 34: New best model saved with val_loss = 0.6493
Epoch 34, accuracy: 0.7286
Epoch 34, Train Loss: 0.0013, Val Loss: 0.6493
batch size: (917, 917)
✅ Epoch 35: New best model saved with val_loss = 0.6449
Epoch 35, accuracy: 0.7294
batch size: (911, 911)
✅ Epoch 36: New best model saved with val_loss = 0.6267
Epoch 36, accuracy: 0.7273
Epoch 36, Train Loss: 0.0013, Val Loss: 0.6267
batch size: (893, 893)
✅ Epoch 37: New best model saved with val_loss = 0.6181
Epoch 37, accuracy: 0.7292
batch size: (890, 890)
✅ Epoch 38: New best model saved with val_loss = 0.6115
Epoch 38, accuracy: 0.7270
Epoch 38, Train Loss: 0.0005, Val Loss: 0.6115
batch size: (905, 905)
Epoch 39, accuracy: 0.7267
batch size: (911, 911)
✅ Epoch 40: New best model saved with val_loss = 0.6001
Epoch 40, accuracy: 0.7264
Epoch 40, Train Loss: 0.0008, Val Loss: 0.6001
batch size: (903, 903)
✅ Epoch 41: New best model saved with val_loss = 0.5897
Epoch 41, accuracy: 0.7267
batch size: (894, 894)
Epoch 42, accuracy: 0.7246
Epoch 42, Train Loss: 0.0010, Val Loss: 0.5897
batch size: (882, 882)
✅ Epoch 43: New best model saved with val_loss = 0.5845
Epoch 43, accuracy: 0.7244
batch size: (894, 894)
✅ Epoch 44: New best model saved with val_loss = 0.5837
Epoch 44, accuracy: 0.7245
Epoch 44, Train Loss: 0.0010, Val Loss: 0.5837
batch size: (899, 899)
✅ Epoch 45: New best model saved with val_loss = 0.5798
Epoch 45, accuracy: 0.7211
batch size: (900, 900)
✅ Epoch 46: New best model saved with val_loss = 0.5752
Epoch 46, accuracy: 0.7210
Epoch 46, Train Loss: 0.0006, Val Loss: 0.5752
batch size: (902, 902)
✅ Epoch 47: New best model saved with val_loss = 0.5751
Epoch 47, accuracy: 0.7222
batch size: (895, 895)
✅ Epoch 48: New best model saved with val_loss = 0.5622
Epoch 48, accuracy: 0.7212
Epoch 48, Train Loss: 0.0005, Val Loss: 0.5622
batch size: (911, 911)
Epoch 49, accuracy: 0.7235
batch size: (912, 912)
Epoch 50, accuracy: 0.7211
Epoch 50, Train Loss: 0.0005, Val Loss: 0.5671
batch size: (926, 926)
Epoch 51, accuracy: 0.7230
batch size: (915, 915)
Epoch 52, accuracy: 0.7222
Epoch 52, Train Loss: 0.0011, Val Loss: 0.5692
batch size: (892, 892)
Epoch 53, accuracy: 0.7219
batch size: (874, 874)
Epoch 54, accuracy: 0.7233
Epoch 54, Train Loss: 0.0010, Val Loss: 0.5737
batch size: (898, 898)
Epoch 55, accuracy: 0.7240
batch size: (892, 892)
Epoch 56, accuracy: 0.7231
Epoch 56, Train Loss: 0.0006, Val Loss: 0.5805
batch size: (896, 896)
Epoch 57, accuracy: 0.7215
batch size: (913, 913)
Epoch 58, accuracy: 0.7238
Epoch 58, Train Loss: 0.0006, Val Loss: 0.5760
batch size: (879, 879)
Epoch 59, accuracy: 0.7236
batch size: (907, 907)
Epoch 60, accuracy: 0.7206
Epoch 60, Train Loss: 0.0004, Val Loss: 0.6116
batch size: (878, 878)
Epoch 61, accuracy: 0.7224
batch size: (884, 884)
Epoch 62, accuracy: 0.7222
Epoch 62, Train Loss: 0.0008, Val Loss: 0.6264
batch size: (907, 907)
Epoch 63, accuracy: 0.7223
batch size: (872, 872)
Epoch 64, accuracy: 0.7246
Epoch 64, Train Loss: 0.0005, Val Loss: 0.6286
batch size: (908, 908)
Epoch 65, accuracy: 0.7239
batch size: (904, 904)
Epoch 66, accuracy: 0.7219
Epoch 66, Train Loss: 0.0005, Val Loss: 0.6686
batch size: (907, 907)
Epoch 67, accuracy: 0.7244
batch size: (908, 908)
Epoch 68, accuracy: 0.7203
Epoch 68, Train Loss: 0.0006, Val Loss: 0.7063
batch size: (882, 882)
Epoch 69, accuracy: 0.7226
batch size: (887, 887)
Epoch 70, accuracy: 0.7232
Epoch 70, Train Loss: 0.0006, Val Loss: 0.7012
batch size: (899, 899)
Epoch 71, accuracy: 0.7239
batch size: (895, 895)
Epoch 72, accuracy: 0.7226
Epoch 72, Train Loss: 0.0009, Val Loss: 0.7343
batch size: (915, 915)
Epoch 73, accuracy: 0.7244
batch size: (919, 919)
Epoch 74, accuracy: 0.7238
Epoch 74, Train Loss: 0.0010, Val Loss: 0.7237
batch size: (911, 911)
Epoch 75, accuracy: 0.7244
batch size: (888, 888)
Epoch 76, accuracy: 0.7236
Epoch 76, Train Loss: 0.0011, Val Loss: 0.7453
batch size: (902, 902)
Epoch 77, accuracy: 0.7252
batch size: (891, 891)
Epoch 78, accuracy: 0.7240
Epoch 78, Train Loss: 0.0007, Val Loss: 0.7651
batch size: (890, 890)
Epoch 79, accuracy: 0.7254
batch size: (900, 900)
Epoch 80, accuracy: 0.7239
Epoch 80, Train Loss: 0.0009, Val Loss: 0.7609
batch size: (905, 905)
Epoch 81, accuracy: 0.7229
batch size: (902, 902)
Epoch 82, accuracy: 0.7230
Epoch 82, Train Loss: 0.0007, Val Loss: 0.7885
batch size: (906, 906)
Epoch 83, accuracy: 0.7225
batch size: (895, 895)
Epoch 84, accuracy: 0.7255
Epoch 84, Train Loss: 0.0008, Val Loss: 0.7503
batch size: (909, 909)
Epoch 85, accuracy: 0.7206
batch size: (900, 900)
Epoch 86, accuracy: 0.7247
Epoch 86, Train Loss: 0.0006, Val Loss: 0.8186
batch size: (890, 890)
Epoch 87, accuracy: 0.7228
batch size: (881, 881)
Epoch 88, accuracy: 0.7231
Epoch 88, Train Loss: 0.0006, Val Loss: 0.7850
batch size: (905, 905)
Epoch 89, accuracy: 0.7221
batch size: (880, 880)
Epoch 90, accuracy: 0.7237
Epoch 90, Train Loss: 0.0005, Val Loss: 0.7694
batch size: (877, 877)
Epoch 91, accuracy: 0.7206
batch size: (890, 890)
Epoch 92, accuracy: 0.7245
Epoch 92, Train Loss: 0.0009, Val Loss: 0.8151
batch size: (898, 898)
Epoch 93, accuracy: 0.7252
batch size: (906, 906)
Epoch 94, accuracy: 0.7264
Epoch 94, Train Loss: 0.0006, Val Loss: 0.8361
batch size: (909, 909)
Epoch 95, accuracy: 0.7223
batch size: (901, 901)
Epoch 96, accuracy: 0.7246
Epoch 96, Train Loss: 0.0007, Val Loss: 0.8339
batch size: (896, 896)
Epoch 97, accuracy: 0.7228
batch size: (891, 891)
Epoch 98, accuracy: 0.7241
Epoch 98, Train Loss: 0.0014, Val Loss: 0.8154
batch size: (895, 895)
Epoch 99, accuracy: 0.7234
batch size: (897, 897)
Epoch 100, accuracy: 0.7210
Epoch 100, Train Loss: 0.0009, Val Loss: 0.8532
batch size: (891, 891)
Epoch 101, accuracy: 0.7227
batch size: (896, 896)
Epoch 102, accuracy: 0.7238
Epoch 102, Train Loss: 0.0005, Val Loss: 0.8017
batch size: (903, 903)
Epoch 103, accuracy: 0.7220
batch size: (901, 901)
Epoch 104, accuracy: 0.7223
Epoch 104, Train Loss: 0.0007, Val Loss: 0.8507
batch size: (898, 898)
Epoch 105, accuracy: 0.7227
batch size: (894, 894)
Epoch 106, accuracy: 0.7222
Epoch 106, Train Loss: 0.0007, Val Loss: 0.8205
batch size: (884, 884)
Epoch 107, accuracy: 0.7251
batch size: (898, 898)
Epoch 108, accuracy: 0.7253
Epoch 108, Train Loss: 0.0005, Val Loss: 0.8186
batch size: (902, 902)
Epoch 109, accuracy: 0.7227
batch size: (916, 916)
Epoch 110, accuracy: 0.7237
Epoch 110, Train Loss: 0.0009, Val Loss: 0.7941
batch size: (903, 903)
Epoch 111, accuracy: 0.7242
batch size: (911, 911)
Epoch 112, accuracy: 0.7222
Epoch 112, Train Loss: 0.0009, Val Loss: 0.8241
batch size: (905, 905)
Epoch 113, accuracy: 0.7218
batch size: (892, 892)
Epoch 114, accuracy: 0.7220
Epoch 114, Train Loss: 0.0004, Val Loss: 0.8332
batch size: (897, 897)
Epoch 115, accuracy: 0.7241
batch size: (892, 892)
Epoch 116, accuracy: 0.7206
Epoch 116, Train Loss: 0.0006, Val Loss: 0.8139
batch size: (899, 899)
Epoch 117, accuracy: 0.7232
batch size: (895, 895)
Epoch 118, accuracy: 0.7251
Epoch 118, Train Loss: 0.0010, Val Loss: 0.8499
batch size: (888, 888)
Epoch 119, accuracy: 0.7221
batch size: (887, 887)
Epoch 120, accuracy: 0.7244
Epoch 120, Train Loss: 0.0007, Val Loss: 0.8295
batch size: (915, 915)
Epoch 121, accuracy: 0.7210
batch size: (906, 906)
Epoch 122, accuracy: 0.7241
Epoch 122, Train Loss: 0.0006, Val Loss: 0.8215
batch size: (889, 889)
Epoch 123, accuracy: 0.7219
batch size: (899, 899)
Epoch 124, accuracy: 0.7233
Epoch 124, Train Loss: 0.0005, Val Loss: 0.8328
batch size: (897, 897)
Epoch 125, accuracy: 0.7236
batch size: (896, 896)
Epoch 126, accuracy: 0.7253
Epoch 126, Train Loss: 0.0007, Val Loss: 0.8287
batch size: (898, 898)
Epoch 127, accuracy: 0.7255
batch size: (901, 901)
Epoch 128, accuracy: 0.7231
Epoch 128, Train Loss: 0.0011, Val Loss: 0.8278
batch size: (908, 908)
Epoch 129, accuracy: 0.7221
batch size: (894, 894)
Epoch 130, accuracy: 0.7236
Epoch 130, Train Loss: 0.0010, Val Loss: 0.8746
batch size: (891, 891)
Epoch 131, accuracy: 0.7228
batch size: (916, 916)
Epoch 132, accuracy: 0.7222
Epoch 132, Train Loss: 0.0015, Val Loss: 0.8506
batch size: (879, 879)
Epoch 133, accuracy: 0.7234
batch size: (907, 907)
Epoch 134, accuracy: 0.7229
Epoch 134, Train Loss: 0.0011, Val Loss: 0.8273
batch size: (904, 904)
Epoch 135, accuracy: 0.7253
batch size: (905, 905)
Epoch 136, accuracy: 0.7227
Epoch 136, Train Loss: 0.0005, Val Loss: 0.8263
batch size: (910, 910)
Epoch 137, accuracy: 0.7228
batch size: (905, 905)
Epoch 138, accuracy: 0.7215
Epoch 138, Train Loss: 0.0009, Val Loss: 0.8111
batch size: (888, 888)
Epoch 139, accuracy: 0.7236
batch size: (897, 897)
Epoch 140, accuracy: 0.7223
Epoch 140, Train Loss: 0.0006, Val Loss: 0.8414
batch size: (885, 885)
Epoch 141, accuracy: 0.7251
batch size: (917, 917)
Epoch 142, accuracy: 0.7225
Epoch 142, Train Loss: 0.0006, Val Loss: 0.8421
batch size: (917, 917)
Epoch 143, accuracy: 0.7250
batch size: (907, 907)
Epoch 144, accuracy: 0.7216
Epoch 144, Train Loss: 0.0012, Val Loss: 0.8030
batch size: (897, 897)
Epoch 145, accuracy: 0.7212
batch size: (893, 893)
Epoch 146, accuracy: 0.7227
Epoch 146, Train Loss: 0.0012, Val Loss: 0.8551
batch size: (923, 923)
Epoch 147, accuracy: 0.7226
batch size: (871, 871)
Epoch 148, accuracy: 0.7241
Epoch 148, Train Loss: 0.0009, Val Loss: 0.8701
batch size: (892, 892)
Epoch 149, accuracy: 0.7212
batch size: (921, 921)
Epoch 150, accuracy: 0.7213
Epoch 150, Train Loss: 0.0023, Val Loss: 0.8577
batch size: (909, 909)
Epoch 151, accuracy: 0.7236
batch size: (908, 908)
Epoch 152, accuracy: 0.7233
Epoch 152, Train Loss: 0.0010, Val Loss: 0.8107
batch size: (909, 909)
Epoch 153, accuracy: 0.7224
batch size: (909, 909)
Epoch 154, accuracy: 0.7226
Epoch 154, Train Loss: 0.0009, Val Loss: 0.8348
batch size: (901, 901)
Epoch 155, accuracy: 0.7227
batch size: (917, 917)
Epoch 156, accuracy: 0.7233
Epoch 156, Train Loss: 0.0006, Val Loss: 0.8154
batch size: (895, 895)
Epoch 157, accuracy: 0.7220
batch size: (889, 889)
Epoch 158, accuracy: 0.7227
Epoch 158, Train Loss: 0.0016, Val Loss: 0.8181
batch size: (903, 903)
Epoch 159, accuracy: 0.7215
batch size: (886, 886)
Epoch 160, accuracy: 0.7244
Epoch 160, Train Loss: 0.0007, Val Loss: 0.8102
batch size: (900, 900)
Epoch 161, accuracy: 0.7214
batch size: (892, 892)
Epoch 162, accuracy: 0.7221
Epoch 162, Train Loss: 0.0012, Val Loss: 0.8619
batch size: (911, 911)
Epoch 163, accuracy: 0.7236
batch size: (895, 895)
Epoch 164, accuracy: 0.7227
Epoch 164, Train Loss: 0.0008, Val Loss: 0.7925
batch size: (896, 896)
Epoch 165, accuracy: 0.7226
batch size: (872, 872)
Epoch 166, accuracy: 0.7231
Epoch 166, Train Loss: 0.0011, Val Loss: 0.8130
batch size: (907, 907)
Epoch 167, accuracy: 0.7240
batch size: (891, 891)
Epoch 168, accuracy: 0.7238
Epoch 168, Train Loss: 0.0006, Val Loss: 0.8472
batch size: (910, 910)
Epoch 169, accuracy: 0.7230
batch size: (908, 908)
Epoch 170, accuracy: 0.7245
Epoch 170, Train Loss: 0.0010, Val Loss: 0.8140
batch size: (897, 897)
Epoch 171, accuracy: 0.7233
batch size: (893, 893)
Epoch 172, accuracy: 0.7193
Epoch 172, Train Loss: 0.0005, Val Loss: 0.8547
batch size: (909, 909)
Epoch 173, accuracy: 0.7252
batch size: (900, 900)
Epoch 174, accuracy: 0.7228
Epoch 174, Train Loss: 0.0006, Val Loss: 0.8403
batch size: (892, 892)
Epoch 175, accuracy: 0.7225
batch size: (916, 916)
Epoch 176, accuracy: 0.7230
Epoch 176, Train Loss: 0.0008, Val Loss: 0.8489
batch size: (884, 884)
Epoch 177, accuracy: 0.7235
batch size: (893, 893)
Epoch 178, accuracy: 0.7241
Epoch 178, Train Loss: 0.0006, Val Loss: 0.8118
batch size: (877, 877)
Epoch 179, accuracy: 0.7240
batch size: (880, 880)
Epoch 180, accuracy: 0.7235
Epoch 180, Train Loss: 0.0009, Val Loss: 0.8966
batch size: (899, 899)
Epoch 181, accuracy: 0.7238
batch size: (899, 899)
Epoch 182, accuracy: 0.7248
Epoch 182, Train Loss: 0.0017, Val Loss: 0.8380
batch size: (888, 888)
Epoch 183, accuracy: 0.7236
batch size: (906, 906)
Epoch 184, accuracy: 0.7210
Epoch 184, Train Loss: 0.0013, Val Loss: 0.8153
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
batch size: (914, 914)
Epoch 185, accuracy: 0.7248
batch size: (891, 891)
Epoch 186, accuracy: 0.7220
Epoch 186, Train Loss: 0.0012, Val Loss: 0.8508
batch size: (873, 873)
Epoch 187, accuracy: 0.7239
batch size: (887, 887)
Epoch 188, accuracy: 0.7222
Epoch 188, Train Loss: 0.0014, Val Loss: 0.8168
batch size: (907, 907)
Epoch 189, accuracy: 0.7219
batch size: (908, 908)
Epoch 190, accuracy: 0.7245
Epoch 190, Train Loss: 0.0006, Val Loss: 0.8510
batch size: (911, 911)
Epoch 191, accuracy: 0.7218
batch size: (906, 906)
Epoch 192, accuracy: 0.7227
Epoch 192, Train Loss: 0.0006, Val Loss: 0.8238
batch size: (880, 880)
Epoch 193, accuracy: 0.7238
batch size: (909, 909)
Epoch 194, accuracy: 0.7253
Epoch 194, Train Loss: 0.0007, Val Loss: 0.8550
batch size: (907, 907)
Epoch 195, accuracy: 0.7230
batch size: (880, 880)
Epoch 196, accuracy: 0.7216
Epoch 196, Train Loss: 0.0006, Val Loss: 0.8759
batch size: (901, 901)
Epoch 197, accuracy: 0.7255
batch size: (896, 896)
Epoch 198, accuracy: 0.7229
Epoch 198, Train Loss: 0.0015, Val Loss: 0.8254
batch size: (897, 897)
Epoch 199, accuracy: 0.7191
Loaded best model with val_loss = 0.5622336268424988
test :accuracy 0.7239, f1_macro: 0.7200, f1_micro: 0.7239, auc: 0.8779
Training GCN with 8 layers...
可训练参数: 532745_GCN
不可训练参数: 0
batch size: (905, 905)
✅ Epoch 0: New best model saved with val_loss = 1.1789
Epoch 0, accuracy: 0.1648
Epoch 0, Train Loss: 1.3355, Val Loss: 1.1789
batch size: (913, 913)
Epoch 1, accuracy: 0.1667
batch size: (897, 897)
Epoch 2, accuracy: 0.1671
Epoch 2, Train Loss: 1.1186, Val Loss: 1.2393
batch size: (900, 900)
Epoch 3, accuracy: 0.2403
batch size: (896, 896)
Epoch 4, accuracy: 0.1655
Epoch 4, Train Loss: 1.1802, Val Loss: 1.2968
batch size: (915, 915)
Epoch 5, accuracy: 0.1705
batch size: (917, 917)
Epoch 6, accuracy: 0.1685
Epoch 6, Train Loss: 0.8879, Val Loss: 1.4342
batch size: (910, 910)
Epoch 7, accuracy: 0.1659
batch size: (922, 922)
Epoch 8, accuracy: 0.1680
Epoch 8, Train Loss: 0.8137, Val Loss: 1.4395
batch size: (910, 910)
Epoch 9, accuracy: 0.1681
batch size: (898, 898)
Epoch 10, accuracy: 0.1671
Epoch 10, Train Loss: 0.6939, Val Loss: 1.4551
batch size: (899, 899)
Epoch 11, accuracy: 0.1684
batch size: (893, 893)
Epoch 12, accuracy: 0.1650
Epoch 12, Train Loss: 0.7671, Val Loss: 1.4819
batch size: (908, 908)
Epoch 13, accuracy: 0.1677
batch size: (899, 899)
Epoch 14, accuracy: 0.1672
Epoch 14, Train Loss: 0.7271, Val Loss: 1.4806
batch size: (885, 885)
Epoch 15, accuracy: 0.1696
batch size: (884, 884)
Epoch 16, accuracy: 0.1673
Epoch 16, Train Loss: 0.7631, Val Loss: 1.5099
batch size: (903, 903)
Epoch 17, accuracy: 0.1687
batch size: (887, 887)
Epoch 18, accuracy: 0.1677
Epoch 18, Train Loss: 0.7578, Val Loss: 1.4772
batch size: (907, 907)
Epoch 19, accuracy: 0.1671
batch size: (890, 890)
Epoch 20, accuracy: 0.1688
Epoch 20, Train Loss: 0.8956, Val Loss: 1.4907
batch size: (897, 897)
Epoch 21, accuracy: 0.1659
batch size: (899, 899)
Epoch 22, accuracy: 0.1669
Epoch 22, Train Loss: 0.7377, Val Loss: 1.4723
batch size: (887, 887)
Epoch 23, accuracy: 0.1652
batch size: (897, 897)
Epoch 24, accuracy: 0.1660
Epoch 24, Train Loss: 0.7400, Val Loss: 1.4846
batch size: (901, 901)
Epoch 25, accuracy: 0.1638
batch size: (906, 906)
Epoch 26, accuracy: 0.1674
Epoch 26, Train Loss: 0.6855, Val Loss: 1.4665
batch size: (904, 904)
Epoch 27, accuracy: 0.1697
batch size: (896, 896)
Epoch 28, accuracy: 0.1650
Epoch 28, Train Loss: 0.7140, Val Loss: 1.4717
batch size: (890, 890)
Epoch 29, accuracy: 0.1678
batch size: (908, 908)
Epoch 30, accuracy: 0.1665
Epoch 30, Train Loss: 0.7166, Val Loss: 1.5124
batch size: (909, 909)
Epoch 31, accuracy: 0.1670
batch size: (886, 886)
Epoch 32, accuracy: 0.1675
Epoch 32, Train Loss: 0.7159, Val Loss: 1.5308
batch size: (901, 901)
Epoch 33, accuracy: 0.1677
batch size: (891, 891)
Epoch 34, accuracy: 0.1639
Epoch 34, Train Loss: 0.6322, Val Loss: 1.5124
batch size: (911, 911)
Epoch 35, accuracy: 0.1711
batch size: (915, 915)
Epoch 36, accuracy: 0.1668
Epoch 36, Train Loss: 0.7021, Val Loss: 1.5062
batch size: (910, 910)
Epoch 37, accuracy: 0.1693
batch size: (916, 916)
Epoch 38, accuracy: 0.1659
Epoch 38, Train Loss: 0.6517, Val Loss: 1.5211
batch size: (902, 902)
Epoch 39, accuracy: 0.1660
batch size: (885, 885)
Epoch 40, accuracy: 0.1662
Epoch 40, Train Loss: 0.7037, Val Loss: 1.4885
batch size: (891, 891)
Epoch 41, accuracy: 0.1689
batch size: (883, 883)
Epoch 42, accuracy: 0.1642
Epoch 42, Train Loss: 0.7243, Val Loss: 1.5208
batch size: (908, 908)
Epoch 43, accuracy: 0.1675
batch size: (910, 910)
Epoch 44, accuracy: 0.1681
Epoch 44, Train Loss: 0.6580, Val Loss: 1.4880
batch size: (905, 905)
Epoch 45, accuracy: 0.1683
batch size: (895, 895)
Epoch 46, accuracy: 0.1668
Epoch 46, Train Loss: 0.8274, Val Loss: 1.4979
batch size: (901, 901)
Epoch 47, accuracy: 0.1699
batch size: (901, 901)
Epoch 48, accuracy: 0.1635
Epoch 48, Train Loss: 0.7383, Val Loss: 1.5131
batch size: (898, 898)
Epoch 49, accuracy: 0.1672
batch size: (893, 893)
Epoch 50, accuracy: 0.1694
Epoch 50, Train Loss: 0.7885, Val Loss: 1.5055
batch size: (895, 895)
Epoch 51, accuracy: 0.1701
batch size: (891, 891)
Epoch 52, accuracy: 0.1660
Epoch 52, Train Loss: 0.7906, Val Loss: 1.5162
batch size: (888, 888)
Epoch 53, accuracy: 0.1680
batch size: (912, 912)
Epoch 54, accuracy: 0.1667
Epoch 54, Train Loss: 0.6399, Val Loss: 1.5032
batch size: (913, 913)
Epoch 55, accuracy: 0.1674
batch size: (888, 888)
Epoch 56, accuracy: 0.1678
Epoch 56, Train Loss: 0.7160, Val Loss: 1.4860
batch size: (890, 890)
Epoch 57, accuracy: 0.1678
batch size: (908, 908)
Epoch 58, accuracy: 0.1678
Epoch 58, Train Loss: 0.8000, Val Loss: 1.4308
batch size: (902, 902)
Epoch 59, accuracy: 0.1681
batch size: (892, 892)
Epoch 60, accuracy: 0.1664
Epoch 60, Train Loss: 0.7862, Val Loss: 1.4740
batch size: (908, 908)
Epoch 61, accuracy: 0.1645
batch size: (903, 903)
Epoch 62, accuracy: 0.1666
Epoch 62, Train Loss: 0.7720, Val Loss: 1.5145
batch size: (903, 903)
Epoch 63, accuracy: 0.1681
batch size: (917, 917)
Epoch 64, accuracy: 0.1672
Epoch 64, Train Loss: 0.6217, Val Loss: 1.5190
batch size: (905, 905)
Epoch 65, accuracy: 0.1652
batch size: (893, 893)
Epoch 66, accuracy: 0.1661
Epoch 66, Train Loss: 0.7045, Val Loss: 1.5065
batch size: (891, 891)
Epoch 67, accuracy: 0.1637
batch size: (915, 915)
Epoch 68, accuracy: 0.1698
Epoch 68, Train Loss: 0.6463, Val Loss: 1.5170
batch size: (892, 892)
Epoch 69, accuracy: 0.1681
batch size: (893, 893)
Epoch 70, accuracy: 0.1668
Epoch 70, Train Loss: 0.7531, Val Loss: 1.5058
batch size: (911, 911)
Epoch 71, accuracy: 0.1671
batch size: (905, 905)
Epoch 72, accuracy: 0.1672
Epoch 72, Train Loss: 0.7317, Val Loss: 1.5105
batch size: (915, 915)
Epoch 73, accuracy: 0.1669
batch size: (885, 885)
Epoch 74, accuracy: 0.1695
Epoch 74, Train Loss: 0.8601, Val Loss: 1.5016
batch size: (916, 916)
Epoch 75, accuracy: 0.1677
batch size: (914, 914)
Epoch 76, accuracy: 0.1651
Epoch 76, Train Loss: 0.6416, Val Loss: 1.4747
batch size: (897, 897)
Epoch 77, accuracy: 0.1692
batch size: (902, 902)
Epoch 78, accuracy: 0.1679
Epoch 78, Train Loss: 0.6646, Val Loss: 1.4933
batch size: (895, 895)
Epoch 79, accuracy: 0.1696
batch size: (898, 898)
Epoch 80, accuracy: 0.1678
Epoch 80, Train Loss: 0.6806, Val Loss: 1.4848
batch size: (892, 892)
Epoch 81, accuracy: 0.1674
batch size: (879, 879)
Epoch 82, accuracy: 0.1646
Epoch 82, Train Loss: 0.6524, Val Loss: 1.4749
batch size: (877, 877)
Epoch 83, accuracy: 0.1657
batch size: (907, 907)
Epoch 84, accuracy: 0.1695
Epoch 84, Train Loss: 0.7160, Val Loss: 1.4862
batch size: (892, 892)
Epoch 85, accuracy: 0.1670
batch size: (912, 912)
Epoch 86, accuracy: 0.1686
Epoch 86, Train Loss: 0.7379, Val Loss: 1.5038
batch size: (901, 901)
Epoch 87, accuracy: 0.1646
batch size: (890, 890)
Epoch 88, accuracy: 0.1682
Epoch 88, Train Loss: 0.7121, Val Loss: 1.5480
batch size: (890, 890)
Epoch 89, accuracy: 0.1654
batch size: (895, 895)
Epoch 90, accuracy: 0.1710
Epoch 90, Train Loss: 0.5732, Val Loss: 1.5112
batch size: (890, 890)
Epoch 91, accuracy: 0.1662
batch size: (901, 901)
Epoch 92, accuracy: 0.1676
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 92, Train Loss: 0.6148, Val Loss: 1.5269
batch size: (899, 899)
Epoch 93, accuracy: 0.1670
batch size: (886, 886)
Epoch 94, accuracy: 0.1674
Epoch 94, Train Loss: 0.9804, Val Loss: 1.5016
batch size: (905, 905)
Epoch 95, accuracy: 0.1677
batch size: (909, 909)
Epoch 96, accuracy: 0.1626
Epoch 96, Train Loss: 0.6567, Val Loss: 1.5145
batch size: (877, 877)
Epoch 97, accuracy: 0.1654
batch size: (922, 922)
Epoch 98, accuracy: 0.1690
Epoch 98, Train Loss: 0.7942, Val Loss: 1.5093
batch size: (894, 894)
Epoch 99, accuracy: 0.1708
batch size: (900, 900)
Epoch 100, accuracy: 0.1652
Epoch 100, Train Loss: 0.7219, Val Loss: 1.4897
batch size: (898, 898)
Epoch 101, accuracy: 0.1691
batch size: (897, 897)
Epoch 102, accuracy: 0.1694
Epoch 102, Train Loss: 0.8116, Val Loss: 1.5126
batch size: (894, 894)
Epoch 103, accuracy: 0.1655
batch size: (902, 902)
Epoch 104, accuracy: 0.1643
Epoch 104, Train Loss: 0.6888, Val Loss: 1.5214
batch size: (900, 900)
Epoch 105, accuracy: 0.1676
batch size: (901, 901)
Epoch 106, accuracy: 0.1665
Epoch 106, Train Loss: 0.7335, Val Loss: 1.4901
batch size: (871, 871)
Epoch 107, accuracy: 0.1689
batch size: (903, 903)
Epoch 108, accuracy: 0.1694
Epoch 108, Train Loss: 0.7012, Val Loss: 1.4916
batch size: (909, 909)
Epoch 109, accuracy: 0.1674
batch size: (907, 907)
Epoch 110, accuracy: 0.1668
Epoch 110, Train Loss: 0.8550, Val Loss: 1.4733
batch size: (903, 903)
Epoch 111, accuracy: 0.1674
batch size: (891, 891)
Epoch 112, accuracy: 0.1685
Epoch 112, Train Loss: 0.7465, Val Loss: 1.5064
batch size: (913, 913)
Epoch 113, accuracy: 0.1668
batch size: (893, 893)
Epoch 114, accuracy: 0.1681
Epoch 114, Train Loss: 0.7463, Val Loss: 1.5423
batch size: (878, 878)
Epoch 115, accuracy: 0.1671
batch size: (910, 910)
Epoch 116, accuracy: 0.1706
Epoch 116, Train Loss: 0.7356, Val Loss: 1.4564
batch size: (902, 902)
Epoch 117, accuracy: 0.1671
batch size: (898, 898)
Epoch 118, accuracy: 0.1665
Epoch 118, Train Loss: 0.7122, Val Loss: 1.4672
batch size: (901, 901)
Epoch 119, accuracy: 0.1665
batch size: (903, 903)
Epoch 120, accuracy: 0.1672
Epoch 120, Train Loss: 0.7584, Val Loss: 1.5007
batch size: (900, 900)
Epoch 121, accuracy: 0.1646
batch size: (893, 893)
Epoch 122, accuracy: 0.1668
Epoch 122, Train Loss: 0.6359, Val Loss: 1.4796
batch size: (883, 883)
Epoch 123, accuracy: 0.1690
batch size: (896, 896)
Epoch 124, accuracy: 0.1666
Epoch 124, Train Loss: 0.7352, Val Loss: 1.4827
batch size: (898, 898)
Epoch 125, accuracy: 0.1701
batch size: (903, 903)
Epoch 126, accuracy: 0.1679
Epoch 126, Train Loss: 0.6517, Val Loss: 1.4953
batch size: (896, 896)
Epoch 127, accuracy: 0.1661
batch size: (895, 895)
Epoch 128, accuracy: 0.1696
Epoch 128, Train Loss: 0.7126, Val Loss: 1.4878
batch size: (885, 885)
Epoch 129, accuracy: 0.1667
batch size: (909, 909)
Epoch 130, accuracy: 0.1683
Epoch 130, Train Loss: 0.6760, Val Loss: 1.5216
batch size: (885, 885)
Epoch 131, accuracy: 0.1695
batch size: (897, 897)
Epoch 132, accuracy: 0.1655
Epoch 132, Train Loss: 0.7772, Val Loss: 1.5039
batch size: (887, 887)
Epoch 133, accuracy: 0.1689
batch size: (891, 891)
Epoch 134, accuracy: 0.1688
Epoch 134, Train Loss: 0.7538, Val Loss: 1.4589
batch size: (914, 914)
Epoch 135, accuracy: 0.1700
batch size: (906, 906)
Epoch 136, accuracy: 0.1670
Epoch 136, Train Loss: 0.6099, Val Loss: 1.4988
batch size: (902, 902)
Epoch 137, accuracy: 0.1691
batch size: (891, 891)
Epoch 138, accuracy: 0.1687
Epoch 138, Train Loss: 0.7765, Val Loss: 1.5057
batch size: (887, 887)
Epoch 139, accuracy: 0.1686
batch size: (910, 910)
Epoch 140, accuracy: 0.1689
Epoch 140, Train Loss: 0.7545, Val Loss: 1.5096
batch size: (898, 898)
Epoch 141, accuracy: 0.1659
batch size: (888, 888)
Epoch 142, accuracy: 0.1685
Epoch 142, Train Loss: 0.6300, Val Loss: 1.5150
batch size: (895, 895)
Epoch 143, accuracy: 0.1665
batch size: (909, 909)
Epoch 144, accuracy: 0.1685
Epoch 144, Train Loss: 0.7574, Val Loss: 1.5075
batch size: (901, 901)
Epoch 145, accuracy: 0.1660
batch size: (878, 878)
Epoch 146, accuracy: 0.1673
Epoch 146, Train Loss: 0.6240, Val Loss: 1.4939
batch size: (899, 899)
Epoch 147, accuracy: 0.1680
batch size: (903, 903)
Epoch 148, accuracy: 0.1653
Epoch 148, Train Loss: 0.6951, Val Loss: 1.4926
batch size: (899, 899)
Epoch 149, accuracy: 0.1628
batch size: (900, 900)
Epoch 150, accuracy: 0.1634
Epoch 150, Train Loss: 0.8133, Val Loss: 1.5181
batch size: (880, 880)
Epoch 151, accuracy: 0.1679
batch size: (883, 883)
Epoch 152, accuracy: 0.1670
Epoch 152, Train Loss: 0.6814, Val Loss: 1.4716
batch size: (900, 900)
Epoch 153, accuracy: 0.1680
batch size: (895, 895)
Epoch 154, accuracy: 0.1683
Epoch 154, Train Loss: 0.8055, Val Loss: 1.5039
batch size: (918, 918)
Epoch 155, accuracy: 0.1702
batch size: (884, 884)
Epoch 156, accuracy: 0.1681
Epoch 156, Train Loss: 0.6852, Val Loss: 1.4766
batch size: (901, 901)
Epoch 157, accuracy: 0.1660
batch size: (917, 917)
Epoch 158, accuracy: 0.1645
Epoch 158, Train Loss: 0.7079, Val Loss: 1.5000
batch size: (892, 892)
Epoch 159, accuracy: 0.1694
batch size: (901, 901)
Epoch 160, accuracy: 0.1646
Epoch 160, Train Loss: 0.6924, Val Loss: 1.5100
batch size: (878, 878)
Epoch 161, accuracy: 0.1683
batch size: (907, 907)
Epoch 162, accuracy: 0.1637
Epoch 162, Train Loss: 0.7304, Val Loss: 1.5382
batch size: (897, 897)
Epoch 163, accuracy: 0.1661
batch size: (903, 903)
Epoch 164, accuracy: 0.1645
Epoch 164, Train Loss: 0.6878, Val Loss: 1.5397
batch size: (905, 905)
Epoch 165, accuracy: 0.1673
batch size: (905, 905)
Epoch 166, accuracy: 0.1633
Epoch 166, Train Loss: 0.7541, Val Loss: 1.5301
batch size: (892, 892)
Epoch 167, accuracy: 0.1639
batch size: (903, 903)
Epoch 168, accuracy: 0.1648
Epoch 168, Train Loss: 0.7278, Val Loss: 1.4993
batch size: (886, 886)
Epoch 169, accuracy: 0.1661
batch size: (881, 881)
Epoch 170, accuracy: 0.1637
Epoch 170, Train Loss: 0.7361, Val Loss: 1.4860
batch size: (903, 903)
Epoch 171, accuracy: 0.1679
batch size: (872, 872)
Epoch 172, accuracy: 0.1663
Epoch 172, Train Loss: 0.7836, Val Loss: 1.5192
batch size: (910, 910)
Epoch 173, accuracy: 0.1687
batch size: (928, 928)
Epoch 174, accuracy: 0.1699
Epoch 174, Train Loss: 0.7086, Val Loss: 1.4887
batch size: (885, 885)
Epoch 175, accuracy: 0.1680
batch size: (904, 904)
Epoch 176, accuracy: 0.1651
Epoch 176, Train Loss: 0.6562, Val Loss: 1.4884
batch size: (908, 908)
Epoch 177, accuracy: 0.1686
batch size: (907, 907)
Epoch 178, accuracy: 0.1675
Epoch 178, Train Loss: 0.7045, Val Loss: 1.5225
batch size: (920, 920)
Epoch 179, accuracy: 0.1671
batch size: (882, 882)
Epoch 180, accuracy: 0.1701
Epoch 180, Train Loss: 0.7149, Val Loss: 1.5191
batch size: (910, 910)
Epoch 181, accuracy: 0.1671
batch size: (908, 908)
Epoch 182, accuracy: 0.1661
Epoch 182, Train Loss: 0.7869, Val Loss: 1.5147
batch size: (926, 926)
Epoch 183, accuracy: 0.1699
batch size: (893, 893)
Epoch 184, accuracy: 0.1671
Epoch 184, Train Loss: 0.7279, Val Loss: 1.5068
batch size: (921, 921)
Epoch 185, accuracy: 0.1683
batch size: (905, 905)
Epoch 186, accuracy: 0.1687
Epoch 186, Train Loss: 0.6867, Val Loss: 1.4830
batch size: (868, 868)
Epoch 187, accuracy: 0.1659
batch size: (903, 903)
Epoch 188, accuracy: 0.1708
Epoch 188, Train Loss: 0.6490, Val Loss: 1.5180
batch size: (881, 881)
Epoch 189, accuracy: 0.1670
batch size: (897, 897)
Epoch 190, accuracy: 0.1692
Epoch 190, Train Loss: 0.6932, Val Loss: 1.4983
batch size: (894, 894)
Epoch 191, accuracy: 0.1685
batch size: (898, 898)
Epoch 192, accuracy: 0.1691
Epoch 192, Train Loss: 0.8673, Val Loss: 1.4814
batch size: (906, 906)
Epoch 193, accuracy: 0.1669
batch size: (906, 906)
Epoch 194, accuracy: 0.1642
Epoch 194, Train Loss: 0.8524, Val Loss: 1.5310
batch size: (899, 899)
Epoch 195, accuracy: 0.1676
batch size: (886, 886)
Epoch 196, accuracy: 0.1653
Epoch 196, Train Loss: 0.6757, Val Loss: 1.5034
batch size: (903, 903)
Epoch 197, accuracy: 0.1661
batch size: (897, 897)
Epoch 198, accuracy: 0.1686
Epoch 198, Train Loss: 0.6534, Val Loss: 1.5272
batch size: (902, 902)
Epoch 199, accuracy: 0.1646
Loaded best model with val_loss = 1.1789363622665405
test :accuracy 0.1673, f1_macro: 0.0955, f1_micro: 0.1673, auc: 0.5024
Training GCN with 32 layers...
可训练参数: 2130185_GCN
不可训练参数: 0
batch size: (896, 896)
✅ Epoch 0: New best model saved with val_loss = 1.2346
Epoch 0, accuracy: 0.1684
Epoch 0, Train Loss: 1.3163, Val Loss: 1.2346
batch size: (903, 903)
Epoch 1, accuracy: 0.1719
batch size: (915, 915)
Epoch 2, accuracy: 0.1673
Epoch 2, Train Loss: 1.2520, Val Loss: 1.2443
batch size: (907, 907)
✅ Epoch 3: New best model saved with val_loss = 1.2082
Epoch 3, accuracy: 0.1707
batch size: (911, 911)
Epoch 4, accuracy: 0.1666
Epoch 4, Train Loss: 1.1209, Val Loss: 1.2477
batch size: (894, 894)
Epoch 5, accuracy: 0.1676
batch size: (892, 892)
Epoch 6, accuracy: 0.1667
Epoch 6, Train Loss: 1.1329, Val Loss: 1.2758
batch size: (888, 888)
Epoch 7, accuracy: 0.1659
batch size: (897, 897)
✅ Epoch 8: New best model saved with val_loss = 1.2007
Epoch 8, accuracy: 0.1669
Epoch 8, Train Loss: 1.2586, Val Loss: 1.2007
batch size: (897, 897)
✅ Epoch 9: New best model saved with val_loss = 1.1919
Epoch 9, accuracy: 0.1681
batch size: (908, 908)
✅ Epoch 10: New best model saved with val_loss = 1.1474
Epoch 10, accuracy: 0.4300
Epoch 10, Train Loss: 1.1229, Val Loss: 1.1474
batch size: (881, 881)
✅ Epoch 11: New best model saved with val_loss = 1.1312
Epoch 11, accuracy: 0.2070
batch size: (878, 878)
✅ Epoch 12: New best model saved with val_loss = 1.1257
Epoch 12, accuracy: 0.4298
Epoch 12, Train Loss: 1.1890, Val Loss: 1.1257
batch size: (916, 916)
✅ Epoch 13: New best model saved with val_loss = 1.1125
Epoch 13, accuracy: 0.2100
batch size: (912, 912)
✅ Epoch 14: New best model saved with val_loss = 1.1105
Epoch 14, accuracy: 0.4239
Epoch 14, Train Loss: 1.0727, Val Loss: 1.1105
batch size: (898, 898)
Epoch 15, accuracy: 0.1971
batch size: (909, 909)
Epoch 16, accuracy: 0.1928
Epoch 16, Train Loss: 1.3420, Val Loss: 1.1124
batch size: (890, 890)
Epoch 17, accuracy: 0.1966
batch size: (906, 906)
✅ Epoch 18: New best model saved with val_loss = 1.1098
Epoch 18, accuracy: 0.1756
Epoch 18, Train Loss: 1.0978, Val Loss: 1.1098
batch size: (882, 882)
Epoch 19, accuracy: 0.1645
batch size: (900, 900)
Epoch 20, accuracy: 0.1714
Epoch 20, Train Loss: 1.2559, Val Loss: 1.1155
batch size: (894, 894)
Epoch 21, accuracy: 0.1657
batch size: (890, 890)
Epoch 22, accuracy: 0.1703
Epoch 22, Train Loss: 1.1591, Val Loss: 1.1181
batch size: (896, 896)
Epoch 23, accuracy: 0.1660
batch size: (889, 889)
Epoch 24, accuracy: 0.1640
Epoch 24, Train Loss: 1.0566, Val Loss: 1.1264
batch size: (889, 889)
Epoch 25, accuracy: 0.1678
batch size: (892, 892)
Epoch 26, accuracy: 0.1675
Epoch 26, Train Loss: 1.1483, Val Loss: 1.1237
batch size: (897, 897)
Epoch 27, accuracy: 0.1681
batch size: (906, 906)
Epoch 28, accuracy: 0.1648
Epoch 28, Train Loss: 1.1077, Val Loss: 1.1242
batch size: (906, 906)
Epoch 29, accuracy: 0.1694
batch size: (903, 903)
Epoch 30, accuracy: 0.1662
Epoch 30, Train Loss: 1.1458, Val Loss: 1.1242
batch size: (917, 917)
Epoch 31, accuracy: 0.1669
batch size: (898, 898)
Epoch 32, accuracy: 0.1695
Epoch 32, Train Loss: 1.0662, Val Loss: 1.1216
batch size: (885, 885)
Epoch 33, accuracy: 0.1655
batch size: (905, 905)
Epoch 34, accuracy: 0.1675
Epoch 34, Train Loss: 1.1385, Val Loss: 1.1201
batch size: (888, 888)
Epoch 35, accuracy: 0.1680
batch size: (895, 895)
Epoch 36, accuracy: 0.1685
Epoch 36, Train Loss: 1.1391, Val Loss: 1.1246
batch size: (904, 904)
Epoch 37, accuracy: 0.1667
batch size: (902, 902)
Epoch 38, accuracy: 0.1692
Epoch 38, Train Loss: 1.0543, Val Loss: 1.1217
batch size: (916, 916)
Epoch 39, accuracy: 0.1696
batch size: (912, 912)
Epoch 40, accuracy: 0.1640
Epoch 40, Train Loss: 1.0361, Val Loss: 1.1188
batch size: (915, 915)
Epoch 41, accuracy: 0.1668
batch size: (886, 886)
Epoch 42, accuracy: 0.1671
Epoch 42, Train Loss: 1.1879, Val Loss: 1.1227
batch size: (881, 881)
Epoch 43, accuracy: 0.1657
batch size: (889, 889)
Epoch 44, accuracy: 0.1671
Epoch 44, Train Loss: 1.0779, Val Loss: 1.1212
batch size: (894, 894)
Epoch 45, accuracy: 0.1697
batch size: (902, 902)
Epoch 46, accuracy: 0.1675
Epoch 46, Train Loss: 1.0982, Val Loss: 1.1177
batch size: (906, 906)
Epoch 47, accuracy: 0.1663
batch size: (917, 917)
Epoch 48, accuracy: 0.1659
Epoch 48, Train Loss: 1.1226, Val Loss: 1.1207
batch size: (910, 910)
Epoch 49, accuracy: 0.1692
batch size: (906, 906)
Epoch 50, accuracy: 0.1645
Epoch 50, Train Loss: 1.0568, Val Loss: 1.1196
batch size: (905, 905)
Epoch 51, accuracy: 0.1643
batch size: (883, 883)
Epoch 52, accuracy: 0.1704
Epoch 52, Train Loss: 1.0988, Val Loss: 1.1214
batch size: (906, 906)
Epoch 53, accuracy: 0.1677
batch size: (893, 893)
Epoch 54, accuracy: 0.1667
Epoch 54, Train Loss: 1.0267, Val Loss: 1.1172
batch size: (906, 906)
Epoch 55, accuracy: 0.1696
batch size: (894, 894)
Epoch 56, accuracy: 0.1669
Epoch 56, Train Loss: 1.0417, Val Loss: 1.1239
batch size: (907, 907)
Epoch 57, accuracy: 0.1663
batch size: (890, 890)
Epoch 58, accuracy: 0.1661
Epoch 58, Train Loss: 1.1354, Val Loss: 1.1289
batch size: (901, 901)
Epoch 59, accuracy: 0.1662
batch size: (898, 898)
Epoch 60, accuracy: 0.1703
Epoch 60, Train Loss: 1.1163, Val Loss: 1.1222
batch size: (933, 933)
Epoch 61, accuracy: 0.1689
batch size: (903, 903)
Epoch 62, accuracy: 0.1667
Epoch 62, Train Loss: 1.2275, Val Loss: 1.1217
batch size: (921, 921)
Epoch 63, accuracy: 0.1684
batch size: (902, 902)
Epoch 64, accuracy: 0.1667
Epoch 64, Train Loss: 1.0686, Val Loss: 1.1260
batch size: (897, 897)
Epoch 65, accuracy: 0.1684
batch size: (891, 891)
Epoch 66, accuracy: 0.1662
Epoch 66, Train Loss: 0.9978, Val Loss: 1.1211
batch size: (904, 904)
Epoch 67, accuracy: 0.1681
batch size: (908, 908)
Epoch 68, accuracy: 0.1642
Epoch 68, Train Loss: 1.0654, Val Loss: 1.1198
batch size: (897, 897)
Epoch 69, accuracy: 0.1698
batch size: (899, 899)
Epoch 70, accuracy: 0.1653
Epoch 70, Train Loss: 1.1110, Val Loss: 1.1261
batch size: (913, 913)
Epoch 71, accuracy: 0.1674
batch size: (890, 890)
Epoch 72, accuracy: 0.1700
Epoch 72, Train Loss: 1.0579, Val Loss: 1.1215
batch size: (931, 931)
Epoch 73, accuracy: 0.1663
batch size: (913, 913)
Epoch 74, accuracy: 0.1662
Epoch 74, Train Loss: 1.0835, Val Loss: 1.1248
batch size: (890, 890)
Epoch 75, accuracy: 0.1686
batch size: (895, 895)
Epoch 76, accuracy: 0.1670
Epoch 76, Train Loss: 1.1962, Val Loss: 1.1222
batch size: (887, 887)
Epoch 77, accuracy: 0.1699
batch size: (897, 897)
Epoch 78, accuracy: 0.1669
Epoch 78, Train Loss: 1.0446, Val Loss: 1.1204
batch size: (906, 906)
Epoch 79, accuracy: 0.1698
batch size: (881, 881)
Epoch 80, accuracy: 0.1674
Epoch 80, Train Loss: 1.1443, Val Loss: 1.1246
batch size: (895, 895)
Epoch 81, accuracy: 0.1660
batch size: (886, 886)
Epoch 82, accuracy: 0.1672
Epoch 82, Train Loss: 1.0391, Val Loss: 1.1244
batch size: (902, 902)
Epoch 83, accuracy: 0.1676
batch size: (912, 912)
Epoch 84, accuracy: 0.1699
Epoch 84, Train Loss: 1.0118, Val Loss: 1.1204
batch size: (897, 897)
Epoch 85, accuracy: 0.1658
batch size: (911, 911)
Epoch 86, accuracy: 0.1676
Epoch 86, Train Loss: 1.1811, Val Loss: 1.1226
batch size: (887, 887)
Epoch 87, accuracy: 0.1675
batch size: (906, 906)
Epoch 88, accuracy: 0.1686
Epoch 88, Train Loss: 1.2082, Val Loss: 1.1203
batch size: (890, 890)
Epoch 89, accuracy: 0.1680
batch size: (905, 905)
Epoch 90, accuracy: 0.1666
Epoch 90, Train Loss: 1.0917, Val Loss: 1.1195
batch size: (887, 887)
Epoch 91, accuracy: 0.1662
batch size: (914, 914)
Epoch 92, accuracy: 0.1702
Epoch 92, Train Loss: 1.0395, Val Loss: 1.1223
batch size: (899, 899)
Epoch 93, accuracy: 0.1654
batch size: (909, 909)
Epoch 94, accuracy: 0.1685
Epoch 94, Train Loss: 1.1704, Val Loss: 1.1221
batch size: (917, 917)
Epoch 95, accuracy: 0.1667
batch size: (898, 898)
Epoch 96, accuracy: 0.1692
Epoch 96, Train Loss: 1.0977, Val Loss: 1.1234
batch size: (904, 904)
Epoch 97, accuracy: 0.1690
batch size: (898, 898)
Epoch 98, accuracy: 0.1684
Epoch 98, Train Loss: 1.1294, Val Loss: 1.1181
batch size: (906, 906)
Epoch 99, accuracy: 0.1662
batch size: (903, 903)
Epoch 100, accuracy: 0.1690
Epoch 100, Train Loss: 1.1791, Val Loss: 1.1208
batch size: (903, 903)
Epoch 101, accuracy: 0.1625
batch size: (894, 894)
Epoch 102, accuracy: 0.1692
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 102, Train Loss: 1.1079, Val Loss: 1.1206
batch size: (888, 888)
Epoch 103, accuracy: 0.1683
batch size: (888, 888)
Epoch 104, accuracy: 0.1661
Epoch 104, Train Loss: 1.0305, Val Loss: 1.1217
batch size: (901, 901)
Epoch 105, accuracy: 0.1722
batch size: (903, 903)
Epoch 106, accuracy: 0.1667
Epoch 106, Train Loss: 1.0828, Val Loss: 1.1183
batch size: (897, 897)
Epoch 107, accuracy: 0.1691
batch size: (901, 901)
Epoch 108, accuracy: 0.1675
Epoch 108, Train Loss: 1.0356, Val Loss: 1.1184
batch size: (885, 885)
Epoch 109, accuracy: 0.1696
batch size: (901, 901)
Epoch 110, accuracy: 0.1675
Epoch 110, Train Loss: 1.1473, Val Loss: 1.1266
batch size: (905, 905)
Epoch 111, accuracy: 0.1644
batch size: (911, 911)
Epoch 112, accuracy: 0.1644
Epoch 112, Train Loss: 1.0698, Val Loss: 1.1216
batch size: (887, 887)
Epoch 113, accuracy: 0.1670
batch size: (903, 903)
Epoch 114, accuracy: 0.1657
Epoch 114, Train Loss: 1.1002, Val Loss: 1.1216
batch size: (901, 901)
Epoch 115, accuracy: 0.1679
batch size: (909, 909)
Epoch 116, accuracy: 0.1661
Epoch 116, Train Loss: 1.0282, Val Loss: 1.1195
batch size: (898, 898)
Epoch 117, accuracy: 0.1683
batch size: (910, 910)
Epoch 118, accuracy: 0.1664
Epoch 118, Train Loss: 1.1650, Val Loss: 1.1230
batch size: (898, 898)
Epoch 119, accuracy: 0.1681
batch size: (894, 894)
Epoch 120, accuracy: 0.1691
Epoch 120, Train Loss: 1.1440, Val Loss: 1.1192
batch size: (909, 909)
Epoch 121, accuracy: 0.1664
batch size: (894, 894)
Epoch 122, accuracy: 0.1668
Epoch 122, Train Loss: 1.1332, Val Loss: 1.1205
batch size: (906, 906)
Epoch 123, accuracy: 0.1648
batch size: (882, 882)
Epoch 124, accuracy: 0.1702
Epoch 124, Train Loss: 1.0417, Val Loss: 1.1227
batch size: (917, 917)
Epoch 125, accuracy: 0.1676
batch size: (913, 913)
Epoch 126, accuracy: 0.1668
Epoch 126, Train Loss: 1.0389, Val Loss: 1.1235
batch size: (907, 907)
Epoch 127, accuracy: 0.1684
batch size: (924, 924)
Epoch 128, accuracy: 0.1647
Epoch 128, Train Loss: 1.4154, Val Loss: 1.1256
batch size: (891, 891)
Epoch 129, accuracy: 0.1660
batch size: (901, 901)
Epoch 130, accuracy: 0.1692
Epoch 130, Train Loss: 1.0271, Val Loss: 1.1243
batch size: (907, 907)
Epoch 131, accuracy: 0.1693
batch size: (888, 888)
Epoch 132, accuracy: 0.1675
Epoch 132, Train Loss: 1.1475, Val Loss: 1.1238
batch size: (906, 906)
Epoch 133, accuracy: 0.1669
batch size: (915, 915)
Epoch 134, accuracy: 0.1670
Epoch 134, Train Loss: 1.0725, Val Loss: 1.1258
batch size: (885, 885)
Epoch 135, accuracy: 0.1680
batch size: (918, 918)
Epoch 136, accuracy: 0.1676
Epoch 136, Train Loss: 1.1339, Val Loss: 1.1201
batch size: (915, 915)
Epoch 137, accuracy: 0.1697
batch size: (874, 874)
Epoch 138, accuracy: 0.1682
Epoch 138, Train Loss: 1.1195, Val Loss: 1.1195
batch size: (894, 894)
Epoch 139, accuracy: 0.1653
batch size: (908, 908)
Epoch 140, accuracy: 0.1667
Epoch 140, Train Loss: 1.1758, Val Loss: 1.1197
batch size: (884, 884)
Epoch 141, accuracy: 0.1691
batch size: (892, 892)
Epoch 142, accuracy: 0.1682
Epoch 142, Train Loss: 1.0589, Val Loss: 1.1226
batch size: (875, 875)
Epoch 143, accuracy: 0.1672
batch size: (903, 903)
Epoch 144, accuracy: 0.1658
Epoch 144, Train Loss: 1.0487, Val Loss: 1.1270
batch size: (905, 905)
Epoch 145, accuracy: 0.1663
batch size: (901, 901)
Epoch 146, accuracy: 0.1685
Epoch 146, Train Loss: 1.1709, Val Loss: 1.1212
batch size: (882, 882)
Epoch 147, accuracy: 0.1675
batch size: (898, 898)
Epoch 148, accuracy: 0.1653
Epoch 148, Train Loss: 1.2679, Val Loss: 1.1233
batch size: (889, 889)
Epoch 149, accuracy: 0.1704
batch size: (902, 902)
Epoch 150, accuracy: 0.1682
Epoch 150, Train Loss: 1.3290, Val Loss: 1.1201
batch size: (898, 898)
Epoch 151, accuracy: 0.1667
batch size: (894, 894)
Epoch 152, accuracy: 0.1706
Epoch 152, Train Loss: 1.1394, Val Loss: 1.1245
batch size: (879, 879)
Epoch 153, accuracy: 0.1699
batch size: (889, 889)
Epoch 154, accuracy: 0.1670
Epoch 154, Train Loss: 1.1226, Val Loss: 1.1227
batch size: (909, 909)
Epoch 155, accuracy: 0.1721
batch size: (893, 893)
Epoch 156, accuracy: 0.1705
Epoch 156, Train Loss: 1.0220, Val Loss: 1.1225
batch size: (912, 912)
Epoch 157, accuracy: 0.1659
batch size: (912, 912)
Epoch 158, accuracy: 0.1663
Epoch 158, Train Loss: 0.9907, Val Loss: 1.1248
batch size: (901, 901)
Epoch 159, accuracy: 0.1652
batch size: (915, 915)
Epoch 160, accuracy: 0.1644
Epoch 160, Train Loss: 1.0207, Val Loss: 1.1217
batch size: (904, 904)
Epoch 161, accuracy: 0.1700
batch size: (914, 914)
Epoch 162, accuracy: 0.1682
Epoch 162, Train Loss: 1.1154, Val Loss: 1.1222
batch size: (900, 900)
Epoch 163, accuracy: 0.1686
batch size: (906, 906)
Epoch 164, accuracy: 0.1656
Epoch 164, Train Loss: 1.0907, Val Loss: 1.1216
batch size: (894, 894)
Epoch 165, accuracy: 0.1663
batch size: (907, 907)
Epoch 166, accuracy: 0.1666
Epoch 166, Train Loss: 1.1169, Val Loss: 1.1181
batch size: (891, 891)
Epoch 167, accuracy: 0.1693
batch size: (888, 888)
Epoch 168, accuracy: 0.1663
Epoch 168, Train Loss: 1.2680, Val Loss: 1.1206
batch size: (893, 893)
Epoch 169, accuracy: 0.1689
batch size: (900, 900)
Epoch 170, accuracy: 0.1686
Epoch 170, Train Loss: 1.0483, Val Loss: 1.1208
batch size: (896, 896)
Epoch 171, accuracy: 0.1684
batch size: (906, 906)
Epoch 172, accuracy: 0.1666
Epoch 172, Train Loss: 1.0491, Val Loss: 1.1177
batch size: (890, 890)
Epoch 173, accuracy: 0.1661
batch size: (898, 898)
Epoch 174, accuracy: 0.1676
Epoch 174, Train Loss: 1.0810, Val Loss: 1.1212
batch size: (879, 879)
Epoch 175, accuracy: 0.1644
batch size: (904, 904)
Epoch 176, accuracy: 0.1657
Epoch 176, Train Loss: 1.1239, Val Loss: 1.1243
batch size: (900, 900)
Epoch 177, accuracy: 0.1658
batch size: (908, 908)
Epoch 178, accuracy: 0.1650
Epoch 178, Train Loss: 1.1555, Val Loss: 1.1235
batch size: (909, 909)
Epoch 179, accuracy: 0.1670
batch size: (917, 917)
Epoch 180, accuracy: 0.1673
Epoch 180, Train Loss: 1.0321, Val Loss: 1.1253
batch size: (898, 898)
Epoch 181, accuracy: 0.1658
batch size: (870, 870)
Epoch 182, accuracy: 0.1697
Epoch 182, Train Loss: 1.0700, Val Loss: 1.1221
batch size: (891, 891)
Epoch 183, accuracy: 0.1710
batch size: (886, 886)
Epoch 184, accuracy: 0.1673
Epoch 184, Train Loss: 1.0362, Val Loss: 1.1236
batch size: (902, 902)
Epoch 185, accuracy: 0.1648
batch size: (895, 895)
Epoch 186, accuracy: 0.1647
Epoch 186, Train Loss: 0.9775, Val Loss: 1.1179
batch size: (902, 902)
Epoch 187, accuracy: 0.1691
batch size: (908, 908)
Epoch 188, accuracy: 0.1695
Epoch 188, Train Loss: 1.0301, Val Loss: 1.1209
batch size: (889, 889)
Epoch 189, accuracy: 0.1668
batch size: (900, 900)
Epoch 190, accuracy: 0.1664
Epoch 190, Train Loss: 1.0464, Val Loss: 1.1198
batch size: (906, 906)
Epoch 191, accuracy: 0.1692
batch size: (892, 892)
Epoch 192, accuracy: 0.1661
Epoch 192, Train Loss: 1.2166, Val Loss: 1.1236
batch size: (886, 886)
Epoch 193, accuracy: 0.1675
batch size: (900, 900)
Epoch 194, accuracy: 0.1693
Epoch 194, Train Loss: 1.1069, Val Loss: 1.1217
batch size: (884, 884)
Epoch 195, accuracy: 0.1651
batch size: (874, 874)
Epoch 196, accuracy: 0.1679
Epoch 196, Train Loss: 0.9867, Val Loss: 1.1209
batch size: (896, 896)
Epoch 197, accuracy: 0.1672
batch size: (905, 905)
Epoch 198, accuracy: 0.1683
Epoch 198, Train Loss: 1.0852, Val Loss: 1.1233
batch size: (926, 926)
Epoch 199, accuracy: 0.1696
Loaded best model with val_loss = 1.109790325164795
test :accuracy 0.1765, f1_macro: 0.1124, f1_micro: 0.1765, auc: 0.5006
Training GraphSAGE with 2 layers...
可训练参数: 262153_GraphSAGE
不可训练参数: 0
batch size: (895, 895)
✅ Epoch 0: New best model saved with val_loss = 1.0921
Epoch 0, accuracy: 0.5494
Epoch 0, Train Loss: 1.1405, Val Loss: 1.0921
batch size: (903, 903)
✅ Epoch 1: New best model saved with val_loss = 1.0825
Epoch 1, accuracy: 0.5982
batch size: (921, 921)
✅ Epoch 2: New best model saved with val_loss = 1.0690
Epoch 2, accuracy: 0.6357
Epoch 2, Train Loss: 0.1719, Val Loss: 1.0690
batch size: (913, 913)
✅ Epoch 3: New best model saved with val_loss = 1.0559
Epoch 3, accuracy: 0.6601
batch size: (891, 891)
✅ Epoch 4: New best model saved with val_loss = 1.0434
Epoch 4, accuracy: 0.6627
Epoch 4, Train Loss: 0.0284, Val Loss: 1.0434
batch size: (905, 905)
✅ Epoch 5: New best model saved with val_loss = 1.0312
Epoch 5, accuracy: 0.6663
batch size: (910, 910)
✅ Epoch 6: New best model saved with val_loss = 1.0195
Epoch 6, accuracy: 0.6654
Epoch 6, Train Loss: 0.0026, Val Loss: 1.0195
batch size: (920, 920)
✅ Epoch 7: New best model saved with val_loss = 1.0082
Epoch 7, accuracy: 0.6621
batch size: (895, 895)
✅ Epoch 8: New best model saved with val_loss = 0.9971
Epoch 8, accuracy: 0.6614
Epoch 8, Train Loss: 0.0010, Val Loss: 0.9971
batch size: (908, 908)
✅ Epoch 9: New best model saved with val_loss = 0.9855
Epoch 9, accuracy: 0.6626
batch size: (895, 895)
✅ Epoch 10: New best model saved with val_loss = 0.9739
Epoch 10, accuracy: 0.6605
Epoch 10, Train Loss: 0.0011, Val Loss: 0.9739
batch size: (891, 891)
✅ Epoch 11: New best model saved with val_loss = 0.9627
Epoch 11, accuracy: 0.6687
batch size: (904, 904)
✅ Epoch 12: New best model saved with val_loss = 0.9499
Epoch 12, accuracy: 0.6741
Epoch 12, Train Loss: 0.0015, Val Loss: 0.9499
batch size: (898, 898)
✅ Epoch 13: New best model saved with val_loss = 0.9383
Epoch 13, accuracy: 0.6862
batch size: (919, 919)
✅ Epoch 14: New best model saved with val_loss = 0.9242
Epoch 14, accuracy: 0.6960
Epoch 14, Train Loss: 0.0003, Val Loss: 0.9242
batch size: (911, 911)
✅ Epoch 15: New best model saved with val_loss = 0.9128
Epoch 15, accuracy: 0.7046
batch size: (890, 890)
✅ Epoch 16: New best model saved with val_loss = 0.9010
Epoch 16, accuracy: 0.7107
Epoch 16, Train Loss: 0.0003, Val Loss: 0.9010
batch size: (881, 881)
✅ Epoch 17: New best model saved with val_loss = 0.8865
Epoch 17, accuracy: 0.7186
batch size: (895, 895)
✅ Epoch 18: New best model saved with val_loss = 0.8747
Epoch 18, accuracy: 0.7183
Epoch 18, Train Loss: 0.0001, Val Loss: 0.8747
batch size: (905, 905)
✅ Epoch 19: New best model saved with val_loss = 0.8623
Epoch 19, accuracy: 0.7195
batch size: (899, 899)
✅ Epoch 20: New best model saved with val_loss = 0.8507
Epoch 20, accuracy: 0.7245
Epoch 20, Train Loss: 0.0001, Val Loss: 0.8507
batch size: (908, 908)
✅ Epoch 21: New best model saved with val_loss = 0.8356
Epoch 21, accuracy: 0.7224
batch size: (911, 911)
✅ Epoch 22: New best model saved with val_loss = 0.8245
Epoch 22, accuracy: 0.7247
Epoch 22, Train Loss: 0.0003, Val Loss: 0.8245
batch size: (888, 888)
✅ Epoch 23: New best model saved with val_loss = 0.8126
Epoch 23, accuracy: 0.7287
batch size: (901, 901)
✅ Epoch 24: New best model saved with val_loss = 0.7969
Epoch 24, accuracy: 0.7241
Epoch 24, Train Loss: 0.0003, Val Loss: 0.7969
batch size: (915, 915)
✅ Epoch 25: New best model saved with val_loss = 0.7858
Epoch 25, accuracy: 0.7271
batch size: (898, 898)
✅ Epoch 26: New best model saved with val_loss = 0.7726
Epoch 26, accuracy: 0.7250
Epoch 26, Train Loss: 0.0001, Val Loss: 0.7726
batch size: (903, 903)
✅ Epoch 27: New best model saved with val_loss = 0.7605
Epoch 27, accuracy: 0.7258
batch size: (897, 897)
✅ Epoch 28: New best model saved with val_loss = 0.7490
Epoch 28, accuracy: 0.7264
Epoch 28, Train Loss: 0.0005, Val Loss: 0.7490
batch size: (903, 903)
✅ Epoch 29: New best model saved with val_loss = 0.7370
Epoch 29, accuracy: 0.7251
batch size: (894, 894)
✅ Epoch 30: New best model saved with val_loss = 0.7254
Epoch 30, accuracy: 0.7266
Epoch 30, Train Loss: 0.0001, Val Loss: 0.7254
batch size: (889, 889)
✅ Epoch 31: New best model saved with val_loss = 0.7134
Epoch 31, accuracy: 0.7265
batch size: (891, 891)
✅ Epoch 32: New best model saved with val_loss = 0.7018
Epoch 32, accuracy: 0.7242
Epoch 32, Train Loss: 0.0001, Val Loss: 0.7018
batch size: (915, 915)
✅ Epoch 33: New best model saved with val_loss = 0.6895
Epoch 33, accuracy: 0.7256
batch size: (879, 879)
✅ Epoch 34: New best model saved with val_loss = 0.6794
Epoch 34, accuracy: 0.7250
Epoch 34, Train Loss: 0.0001, Val Loss: 0.6794
batch size: (898, 898)
✅ Epoch 35: New best model saved with val_loss = 0.6696
Epoch 35, accuracy: 0.7288
batch size: (887, 887)
✅ Epoch 36: New best model saved with val_loss = 0.6548
Epoch 36, accuracy: 0.7268
Epoch 36, Train Loss: 0.0001, Val Loss: 0.6548
batch size: (875, 875)
✅ Epoch 37: New best model saved with val_loss = 0.6540
Epoch 37, accuracy: 0.7301
batch size: (904, 904)
✅ Epoch 38: New best model saved with val_loss = 0.6419
Epoch 38, accuracy: 0.7284
Epoch 38, Train Loss: 0.0003, Val Loss: 0.6419
batch size: (895, 895)
✅ Epoch 39: New best model saved with val_loss = 0.6300
Epoch 39, accuracy: 0.7291
batch size: (893, 893)
✅ Epoch 40: New best model saved with val_loss = 0.6222
Epoch 40, accuracy: 0.7292
Epoch 40, Train Loss: 0.0001, Val Loss: 0.6222
batch size: (901, 901)
✅ Epoch 41: New best model saved with val_loss = 0.6202
Epoch 41, accuracy: 0.7281
batch size: (900, 900)
✅ Epoch 42: New best model saved with val_loss = 0.6035
Epoch 42, accuracy: 0.7284
Epoch 42, Train Loss: 0.0001, Val Loss: 0.6035
batch size: (884, 884)
✅ Epoch 43: New best model saved with val_loss = 0.5994
Epoch 43, accuracy: 0.7290
batch size: (898, 898)
✅ Epoch 44: New best model saved with val_loss = 0.5947
Epoch 44, accuracy: 0.7293
Epoch 44, Train Loss: 0.0003, Val Loss: 0.5947
batch size: (900, 900)
✅ Epoch 45: New best model saved with val_loss = 0.5940
Epoch 45, accuracy: 0.7289
batch size: (899, 899)
✅ Epoch 46: New best model saved with val_loss = 0.5875
Epoch 46, accuracy: 0.7285
Epoch 46, Train Loss: 0.0001, Val Loss: 0.5875
batch size: (900, 900)
✅ Epoch 47: New best model saved with val_loss = 0.5807
Epoch 47, accuracy: 0.7289
batch size: (888, 888)
✅ Epoch 48: New best model saved with val_loss = 0.5751
Epoch 48, accuracy: 0.7318
Epoch 48, Train Loss: 0.0002, Val Loss: 0.5751
batch size: (894, 894)
Epoch 49, accuracy: 0.7325
batch size: (898, 898)
Epoch 50, accuracy: 0.7304
Epoch 50, Train Loss: 0.0001, Val Loss: 0.5796
batch size: (906, 906)
✅ Epoch 51: New best model saved with val_loss = 0.5749
Epoch 51, accuracy: 0.7323
batch size: (892, 892)
✅ Epoch 52: New best model saved with val_loss = 0.5730
Epoch 52, accuracy: 0.7287
Epoch 52, Train Loss: 0.0001, Val Loss: 0.5730
batch size: (897, 897)
✅ Epoch 53: New best model saved with val_loss = 0.5670
Epoch 53, accuracy: 0.7304
batch size: (886, 886)
Epoch 54, accuracy: 0.7325
Epoch 54, Train Loss: 0.0002, Val Loss: 0.5709
batch size: (915, 915)
Epoch 55, accuracy: 0.7289
batch size: (895, 895)
Epoch 56, accuracy: 0.7291
Epoch 56, Train Loss: 0.0001, Val Loss: 0.5748
batch size: (900, 900)
Epoch 57, accuracy: 0.7302
batch size: (891, 891)
Epoch 58, accuracy: 0.7286
Epoch 58, Train Loss: 0.0001, Val Loss: 0.5752
batch size: (888, 888)
Epoch 59, accuracy: 0.7321
batch size: (907, 907)
Epoch 60, accuracy: 0.7301
Epoch 60, Train Loss: 0.0001, Val Loss: 0.5968
batch size: (902, 902)
Epoch 61, accuracy: 0.7324
batch size: (893, 893)
Epoch 62, accuracy: 0.7297
Epoch 62, Train Loss: 0.0001, Val Loss: 0.5982
batch size: (895, 895)
Epoch 63, accuracy: 0.7311
batch size: (914, 914)
Epoch 64, accuracy: 0.7323
Epoch 64, Train Loss: 0.0002, Val Loss: 0.6233
batch size: (906, 906)
Epoch 65, accuracy: 0.7300
batch size: (891, 891)
Epoch 66, accuracy: 0.7295
Epoch 66, Train Loss: 0.0001, Val Loss: 0.6242
batch size: (889, 889)
Epoch 67, accuracy: 0.7320
batch size: (906, 906)
Epoch 68, accuracy: 0.7301
Epoch 68, Train Loss: 0.0006, Val Loss: 0.6436
batch size: (885, 885)
Epoch 69, accuracy: 0.7326
batch size: (903, 903)
Epoch 70, accuracy: 0.7290
Epoch 70, Train Loss: 0.0001, Val Loss: 0.6680
batch size: (907, 907)
Epoch 71, accuracy: 0.7319
batch size: (916, 916)
Epoch 72, accuracy: 0.7319
Epoch 72, Train Loss: 0.0001, Val Loss: 0.6873
batch size: (889, 889)
Epoch 73, accuracy: 0.7303
batch size: (896, 896)
Epoch 74, accuracy: 0.7293
Epoch 74, Train Loss: 0.0001, Val Loss: 0.7022
batch size: (908, 908)
Epoch 75, accuracy: 0.7313
batch size: (902, 902)
Epoch 76, accuracy: 0.7290
Epoch 76, Train Loss: 0.0001, Val Loss: 0.7143
batch size: (914, 914)
Epoch 77, accuracy: 0.7312
batch size: (890, 890)
Epoch 78, accuracy: 0.7286
Epoch 78, Train Loss: 0.0001, Val Loss: 0.7188
batch size: (905, 905)
Epoch 79, accuracy: 0.7306
batch size: (918, 918)
Epoch 80, accuracy: 0.7305
Epoch 80, Train Loss: 0.0003, Val Loss: 0.7382
batch size: (923, 923)
Epoch 81, accuracy: 0.7285
batch size: (909, 909)
Epoch 82, accuracy: 0.7322
Epoch 82, Train Loss: 0.0001, Val Loss: 0.7609
batch size: (931, 931)
Epoch 83, accuracy: 0.7306
batch size: (907, 907)
Epoch 84, accuracy: 0.7284
Epoch 84, Train Loss: 0.0002, Val Loss: 0.7861
batch size: (903, 903)
Epoch 85, accuracy: 0.7283
batch size: (902, 902)
Epoch 86, accuracy: 0.7328
Epoch 86, Train Loss: 0.0002, Val Loss: 0.7765
batch size: (906, 906)
Epoch 87, accuracy: 0.7298
batch size: (914, 914)
Epoch 88, accuracy: 0.7291
Epoch 88, Train Loss: 0.0001, Val Loss: 0.8051
batch size: (903, 903)
Epoch 89, accuracy: 0.7305
batch size: (883, 883)
Epoch 90, accuracy: 0.7278
Epoch 90, Train Loss: 0.0001, Val Loss: 0.8081
batch size: (888, 888)
Epoch 91, accuracy: 0.7271
batch size: (896, 896)
Epoch 92, accuracy: 0.7303
Epoch 92, Train Loss: 0.0001, Val Loss: 0.8056
batch size: (934, 934)
Epoch 93, accuracy: 0.7292
batch size: (905, 905)
Epoch 94, accuracy: 0.7294
Epoch 94, Train Loss: 0.0001, Val Loss: 0.7988
batch size: (917, 917)
Epoch 95, accuracy: 0.7276
batch size: (885, 885)
Epoch 96, accuracy: 0.7286
Epoch 96, Train Loss: 0.0001, Val Loss: 0.8089
batch size: (891, 891)
Epoch 97, accuracy: 0.7296
batch size: (919, 919)
Epoch 98, accuracy: 0.7274
Epoch 98, Train Loss: 0.0001, Val Loss: 0.8107
batch size: (882, 882)
Epoch 99, accuracy: 0.7289
batch size: (907, 907)
Epoch 100, accuracy: 0.7282
Epoch 100, Train Loss: 0.0001, Val Loss: 0.8012
batch size: (914, 914)
Epoch 101, accuracy: 0.7286
batch size: (888, 888)
Epoch 102, accuracy: 0.7283
Epoch 102, Train Loss: 0.0002, Val Loss: 0.8108
batch size: (900, 900)
Epoch 103, accuracy: 0.7304
batch size: (890, 890)
Epoch 104, accuracy: 0.7292
Epoch 104, Train Loss: 0.0001, Val Loss: 0.8030
batch size: (905, 905)
Epoch 105, accuracy: 0.7295
batch size: (929, 929)
Epoch 106, accuracy: 0.7299
Epoch 106, Train Loss: 0.0002, Val Loss: 0.8239
batch size: (893, 893)
Epoch 107, accuracy: 0.7291
batch size: (890, 890)
Epoch 108, accuracy: 0.7300
Epoch 108, Train Loss: 0.0001, Val Loss: 0.8216
batch size: (920, 920)
Epoch 109, accuracy: 0.7300
batch size: (891, 891)
Epoch 110, accuracy: 0.7262
Epoch 110, Train Loss: 0.0001, Val Loss: 0.8210
batch size: (893, 893)
Epoch 111, accuracy: 0.7265
batch size: (900, 900)
Epoch 112, accuracy: 0.7282
Epoch 112, Train Loss: 0.0001, Val Loss: 0.8181
batch size: (886, 886)
Epoch 113, accuracy: 0.7298
batch size: (891, 891)
Epoch 114, accuracy: 0.7280
Epoch 114, Train Loss: 0.0001, Val Loss: 0.8482
batch size: (883, 883)
Epoch 115, accuracy: 0.7321
batch size: (898, 898)
Epoch 116, accuracy: 0.7271
Epoch 116, Train Loss: 0.0001, Val Loss: 0.8231
batch size: (890, 890)
Epoch 117, accuracy: 0.7267
batch size: (895, 895)
Epoch 118, accuracy: 0.7293
Epoch 118, Train Loss: 0.0001, Val Loss: 0.8137
batch size: (906, 906)
Epoch 119, accuracy: 0.7289
batch size: (898, 898)
Epoch 120, accuracy: 0.7284
Epoch 120, Train Loss: 0.0001, Val Loss: 0.8235
batch size: (904, 904)
Epoch 121, accuracy: 0.7287
batch size: (895, 895)
Epoch 122, accuracy: 0.7305
Epoch 122, Train Loss: 0.0002, Val Loss: 0.8132
batch size: (894, 894)
Epoch 123, accuracy: 0.7284
batch size: (882, 882)
Epoch 124, accuracy: 0.7283
Epoch 124, Train Loss: 0.0002, Val Loss: 0.8330
batch size: (917, 917)
Epoch 125, accuracy: 0.7286
batch size: (897, 897)
Epoch 126, accuracy: 0.7282
Epoch 126, Train Loss: 0.0002, Val Loss: 0.8187
batch size: (904, 904)
Epoch 127, accuracy: 0.7292
batch size: (925, 925)
Epoch 128, accuracy: 0.7295
Epoch 128, Train Loss: 0.0002, Val Loss: 0.8237
batch size: (890, 890)
Epoch 129, accuracy: 0.7282
batch size: (897, 897)
Epoch 130, accuracy: 0.7299
Epoch 130, Train Loss: 0.0002, Val Loss: 0.8176
batch size: (900, 900)
Epoch 131, accuracy: 0.7290
batch size: (892, 892)
Epoch 132, accuracy: 0.7282
Epoch 132, Train Loss: 0.0003, Val Loss: 0.8431
batch size: (886, 886)
Epoch 133, accuracy: 0.7263
batch size: (902, 902)
Epoch 134, accuracy: 0.7294
Epoch 134, Train Loss: 0.0001, Val Loss: 0.8237
batch size: (887, 887)
Epoch 135, accuracy: 0.7288
batch size: (914, 914)
Epoch 136, accuracy: 0.7314
Epoch 136, Train Loss: 0.0001, Val Loss: 0.8119
batch size: (891, 891)
Epoch 137, accuracy: 0.7268
batch size: (901, 901)
Epoch 138, accuracy: 0.7288
Epoch 138, Train Loss: 0.0002, Val Loss: 0.8264
batch size: (898, 898)
Epoch 139, accuracy: 0.7285
batch size: (910, 910)
Epoch 140, accuracy: 0.7304
Epoch 140, Train Loss: 0.0002, Val Loss: 0.8301
batch size: (906, 906)
Epoch 141, accuracy: 0.7294
batch size: (902, 902)
Epoch 142, accuracy: 0.7267
Epoch 142, Train Loss: 0.0002, Val Loss: 0.8091
batch size: (890, 890)
Epoch 143, accuracy: 0.7315
batch size: (867, 867)
Epoch 144, accuracy: 0.7268
Epoch 144, Train Loss: 0.0001, Val Loss: 0.8264
batch size: (882, 882)
Epoch 145, accuracy: 0.7266
batch size: (897, 897)
Epoch 146, accuracy: 0.7280
Epoch 146, Train Loss: 0.0002, Val Loss: 0.8176
batch size: (897, 897)
Epoch 147, accuracy: 0.7269
batch size: (899, 899)
Epoch 148, accuracy: 0.7280
Epoch 148, Train Loss: 0.0001, Val Loss: 0.8258
batch size: (884, 884)
Epoch 149, accuracy: 0.7283
batch size: (898, 898)
Epoch 150, accuracy: 0.7280
Epoch 150, Train Loss: 0.0001, Val Loss: 0.8257
batch size: (895, 895)
Epoch 151, accuracy: 0.7279
batch size: (902, 902)
Epoch 152, accuracy: 0.7291
Epoch 152, Train Loss: 0.0002, Val Loss: 0.8068
batch size: (890, 890)
Epoch 153, accuracy: 0.7283
batch size: (897, 897)
Epoch 154, accuracy: 0.7272
Epoch 154, Train Loss: 0.0002, Val Loss: 0.8342
batch size: (893, 893)
Epoch 155, accuracy: 0.7279
batch size: (890, 890)
Epoch 156, accuracy: 0.7286
Epoch 156, Train Loss: 0.0002, Val Loss: 0.8039
batch size: (904, 904)
Epoch 157, accuracy: 0.7293
batch size: (905, 905)
Epoch 158, accuracy: 0.7272
Epoch 158, Train Loss: 0.0001, Val Loss: 0.8122
batch size: (898, 898)
Epoch 159, accuracy: 0.7268
batch size: (899, 899)
Epoch 160, accuracy: 0.7265
Epoch 160, Train Loss: 0.0001, Val Loss: 0.8280
batch size: (903, 903)
Epoch 161, accuracy: 0.7290
batch size: (893, 893)
Epoch 162, accuracy: 0.7268
Epoch 162, Train Loss: 0.0001, Val Loss: 0.8307
batch size: (905, 905)
Epoch 163, accuracy: 0.7281
batch size: (926, 926)
Epoch 164, accuracy: 0.7300
Epoch 164, Train Loss: 0.0001, Val Loss: 0.8364
batch size: (898, 898)
Epoch 165, accuracy: 0.7287
batch size: (899, 899)
Epoch 166, accuracy: 0.7303
Epoch 166, Train Loss: 0.0001, Val Loss: 0.8343
batch size: (881, 881)
Epoch 167, accuracy: 0.7274
batch size: (917, 917)
Epoch 168, accuracy: 0.7250
Epoch 168, Train Loss: 0.0003, Val Loss: 0.8275
batch size: (882, 882)
Epoch 169, accuracy: 0.7309
batch size: (906, 906)
Epoch 170, accuracy: 0.7277
Epoch 170, Train Loss: 0.0002, Val Loss: 0.8354
batch size: (892, 892)
Epoch 171, accuracy: 0.7298
batch size: (899, 899)
Epoch 172, accuracy: 0.7281
Epoch 172, Train Loss: 0.0004, Val Loss: 0.8255
batch size: (910, 910)
Epoch 173, accuracy: 0.7279
batch size: (888, 888)
Epoch 174, accuracy: 0.7287
Epoch 174, Train Loss: 0.0001, Val Loss: 0.8223
batch size: (913, 913)
Epoch 175, accuracy: 0.7291
batch size: (886, 886)
Epoch 176, accuracy: 0.7291
Epoch 176, Train Loss: 0.0002, Val Loss: 0.8181
batch size: (878, 878)
Epoch 177, accuracy: 0.7289
batch size: (896, 896)
Epoch 178, accuracy: 0.7255
Epoch 178, Train Loss: 0.0001, Val Loss: 0.8224
batch size: (906, 906)
Epoch 179, accuracy: 0.7294
batch size: (908, 908)
Epoch 180, accuracy: 0.7291
Epoch 180, Train Loss: 0.0004, Val Loss: 0.8290
batch size: (910, 910)
Epoch 181, accuracy: 0.7277
batch size: (907, 907)
Epoch 182, accuracy: 0.7279
Epoch 182, Train Loss: 0.0001, Val Loss: 0.8327
batch size: (895, 895)
Epoch 183, accuracy: 0.7275
batch size: (885, 885)
Epoch 184, accuracy: 0.7283
Epoch 184, Train Loss: 0.0002, Val Loss: 0.8350
batch size: (906, 906)
Epoch 185, accuracy: 0.7259
batch size: (890, 890)
Epoch 186, accuracy: 0.7276
Epoch 186, Train Loss: 0.0003, Val Loss: 0.8359
batch size: (906, 906)
Epoch 187, accuracy: 0.7283
batch size: (891, 891)
Epoch 188, accuracy: 0.7278
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 188, Train Loss: 0.0001, Val Loss: 0.8273
batch size: (879, 879)
Epoch 189, accuracy: 0.7253
batch size: (891, 891)
Epoch 190, accuracy: 0.7288
Epoch 190, Train Loss: 0.0002, Val Loss: 0.8335
batch size: (891, 891)
Epoch 191, accuracy: 0.7281
batch size: (897, 897)
Epoch 192, accuracy: 0.7275
Epoch 192, Train Loss: 0.0005, Val Loss: 0.8323
batch size: (908, 908)
Epoch 193, accuracy: 0.7279
batch size: (894, 894)
Epoch 194, accuracy: 0.7263
Epoch 194, Train Loss: 0.0001, Val Loss: 0.8314
batch size: (896, 896)
Epoch 195, accuracy: 0.7248
batch size: (909, 909)
Epoch 196, accuracy: 0.7273
Epoch 196, Train Loss: 0.0002, Val Loss: 0.8277
batch size: (902, 902)
Epoch 197, accuracy: 0.7263
batch size: (897, 897)
Epoch 198, accuracy: 0.7256
Epoch 198, Train Loss: 0.0002, Val Loss: 0.8338
batch size: (902, 902)
Epoch 199, accuracy: 0.7289
Loaded best model with val_loss = 0.5669633150100708
test :accuracy 0.7306, f1_macro: 0.7305, f1_micro: 0.7306, auc: 0.8602
Training GraphSAGE with 8 layers...
可训练参数: 45833_GraphSAGE
不可训练参数: 0
batch size: (900, 900)
✅ Epoch 0: New best model saved with val_loss = 1.0986
Epoch 0, accuracy: 0.1667
Epoch 0, Train Loss: 1.1488, Val Loss: 1.0986
batch size: (899, 899)
Epoch 1, accuracy: 0.1682
batch size: (886, 886)
Epoch 2, accuracy: 0.1663
Epoch 2, Train Loss: 1.2332, Val Loss: 1.0986
batch size: (914, 914)
Epoch 3, accuracy: 0.1697
batch size: (919, 919)
Epoch 4, accuracy: 0.1667
Epoch 4, Train Loss: 1.2907, Val Loss: 1.0986
batch size: (896, 896)
Epoch 5, accuracy: 0.1701
batch size: (899, 899)
Epoch 6, accuracy: 0.1705
Epoch 6, Train Loss: 1.1567, Val Loss: 1.0986
batch size: (893, 893)
Epoch 7, accuracy: 0.1689
batch size: (913, 913)
Epoch 8, accuracy: 0.1649
Epoch 8, Train Loss: 1.1362, Val Loss: 1.0986
batch size: (911, 911)
Epoch 9, accuracy: 0.1658
batch size: (910, 910)
Epoch 10, accuracy: 0.1608
Epoch 10, Train Loss: 1.0905, Val Loss: 1.0986
batch size: (893, 893)
Epoch 11, accuracy: 0.1694
batch size: (915, 915)
Epoch 12, accuracy: 0.1679
Epoch 12, Train Loss: 1.1395, Val Loss: 1.0986
batch size: (918, 918)
Epoch 13, accuracy: 0.1683
batch size: (906, 906)
Epoch 14, accuracy: 0.1662
Epoch 14, Train Loss: 1.0614, Val Loss: 1.0986
batch size: (897, 897)
Epoch 15, accuracy: 0.1706
batch size: (890, 890)
Epoch 16, accuracy: 0.1658
Epoch 16, Train Loss: 1.1137, Val Loss: 1.0986
batch size: (883, 883)
Epoch 17, accuracy: 0.1689
batch size: (898, 898)
Epoch 18, accuracy: 0.1649
Epoch 18, Train Loss: 1.1199, Val Loss: 1.0986
batch size: (900, 900)
Epoch 19, accuracy: 0.1672
batch size: (909, 909)
Epoch 20, accuracy: 0.1669
Epoch 20, Train Loss: 1.1240, Val Loss: 1.0986
batch size: (889, 889)
Epoch 21, accuracy: 0.1688
batch size: (910, 910)
Epoch 22, accuracy: 0.1675
Epoch 22, Train Loss: 1.1147, Val Loss: 1.0986
batch size: (886, 886)
Epoch 23, accuracy: 0.1644
batch size: (912, 912)
Epoch 24, accuracy: 0.1688
Epoch 24, Train Loss: 1.1010, Val Loss: 1.0986
batch size: (892, 892)
Epoch 25, accuracy: 0.1677
batch size: (911, 911)
Epoch 26, accuracy: 0.1654
Epoch 26, Train Loss: 1.1041, Val Loss: 1.0986
batch size: (906, 906)
Epoch 27, accuracy: 0.1682
batch size: (896, 896)
Epoch 28, accuracy: 0.1680
Epoch 28, Train Loss: 1.1443, Val Loss: 1.0986
batch size: (910, 910)
Epoch 29, accuracy: 0.1687
batch size: (912, 912)
Epoch 30, accuracy: 0.1656
Epoch 30, Train Loss: 1.0913, Val Loss: 1.0986
batch size: (894, 894)
Epoch 31, accuracy: 0.1645
batch size: (906, 906)
Epoch 32, accuracy: 0.1699
Epoch 32, Train Loss: 1.1273, Val Loss: 1.0986
batch size: (883, 883)
Epoch 33, accuracy: 0.1672
batch size: (897, 897)
Epoch 34, accuracy: 0.1662
Epoch 34, Train Loss: 1.0753, Val Loss: 1.0986
batch size: (897, 897)
Epoch 35, accuracy: 0.1675
batch size: (891, 891)
Epoch 36, accuracy: 0.1660
Epoch 36, Train Loss: 1.1361, Val Loss: 1.0986
batch size: (887, 887)
Epoch 37, accuracy: 0.1696
batch size: (909, 909)
Epoch 38, accuracy: 0.1672
Epoch 38, Train Loss: 1.1440, Val Loss: 1.0986
batch size: (904, 904)
Epoch 39, accuracy: 0.1686
batch size: (900, 900)
Epoch 40, accuracy: 0.1654
Epoch 40, Train Loss: 1.1164, Val Loss: 1.0986
batch size: (887, 887)
Epoch 41, accuracy: 0.1664
batch size: (895, 895)
Epoch 42, accuracy: 0.1696
Epoch 42, Train Loss: 1.0952, Val Loss: 1.0986
batch size: (908, 908)
Epoch 43, accuracy: 0.1662
batch size: (902, 902)
Epoch 44, accuracy: 0.1686
Epoch 44, Train Loss: 1.1750, Val Loss: 1.0986
batch size: (887, 887)
Epoch 45, accuracy: 0.1673
batch size: (880, 880)
Epoch 46, accuracy: 0.1673
Epoch 46, Train Loss: 1.1180, Val Loss: 1.0986
batch size: (910, 910)
Epoch 47, accuracy: 0.1668
batch size: (908, 908)
Epoch 48, accuracy: 0.1650
Epoch 48, Train Loss: 1.1242, Val Loss: 1.0986
batch size: (901, 901)
Epoch 49, accuracy: 0.1666
batch size: (895, 895)
Epoch 50, accuracy: 0.1685
Epoch 50, Train Loss: 1.1149, Val Loss: 1.0986
batch size: (880, 880)
Epoch 51, accuracy: 0.1690
batch size: (923, 923)
Epoch 52, accuracy: 0.1700
Epoch 52, Train Loss: 1.1138, Val Loss: 1.0986
batch size: (900, 900)
Epoch 53, accuracy: 0.1678
batch size: (911, 911)
Epoch 54, accuracy: 0.1678
Epoch 54, Train Loss: 1.0891, Val Loss: 1.0986
batch size: (903, 903)
Epoch 55, accuracy: 0.1664
batch size: (891, 891)
Epoch 56, accuracy: 0.1682
Epoch 56, Train Loss: 1.0986, Val Loss: 1.0986
batch size: (912, 912)
Epoch 57, accuracy: 0.1687
batch size: (913, 913)
Epoch 58, accuracy: 0.1666
Epoch 58, Train Loss: 1.1333, Val Loss: 1.0986
batch size: (893, 893)
Epoch 59, accuracy: 0.1676
batch size: (884, 884)
Epoch 60, accuracy: 0.1675
Epoch 60, Train Loss: 1.1005, Val Loss: 1.0986
batch size: (908, 908)
Epoch 61, accuracy: 0.1665
batch size: (916, 916)
Epoch 62, accuracy: 0.1637
Epoch 62, Train Loss: 1.1329, Val Loss: 1.0986
batch size: (910, 910)
Epoch 63, accuracy: 0.1681
batch size: (903, 903)
Epoch 64, accuracy: 0.1695
Epoch 64, Train Loss: 1.0905, Val Loss: 1.0986
batch size: (909, 909)
Epoch 65, accuracy: 0.1673
batch size: (903, 903)
Epoch 66, accuracy: 0.1661
Epoch 66, Train Loss: 1.0952, Val Loss: 1.0986
batch size: (897, 897)
Epoch 67, accuracy: 0.1677
batch size: (897, 897)
Epoch 68, accuracy: 0.1657
Epoch 68, Train Loss: 1.1126, Val Loss: 1.0986
batch size: (885, 885)
Epoch 69, accuracy: 0.1654
batch size: (892, 892)
Epoch 70, accuracy: 0.1666
Epoch 70, Train Loss: 1.1254, Val Loss: 1.0986
batch size: (886, 886)
Epoch 71, accuracy: 0.1645
batch size: (889, 889)
Epoch 72, accuracy: 0.1668
Epoch 72, Train Loss: 1.1245, Val Loss: 1.0986
batch size: (909, 909)
Epoch 73, accuracy: 0.1690
batch size: (886, 886)
Epoch 74, accuracy: 0.1672
Epoch 74, Train Loss: 1.1062, Val Loss: 1.0986
batch size: (897, 897)
Epoch 75, accuracy: 0.1669
batch size: (921, 921)
Epoch 76, accuracy: 0.1654
Epoch 76, Train Loss: 1.0848, Val Loss: 1.0986
batch size: (894, 894)
Epoch 77, accuracy: 0.1676
batch size: (895, 895)
Epoch 78, accuracy: 0.1660
Epoch 78, Train Loss: 1.0899, Val Loss: 1.0986
batch size: (904, 904)
Epoch 79, accuracy: 0.1710
batch size: (932, 932)
Epoch 80, accuracy: 0.1688
Epoch 80, Train Loss: 1.1069, Val Loss: 1.0986
batch size: (908, 908)
Epoch 81, accuracy: 0.1691
batch size: (880, 880)
Epoch 82, accuracy: 0.1655
Epoch 82, Train Loss: 1.0782, Val Loss: 1.0986
batch size: (893, 893)
Epoch 83, accuracy: 0.1688
batch size: (899, 899)
Epoch 84, accuracy: 0.1679
Epoch 84, Train Loss: 1.1218, Val Loss: 1.0986
batch size: (902, 902)
Epoch 85, accuracy: 0.1665
batch size: (905, 905)
Epoch 86, accuracy: 0.1688
Epoch 86, Train Loss: 1.1009, Val Loss: 1.0986
batch size: (898, 898)
Epoch 87, accuracy: 0.1674
batch size: (893, 893)
Epoch 88, accuracy: 0.1662
Epoch 88, Train Loss: 1.1048, Val Loss: 1.0986
batch size: (895, 895)
Epoch 89, accuracy: 0.1676
batch size: (897, 897)
Epoch 90, accuracy: 0.1670
Epoch 90, Train Loss: 1.1024, Val Loss: 1.0986
batch size: (895, 895)
Epoch 91, accuracy: 0.1652
batch size: (900, 900)
Epoch 92, accuracy: 0.1684
Epoch 92, Train Loss: 1.0888, Val Loss: 1.0986
batch size: (901, 901)
Epoch 93, accuracy: 0.1691
batch size: (905, 905)
Epoch 94, accuracy: 0.1651
Epoch 94, Train Loss: 1.1204, Val Loss: 1.0986
batch size: (910, 910)
Epoch 95, accuracy: 0.1662
batch size: (902, 902)
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 96, accuracy: 0.1692
Epoch 96, Train Loss: 1.1172, Val Loss: 1.0986
batch size: (912, 912)
Epoch 97, accuracy: 0.1675
batch size: (904, 904)
Epoch 98, accuracy: 0.1668
Epoch 98, Train Loss: 1.1136, Val Loss: 1.0986
batch size: (908, 908)
Epoch 99, accuracy: 0.1688
batch size: (895, 895)
Epoch 100, accuracy: 0.1672
Epoch 100, Train Loss: 1.1255, Val Loss: 1.0986
batch size: (888, 888)
Epoch 101, accuracy: 0.1679
batch size: (885, 885)
Epoch 102, accuracy: 0.1697
Epoch 102, Train Loss: 1.1121, Val Loss: 1.0986
batch size: (883, 883)
Epoch 103, accuracy: 0.1691
batch size: (882, 882)
Epoch 104, accuracy: 0.1663
Epoch 104, Train Loss: 1.1105, Val Loss: 1.0986
batch size: (897, 897)
Epoch 105, accuracy: 0.1677
batch size: (894, 894)
Epoch 106, accuracy: 0.1660
Epoch 106, Train Loss: 1.1399, Val Loss: 1.0986
batch size: (901, 901)
Epoch 107, accuracy: 0.1689
batch size: (900, 900)
Epoch 108, accuracy: 0.1666
Epoch 108, Train Loss: 1.0960, Val Loss: 1.0986
batch size: (894, 894)
Epoch 109, accuracy: 0.1673
batch size: (904, 904)
Epoch 110, accuracy: 0.1692
Epoch 110, Train Loss: 1.0909, Val Loss: 1.0986
batch size: (892, 892)
Epoch 111, accuracy: 0.1674
batch size: (911, 911)
Epoch 112, accuracy: 0.1683
Epoch 112, Train Loss: 1.0749, Val Loss: 1.0986
batch size: (898, 898)
Epoch 113, accuracy: 0.1661
batch size: (913, 913)
Epoch 114, accuracy: 0.1684
Epoch 114, Train Loss: 1.1239, Val Loss: 1.0986
batch size: (886, 886)
Epoch 115, accuracy: 0.1651
batch size: (904, 904)
Epoch 116, accuracy: 0.1684
Epoch 116, Train Loss: 1.0774, Val Loss: 1.0986
batch size: (899, 899)
Epoch 117, accuracy: 0.1669
batch size: (908, 908)
Epoch 118, accuracy: 0.1680
Epoch 118, Train Loss: 1.1050, Val Loss: 1.0986
batch size: (884, 884)
Epoch 119, accuracy: 0.1698
batch size: (901, 901)
Epoch 120, accuracy: 0.1649
Epoch 120, Train Loss: 1.1167, Val Loss: 1.0986
batch size: (890, 890)
Epoch 121, accuracy: 0.1665
batch size: (903, 903)
Epoch 122, accuracy: 0.1692
Epoch 122, Train Loss: 1.1583, Val Loss: 1.0986
batch size: (901, 901)
Epoch 123, accuracy: 0.1664
batch size: (919, 919)
Epoch 124, accuracy: 0.1683
Epoch 124, Train Loss: 1.1060, Val Loss: 1.0986
batch size: (926, 926)
Epoch 125, accuracy: 0.1684
batch size: (900, 900)
Epoch 126, accuracy: 0.1677
Epoch 126, Train Loss: 1.1152, Val Loss: 1.0986
batch size: (892, 892)
Epoch 127, accuracy: 0.1685
batch size: (912, 912)
Epoch 128, accuracy: 0.1669
Epoch 128, Train Loss: 1.1057, Val Loss: 1.0986
batch size: (885, 885)
Epoch 129, accuracy: 0.1669
batch size: (883, 883)
Epoch 130, accuracy: 0.1671
Epoch 130, Train Loss: 1.0747, Val Loss: 1.0986
batch size: (913, 913)
Epoch 131, accuracy: 0.1690
batch size: (899, 899)
Epoch 132, accuracy: 0.1674
Epoch 132, Train Loss: 1.1054, Val Loss: 1.0986
batch size: (912, 912)
Epoch 133, accuracy: 0.1677
batch size: (889, 889)
Epoch 134, accuracy: 0.1700
Epoch 134, Train Loss: 1.1382, Val Loss: 1.0986
batch size: (904, 904)
Epoch 135, accuracy: 0.1656
batch size: (913, 913)
Epoch 136, accuracy: 0.1662
Epoch 136, Train Loss: 1.1121, Val Loss: 1.0986
batch size: (894, 894)
Epoch 137, accuracy: 0.1690
batch size: (910, 910)
Epoch 138, accuracy: 0.1698
Epoch 138, Train Loss: 1.1398, Val Loss: 1.0986
batch size: (900, 900)
Epoch 139, accuracy: 0.1684
batch size: (908, 908)
Epoch 140, accuracy: 0.1685
Epoch 140, Train Loss: 1.1061, Val Loss: 1.0986
batch size: (901, 901)
Epoch 141, accuracy: 0.1706
batch size: (909, 909)
Epoch 142, accuracy: 0.1687
Epoch 142, Train Loss: 1.0855, Val Loss: 1.0986
batch size: (890, 890)
Epoch 143, accuracy: 0.1669
batch size: (897, 897)
Epoch 144, accuracy: 0.1647
Epoch 144, Train Loss: 1.1321, Val Loss: 1.0986
batch size: (898, 898)
Epoch 145, accuracy: 0.1676
batch size: (911, 911)
Epoch 146, accuracy: 0.1666
Epoch 146, Train Loss: 1.0984, Val Loss: 1.0986
batch size: (886, 886)
Epoch 147, accuracy: 0.1667
batch size: (915, 915)
Epoch 148, accuracy: 0.1651
Epoch 148, Train Loss: 1.1570, Val Loss: 1.0986
batch size: (905, 905)
Epoch 149, accuracy: 0.1651
batch size: (915, 915)
Epoch 150, accuracy: 0.1689
Epoch 150, Train Loss: 1.1150, Val Loss: 1.0986
batch size: (897, 897)
Epoch 151, accuracy: 0.1661
batch size: (894, 894)
Epoch 152, accuracy: 0.1678
Epoch 152, Train Loss: 1.0827, Val Loss: 1.0986
batch size: (888, 888)
Epoch 153, accuracy: 0.1700
batch size: (903, 903)
Epoch 154, accuracy: 0.1692
Epoch 154, Train Loss: 1.1133, Val Loss: 1.0986
batch size: (893, 893)
Epoch 155, accuracy: 0.1680
batch size: (887, 887)
Epoch 156, accuracy: 0.1682
Epoch 156, Train Loss: 1.1090, Val Loss: 1.0986
batch size: (892, 892)
Epoch 157, accuracy: 0.1675
batch size: (892, 892)
Epoch 158, accuracy: 0.1637
Epoch 158, Train Loss: 1.1200, Val Loss: 1.0986
batch size: (890, 890)
Epoch 159, accuracy: 0.1673
batch size: (901, 901)
Epoch 160, accuracy: 0.1655
Epoch 160, Train Loss: 1.1034, Val Loss: 1.0986
batch size: (890, 890)
Epoch 161, accuracy: 0.1698
batch size: (922, 922)
Epoch 162, accuracy: 0.1666
Epoch 162, Train Loss: 1.1136, Val Loss: 1.0986
batch size: (916, 916)
Epoch 163, accuracy: 0.1699
batch size: (901, 901)
Epoch 164, accuracy: 0.1643
Epoch 164, Train Loss: 1.1016, Val Loss: 1.0986
batch size: (900, 900)
Epoch 165, accuracy: 0.1654
batch size: (901, 901)
Epoch 166, accuracy: 0.1673
Epoch 166, Train Loss: 1.1361, Val Loss: 1.0986
batch size: (917, 917)
Epoch 167, accuracy: 0.1667
batch size: (908, 908)
Epoch 168, accuracy: 0.1664
Epoch 168, Train Loss: 1.1540, Val Loss: 1.0986
batch size: (910, 910)
Epoch 169, accuracy: 0.1698
batch size: (897, 897)
Epoch 170, accuracy: 0.1649
Epoch 170, Train Loss: 1.0912, Val Loss: 1.0986
batch size: (897, 897)
Epoch 171, accuracy: 0.1662
batch size: (902, 902)
Epoch 172, accuracy: 0.1678
Epoch 172, Train Loss: 1.1105, Val Loss: 1.0986
batch size: (901, 901)
Epoch 173, accuracy: 0.1649
batch size: (902, 902)
Epoch 174, accuracy: 0.1675
Epoch 174, Train Loss: 1.0918, Val Loss: 1.0986
batch size: (900, 900)
Epoch 175, accuracy: 0.1659
batch size: (910, 910)
Epoch 176, accuracy: 0.1697
Epoch 176, Train Loss: 1.1334, Val Loss: 1.0986
batch size: (903, 903)
Epoch 177, accuracy: 0.1693
batch size: (904, 904)
Epoch 178, accuracy: 0.1666
Epoch 178, Train Loss: 1.0864, Val Loss: 1.0986
batch size: (906, 906)
Epoch 179, accuracy: 0.1644
batch size: (893, 893)
Epoch 180, accuracy: 0.1691
Epoch 180, Train Loss: 1.1270, Val Loss: 1.0986
batch size: (895, 895)
Epoch 181, accuracy: 0.1697
batch size: (903, 903)
Epoch 182, accuracy: 0.1695
Epoch 182, Train Loss: 1.1035, Val Loss: 1.0986
batch size: (880, 880)
Epoch 183, accuracy: 0.1674
batch size: (930, 930)
Epoch 184, accuracy: 0.1671
Epoch 184, Train Loss: 1.1190, Val Loss: 1.0986
batch size: (902, 902)
Epoch 185, accuracy: 0.1683
batch size: (884, 884)
Epoch 186, accuracy: 0.1680
Epoch 186, Train Loss: 1.1354, Val Loss: 1.0986
batch size: (904, 904)
Epoch 187, accuracy: 0.1688
batch size: (899, 899)
Epoch 188, accuracy: 0.1677
Epoch 188, Train Loss: 1.0981, Val Loss: 1.0986
batch size: (896, 896)
Epoch 189, accuracy: 0.1691
batch size: (897, 897)
Epoch 190, accuracy: 0.1657
Epoch 190, Train Loss: 1.1022, Val Loss: 1.0986
batch size: (910, 910)
Epoch 191, accuracy: 0.1684
batch size: (921, 921)
Epoch 192, accuracy: 0.1655
Epoch 192, Train Loss: 1.0852, Val Loss: 1.0986
batch size: (919, 919)
Epoch 193, accuracy: 0.1657
batch size: (888, 888)
Epoch 194, accuracy: 0.1703
Epoch 194, Train Loss: 1.0925, Val Loss: 1.0986
batch size: (905, 905)
Epoch 195, accuracy: 0.1676
batch size: (886, 886)
Epoch 196, accuracy: 0.1709
Epoch 196, Train Loss: 1.0771, Val Loss: 1.0986
batch size: (918, 918)
Epoch 197, accuracy: 0.1675
batch size: (899, 899)
Epoch 198, accuracy: 0.1687
Epoch 198, Train Loss: 1.0946, Val Loss: 1.0986
batch size: (894, 894)
Epoch 199, accuracy: 0.1644
Loaded best model with val_loss = 1.0986120700836182
test :accuracy 0.1674, f1_macro: 0.0956, f1_micro: 0.1674, auc: 0.4884
Training GraphSAGE with 32 layers...
可训练参数: 98057_GraphSAGE
不可训练参数: 0
batch size: (899, 899)
✅ Epoch 0: New best model saved with val_loss = 1.1024
Epoch 0, accuracy: 0.1848
Epoch 0, Train Loss: 1.1994, Val Loss: 1.1024
batch size: (907, 907)
Epoch 1, accuracy: 0.1639
batch size: (898, 898)
Epoch 2, accuracy: 0.1638
Epoch 2, Train Loss: 1.1155, Val Loss: 1.1093
batch size: (889, 889)
Epoch 3, accuracy: 0.1687
batch size: (907, 907)
Epoch 4, accuracy: 0.1659
Epoch 4, Train Loss: 1.2264, Val Loss: 1.1117
batch size: (905, 905)
Epoch 5, accuracy: 0.1673
batch size: (886, 886)
Epoch 6, accuracy: 0.1664
Epoch 6, Train Loss: 1.1703, Val Loss: 1.1094
batch size: (897, 897)
Epoch 7, accuracy: 0.1678
batch size: (901, 901)
Epoch 8, accuracy: 0.1633
Epoch 8, Train Loss: 1.1365, Val Loss: 1.1115
batch size: (899, 899)
Epoch 9, accuracy: 0.1669
batch size: (908, 908)
Epoch 10, accuracy: 0.1678
Epoch 10, Train Loss: 1.1056, Val Loss: 1.1093
batch size: (890, 890)
Epoch 11, accuracy: 0.1686
batch size: (911, 911)
Epoch 12, accuracy: 0.1707
Epoch 12, Train Loss: 1.1361, Val Loss: 1.1115
batch size: (885, 885)
Epoch 13, accuracy: 0.1662
batch size: (908, 908)
Epoch 14, accuracy: 0.1681
Epoch 14, Train Loss: 1.1549, Val Loss: 1.1090
batch size: (913, 913)
Epoch 15, accuracy: 0.1664
batch size: (875, 875)
Epoch 16, accuracy: 0.1677
Epoch 16, Train Loss: 1.1084, Val Loss: 1.1116
batch size: (895, 895)
Epoch 17, accuracy: 0.1681
batch size: (900, 900)
Epoch 18, accuracy: 0.1654
Epoch 18, Train Loss: 1.1311, Val Loss: 1.1084
batch size: (880, 880)
Epoch 19, accuracy: 0.1683
batch size: (902, 902)
Epoch 20, accuracy: 0.1680
Epoch 20, Train Loss: 1.1352, Val Loss: 1.1117
batch size: (885, 885)
Epoch 21, accuracy: 0.1678
batch size: (910, 910)
Epoch 22, accuracy: 0.1684
Epoch 22, Train Loss: 1.0945, Val Loss: 1.1099
batch size: (896, 896)
Epoch 23, accuracy: 0.1689
batch size: (901, 901)
Epoch 24, accuracy: 0.1699
Epoch 24, Train Loss: 1.1302, Val Loss: 1.1105
batch size: (906, 906)
Epoch 25, accuracy: 0.1695
batch size: (911, 911)
Epoch 26, accuracy: 0.1680
Epoch 26, Train Loss: 1.3075, Val Loss: 1.1119
batch size: (894, 894)
Epoch 27, accuracy: 0.1661
batch size: (877, 877)
Epoch 28, accuracy: 0.1673
Epoch 28, Train Loss: 1.2942, Val Loss: 1.1090
batch size: (887, 887)
Epoch 29, accuracy: 0.1664
batch size: (914, 914)
Epoch 30, accuracy: 0.1672
Epoch 30, Train Loss: 1.1999, Val Loss: 1.1094
batch size: (878, 878)
Epoch 31, accuracy: 0.1685
batch size: (867, 867)
Epoch 32, accuracy: 0.1657
Epoch 32, Train Loss: 1.1095, Val Loss: 1.1107
batch size: (913, 913)
Epoch 33, accuracy: 0.1680
batch size: (902, 902)
Epoch 34, accuracy: 0.1685
Epoch 34, Train Loss: 1.1394, Val Loss: 1.1122
batch size: (911, 911)
Epoch 35, accuracy: 0.1685
batch size: (896, 896)
Epoch 36, accuracy: 0.1693
Epoch 36, Train Loss: 1.2396, Val Loss: 1.1114
batch size: (897, 897)
Epoch 37, accuracy: 0.1676
batch size: (894, 894)
Epoch 38, accuracy: 0.1684
Epoch 38, Train Loss: 1.0975, Val Loss: 1.1096
batch size: (902, 902)
Epoch 39, accuracy: 0.1694
batch size: (891, 891)
Epoch 40, accuracy: 0.1691
Epoch 40, Train Loss: 1.1096, Val Loss: 1.1106
batch size: (910, 910)
Epoch 41, accuracy: 0.1701
batch size: (883, 883)
Epoch 42, accuracy: 0.1679
Epoch 42, Train Loss: 1.2762, Val Loss: 1.1125
batch size: (894, 894)
Epoch 43, accuracy: 0.1676
batch size: (903, 903)
Epoch 44, accuracy: 0.1660
Epoch 44, Train Loss: 1.1211, Val Loss: 1.1101
batch size: (884, 884)
Epoch 45, accuracy: 0.1674
batch size: (898, 898)
Epoch 46, accuracy: 0.1685
Epoch 46, Train Loss: 1.1515, Val Loss: 1.1101
batch size: (892, 892)
Epoch 47, accuracy: 0.1691
batch size: (908, 908)
Epoch 48, accuracy: 0.1703
Epoch 48, Train Loss: 1.1204, Val Loss: 1.1105
batch size: (896, 896)
Epoch 49, accuracy: 0.1680
batch size: (926, 926)
Epoch 50, accuracy: 0.1686
Epoch 50, Train Loss: 1.0808, Val Loss: 1.1104
batch size: (924, 924)
Epoch 51, accuracy: 0.1667
batch size: (888, 888)
Epoch 52, accuracy: 0.1668
Epoch 52, Train Loss: 1.0894, Val Loss: 1.1100
batch size: (894, 894)
Epoch 53, accuracy: 0.1674
batch size: (898, 898)
Epoch 54, accuracy: 0.1655
Epoch 54, Train Loss: 1.1855, Val Loss: 1.1111
batch size: (909, 909)
Epoch 55, accuracy: 0.1670
batch size: (890, 890)
Epoch 56, accuracy: 0.1656
Epoch 56, Train Loss: 1.1759, Val Loss: 1.1097
batch size: (911, 911)
Epoch 57, accuracy: 0.1659
batch size: (898, 898)
Epoch 58, accuracy: 0.1678
Epoch 58, Train Loss: 1.1085, Val Loss: 1.1133
batch size: (915, 915)
Epoch 59, accuracy: 0.1686
batch size: (884, 884)
Epoch 60, accuracy: 0.1683
Epoch 60, Train Loss: 1.3321, Val Loss: 1.1082
batch size: (899, 899)
Epoch 61, accuracy: 0.1682
batch size: (894, 894)
Epoch 62, accuracy: 0.1690
Epoch 62, Train Loss: 1.1997, Val Loss: 1.1111
batch size: (902, 902)
Epoch 63, accuracy: 0.1674
batch size: (890, 890)
Epoch 64, accuracy: 0.1677
Epoch 64, Train Loss: 1.2159, Val Loss: 1.1094
batch size: (932, 932)
Epoch 65, accuracy: 0.1697
batch size: (896, 896)
Epoch 66, accuracy: 0.1675
Epoch 66, Train Loss: 1.1729, Val Loss: 1.1105
batch size: (881, 881)
Epoch 67, accuracy: 0.1687
batch size: (884, 884)
Epoch 68, accuracy: 0.1669
Epoch 68, Train Loss: 1.1572, Val Loss: 1.1110
batch size: (900, 900)
Epoch 69, accuracy: 0.1703
batch size: (914, 914)
Epoch 70, accuracy: 0.1663
Epoch 70, Train Loss: 1.1001, Val Loss: 1.1113
batch size: (905, 905)
Epoch 71, accuracy: 0.1654
batch size: (906, 906)
Epoch 72, accuracy: 0.1634
Epoch 72, Train Loss: 1.0949, Val Loss: 1.1102
batch size: (898, 898)
Epoch 73, accuracy: 0.1660
batch size: (900, 900)
Epoch 74, accuracy: 0.1654
Epoch 74, Train Loss: 1.1687, Val Loss: 1.1064
batch size: (920, 920)
Epoch 75, accuracy: 0.1689
batch size: (875, 875)
Epoch 76, accuracy: 0.1700
Epoch 76, Train Loss: 1.1476, Val Loss: 1.1080
batch size: (895, 895)
Epoch 77, accuracy: 0.1647
batch size: (893, 893)
Epoch 78, accuracy: 0.1660
Epoch 78, Train Loss: 1.1461, Val Loss: 1.1104
batch size: (917, 917)
Epoch 79, accuracy: 0.1692
batch size: (891, 891)
Epoch 80, accuracy: 0.1658
Epoch 80, Train Loss: 1.1280, Val Loss: 1.1102
batch size: (905, 905)
Epoch 81, accuracy: 0.1657
batch size: (908, 908)
Epoch 82, accuracy: 0.1692
Epoch 82, Train Loss: 1.1822, Val Loss: 1.1121
batch size: (890, 890)
Epoch 83, accuracy: 0.1679
batch size: (903, 903)
Epoch 84, accuracy: 0.1661
Epoch 84, Train Loss: 1.2110, Val Loss: 1.1084
batch size: (884, 884)
Epoch 85, accuracy: 0.1670
batch size: (905, 905)
Epoch 86, accuracy: 0.1640
Epoch 86, Train Loss: 1.1064, Val Loss: 1.1116
batch size: (878, 878)
Epoch 87, accuracy: 0.1668
batch size: (913, 913)
Epoch 88, accuracy: 0.1677
Epoch 88, Train Loss: 1.1751, Val Loss: 1.1085
batch size: (908, 908)
Epoch 89, accuracy: 0.1691
batch size: (906, 906)
Epoch 90, accuracy: 0.1672
Epoch 90, Train Loss: 1.1456, Val Loss: 1.1098
batch size: (890, 890)
Epoch 91, accuracy: 0.1676
batch size: (891, 891)
Epoch 92, accuracy: 0.1706
Epoch 92, Train Loss: 1.1940, Val Loss: 1.1111
batch size: (901, 901)
Epoch 93, accuracy: 0.1637
batch size: (887, 887)
Epoch 94, accuracy: 0.1671
Epoch 94, Train Loss: 1.0918, Val Loss: 1.1117
batch size: (913, 913)
Epoch 95, accuracy: 0.1678
batch size: (892, 892)
Epoch 96, accuracy: 0.1681
Epoch 96, Train Loss: 1.0719, Val Loss: 1.1098
batch size: (900, 900)
Epoch 97, accuracy: 0.1661
batch size: (912, 912)
Epoch 98, accuracy: 0.1717
Epoch 98, Train Loss: 1.2424, Val Loss: 1.1105
batch size: (891, 891)
Epoch 99, accuracy: 0.1649
batch size: (882, 882)
Epoch 100, accuracy: 0.1674
Epoch 100, Train Loss: 1.1916, Val Loss: 1.1120
batch size: (889, 889)
Epoch 101, accuracy: 0.1699
batch size: (915, 915)
Epoch 102, accuracy: 0.1669
Epoch 102, Train Loss: 1.2351, Val Loss: 1.1100
batch size: (897, 897)
Epoch 103, accuracy: 0.1669
batch size: (898, 898)
Epoch 104, accuracy: 0.1689
Epoch 104, Train Loss: 1.1405, Val Loss: 1.1122
batch size: (916, 916)
Epoch 105, accuracy: 0.1681
batch size: (898, 898)
Epoch 106, accuracy: 0.1695
Epoch 106, Train Loss: 1.0998, Val Loss: 1.1094
batch size: (884, 884)
Epoch 107, accuracy: 0.1661
batch size: (903, 903)
Epoch 108, accuracy: 0.1678
Epoch 108, Train Loss: 1.1488, Val Loss: 1.1115
batch size: (902, 902)
Epoch 109, accuracy: 0.1700
batch size: (899, 899)
Epoch 110, accuracy: 0.1656
Epoch 110, Train Loss: 1.1567, Val Loss: 1.1091
batch size: (890, 890)
Epoch 111, accuracy: 0.1663
batch size: (917, 917)
Epoch 112, accuracy: 0.1672
Epoch 112, Train Loss: 1.1039, Val Loss: 1.1122
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
batch size: (902, 902)
Epoch 113, accuracy: 0.1652
batch size: (918, 918)
Epoch 114, accuracy: 0.1656
Epoch 114, Train Loss: 1.1966, Val Loss: 1.1115
batch size: (903, 903)
Epoch 115, accuracy: 0.1679
batch size: (878, 878)
Epoch 116, accuracy: 0.1718
Epoch 116, Train Loss: 1.1186, Val Loss: 1.1101
batch size: (896, 896)
Epoch 117, accuracy: 0.1661
batch size: (896, 896)
Epoch 118, accuracy: 0.1711
Epoch 118, Train Loss: 1.1119, Val Loss: 1.1083
batch size: (900, 900)
Epoch 119, accuracy: 0.1679
batch size: (900, 900)
Epoch 120, accuracy: 0.1683
Epoch 120, Train Loss: 1.2088, Val Loss: 1.1110
batch size: (913, 913)
Epoch 121, accuracy: 0.1648
batch size: (908, 908)
Epoch 122, accuracy: 0.1675
Epoch 122, Train Loss: 1.2660, Val Loss: 1.1099
batch size: (883, 883)
Epoch 123, accuracy: 0.1654
batch size: (879, 879)
Epoch 124, accuracy: 0.1676
Epoch 124, Train Loss: 1.1808, Val Loss: 1.1117
batch size: (888, 888)
Epoch 125, accuracy: 0.1678
batch size: (884, 884)
Epoch 126, accuracy: 0.1666
Epoch 126, Train Loss: 1.2301, Val Loss: 1.1096
batch size: (895, 895)
Epoch 127, accuracy: 0.1660
batch size: (907, 907)
Epoch 128, accuracy: 0.1684
Epoch 128, Train Loss: 1.1609, Val Loss: 1.1110
batch size: (902, 902)
Epoch 129, accuracy: 0.1686
batch size: (898, 898)
Epoch 130, accuracy: 0.1657
Epoch 130, Train Loss: 1.1214, Val Loss: 1.1103
batch size: (896, 896)
Epoch 131, accuracy: 0.1679
batch size: (884, 884)
Epoch 132, accuracy: 0.1640
Epoch 132, Train Loss: 1.0937, Val Loss: 1.1114
batch size: (916, 916)
Epoch 133, accuracy: 0.1676
batch size: (907, 907)
Epoch 134, accuracy: 0.1676
Epoch 134, Train Loss: 1.1698, Val Loss: 1.1095
batch size: (897, 897)
Epoch 135, accuracy: 0.1677
batch size: (904, 904)
Epoch 136, accuracy: 0.1671
Epoch 136, Train Loss: 1.0961, Val Loss: 1.1111
batch size: (915, 915)
Epoch 137, accuracy: 0.1678
batch size: (904, 904)
Epoch 138, accuracy: 0.1669
Epoch 138, Train Loss: 1.2214, Val Loss: 1.1102
batch size: (915, 915)
Epoch 139, accuracy: 0.1691
batch size: (924, 924)
Epoch 140, accuracy: 0.1703
Epoch 140, Train Loss: 1.0961, Val Loss: 1.1103
batch size: (914, 914)
Epoch 141, accuracy: 0.1697
batch size: (895, 895)
Epoch 142, accuracy: 0.1680
Epoch 142, Train Loss: 1.1597, Val Loss: 1.1097
batch size: (900, 900)
Epoch 143, accuracy: 0.1673
batch size: (888, 888)
Epoch 144, accuracy: 0.1680
Epoch 144, Train Loss: 1.1634, Val Loss: 1.1104
batch size: (892, 892)
Epoch 145, accuracy: 0.1686
batch size: (893, 893)
Epoch 146, accuracy: 0.1665
Epoch 146, Train Loss: 1.1427, Val Loss: 1.1110
batch size: (908, 908)
Epoch 147, accuracy: 0.1666
batch size: (911, 911)
Epoch 148, accuracy: 0.1677
Epoch 148, Train Loss: 1.2640, Val Loss: 1.1105
batch size: (908, 908)
Epoch 149, accuracy: 0.1673
batch size: (909, 909)
Epoch 150, accuracy: 0.1659
Epoch 150, Train Loss: 1.2792, Val Loss: 1.1123
batch size: (888, 888)
Epoch 151, accuracy: 0.1637
batch size: (887, 887)
Epoch 152, accuracy: 0.1679
Epoch 152, Train Loss: 1.1327, Val Loss: 1.1099
batch size: (905, 905)
Epoch 153, accuracy: 0.1673
batch size: (894, 894)
Epoch 154, accuracy: 0.1680
Epoch 154, Train Loss: 1.1262, Val Loss: 1.1094
batch size: (908, 908)
Epoch 155, accuracy: 0.1661
batch size: (904, 904)
Epoch 156, accuracy: 0.1671
Epoch 156, Train Loss: 1.1614, Val Loss: 1.1088
batch size: (900, 900)
Epoch 157, accuracy: 0.1645
batch size: (900, 900)
Epoch 158, accuracy: 0.1659
Epoch 158, Train Loss: 1.1742, Val Loss: 1.1098
batch size: (910, 910)
Epoch 159, accuracy: 0.1683
batch size: (908, 908)
Epoch 160, accuracy: 0.1669
Epoch 160, Train Loss: 1.1571, Val Loss: 1.1092
batch size: (913, 913)
Epoch 161, accuracy: 0.1682
batch size: (903, 903)
Epoch 162, accuracy: 0.1696
Epoch 162, Train Loss: 1.1374, Val Loss: 1.1104
batch size: (882, 882)
Epoch 163, accuracy: 0.1690
batch size: (906, 906)
Epoch 164, accuracy: 0.1660
Epoch 164, Train Loss: 1.2325, Val Loss: 1.1108
batch size: (904, 904)
Epoch 165, accuracy: 0.1707
batch size: (902, 902)
Epoch 166, accuracy: 0.1689
Epoch 166, Train Loss: 1.1552, Val Loss: 1.1101
batch size: (893, 893)
Epoch 167, accuracy: 0.1648
batch size: (911, 911)
Epoch 168, accuracy: 0.1676
Epoch 168, Train Loss: 1.0944, Val Loss: 1.1091
batch size: (879, 879)
Epoch 169, accuracy: 0.1679
batch size: (918, 918)
Epoch 170, accuracy: 0.1680
Epoch 170, Train Loss: 1.1461, Val Loss: 1.1111
batch size: (892, 892)
Epoch 171, accuracy: 0.1673
batch size: (904, 904)
Epoch 172, accuracy: 0.1678
Epoch 172, Train Loss: 1.1353, Val Loss: 1.1121
batch size: (890, 890)
Epoch 173, accuracy: 0.1669
batch size: (896, 896)
Epoch 174, accuracy: 0.1679
Epoch 174, Train Loss: 1.1354, Val Loss: 1.1088
batch size: (903, 903)
Epoch 175, accuracy: 0.1668
batch size: (888, 888)
Epoch 176, accuracy: 0.1664
Epoch 176, Train Loss: 1.2852, Val Loss: 1.1121
batch size: (899, 899)
Epoch 177, accuracy: 0.1692
batch size: (919, 919)
Epoch 178, accuracy: 0.1665
Epoch 178, Train Loss: 1.3698, Val Loss: 1.1111
batch size: (888, 888)
Epoch 179, accuracy: 0.1709
batch size: (898, 898)
Epoch 180, accuracy: 0.1640
Epoch 180, Train Loss: 1.1792, Val Loss: 1.1142
batch size: (907, 907)
Epoch 181, accuracy: 0.1697
batch size: (896, 896)
Epoch 182, accuracy: 0.1660
Epoch 182, Train Loss: 1.1217, Val Loss: 1.1090
batch size: (889, 889)
Epoch 183, accuracy: 0.1642
batch size: (909, 909)
Epoch 184, accuracy: 0.1677
Epoch 184, Train Loss: 1.1281, Val Loss: 1.1109
batch size: (891, 891)
Epoch 185, accuracy: 0.1708
batch size: (906, 906)
Epoch 186, accuracy: 0.1681
Epoch 186, Train Loss: 1.1449, Val Loss: 1.1119
batch size: (888, 888)
Epoch 187, accuracy: 0.1689
batch size: (905, 905)
Epoch 188, accuracy: 0.1662
Epoch 188, Train Loss: 1.1610, Val Loss: 1.1086
batch size: (912, 912)
Epoch 189, accuracy: 0.1683
batch size: (899, 899)
Epoch 190, accuracy: 0.1678
Epoch 190, Train Loss: 1.1666, Val Loss: 1.1119
batch size: (919, 919)
Epoch 191, accuracy: 0.1664
batch size: (891, 891)
Epoch 192, accuracy: 0.1701
Epoch 192, Train Loss: 1.2066, Val Loss: 1.1101
batch size: (892, 892)
Epoch 193, accuracy: 0.1701
batch size: (895, 895)
Epoch 194, accuracy: 0.1692
Epoch 194, Train Loss: 1.1029, Val Loss: 1.1105
batch size: (892, 892)
Epoch 195, accuracy: 0.1663
batch size: (888, 888)
Epoch 196, accuracy: 0.1666
Epoch 196, Train Loss: 1.0902, Val Loss: 1.1107
batch size: (901, 901)
Epoch 197, accuracy: 0.1659
batch size: (894, 894)
Epoch 198, accuracy: 0.1665
Epoch 198, Train Loss: 1.1795, Val Loss: 1.1118
batch size: (906, 906)
Epoch 199, accuracy: 0.1661
Loaded best model with val_loss = 1.102371096611023
test :accuracy 0.1882, f1_macro: 0.1404, f1_micro: 0.1882, auc: 0.4945
Training GAT with 2 layers...
可训练参数: 196495_GAT
不可训练参数: 0
batch size: (901, 901)
✅ Epoch 0: New best model saved with val_loss = 1.0800
Epoch 0, accuracy: 0.6812
Epoch 0, Train Loss: 1.1009, Val Loss: 1.0800
batch size: (913, 913)
✅ Epoch 1: New best model saved with val_loss = 1.0592
Epoch 1, accuracy: 0.7179
batch size: (882, 882)
✅ Epoch 2: New best model saved with val_loss = 1.0319
Epoch 2, accuracy: 0.7293
Epoch 2, Train Loss: 1.0281, Val Loss: 1.0319
batch size: (886, 886)
✅ Epoch 3: New best model saved with val_loss = 0.9996
Epoch 3, accuracy: 0.7270
batch size: (911, 911)
✅ Epoch 4: New best model saved with val_loss = 0.9641
Epoch 4, accuracy: 0.7292
Epoch 4, Train Loss: 0.9400, Val Loss: 0.9641
batch size: (906, 906)
✅ Epoch 5: New best model saved with val_loss = 0.9259
Epoch 5, accuracy: 0.7308
batch size: (904, 904)
✅ Epoch 6: New best model saved with val_loss = 0.8884
Epoch 6, accuracy: 0.7329
Epoch 6, Train Loss: 0.8223, Val Loss: 0.8884
batch size: (890, 890)
✅ Epoch 7: New best model saved with val_loss = 0.8523
Epoch 7, accuracy: 0.7347
batch size: (890, 890)
✅ Epoch 8: New best model saved with val_loss = 0.8157
Epoch 8, accuracy: 0.7360
Epoch 8, Train Loss: 0.7127, Val Loss: 0.8157
batch size: (881, 881)
✅ Epoch 9: New best model saved with val_loss = 0.7795
Epoch 9, accuracy: 0.7383
batch size: (908, 908)
✅ Epoch 10: New best model saved with val_loss = 0.7454
Epoch 10, accuracy: 0.7393
Epoch 10, Train Loss: 0.5890, Val Loss: 0.7454
batch size: (894, 894)
✅ Epoch 11: New best model saved with val_loss = 0.7120
Epoch 11, accuracy: 0.7389
batch size: (905, 905)
✅ Epoch 12: New best model saved with val_loss = 0.6871
Epoch 12, accuracy: 0.7406
Epoch 12, Train Loss: 0.4693, Val Loss: 0.6871
batch size: (882, 882)
✅ Epoch 13: New best model saved with val_loss = 0.6572
Epoch 13, accuracy: 0.7445
batch size: (896, 896)
✅ Epoch 14: New best model saved with val_loss = 0.6376
Epoch 14, accuracy: 0.7462
Epoch 14, Train Loss: 0.3760, Val Loss: 0.6376
batch size: (898, 898)
✅ Epoch 15: New best model saved with val_loss = 0.6147
Epoch 15, accuracy: 0.7505
batch size: (908, 908)
✅ Epoch 16: New best model saved with val_loss = 0.6022
Epoch 16, accuracy: 0.7499
Epoch 16, Train Loss: 0.3060, Val Loss: 0.6022
batch size: (893, 893)
✅ Epoch 17: New best model saved with val_loss = 0.5806
Epoch 17, accuracy: 0.7517
batch size: (911, 911)
✅ Epoch 18: New best model saved with val_loss = 0.5744
Epoch 18, accuracy: 0.7536
Epoch 18, Train Loss: 0.2334, Val Loss: 0.5744
batch size: (902, 902)
✅ Epoch 19: New best model saved with val_loss = 0.5672
Epoch 19, accuracy: 0.7568
batch size: (882, 882)
✅ Epoch 20: New best model saved with val_loss = 0.5609
Epoch 20, accuracy: 0.7508
Epoch 20, Train Loss: 0.1976, Val Loss: 0.5609
batch size: (913, 913)
✅ Epoch 21: New best model saved with val_loss = 0.5523
Epoch 21, accuracy: 0.7502
batch size: (910, 910)
✅ Epoch 22: New best model saved with val_loss = 0.5501
Epoch 22, accuracy: 0.7491
Epoch 22, Train Loss: 0.1500, Val Loss: 0.5501
batch size: (899, 899)
Epoch 23, accuracy: 0.7479
batch size: (895, 895)
✅ Epoch 24: New best model saved with val_loss = 0.5417
Epoch 24, accuracy: 0.7466
Epoch 24, Train Loss: 0.1150, Val Loss: 0.5417
batch size: (877, 877)
Epoch 25, accuracy: 0.7474
batch size: (906, 906)
Epoch 26, accuracy: 0.7465
Epoch 26, Train Loss: 0.0960, Val Loss: 0.5524
batch size: (900, 900)
Epoch 27, accuracy: 0.7432
batch size: (889, 889)
Epoch 28, accuracy: 0.7416
Epoch 28, Train Loss: 0.0899, Val Loss: 0.5708
batch size: (897, 897)
Epoch 29, accuracy: 0.7362
batch size: (892, 892)
Epoch 30, accuracy: 0.7336
Epoch 30, Train Loss: 0.0751, Val Loss: 0.5588
batch size: (895, 895)
Epoch 31, accuracy: 0.7338
batch size: (889, 889)
Epoch 32, accuracy: 0.7329
Epoch 32, Train Loss: 0.0622, Val Loss: 0.5624
batch size: (913, 913)
Epoch 33, accuracy: 0.7326
batch size: (902, 902)
Epoch 34, accuracy: 0.7321
Epoch 34, Train Loss: 0.0632, Val Loss: 0.5562
batch size: (876, 876)
Epoch 35, accuracy: 0.7332
batch size: (919, 919)
Epoch 36, accuracy: 0.7349
Epoch 36, Train Loss: 0.0629, Val Loss: 0.5473
batch size: (880, 880)
Epoch 37, accuracy: 0.7324
batch size: (887, 887)
Epoch 38, accuracy: 0.7339
Epoch 38, Train Loss: 0.0602, Val Loss: 0.5649
batch size: (898, 898)
Epoch 39, accuracy: 0.7356
batch size: (892, 892)
Epoch 40, accuracy: 0.7359
Epoch 40, Train Loss: 0.0531, Val Loss: 0.5562
batch size: (887, 887)
Epoch 41, accuracy: 0.7343
batch size: (892, 892)
Epoch 42, accuracy: 0.7340
Epoch 42, Train Loss: 0.0560, Val Loss: 0.5596
batch size: (896, 896)
Epoch 43, accuracy: 0.7342
batch size: (907, 907)
Epoch 44, accuracy: 0.7313
Epoch 44, Train Loss: 0.0596, Val Loss: 0.5661
batch size: (902, 902)
Epoch 45, accuracy: 0.7326
batch size: (901, 901)
Epoch 46, accuracy: 0.7364
Epoch 46, Train Loss: 0.0611, Val Loss: 0.5536
batch size: (900, 900)
Epoch 47, accuracy: 0.7336
batch size: (903, 903)
Epoch 48, accuracy: 0.7340
Epoch 48, Train Loss: 0.0612, Val Loss: 0.5701
batch size: (905, 905)
Epoch 49, accuracy: 0.7306
batch size: (898, 898)
Epoch 50, accuracy: 0.7347
Epoch 50, Train Loss: 0.0660, Val Loss: 0.5565
batch size: (897, 897)
Epoch 51, accuracy: 0.7349
batch size: (893, 893)
Epoch 52, accuracy: 0.7329
Epoch 52, Train Loss: 0.0628, Val Loss: 0.5565
batch size: (902, 902)
Epoch 53, accuracy: 0.7350
batch size: (912, 912)
Epoch 54, accuracy: 0.7347
Epoch 54, Train Loss: 0.0518, Val Loss: 0.5842
batch size: (890, 890)
Epoch 55, accuracy: 0.7351
batch size: (903, 903)
Epoch 56, accuracy: 0.7357
Epoch 56, Train Loss: 0.0634, Val Loss: 0.5623
batch size: (897, 897)
Epoch 57, accuracy: 0.7348
batch size: (907, 907)
Epoch 58, accuracy: 0.7352
Epoch 58, Train Loss: 0.0606, Val Loss: 0.5544
batch size: (896, 896)
Epoch 59, accuracy: 0.7309
batch size: (899, 899)
Epoch 60, accuracy: 0.7331
Epoch 60, Train Loss: 0.0550, Val Loss: 0.5618
batch size: (882, 882)
Epoch 61, accuracy: 0.7336
batch size: (888, 888)
Epoch 62, accuracy: 0.7342
Epoch 62, Train Loss: 0.0613, Val Loss: 0.5544
batch size: (901, 901)
Epoch 63, accuracy: 0.7352
batch size: (903, 903)
Epoch 64, accuracy: 0.7345
Epoch 64, Train Loss: 0.0669, Val Loss: 0.5535
batch size: (892, 892)
Epoch 65, accuracy: 0.7341
batch size: (904, 904)
Epoch 66, accuracy: 0.7341
Epoch 66, Train Loss: 0.0605, Val Loss: 0.5601
batch size: (891, 891)
Epoch 67, accuracy: 0.7321
batch size: (888, 888)
Epoch 68, accuracy: 0.7347
Epoch 68, Train Loss: 0.0555, Val Loss: 0.5665
batch size: (889, 889)
Epoch 69, accuracy: 0.7315
batch size: (913, 913)
Epoch 70, accuracy: 0.7331
Epoch 70, Train Loss: 0.0578, Val Loss: 0.5629
batch size: (901, 901)
Epoch 71, accuracy: 0.7340
batch size: (906, 906)
Epoch 72, accuracy: 0.7350
Epoch 72, Train Loss: 0.0517, Val Loss: 0.5698
batch size: (902, 902)
Epoch 73, accuracy: 0.7339
batch size: (919, 919)
Epoch 74, accuracy: 0.7351
Epoch 74, Train Loss: 0.0608, Val Loss: 0.5621
batch size: (911, 911)
Epoch 75, accuracy: 0.7333
batch size: (905, 905)
Epoch 76, accuracy: 0.7332
Epoch 76, Train Loss: 0.0614, Val Loss: 0.5609
batch size: (898, 898)
Epoch 77, accuracy: 0.7352
batch size: (909, 909)
Epoch 78, accuracy: 0.7338
Epoch 78, Train Loss: 0.0740, Val Loss: 0.5594
batch size: (891, 891)
Epoch 79, accuracy: 0.7335
batch size: (904, 904)
Epoch 80, accuracy: 0.7344
Epoch 80, Train Loss: 0.0690, Val Loss: 0.5675
batch size: (910, 910)
Epoch 81, accuracy: 0.7364
batch size: (916, 916)
Epoch 82, accuracy: 0.7337
Epoch 82, Train Loss: 0.0621, Val Loss: 0.5527
batch size: (876, 876)
Epoch 83, accuracy: 0.7331
batch size: (872, 872)
Epoch 84, accuracy: 0.7335
Epoch 84, Train Loss: 0.0560, Val Loss: 0.5633
batch size: (900, 900)
Epoch 85, accuracy: 0.7343
batch size: (898, 898)
Epoch 86, accuracy: 0.7342
Epoch 86, Train Loss: 0.0629, Val Loss: 0.5614
batch size: (911, 911)
Epoch 87, accuracy: 0.7345
batch size: (899, 899)
Epoch 88, accuracy: 0.7327
Epoch 88, Train Loss: 0.0621, Val Loss: 0.5533
batch size: (913, 913)
Epoch 89, accuracy: 0.7350
batch size: (918, 918)
Epoch 90, accuracy: 0.7320
Epoch 90, Train Loss: 0.0703, Val Loss: 0.5595
batch size: (890, 890)
Epoch 91, accuracy: 0.7353
batch size: (909, 909)
Epoch 92, accuracy: 0.7350
Epoch 92, Train Loss: 0.0662, Val Loss: 0.5639
batch size: (912, 912)
Epoch 93, accuracy: 0.7354
batch size: (887, 887)
Epoch 94, accuracy: 0.7345
Epoch 94, Train Loss: 0.0597, Val Loss: 0.5559
batch size: (911, 911)
Epoch 95, accuracy: 0.7332
batch size: (901, 901)
Epoch 96, accuracy: 0.7346
Epoch 96, Train Loss: 0.0645, Val Loss: 0.5647
batch size: (899, 899)
Epoch 97, accuracy: 0.7343
batch size: (907, 907)
Epoch 98, accuracy: 0.7335
Epoch 98, Train Loss: 0.0622, Val Loss: 0.5619
batch size: (907, 907)
Epoch 99, accuracy: 0.7342
batch size: (903, 903)
Epoch 100, accuracy: 0.7350
Epoch 100, Train Loss: 0.0621, Val Loss: 0.5575
batch size: (911, 911)
Epoch 101, accuracy: 0.7341
batch size: (898, 898)
Epoch 102, accuracy: 0.7339
Epoch 102, Train Loss: 0.0731, Val Loss: 0.5568
batch size: (907, 907)
Epoch 103, accuracy: 0.7344
batch size: (912, 912)
Epoch 104, accuracy: 0.7329
Epoch 104, Train Loss: 0.0674, Val Loss: 0.5673
batch size: (897, 897)
Epoch 105, accuracy: 0.7326
batch size: (908, 908)
Epoch 106, accuracy: 0.7348
Epoch 106, Train Loss: 0.0593, Val Loss: 0.5495
batch size: (891, 891)
Epoch 107, accuracy: 0.7329
batch size: (902, 902)
Epoch 108, accuracy: 0.7339
Epoch 108, Train Loss: 0.0569, Val Loss: 0.5609
batch size: (918, 918)
Epoch 109, accuracy: 0.7336
batch size: (900, 900)
Epoch 110, accuracy: 0.7337
Epoch 110, Train Loss: 0.0652, Val Loss: 0.5599
batch size: (908, 908)
Epoch 111, accuracy: 0.7369
batch size: (903, 903)
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 112, accuracy: 0.7329
Epoch 112, Train Loss: 0.0633, Val Loss: 0.5738
batch size: (900, 900)
Epoch 113, accuracy: 0.7315
batch size: (890, 890)
Epoch 114, accuracy: 0.7327
Epoch 114, Train Loss: 0.0558, Val Loss: 0.5656
batch size: (920, 920)
Epoch 115, accuracy: 0.7348
batch size: (896, 896)
Epoch 116, accuracy: 0.7344
Epoch 116, Train Loss: 0.0860, Val Loss: 0.5492
batch size: (892, 892)
Epoch 117, accuracy: 0.7343
batch size: (898, 898)
Epoch 118, accuracy: 0.7326
Epoch 118, Train Loss: 0.0756, Val Loss: 0.5606
batch size: (901, 901)
Epoch 119, accuracy: 0.7326
batch size: (894, 894)
Epoch 120, accuracy: 0.7325
Epoch 120, Train Loss: 0.0648, Val Loss: 0.5592
batch size: (898, 898)
Epoch 121, accuracy: 0.7332
batch size: (911, 911)
Epoch 122, accuracy: 0.7341
Epoch 122, Train Loss: 0.0695, Val Loss: 0.5485
batch size: (910, 910)
Epoch 123, accuracy: 0.7361
batch size: (900, 900)
Epoch 124, accuracy: 0.7368
Epoch 124, Train Loss: 0.0679, Val Loss: 0.5576
batch size: (877, 877)
Epoch 125, accuracy: 0.7355
batch size: (898, 898)
Epoch 126, accuracy: 0.7330
Epoch 126, Train Loss: 0.0709, Val Loss: 0.5605
batch size: (888, 888)
Epoch 127, accuracy: 0.7344
batch size: (899, 899)
Epoch 128, accuracy: 0.7342
Epoch 128, Train Loss: 0.0572, Val Loss: 0.5676
batch size: (876, 876)
Epoch 129, accuracy: 0.7307
batch size: (884, 884)
Epoch 130, accuracy: 0.7343
Epoch 130, Train Loss: 0.0590, Val Loss: 0.5612
batch size: (884, 884)
Epoch 131, accuracy: 0.7343
batch size: (900, 900)
Epoch 132, accuracy: 0.7300
Epoch 132, Train Loss: 0.0577, Val Loss: 0.5527
batch size: (915, 915)
Epoch 133, accuracy: 0.7340
batch size: (909, 909)
Epoch 134, accuracy: 0.7348
Epoch 134, Train Loss: 0.0584, Val Loss: 0.5654
batch size: (891, 891)
Epoch 135, accuracy: 0.7317
batch size: (916, 916)
Epoch 136, accuracy: 0.7334
Epoch 136, Train Loss: 0.0699, Val Loss: 0.5505
batch size: (894, 894)
Epoch 137, accuracy: 0.7339
batch size: (917, 917)
Epoch 138, accuracy: 0.7348
Epoch 138, Train Loss: 0.0625, Val Loss: 0.5527
batch size: (901, 901)
Epoch 139, accuracy: 0.7331
batch size: (911, 911)
Epoch 140, accuracy: 0.7343
Epoch 140, Train Loss: 0.0649, Val Loss: 0.5677
batch size: (914, 914)
Epoch 141, accuracy: 0.7357
batch size: (904, 904)
Epoch 142, accuracy: 0.7336
Epoch 142, Train Loss: 0.0633, Val Loss: 0.5623
batch size: (899, 899)
Epoch 143, accuracy: 0.7339
batch size: (904, 904)
Epoch 144, accuracy: 0.7349
Epoch 144, Train Loss: 0.0603, Val Loss: 0.5683
batch size: (891, 891)
Epoch 145, accuracy: 0.7311
batch size: (910, 910)
Epoch 146, accuracy: 0.7344
Epoch 146, Train Loss: 0.0590, Val Loss: 0.5603
batch size: (882, 882)
Epoch 147, accuracy: 0.7351
batch size: (900, 900)
Epoch 148, accuracy: 0.7337
Epoch 148, Train Loss: 0.0620, Val Loss: 0.5714
batch size: (887, 887)
Epoch 149, accuracy: 0.7331
batch size: (906, 906)
Epoch 150, accuracy: 0.7339
Epoch 150, Train Loss: 0.0595, Val Loss: 0.5421
batch size: (912, 912)
Epoch 151, accuracy: 0.7331
batch size: (905, 905)
Epoch 152, accuracy: 0.7332
Epoch 152, Train Loss: 0.0583, Val Loss: 0.5494
batch size: (910, 910)
Epoch 153, accuracy: 0.7360
batch size: (909, 909)
Epoch 154, accuracy: 0.7361
Epoch 154, Train Loss: 0.0718, Val Loss: 0.5537
batch size: (907, 907)
Epoch 155, accuracy: 0.7323
batch size: (895, 895)
Epoch 156, accuracy: 0.7340
Epoch 156, Train Loss: 0.0638, Val Loss: 0.5525
batch size: (891, 891)
Epoch 157, accuracy: 0.7315
batch size: (880, 880)
Epoch 158, accuracy: 0.7344
Epoch 158, Train Loss: 0.0622, Val Loss: 0.5564
batch size: (902, 902)
Epoch 159, accuracy: 0.7340
batch size: (911, 911)
Epoch 160, accuracy: 0.7345
Epoch 160, Train Loss: 0.0623, Val Loss: 0.5556
batch size: (886, 886)
Epoch 161, accuracy: 0.7322
batch size: (899, 899)
Epoch 162, accuracy: 0.7344
Epoch 162, Train Loss: 0.0568, Val Loss: 0.5533
batch size: (918, 918)
Epoch 163, accuracy: 0.7340
batch size: (896, 896)
Epoch 164, accuracy: 0.7341
Epoch 164, Train Loss: 0.0554, Val Loss: 0.5716
batch size: (903, 903)
Epoch 165, accuracy: 0.7366
batch size: (894, 894)
Epoch 166, accuracy: 0.7319
Epoch 166, Train Loss: 0.0525, Val Loss: 0.5498
batch size: (886, 886)
Epoch 167, accuracy: 0.7345
batch size: (891, 891)
Epoch 168, accuracy: 0.7352
Epoch 168, Train Loss: 0.0600, Val Loss: 0.5664
batch size: (907, 907)
Epoch 169, accuracy: 0.7355
batch size: (905, 905)
Epoch 170, accuracy: 0.7339
Epoch 170, Train Loss: 0.0677, Val Loss: 0.5538
batch size: (878, 878)
Epoch 171, accuracy: 0.7338
batch size: (891, 891)
Epoch 172, accuracy: 0.7338
Epoch 172, Train Loss: 0.0563, Val Loss: 0.5604
batch size: (889, 889)
Epoch 173, accuracy: 0.7349
batch size: (904, 904)
Epoch 174, accuracy: 0.7355
Epoch 174, Train Loss: 0.0624, Val Loss: 0.5553
batch size: (906, 906)
Epoch 175, accuracy: 0.7337
batch size: (910, 910)
Epoch 176, accuracy: 0.7349
Epoch 176, Train Loss: 0.0743, Val Loss: 0.5513
batch size: (906, 906)
Epoch 177, accuracy: 0.7332
batch size: (893, 893)
Epoch 178, accuracy: 0.7328
Epoch 178, Train Loss: 0.0538, Val Loss: 0.5651
batch size: (901, 901)
Epoch 179, accuracy: 0.7334
batch size: (891, 891)
Epoch 180, accuracy: 0.7313
Epoch 180, Train Loss: 0.0654, Val Loss: 0.5671
batch size: (889, 889)
Epoch 181, accuracy: 0.7340
batch size: (931, 931)
Epoch 182, accuracy: 0.7334
Epoch 182, Train Loss: 0.0635, Val Loss: 0.5558
batch size: (904, 904)
Epoch 183, accuracy: 0.7336
batch size: (890, 890)
Epoch 184, accuracy: 0.7332
Epoch 184, Train Loss: 0.0573, Val Loss: 0.5587
batch size: (899, 899)
Epoch 185, accuracy: 0.7344
batch size: (908, 908)
Epoch 186, accuracy: 0.7330
Epoch 186, Train Loss: 0.0695, Val Loss: 0.5569
batch size: (912, 912)
Epoch 187, accuracy: 0.7337
batch size: (914, 914)
Epoch 188, accuracy: 0.7302
Epoch 188, Train Loss: 0.0700, Val Loss: 0.5585
batch size: (904, 904)
Epoch 189, accuracy: 0.7319
batch size: (898, 898)
Epoch 190, accuracy: 0.7340
Epoch 190, Train Loss: 0.0678, Val Loss: 0.5532
batch size: (901, 901)
Epoch 191, accuracy: 0.7338
batch size: (897, 897)
Epoch 192, accuracy: 0.7376
Epoch 192, Train Loss: 0.0533, Val Loss: 0.5713
batch size: (896, 896)
Epoch 193, accuracy: 0.7331
batch size: (900, 900)
Epoch 194, accuracy: 0.7347
Epoch 194, Train Loss: 0.0648, Val Loss: 0.5680
batch size: (911, 911)
Epoch 195, accuracy: 0.7334
batch size: (889, 889)
Epoch 196, accuracy: 0.7348
Epoch 196, Train Loss: 0.0570, Val Loss: 0.5556
batch size: (925, 925)
Epoch 197, accuracy: 0.7346
batch size: (904, 904)
Epoch 198, accuracy: 0.7337
Epoch 198, Train Loss: 0.0609, Val Loss: 0.5692
batch size: (895, 895)
Epoch 199, accuracy: 0.7333
Loaded best model with val_loss = 0.541668176651001
test :accuracy 0.7468, f1_macro: 0.7419, f1_micro: 0.7468, auc: 0.8934
Training GAT with 8 layers...
可训练参数: 1090447_GAT
不可训练参数: 0
batch size: (914, 914)
✅ Epoch 0: New best model saved with val_loss = 1.1108
Epoch 0, accuracy: 0.1675
Epoch 0, Train Loss: 1.0986, Val Loss: 1.1108
batch size: (892, 892)
Epoch 1, accuracy: 0.1687
batch size: (895, 895)
✅ Epoch 2: New best model saved with val_loss = 1.0817
Epoch 2, accuracy: 0.4000
Epoch 2, Train Loss: 1.0897, Val Loss: 1.0817
batch size: (902, 902)
Epoch 3, accuracy: 0.1699
batch size: (891, 891)
✅ Epoch 4: New best model saved with val_loss = 1.0550
Epoch 4, accuracy: 0.4174
Epoch 4, Train Loss: 1.0628, Val Loss: 1.0550
batch size: (916, 916)
✅ Epoch 5: New best model saved with val_loss = 1.0395
Epoch 5, accuracy: 0.4260
batch size: (893, 893)
✅ Epoch 6: New best model saved with val_loss = 0.9715
Epoch 6, accuracy: 0.4936
Epoch 6, Train Loss: 1.0385, Val Loss: 0.9715
batch size: (894, 894)
Epoch 7, accuracy: 0.3844
batch size: (910, 910)
✅ Epoch 8: New best model saved with val_loss = 0.9177
Epoch 8, accuracy: 0.5352
Epoch 8, Train Loss: 1.0433, Val Loss: 0.9177
batch size: (893, 893)
✅ Epoch 9: New best model saved with val_loss = 0.8931
Epoch 9, accuracy: 0.5221
batch size: (891, 891)
Epoch 10, accuracy: 0.5067
Epoch 10, Train Loss: 0.7710, Val Loss: 0.9099
batch size: (908, 908)
✅ Epoch 11: New best model saved with val_loss = 0.8838
Epoch 11, accuracy: 0.5164
batch size: (894, 894)
✅ Epoch 12: New best model saved with val_loss = 0.8690
Epoch 12, accuracy: 0.5354
Epoch 12, Train Loss: 0.6982, Val Loss: 0.8690
batch size: (895, 895)
Epoch 13, accuracy: 0.5322
batch size: (886, 886)
Epoch 14, accuracy: 0.5316
Epoch 14, Train Loss: 0.6906, Val Loss: 1.3976
batch size: (897, 897)
Epoch 15, accuracy: 0.5284
batch size: (896, 896)
Epoch 16, accuracy: 0.5279
Epoch 16, Train Loss: 0.6495, Val Loss: 1.1238
batch size: (902, 902)
Epoch 17, accuracy: 0.5197
batch size: (887, 887)
Epoch 18, accuracy: 0.5126
Epoch 18, Train Loss: 0.5529, Val Loss: 1.0376
batch size: (902, 902)
Epoch 19, accuracy: 0.5148
batch size: (894, 894)
Epoch 20, accuracy: 0.5100
Epoch 20, Train Loss: 0.6634, Val Loss: 1.0047
batch size: (913, 913)
Epoch 21, accuracy: 0.5152
batch size: (897, 897)
Epoch 22, accuracy: 0.5160
Epoch 22, Train Loss: 0.5427, Val Loss: 0.9752
batch size: (903, 903)
Epoch 23, accuracy: 0.5184
batch size: (886, 886)
Epoch 24, accuracy: 0.5172
Epoch 24, Train Loss: 0.6012, Val Loss: 0.9453
batch size: (903, 903)
Epoch 25, accuracy: 0.5178
batch size: (905, 905)
Epoch 26, accuracy: 0.5145
Epoch 26, Train Loss: 0.5782, Val Loss: 0.9216
batch size: (911, 911)
Epoch 27, accuracy: 0.5143
batch size: (903, 903)
Epoch 28, accuracy: 0.5172
Epoch 28, Train Loss: 0.5077, Val Loss: 0.9127
batch size: (898, 898)
Epoch 29, accuracy: 0.5155
batch size: (903, 903)
Epoch 30, accuracy: 0.5163
Epoch 30, Train Loss: 0.5715, Val Loss: 0.9493
batch size: (927, 927)
Epoch 31, accuracy: 0.5135
batch size: (893, 893)
Epoch 32, accuracy: 0.5178
Epoch 32, Train Loss: 0.6005, Val Loss: 0.9348
batch size: (896, 896)
Epoch 33, accuracy: 0.5175
batch size: (906, 906)
Epoch 34, accuracy: 0.5148
Epoch 34, Train Loss: 0.6625, Val Loss: 0.9609
batch size: (880, 880)
Epoch 35, accuracy: 0.5168
batch size: (905, 905)
Epoch 36, accuracy: 0.5180
Epoch 36, Train Loss: 0.6115, Val Loss: 0.9156
batch size: (887, 887)
Epoch 37, accuracy: 0.5186
batch size: (881, 881)
Epoch 38, accuracy: 0.5189
Epoch 38, Train Loss: 0.6333, Val Loss: 0.9482
batch size: (903, 903)
Epoch 39, accuracy: 0.5129
batch size: (896, 896)
Epoch 40, accuracy: 0.5143
Epoch 40, Train Loss: 0.6096, Val Loss: 0.9646
batch size: (878, 878)
Epoch 41, accuracy: 0.5150
batch size: (892, 892)
Epoch 42, accuracy: 0.5181
Epoch 42, Train Loss: 0.5619, Val Loss: 0.9850
batch size: (898, 898)
Epoch 43, accuracy: 0.5155
batch size: (892, 892)
Epoch 44, accuracy: 0.5158
Epoch 44, Train Loss: 0.6106, Val Loss: 0.9196
batch size: (909, 909)
Epoch 45, accuracy: 0.5187
batch size: (899, 899)
Epoch 46, accuracy: 0.5165
Epoch 46, Train Loss: 0.6020, Val Loss: 0.9423
batch size: (925, 925)
Epoch 47, accuracy: 0.5164
batch size: (902, 902)
Epoch 48, accuracy: 0.5159
Epoch 48, Train Loss: 0.5926, Val Loss: 0.9915
batch size: (892, 892)
Epoch 49, accuracy: 0.5097
batch size: (905, 905)
Epoch 50, accuracy: 0.5170
Epoch 50, Train Loss: 0.5211, Val Loss: 0.9605
batch size: (904, 904)
Epoch 51, accuracy: 0.5182
batch size: (910, 910)
Epoch 52, accuracy: 0.5163
Epoch 52, Train Loss: 0.5706, Val Loss: 0.9532
batch size: (894, 894)
Epoch 53, accuracy: 0.5152
batch size: (900, 900)
Epoch 54, accuracy: 0.5138
Epoch 54, Train Loss: 0.6183, Val Loss: 0.8917
batch size: (891, 891)
Epoch 55, accuracy: 0.5184
batch size: (887, 887)
Epoch 56, accuracy: 0.5143
Epoch 56, Train Loss: 0.5961, Val Loss: 0.9397
batch size: (892, 892)
Epoch 57, accuracy: 0.5195
batch size: (914, 914)
Epoch 58, accuracy: 0.5182
Epoch 58, Train Loss: 0.5387, Val Loss: 0.9068
batch size: (898, 898)
Epoch 59, accuracy: 0.5170
batch size: (912, 912)
Epoch 60, accuracy: 0.5194
Epoch 60, Train Loss: 0.5412, Val Loss: 0.9398
batch size: (892, 892)
Epoch 61, accuracy: 0.5189
batch size: (913, 913)
Epoch 62, accuracy: 0.5143
Epoch 62, Train Loss: 0.5636, Val Loss: 0.9659
batch size: (905, 905)
Epoch 63, accuracy: 0.5185
batch size: (917, 917)
Epoch 64, accuracy: 0.5191
Epoch 64, Train Loss: 0.5775, Val Loss: 0.9447
batch size: (922, 922)
Epoch 65, accuracy: 0.5132
batch size: (924, 924)
Epoch 66, accuracy: 0.5183
Epoch 66, Train Loss: 0.5744, Val Loss: 0.9600
batch size: (878, 878)
Epoch 67, accuracy: 0.5169
batch size: (899, 899)
Epoch 68, accuracy: 0.5177
Epoch 68, Train Loss: 0.5403, Val Loss: 0.9347
batch size: (892, 892)
Epoch 69, accuracy: 0.5173
batch size: (889, 889)
Epoch 70, accuracy: 0.5146
Epoch 70, Train Loss: 0.5677, Val Loss: 0.9214
batch size: (918, 918)
Epoch 71, accuracy: 0.5176
batch size: (894, 894)
Epoch 72, accuracy: 0.5165
Epoch 72, Train Loss: 0.5579, Val Loss: 0.9576
batch size: (885, 885)
Epoch 73, accuracy: 0.5177
batch size: (906, 906)
Epoch 74, accuracy: 0.5103
Epoch 74, Train Loss: 0.6577, Val Loss: 0.9709
batch size: (887, 887)
Epoch 75, accuracy: 0.5149
batch size: (926, 926)
Epoch 76, accuracy: 0.5152
Epoch 76, Train Loss: 0.5217, Val Loss: 0.9523
batch size: (884, 884)
Epoch 77, accuracy: 0.5128
batch size: (906, 906)
Epoch 78, accuracy: 0.5161
Epoch 78, Train Loss: 0.5297, Val Loss: 0.9387
batch size: (886, 886)
Epoch 79, accuracy: 0.5162
batch size: (903, 903)
Epoch 80, accuracy: 0.5194
Epoch 80, Train Loss: 0.5920, Val Loss: 0.9254
batch size: (908, 908)
Epoch 81, accuracy: 0.5163
batch size: (897, 897)
Epoch 82, accuracy: 0.5152
Epoch 82, Train Loss: 0.5303, Val Loss: 0.9582
batch size: (890, 890)
Epoch 83, accuracy: 0.5176
batch size: (901, 901)
Epoch 84, accuracy: 0.5162
Epoch 84, Train Loss: 0.5454, Val Loss: 0.9379
batch size: (911, 911)
Epoch 85, accuracy: 0.5179
batch size: (885, 885)
Epoch 86, accuracy: 0.5172
Epoch 86, Train Loss: 0.6545, Val Loss: 0.9906
batch size: (901, 901)
Epoch 87, accuracy: 0.5157
batch size: (903, 903)
Epoch 88, accuracy: 0.5208
Epoch 88, Train Loss: 0.5828, Val Loss: 0.9446
batch size: (900, 900)
Epoch 89, accuracy: 0.5177
batch size: (897, 897)
Epoch 90, accuracy: 0.5178
Epoch 90, Train Loss: 0.5800, Val Loss: 0.9673
batch size: (898, 898)
Epoch 91, accuracy: 0.5148
batch size: (893, 893)
Epoch 92, accuracy: 0.5173
Epoch 92, Train Loss: 0.5602, Val Loss: 0.9538
batch size: (910, 910)
Epoch 93, accuracy: 0.5154
batch size: (900, 900)
Epoch 94, accuracy: 0.5197
Epoch 94, Train Loss: 0.6027, Val Loss: 0.9597
batch size: (889, 889)
Epoch 95, accuracy: 0.5146
batch size: (906, 906)
Epoch 96, accuracy: 0.5151
Epoch 96, Train Loss: 0.5108, Val Loss: 0.9201
batch size: (910, 910)
Epoch 97, accuracy: 0.5154
batch size: (899, 899)
Epoch 98, accuracy: 0.5139
Epoch 98, Train Loss: 0.5263, Val Loss: 0.9284
batch size: (901, 901)
Epoch 99, accuracy: 0.5162
batch size: (894, 894)
Epoch 100, accuracy: 0.5148
Epoch 100, Train Loss: 0.5278, Val Loss: 0.9476
batch size: (883, 883)
Epoch 101, accuracy: 0.5206
batch size: (893, 893)
Epoch 102, accuracy: 0.5155
Epoch 102, Train Loss: 0.5333, Val Loss: 0.9389
batch size: (908, 908)
Epoch 103, accuracy: 0.5169
batch size: (879, 879)
Epoch 104, accuracy: 0.5200
Epoch 104, Train Loss: 0.5609, Val Loss: 0.9683
batch size: (900, 900)
Epoch 105, accuracy: 0.5135
batch size: (905, 905)
Epoch 106, accuracy: 0.5152
Epoch 106, Train Loss: 0.6017, Val Loss: 0.9759
batch size: (911, 911)
Epoch 107, accuracy: 0.5170
batch size: (914, 914)
Epoch 108, accuracy: 0.5143
Epoch 108, Train Loss: 0.5474, Val Loss: 0.9556
batch size: (889, 889)
Epoch 109, accuracy: 0.5174
batch size: (894, 894)
Epoch 110, accuracy: 0.5193
Epoch 110, Train Loss: 0.5239, Val Loss: 0.9786
batch size: (904, 904)
Epoch 111, accuracy: 0.5193
batch size: (895, 895)
Epoch 112, accuracy: 0.5140
Epoch 112, Train Loss: 0.5700, Val Loss: 0.9316
batch size: (910, 910)
Epoch 113, accuracy: 0.5172
batch size: (900, 900)
Epoch 114, accuracy: 0.5147
Epoch 114, Train Loss: 0.5382, Val Loss: 0.9620
batch size: (888, 888)
Epoch 115, accuracy: 0.5137
batch size: (890, 890)
Epoch 116, accuracy: 0.5176
Epoch 116, Train Loss: 0.5343, Val Loss: 0.9625
batch size: (910, 910)
Epoch 117, accuracy: 0.5169
batch size: (902, 902)
Epoch 118, accuracy: 0.5163
Epoch 118, Train Loss: 0.5903, Val Loss: 0.9438
batch size: (913, 913)
Epoch 119, accuracy: 0.5172
batch size: (906, 906)
Epoch 120, accuracy: 0.5172
Epoch 120, Train Loss: 0.6315, Val Loss: 0.9101
batch size: (896, 896)
Epoch 121, accuracy: 0.5146
batch size: (874, 874)
Epoch 122, accuracy: 0.5140
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 122, Train Loss: 0.5319, Val Loss: 0.9800
batch size: (908, 908)
Epoch 123, accuracy: 0.5181
batch size: (893, 893)
Epoch 124, accuracy: 0.5159
Epoch 124, Train Loss: 0.5785, Val Loss: 0.8985
batch size: (886, 886)
Epoch 125, accuracy: 0.5163
batch size: (908, 908)
Epoch 126, accuracy: 0.5163
Epoch 126, Train Loss: 0.5052, Val Loss: 0.9670
batch size: (910, 910)
Epoch 127, accuracy: 0.5150
batch size: (884, 884)
Epoch 128, accuracy: 0.5153
Epoch 128, Train Loss: 0.5989, Val Loss: 0.9647
batch size: (903, 903)
Epoch 129, accuracy: 0.5169
batch size: (925, 925)
Epoch 130, accuracy: 0.5164
Epoch 130, Train Loss: 0.5782, Val Loss: 0.9653
batch size: (887, 887)
Epoch 131, accuracy: 0.5128
batch size: (911, 911)
Epoch 132, accuracy: 0.5146
Epoch 132, Train Loss: 0.5753, Val Loss: 0.9893
batch size: (899, 899)
Epoch 133, accuracy: 0.5181
batch size: (904, 904)
Epoch 134, accuracy: 0.5110
Epoch 134, Train Loss: 0.5253, Val Loss: 0.9400
batch size: (898, 898)
Epoch 135, accuracy: 0.5172
batch size: (897, 897)
Epoch 136, accuracy: 0.5177
Epoch 136, Train Loss: 0.5261, Val Loss: 0.9264
batch size: (921, 921)
Epoch 137, accuracy: 0.5160
batch size: (902, 902)
Epoch 138, accuracy: 0.5178
Epoch 138, Train Loss: 0.5737, Val Loss: 0.9401
batch size: (905, 905)
Epoch 139, accuracy: 0.5178
batch size: (919, 919)
Epoch 140, accuracy: 0.5181
Epoch 140, Train Loss: 0.5356, Val Loss: 0.9619
batch size: (896, 896)
Epoch 141, accuracy: 0.5156
batch size: (913, 913)
Epoch 142, accuracy: 0.5202
Epoch 142, Train Loss: 0.5217, Val Loss: 0.9681
batch size: (887, 887)
Epoch 143, accuracy: 0.5150
batch size: (900, 900)
Epoch 144, accuracy: 0.5154
Epoch 144, Train Loss: 0.5885, Val Loss: 0.9539
batch size: (890, 890)
Epoch 145, accuracy: 0.5189
batch size: (899, 899)
Epoch 146, accuracy: 0.5190
Epoch 146, Train Loss: 0.5792, Val Loss: 0.9438
batch size: (907, 907)
Epoch 147, accuracy: 0.5172
batch size: (904, 904)
Epoch 148, accuracy: 0.5168
Epoch 148, Train Loss: 0.6810, Val Loss: 0.9894
batch size: (891, 891)
Epoch 149, accuracy: 0.5156
batch size: (906, 906)
Epoch 150, accuracy: 0.5167
Epoch 150, Train Loss: 0.5153, Val Loss: 0.8973
batch size: (907, 907)
Epoch 151, accuracy: 0.5162
batch size: (888, 888)
Epoch 152, accuracy: 0.5126
Epoch 152, Train Loss: 0.5914, Val Loss: 0.9475
batch size: (894, 894)
Epoch 153, accuracy: 0.5151
batch size: (914, 914)
Epoch 154, accuracy: 0.5215
Epoch 154, Train Loss: 0.5508, Val Loss: 0.9854
batch size: (910, 910)
Epoch 155, accuracy: 0.5155
batch size: (887, 887)
Epoch 156, accuracy: 0.5194
Epoch 156, Train Loss: 0.6174, Val Loss: 0.9420
batch size: (904, 904)
Epoch 157, accuracy: 0.5156
batch size: (891, 891)
Epoch 158, accuracy: 0.5159
Epoch 158, Train Loss: 0.5362, Val Loss: 0.9485
batch size: (906, 906)
Epoch 159, accuracy: 0.5172
batch size: (882, 882)
Epoch 160, accuracy: 0.5115
Epoch 160, Train Loss: 0.5368, Val Loss: 0.9842
batch size: (892, 892)
Epoch 161, accuracy: 0.5175
batch size: (909, 909)
Epoch 162, accuracy: 0.5172
Epoch 162, Train Loss: 0.6097, Val Loss: 0.9411
batch size: (899, 899)
Epoch 163, accuracy: 0.5146
batch size: (896, 896)
Epoch 164, accuracy: 0.5155
Epoch 164, Train Loss: 0.6259, Val Loss: 0.9368
batch size: (889, 889)
Epoch 165, accuracy: 0.5188
batch size: (897, 897)
Epoch 166, accuracy: 0.5164
Epoch 166, Train Loss: 0.6530, Val Loss: 0.9745
batch size: (912, 912)
Epoch 167, accuracy: 0.5152
batch size: (905, 905)
Epoch 168, accuracy: 0.5185
Epoch 168, Train Loss: 0.5602, Val Loss: 0.9727
batch size: (898, 898)
Epoch 169, accuracy: 0.5163
batch size: (891, 891)
Epoch 170, accuracy: 0.5158
Epoch 170, Train Loss: 0.5252, Val Loss: 0.9502
batch size: (915, 915)
Epoch 171, accuracy: 0.5194
batch size: (914, 914)
Epoch 172, accuracy: 0.5098
Epoch 172, Train Loss: 0.6161, Val Loss: 0.9699
batch size: (911, 911)
Epoch 173, accuracy: 0.5174
batch size: (897, 897)
Epoch 174, accuracy: 0.5156
Epoch 174, Train Loss: 0.5446, Val Loss: 0.9452
batch size: (898, 898)
Epoch 175, accuracy: 0.5152
batch size: (907, 907)
Epoch 176, accuracy: 0.5175
Epoch 176, Train Loss: 0.6215, Val Loss: 0.9584
batch size: (912, 912)
Epoch 177, accuracy: 0.5155
batch size: (878, 878)
Epoch 178, accuracy: 0.5182
Epoch 178, Train Loss: 0.5745, Val Loss: 0.8973
batch size: (904, 904)
Epoch 179, accuracy: 0.5183
batch size: (915, 915)
Epoch 180, accuracy: 0.5176
Epoch 180, Train Loss: 0.5941, Val Loss: 0.9598
batch size: (886, 886)
Epoch 181, accuracy: 0.5173
batch size: (896, 896)
Epoch 182, accuracy: 0.5159
Epoch 182, Train Loss: 0.5719, Val Loss: 0.9355
batch size: (896, 896)
Epoch 183, accuracy: 0.5170
batch size: (895, 895)
Epoch 184, accuracy: 0.5212
Epoch 184, Train Loss: 0.5191, Val Loss: 0.9423
batch size: (910, 910)
Epoch 185, accuracy: 0.5095
batch size: (888, 888)
Epoch 186, accuracy: 0.5198
Epoch 186, Train Loss: 0.5186, Val Loss: 0.9805
batch size: (907, 907)
Epoch 187, accuracy: 0.5167
batch size: (908, 908)
Epoch 188, accuracy: 0.5125
Epoch 188, Train Loss: 0.5494, Val Loss: 0.9792
batch size: (913, 913)
Epoch 189, accuracy: 0.5159
batch size: (888, 888)
Epoch 190, accuracy: 0.5174
Epoch 190, Train Loss: 0.5134, Val Loss: 0.9073
batch size: (888, 888)
Epoch 191, accuracy: 0.5184
batch size: (888, 888)
Epoch 192, accuracy: 0.5173
Epoch 192, Train Loss: 0.6090, Val Loss: 0.9250
batch size: (903, 903)
Epoch 193, accuracy: 0.5182
batch size: (904, 904)
Epoch 194, accuracy: 0.5189
Epoch 194, Train Loss: 0.5144, Val Loss: 0.9721
batch size: (900, 900)
Epoch 195, accuracy: 0.5149
batch size: (888, 888)
Epoch 196, accuracy: 0.5186
Epoch 196, Train Loss: 0.5424, Val Loss: 0.9430
batch size: (893, 893)
Epoch 197, accuracy: 0.5178
batch size: (913, 913)
Epoch 198, accuracy: 0.5176
Epoch 198, Train Loss: 0.5784, Val Loss: 0.9445
batch size: (908, 908)
Epoch 199, accuracy: 0.5179
Loaded best model with val_loss = 0.8689503073692322
test :accuracy 0.5306, f1_macro: 0.4680, f1_micro: 0.5306, auc: 0.7293
Training GAT with 32 layers...
可训练参数: 4666255_GAT
不可训练参数: 0
batch size: (907, 907)
✅ Epoch 0: New best model saved with val_loss = 1.0987
Epoch 0, accuracy: 0.4242
Epoch 0, Train Loss: 1.0980, Val Loss: 1.0987
batch size: (920, 920)
✅ Epoch 1: New best model saved with val_loss = 1.0871
Epoch 1, accuracy: 0.4298
batch size: (899, 899)
Epoch 2, accuracy: 0.1667
Epoch 2, Train Loss: 1.0925, Val Loss: 1.1452
batch size: (901, 901)
Epoch 3, accuracy: 0.4333
batch size: (890, 890)
✅ Epoch 4: New best model saved with val_loss = 1.0829
Epoch 4, accuracy: 0.4284
Epoch 4, Train Loss: 1.0991, Val Loss: 1.0829
batch size: (906, 906)
✅ Epoch 5: New best model saved with val_loss = 1.0805
Epoch 5, accuracy: 0.4281
batch size: (897, 897)
✅ Epoch 6: New best model saved with val_loss = 1.0803
Epoch 6, accuracy: 0.4268
Epoch 6, Train Loss: 1.1039, Val Loss: 1.0803
batch size: (898, 898)
Epoch 7, accuracy: 0.4276
batch size: (897, 897)
Epoch 8, accuracy: 0.4313
Epoch 8, Train Loss: 1.1022, Val Loss: 1.0840
batch size: (901, 901)
Epoch 9, accuracy: 0.4351
batch size: (909, 909)
Epoch 10, accuracy: 0.4303
Epoch 10, Train Loss: 1.1021, Val Loss: 1.0894
batch size: (896, 896)
Epoch 11, accuracy: 0.4279
batch size: (897, 897)
Epoch 12, accuracy: 0.4289
Epoch 12, Train Loss: 1.1002, Val Loss: 1.0951
batch size: (914, 914)
Epoch 13, accuracy: 0.4293
batch size: (912, 912)
Epoch 14, accuracy: 0.4319
Epoch 14, Train Loss: 1.1009, Val Loss: 1.0956
batch size: (898, 898)
Epoch 15, accuracy: 0.4303
batch size: (923, 923)
Epoch 16, accuracy: 0.4271
Epoch 16, Train Loss: 1.0979, Val Loss: 1.0961
batch size: (896, 896)
Epoch 17, accuracy: 0.3988
batch size: (877, 877)
Epoch 18, accuracy: 0.4017
Epoch 18, Train Loss: 1.0999, Val Loss: 1.0965
batch size: (919, 919)
Epoch 19, accuracy: 0.4037
batch size: (891, 891)
Epoch 20, accuracy: 0.4045
Epoch 20, Train Loss: 1.0995, Val Loss: 1.0965
batch size: (909, 909)
Epoch 21, accuracy: 0.4032
batch size: (908, 908)
Epoch 22, accuracy: 0.4027
Epoch 22, Train Loss: 1.0983, Val Loss: 1.0966
batch size: (903, 903)
Epoch 23, accuracy: 0.3991
batch size: (898, 898)
Epoch 24, accuracy: 0.4029
Epoch 24, Train Loss: 1.1003, Val Loss: 1.0966
batch size: (908, 908)
Epoch 25, accuracy: 0.4013
batch size: (898, 898)
Epoch 26, accuracy: 0.4010
Epoch 26, Train Loss: 1.0993, Val Loss: 1.0966
batch size: (893, 893)
Epoch 27, accuracy: 0.3973
batch size: (907, 907)
Epoch 28, accuracy: 0.3970
Epoch 28, Train Loss: 1.1000, Val Loss: 1.0966
batch size: (904, 904)
Epoch 29, accuracy: 0.4041
batch size: (902, 902)
Epoch 30, accuracy: 0.3996
Epoch 30, Train Loss: 1.0985, Val Loss: 1.0966
batch size: (912, 912)
Epoch 31, accuracy: 0.4043
batch size: (896, 896)
Epoch 32, accuracy: 0.4034
Epoch 32, Train Loss: 1.0980, Val Loss: 1.0966
batch size: (919, 919)
Epoch 33, accuracy: 0.4019
batch size: (904, 904)
Epoch 34, accuracy: 0.4020
Epoch 34, Train Loss: 1.0982, Val Loss: 1.0966
batch size: (887, 887)
Epoch 35, accuracy: 0.4032
batch size: (893, 893)
Epoch 36, accuracy: 0.4051
Epoch 36, Train Loss: 1.0983, Val Loss: 1.0966
batch size: (876, 876)
Epoch 37, accuracy: 0.4013
batch size: (894, 894)
Epoch 38, accuracy: 0.3994
Epoch 38, Train Loss: 1.0985, Val Loss: 1.0966
batch size: (897, 897)
Epoch 39, accuracy: 0.4050
batch size: (896, 896)
Epoch 40, accuracy: 0.4025
Epoch 40, Train Loss: 1.0990, Val Loss: 1.0966
batch size: (897, 897)
Epoch 41, accuracy: 0.4017
batch size: (905, 905)
Epoch 42, accuracy: 0.4020
Epoch 42, Train Loss: 1.1021, Val Loss: 1.0966
batch size: (887, 887)
Epoch 43, accuracy: 0.4027
batch size: (901, 901)
Epoch 44, accuracy: 0.4005
Epoch 44, Train Loss: 1.0969, Val Loss: 1.0966
batch size: (901, 901)
Epoch 45, accuracy: 0.4001
batch size: (900, 900)
Epoch 46, accuracy: 0.4008
Epoch 46, Train Loss: 1.0980, Val Loss: 1.0966
batch size: (915, 915)
Epoch 47, accuracy: 0.4041
batch size: (890, 890)
Epoch 48, accuracy: 0.4014
Epoch 48, Train Loss: 1.0963, Val Loss: 1.0966
batch size: (911, 911)
Epoch 49, accuracy: 0.4056
batch size: (911, 911)
Epoch 50, accuracy: 0.4058
Epoch 50, Train Loss: 1.1005, Val Loss: 1.0966
batch size: (889, 889)
Epoch 51, accuracy: 0.4013
batch size: (900, 900)
Epoch 52, accuracy: 0.3998
Epoch 52, Train Loss: 1.1001, Val Loss: 1.0966
batch size: (915, 915)
Epoch 53, accuracy: 0.4033
batch size: (874, 874)
Epoch 54, accuracy: 0.4050
Epoch 54, Train Loss: 1.1002, Val Loss: 1.0966
batch size: (898, 898)
Epoch 55, accuracy: 0.4072
batch size: (913, 913)
Epoch 56, accuracy: 0.4024
Epoch 56, Train Loss: 1.0956, Val Loss: 1.0966
batch size: (898, 898)
Epoch 57, accuracy: 0.4019
batch size: (903, 903)
Epoch 58, accuracy: 0.4036
Epoch 58, Train Loss: 1.0981, Val Loss: 1.0966
batch size: (909, 909)
Epoch 59, accuracy: 0.3972
batch size: (887, 887)
Epoch 60, accuracy: 0.4027
Epoch 60, Train Loss: 1.0984, Val Loss: 1.0966
batch size: (885, 885)
Epoch 61, accuracy: 0.4021
batch size: (914, 914)
Epoch 62, accuracy: 0.4049
Epoch 62, Train Loss: 1.0973, Val Loss: 1.0966
batch size: (892, 892)
Epoch 63, accuracy: 0.4011
batch size: (886, 886)
Epoch 64, accuracy: 0.4030
Epoch 64, Train Loss: 1.0988, Val Loss: 1.0966
batch size: (889, 889)
Epoch 65, accuracy: 0.4051
batch size: (899, 899)
Epoch 66, accuracy: 0.3983
Epoch 66, Train Loss: 1.0991, Val Loss: 1.0966
batch size: (910, 910)
Epoch 67, accuracy: 0.4050
batch size: (887, 887)
Epoch 68, accuracy: 0.3983
Epoch 68, Train Loss: 1.0993, Val Loss: 1.0966
batch size: (904, 904)
Epoch 69, accuracy: 0.4001
batch size: (893, 893)
Epoch 70, accuracy: 0.4042
Epoch 70, Train Loss: 1.0995, Val Loss: 1.0966
batch size: (884, 884)
Epoch 71, accuracy: 0.3994
batch size: (891, 891)
Epoch 72, accuracy: 0.4010
Epoch 72, Train Loss: 1.1007, Val Loss: 1.0966
batch size: (918, 918)
Epoch 73, accuracy: 0.4052
batch size: (908, 908)
Epoch 74, accuracy: 0.4073
Epoch 74, Train Loss: 1.1000, Val Loss: 1.0966
batch size: (890, 890)
Epoch 75, accuracy: 0.4017
batch size: (918, 918)
Epoch 76, accuracy: 0.4040
Epoch 76, Train Loss: 1.0994, Val Loss: 1.0966
batch size: (887, 887)
Epoch 77, accuracy: 0.4002
batch size: (924, 924)
Epoch 78, accuracy: 0.4046
Epoch 78, Train Loss: 1.0978, Val Loss: 1.0966
batch size: (902, 902)
Epoch 79, accuracy: 0.4031
batch size: (893, 893)
Epoch 80, accuracy: 0.4039
Epoch 80, Train Loss: 1.0966, Val Loss: 1.0966
batch size: (905, 905)
Epoch 81, accuracy: 0.4023
batch size: (897, 897)
Epoch 82, accuracy: 0.4044
Epoch 82, Train Loss: 1.0994, Val Loss: 1.0966
batch size: (894, 894)
Epoch 83, accuracy: 0.4030
batch size: (883, 883)
Epoch 84, accuracy: 0.4036
Epoch 84, Train Loss: 1.1010, Val Loss: 1.0966
batch size: (922, 922)
Epoch 85, accuracy: 0.4015
batch size: (897, 897)
Epoch 86, accuracy: 0.3999
Epoch 86, Train Loss: 1.0996, Val Loss: 1.0966
batch size: (898, 898)
Epoch 87, accuracy: 0.4060
batch size: (899, 899)
Epoch 88, accuracy: 0.4024
Epoch 88, Train Loss: 1.0996, Val Loss: 1.0966
batch size: (888, 888)
Epoch 89, accuracy: 0.3993
batch size: (886, 886)
Epoch 90, accuracy: 0.4023
Epoch 90, Train Loss: 1.0984, Val Loss: 1.0966
batch size: (907, 907)
Epoch 91, accuracy: 0.4009
batch size: (881, 881)
Epoch 92, accuracy: 0.4039
Epoch 92, Train Loss: 1.0995, Val Loss: 1.0966
batch size: (893, 893)
Epoch 93, accuracy: 0.3986
batch size: (908, 908)
Epoch 94, accuracy: 0.4036
Epoch 94, Train Loss: 1.1002, Val Loss: 1.0966
batch size: (916, 916)
Epoch 95, accuracy: 0.3975
batch size: (905, 905)
Epoch 96, accuracy: 0.3993
Epoch 96, Train Loss: 1.0990, Val Loss: 1.0966
batch size: (900, 900)
Epoch 97, accuracy: 0.4048
batch size: (898, 898)
Epoch 98, accuracy: 0.4030
Epoch 98, Train Loss: 1.1004, Val Loss: 1.0966
batch size: (910, 910)
Epoch 99, accuracy: 0.4010
batch size: (897, 897)
Epoch 100, accuracy: 0.4030
Epoch 100, Train Loss: 1.0960, Val Loss: 1.0966
batch size: (893, 893)
Epoch 101, accuracy: 0.4018
batch size: (906, 906)
Epoch 102, accuracy: 0.3987
Epoch 102, Train Loss: 1.1006, Val Loss: 1.0966
batch size: (913, 913)
Epoch 103, accuracy: 0.3992
batch size: (881, 881)
Epoch 104, accuracy: 0.3995
Epoch 104, Train Loss: 1.0998, Val Loss: 1.0966
batch size: (913, 913)
Epoch 105, accuracy: 0.4016
batch size: (893, 893)
Epoch 106, accuracy: 0.4030
Epoch 106, Train Loss: 1.0997, Val Loss: 1.0966
batch size: (907, 907)
Epoch 107, accuracy: 0.4060
batch size: (895, 895)
Epoch 108, accuracy: 0.4038
Epoch 108, Train Loss: 1.0989, Val Loss: 1.0966
batch size: (906, 906)
Epoch 109, accuracy: 0.4050
batch size: (904, 904)
Epoch 110, accuracy: 0.4071
Epoch 110, Train Loss: 1.0984, Val Loss: 1.0966
batch size: (895, 895)
Epoch 111, accuracy: 0.4018
batch size: (902, 902)
Epoch 112, accuracy: 0.3977
Epoch 112, Train Loss: 1.0998, Val Loss: 1.0966
batch size: (894, 894)
Epoch 113, accuracy: 0.4059
batch size: (887, 887)
Epoch 114, accuracy: 0.4014
Epoch 114, Train Loss: 1.0997, Val Loss: 1.0966
batch size: (892, 892)
Epoch 115, accuracy: 0.4009
batch size: (892, 892)
Epoch 116, accuracy: 0.4014
Epoch 116, Train Loss: 1.1001, Val Loss: 1.0966
batch size: (921, 921)
Epoch 117, accuracy: 0.4023
batch size: (892, 892)
Epoch 118, accuracy: 0.4037
Epoch 118, Train Loss: 1.0976, Val Loss: 1.0966
batch size: (922, 922)
Epoch 119, accuracy: 0.3994
batch size: (893, 893)
Epoch 120, accuracy: 0.4023
Epoch 120, Train Loss: 1.0988, Val Loss: 1.0966
batch size: (876, 876)
Epoch 121, accuracy: 0.4090
batch size: (907, 907)
Epoch 122, accuracy: 0.4029
Epoch 122, Train Loss: 1.1002, Val Loss: 1.0966
batch size: (912, 912)
Epoch 123, accuracy: 0.4033
batch size: (895, 895)
Epoch 124, accuracy: 0.4013
Epoch 124, Train Loss: 1.0980, Val Loss: 1.0966
batch size: (900, 900)
Epoch 125, accuracy: 0.4036
batch size: (896, 896)
Epoch 126, accuracy: 0.4030
Epoch 126, Train Loss: 1.0994, Val Loss: 1.0966
batch size: (894, 894)
Epoch 127, accuracy: 0.4091
batch size: (895, 895)
Epoch 128, accuracy: 0.4039
Epoch 128, Train Loss: 1.1010, Val Loss: 1.0966
batch size: (904, 904)
Epoch 129, accuracy: 0.4014
batch size: (884, 884)
Epoch 130, accuracy: 0.4027
Epoch 130, Train Loss: 1.0992, Val Loss: 1.0966
batch size: (896, 896)
Epoch 131, accuracy: 0.4000
batch size: (883, 883)
Epoch 132, accuracy: 0.4035
Epoch 132, Train Loss: 1.0985, Val Loss: 1.0966
batch size: (920, 920)
Epoch 133, accuracy: 0.3997
batch size: (894, 894)
Epoch 134, accuracy: 0.4047
Epoch 134, Train Loss: 1.1011, Val Loss: 1.0966
batch size: (906, 906)
Epoch 135, accuracy: 0.4024
batch size: (912, 912)
Epoch 136, accuracy: 0.4030
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 136, Train Loss: 1.1003, Val Loss: 1.0966
batch size: (890, 890)
Epoch 137, accuracy: 0.3983
batch size: (905, 905)
Epoch 138, accuracy: 0.4068
Epoch 138, Train Loss: 1.0986, Val Loss: 1.0966
batch size: (897, 897)
Epoch 139, accuracy: 0.4023
batch size: (892, 892)
Epoch 140, accuracy: 0.3999
Epoch 140, Train Loss: 1.1000, Val Loss: 1.0966
batch size: (894, 894)
Epoch 141, accuracy: 0.4035
batch size: (916, 916)
Epoch 142, accuracy: 0.4031
Epoch 142, Train Loss: 1.0951, Val Loss: 1.0966
batch size: (916, 916)
Epoch 143, accuracy: 0.3977
batch size: (886, 886)
Epoch 144, accuracy: 0.4022
Epoch 144, Train Loss: 1.0983, Val Loss: 1.0966
batch size: (882, 882)
Epoch 145, accuracy: 0.4060
batch size: (906, 906)
Epoch 146, accuracy: 0.4051
Epoch 146, Train Loss: 1.1006, Val Loss: 1.0966
batch size: (904, 904)
Epoch 147, accuracy: 0.4045
batch size: (927, 927)
Epoch 148, accuracy: 0.4023
Epoch 148, Train Loss: 1.1008, Val Loss: 1.0966
batch size: (882, 882)
Epoch 149, accuracy: 0.4003
batch size: (890, 890)
Epoch 150, accuracy: 0.4030
Epoch 150, Train Loss: 1.0996, Val Loss: 1.0966
batch size: (904, 904)
Epoch 151, accuracy: 0.4032
batch size: (909, 909)
Epoch 152, accuracy: 0.3976
Epoch 152, Train Loss: 1.1025, Val Loss: 1.0966
batch size: (902, 902)
Epoch 153, accuracy: 0.4023
batch size: (912, 912)
Epoch 154, accuracy: 0.4042
Epoch 154, Train Loss: 1.1005, Val Loss: 1.0966
batch size: (908, 908)
Epoch 155, accuracy: 0.4032
batch size: (903, 903)
Epoch 156, accuracy: 0.3964
Epoch 156, Train Loss: 1.0988, Val Loss: 1.0966
batch size: (896, 896)
Epoch 157, accuracy: 0.4017
batch size: (881, 881)
Epoch 158, accuracy: 0.4052
Epoch 158, Train Loss: 1.0976, Val Loss: 1.0966
batch size: (927, 927)
Epoch 159, accuracy: 0.4055
batch size: (907, 907)
Epoch 160, accuracy: 0.4026
Epoch 160, Train Loss: 1.0994, Val Loss: 1.0966
batch size: (903, 903)
Epoch 161, accuracy: 0.4068
batch size: (896, 896)
Epoch 162, accuracy: 0.4049
Epoch 162, Train Loss: 1.0986, Val Loss: 1.0966
batch size: (885, 885)
Epoch 163, accuracy: 0.4028
batch size: (903, 903)
Epoch 164, accuracy: 0.4018
Epoch 164, Train Loss: 1.0993, Val Loss: 1.0966
batch size: (901, 901)
Epoch 165, accuracy: 0.4004
batch size: (894, 894)
Epoch 166, accuracy: 0.4052
Epoch 166, Train Loss: 1.0980, Val Loss: 1.0966
batch size: (901, 901)
Epoch 167, accuracy: 0.4036
batch size: (915, 915)
Epoch 168, accuracy: 0.4039
Epoch 168, Train Loss: 1.0981, Val Loss: 1.0966
batch size: (895, 895)
Epoch 169, accuracy: 0.4032
batch size: (895, 895)
Epoch 170, accuracy: 0.4021
Epoch 170, Train Loss: 1.0991, Val Loss: 1.0966
batch size: (903, 903)
Epoch 171, accuracy: 0.4025
batch size: (894, 894)
Epoch 172, accuracy: 0.4044
Epoch 172, Train Loss: 1.0971, Val Loss: 1.0966
batch size: (884, 884)
Epoch 173, accuracy: 0.4031
batch size: (912, 912)
Epoch 174, accuracy: 0.4042
Epoch 174, Train Loss: 1.0982, Val Loss: 1.0966
batch size: (902, 902)
Epoch 175, accuracy: 0.4072
batch size: (897, 897)
Epoch 176, accuracy: 0.4005
Epoch 176, Train Loss: 1.0997, Val Loss: 1.0966
batch size: (903, 903)
Epoch 177, accuracy: 0.4063
batch size: (894, 894)
Epoch 178, accuracy: 0.4046
Epoch 178, Train Loss: 1.0984, Val Loss: 1.0966
batch size: (882, 882)
Epoch 179, accuracy: 0.3994
batch size: (889, 889)
Epoch 180, accuracy: 0.4015
Epoch 180, Train Loss: 1.0961, Val Loss: 1.0966
batch size: (894, 894)
Epoch 181, accuracy: 0.4077
batch size: (900, 900)
Epoch 182, accuracy: 0.4019
Epoch 182, Train Loss: 1.0981, Val Loss: 1.0966
batch size: (892, 892)
Epoch 183, accuracy: 0.4013
batch size: (891, 891)
Epoch 184, accuracy: 0.4005
Epoch 184, Train Loss: 1.0969, Val Loss: 1.0966
batch size: (893, 893)
Epoch 185, accuracy: 0.3994
batch size: (903, 903)
Epoch 186, accuracy: 0.4029
Epoch 186, Train Loss: 1.0974, Val Loss: 1.0966
batch size: (915, 915)
Epoch 187, accuracy: 0.4054
batch size: (898, 898)
Epoch 188, accuracy: 0.4048
Epoch 188, Train Loss: 1.0972, Val Loss: 1.0966
batch size: (898, 898)
Epoch 189, accuracy: 0.4006
batch size: (895, 895)
Epoch 190, accuracy: 0.4013
Epoch 190, Train Loss: 1.1007, Val Loss: 1.0966
batch size: (897, 897)
Epoch 191, accuracy: 0.4046
batch size: (877, 877)
Epoch 192, accuracy: 0.4036
Epoch 192, Train Loss: 1.0991, Val Loss: 1.0966
batch size: (899, 899)
Epoch 193, accuracy: 0.3985
batch size: (917, 917)
Epoch 194, accuracy: 0.4044
Epoch 194, Train Loss: 1.0999, Val Loss: 1.0966
batch size: (933, 933)
Epoch 195, accuracy: 0.3972
batch size: (886, 886)
Epoch 196, accuracy: 0.4055
Epoch 196, Train Loss: 1.1011, Val Loss: 1.0966
batch size: (907, 907)
Epoch 197, accuracy: 0.4001
batch size: (901, 901)
Epoch 198, accuracy: 0.4033
Epoch 198, Train Loss: 1.1000, Val Loss: 1.0966
batch size: (898, 898)
Epoch 199, accuracy: 0.3977
Loaded best model with val_loss = 1.080345869064331
test :accuracy 0.4299, f1_macro: 0.2004, f1_micro: 0.4299, auc: 0.5031
Training JKNet with 2 layers...
可训练参数: 391942_JKNet
不可训练参数: 0
batch size: (908, 908)
✅ Epoch 0: New best model saved with val_loss = 1.0886
Epoch 0, accuracy: 0.4375
Epoch 0, Train Loss: 1.2153, Val Loss: 1.0886
batch size: (912, 912)
✅ Epoch 1: New best model saved with val_loss = 1.0830
Epoch 1, accuracy: 0.4351
batch size: (906, 906)
✅ Epoch 2: New best model saved with val_loss = 1.0741
Epoch 2, accuracy: 0.4046
Epoch 2, Train Loss: 0.0945, Val Loss: 1.0741
batch size: (898, 898)
✅ Epoch 3: New best model saved with val_loss = 1.0615
Epoch 3, accuracy: 0.4089
batch size: (909, 909)
✅ Epoch 4: New best model saved with val_loss = 1.0452
Epoch 4, accuracy: 0.4048
Epoch 4, Train Loss: 0.0224, Val Loss: 1.0452
batch size: (893, 893)
✅ Epoch 5: New best model saved with val_loss = 1.0278
Epoch 5, accuracy: 0.4085
batch size: (891, 891)
✅ Epoch 6: New best model saved with val_loss = 1.0118
Epoch 6, accuracy: 0.4074
Epoch 6, Train Loss: 0.0034, Val Loss: 1.0118
batch size: (907, 907)
✅ Epoch 7: New best model saved with val_loss = 0.9976
Epoch 7, accuracy: 0.4142
batch size: (912, 912)
✅ Epoch 8: New best model saved with val_loss = 0.9837
Epoch 8, accuracy: 0.4138
Epoch 8, Train Loss: 0.0009, Val Loss: 0.9837
batch size: (896, 896)
✅ Epoch 9: New best model saved with val_loss = 0.9728
Epoch 9, accuracy: 0.4118
batch size: (911, 911)
✅ Epoch 10: New best model saved with val_loss = 0.9650
Epoch 10, accuracy: 0.4124
Epoch 10, Train Loss: 0.0004, Val Loss: 0.9650
batch size: (910, 910)
✅ Epoch 11: New best model saved with val_loss = 0.9599
Epoch 11, accuracy: 0.4108
batch size: (911, 911)
✅ Epoch 12: New best model saved with val_loss = 0.9574
Epoch 12, accuracy: 0.4061
Epoch 12, Train Loss: 0.0025, Val Loss: 0.9574
batch size: (889, 889)
✅ Epoch 13: New best model saved with val_loss = 0.9535
Epoch 13, accuracy: 0.4068
batch size: (895, 895)
Epoch 14, accuracy: 0.4095
Epoch 14, Train Loss: 0.0003, Val Loss: 0.9545
batch size: (901, 901)
Epoch 15, accuracy: 0.4080
batch size: (895, 895)
Epoch 16, accuracy: 0.4075
Epoch 16, Train Loss: 0.0002, Val Loss: 0.9543
batch size: (913, 913)
Epoch 17, accuracy: 0.4006
batch size: (907, 907)
Epoch 18, accuracy: 0.4071
Epoch 18, Train Loss: 0.0000, Val Loss: 0.9567
batch size: (908, 908)
Epoch 19, accuracy: 0.4021
batch size: (921, 921)
Epoch 20, accuracy: 0.4065
Epoch 20, Train Loss: 0.0003, Val Loss: 0.9591
batch size: (903, 903)
Epoch 21, accuracy: 0.4036
batch size: (871, 871)
Epoch 22, accuracy: 0.4040
Epoch 22, Train Loss: 0.0000, Val Loss: 0.9631
batch size: (896, 896)
Epoch 23, accuracy: 0.4059
batch size: (903, 903)
Epoch 24, accuracy: 0.4073
Epoch 24, Train Loss: 0.0001, Val Loss: 0.9661
batch size: (908, 908)
Epoch 25, accuracy: 0.4015
batch size: (914, 914)
Epoch 26, accuracy: 0.4064
Epoch 26, Train Loss: 0.0000, Val Loss: 0.9687
batch size: (897, 897)
Epoch 27, accuracy: 0.4046
batch size: (898, 898)
Epoch 28, accuracy: 0.4060
Epoch 28, Train Loss: 0.0001, Val Loss: 0.9710
batch size: (889, 889)
Epoch 29, accuracy: 0.4095
batch size: (902, 902)
Epoch 30, accuracy: 0.4103
Epoch 30, Train Loss: 0.0000, Val Loss: 0.9693
batch size: (911, 911)
Epoch 31, accuracy: 0.4071
batch size: (925, 925)
Epoch 32, accuracy: 0.4079
Epoch 32, Train Loss: 0.0000, Val Loss: 0.9698
batch size: (896, 896)
Epoch 33, accuracy: 0.4027
batch size: (900, 900)
Epoch 34, accuracy: 0.4049
Epoch 34, Train Loss: 0.0000, Val Loss: 0.9703
batch size: (898, 898)
Epoch 35, accuracy: 0.4005
batch size: (895, 895)
Epoch 36, accuracy: 0.4053
Epoch 36, Train Loss: 0.0001, Val Loss: 0.9705
batch size: (906, 906)
Epoch 37, accuracy: 0.4042
batch size: (899, 899)
Epoch 38, accuracy: 0.4070
Epoch 38, Train Loss: 0.0000, Val Loss: 0.9695
batch size: (903, 903)
Epoch 39, accuracy: 0.4079
batch size: (902, 902)
Epoch 40, accuracy: 0.4072
Epoch 40, Train Loss: 0.0027, Val Loss: 0.9697
batch size: (896, 896)
Epoch 41, accuracy: 0.4056
batch size: (909, 909)
Epoch 42, accuracy: 0.4047
Epoch 42, Train Loss: 0.0001, Val Loss: 0.9717
batch size: (886, 886)
Epoch 43, accuracy: 0.4053
batch size: (897, 897)
Epoch 44, accuracy: 0.4063
Epoch 44, Train Loss: 0.0001, Val Loss: 0.9723
batch size: (911, 911)
Epoch 45, accuracy: 0.4041
batch size: (883, 883)
Epoch 46, accuracy: 0.4048
Epoch 46, Train Loss: 0.0001, Val Loss: 0.9709
batch size: (901, 901)
Epoch 47, accuracy: 0.4088
batch size: (891, 891)
Epoch 48, accuracy: 0.4035
Epoch 48, Train Loss: 0.0001, Val Loss: 0.9717
batch size: (896, 896)
Epoch 49, accuracy: 0.4016
batch size: (908, 908)
Epoch 50, accuracy: 0.4060
Epoch 50, Train Loss: 0.0000, Val Loss: 0.9717
batch size: (894, 894)
Epoch 51, accuracy: 0.4052
batch size: (901, 901)
Epoch 52, accuracy: 0.4074
Epoch 52, Train Loss: 0.0000, Val Loss: 0.9707
batch size: (899, 899)
Epoch 53, accuracy: 0.4010
batch size: (890, 890)
Epoch 54, accuracy: 0.4074
Epoch 54, Train Loss: 0.0000, Val Loss: 0.9714
batch size: (904, 904)
Epoch 55, accuracy: 0.4063
batch size: (894, 894)
Epoch 56, accuracy: 0.4102
Epoch 56, Train Loss: 0.0000, Val Loss: 0.9702
batch size: (886, 886)
Epoch 57, accuracy: 0.4054
batch size: (897, 897)
Epoch 58, accuracy: 0.4042
Epoch 58, Train Loss: 0.0003, Val Loss: 0.9710
batch size: (885, 885)
Epoch 59, accuracy: 0.4010
batch size: (893, 893)
Epoch 60, accuracy: 0.4069
Epoch 60, Train Loss: 0.0008, Val Loss: 0.9707
batch size: (912, 912)
Epoch 61, accuracy: 0.4065
batch size: (899, 899)
Epoch 62, accuracy: 0.4082
Epoch 62, Train Loss: 0.0001, Val Loss: 0.9727
batch size: (898, 898)
Epoch 63, accuracy: 0.4061
batch size: (888, 888)
Epoch 64, accuracy: 0.4050
Epoch 64, Train Loss: 0.0012, Val Loss: 0.9708
batch size: (907, 907)
Epoch 65, accuracy: 0.4094
batch size: (905, 905)
Epoch 66, accuracy: 0.4054
Epoch 66, Train Loss: 0.0001, Val Loss: 0.9714
batch size: (894, 894)
Epoch 67, accuracy: 0.4046
batch size: (906, 906)
Epoch 68, accuracy: 0.4068
Epoch 68, Train Loss: 0.0009, Val Loss: 0.9728
batch size: (901, 901)
Epoch 69, accuracy: 0.4039
batch size: (907, 907)
Epoch 70, accuracy: 0.4039
Epoch 70, Train Loss: 0.0001, Val Loss: 0.9713
batch size: (917, 917)
Epoch 71, accuracy: 0.4033
batch size: (907, 907)
Epoch 72, accuracy: 0.4078
Epoch 72, Train Loss: 0.0006, Val Loss: 0.9729
batch size: (910, 910)
Epoch 73, accuracy: 0.4052
batch size: (889, 889)
Epoch 74, accuracy: 0.4030
Epoch 74, Train Loss: 0.0001, Val Loss: 0.9715
batch size: (894, 894)
Epoch 75, accuracy: 0.3980
batch size: (902, 902)
Epoch 76, accuracy: 0.4015
Epoch 76, Train Loss: 0.0001, Val Loss: 0.9744
batch size: (897, 897)
Epoch 77, accuracy: 0.4076
batch size: (913, 913)
Epoch 78, accuracy: 0.4083
Epoch 78, Train Loss: 0.0005, Val Loss: 0.9702
batch size: (888, 888)
Epoch 79, accuracy: 0.4090
batch size: (889, 889)
Epoch 80, accuracy: 0.4054
Epoch 80, Train Loss: 0.0001, Val Loss: 0.9696
batch size: (892, 892)
Epoch 81, accuracy: 0.4052
batch size: (900, 900)
Epoch 82, accuracy: 0.4032
Epoch 82, Train Loss: 0.0002, Val Loss: 0.9701
batch size: (909, 909)
Epoch 83, accuracy: 0.4063
batch size: (899, 899)
Epoch 84, accuracy: 0.4061
Epoch 84, Train Loss: 0.0001, Val Loss: 0.9697
batch size: (919, 919)
Epoch 85, accuracy: 0.4027
batch size: (894, 894)
Epoch 86, accuracy: 0.4059
Epoch 86, Train Loss: 0.0000, Val Loss: 0.9710
batch size: (874, 874)
Epoch 87, accuracy: 0.4113
batch size: (907, 907)
Epoch 88, accuracy: 0.3999
Epoch 88, Train Loss: 0.0000, Val Loss: 0.9716
batch size: (911, 911)
Epoch 89, accuracy: 0.4093
batch size: (902, 902)
Epoch 90, accuracy: 0.4064
Epoch 90, Train Loss: 0.0001, Val Loss: 0.9713
batch size: (882, 882)
Epoch 91, accuracy: 0.4056
batch size: (898, 898)
Epoch 92, accuracy: 0.4045
Epoch 92, Train Loss: 0.0002, Val Loss: 0.9711
batch size: (896, 896)
Epoch 93, accuracy: 0.4122
batch size: (896, 896)
Epoch 94, accuracy: 0.4046
Epoch 94, Train Loss: 0.0002, Val Loss: 0.9706
batch size: (893, 893)
Epoch 95, accuracy: 0.4055
batch size: (904, 904)
Epoch 96, accuracy: 0.4060
Epoch 96, Train Loss: 0.0000, Val Loss: 0.9701
batch size: (906, 906)
Epoch 97, accuracy: 0.4037
batch size: (892, 892)
Epoch 98, accuracy: 0.4074
Epoch 98, Train Loss: 0.0003, Val Loss: 0.9721
batch size: (892, 892)
Epoch 99, accuracy: 0.4050
batch size: (901, 901)
Epoch 100, accuracy: 0.4068
Epoch 100, Train Loss: 0.0000, Val Loss: 0.9743
batch size: (877, 877)
Epoch 101, accuracy: 0.4059
batch size: (922, 922)
Epoch 102, accuracy: 0.4077
Epoch 102, Train Loss: 0.0001, Val Loss: 0.9744
batch size: (906, 906)
Epoch 103, accuracy: 0.4036
batch size: (916, 916)
Epoch 104, accuracy: 0.4012
Epoch 104, Train Loss: 0.0000, Val Loss: 0.9734
batch size: (886, 886)
Epoch 105, accuracy: 0.4057
batch size: (883, 883)
Epoch 106, accuracy: 0.4113
Epoch 106, Train Loss: 0.0001, Val Loss: 0.9744
batch size: (882, 882)
Epoch 107, accuracy: 0.4050
batch size: (884, 884)
Epoch 108, accuracy: 0.4068
Epoch 108, Train Loss: 0.0000, Val Loss: 0.9732
batch size: (891, 891)
Epoch 109, accuracy: 0.4032
batch size: (907, 907)
Epoch 110, accuracy: 0.4070
Epoch 110, Train Loss: 0.0001, Val Loss: 0.9714
batch size: (910, 910)
Epoch 111, accuracy: 0.4069
batch size: (887, 887)
Epoch 112, accuracy: 0.4052
Epoch 112, Train Loss: 0.0002, Val Loss: 0.9712
batch size: (908, 908)
Epoch 113, accuracy: 0.4044
batch size: (909, 909)
Epoch 114, accuracy: 0.4110
Epoch 114, Train Loss: 0.0016, Val Loss: 0.9721
batch size: (889, 889)
Epoch 115, accuracy: 0.4067
batch size: (904, 904)
Epoch 116, accuracy: 0.4044
Epoch 116, Train Loss: 0.0001, Val Loss: 0.9712
batch size: (898, 898)
Epoch 117, accuracy: 0.4057
batch size: (918, 918)
Epoch 118, accuracy: 0.4053
Epoch 118, Train Loss: 0.0000, Val Loss: 0.9722
batch size: (892, 892)
Epoch 119, accuracy: 0.4095
batch size: (910, 910)
Epoch 120, accuracy: 0.4042
Epoch 120, Train Loss: 0.0001, Val Loss: 0.9714
batch size: (898, 898)
Epoch 121, accuracy: 0.4089
batch size: (907, 907)
Epoch 122, accuracy: 0.4063
Epoch 122, Train Loss: 0.0002, Val Loss: 0.9712
batch size: (901, 901)
Epoch 123, accuracy: 0.4037
batch size: (909, 909)
Epoch 124, accuracy: 0.4048
Epoch 124, Train Loss: 0.0000, Val Loss: 0.9697
batch size: (904, 904)
Epoch 125, accuracy: 0.4039
batch size: (913, 913)
Epoch 126, accuracy: 0.4032
Epoch 126, Train Loss: 0.0006, Val Loss: 0.9682
batch size: (910, 910)
Epoch 127, accuracy: 0.4090
batch size: (895, 895)
Epoch 128, accuracy: 0.4055
Epoch 128, Train Loss: 0.0003, Val Loss: 0.9696
batch size: (890, 890)
Epoch 129, accuracy: 0.4068
batch size: (901, 901)
Epoch 130, accuracy: 0.4082
Epoch 130, Train Loss: 0.0002, Val Loss: 0.9709
batch size: (895, 895)
Epoch 131, accuracy: 0.4064
batch size: (888, 888)
Epoch 132, accuracy: 0.4058
Epoch 132, Train Loss: 0.0000, Val Loss: 0.9708
batch size: (891, 891)
Epoch 133, accuracy: 0.4089
batch size: (884, 884)
Epoch 134, accuracy: 0.4034
Epoch 134, Train Loss: 0.0014, Val Loss: 0.9712
batch size: (905, 905)
Epoch 135, accuracy: 0.4019
batch size: (906, 906)
Epoch 136, accuracy: 0.4031
Epoch 136, Train Loss: 0.0000, Val Loss: 0.9715
batch size: (908, 908)
Epoch 137, accuracy: 0.4046
batch size: (889, 889)
Epoch 138, accuracy: 0.4035
Epoch 138, Train Loss: 0.0001, Val Loss: 0.9722
batch size: (916, 916)
Epoch 139, accuracy: 0.4085
batch size: (881, 881)
Epoch 140, accuracy: 0.4038
Epoch 140, Train Loss: 0.0001, Val Loss: 0.9718
batch size: (881, 881)
Epoch 141, accuracy: 0.4044
batch size: (902, 902)
Epoch 142, accuracy: 0.4026
Epoch 142, Train Loss: 0.0001, Val Loss: 0.9699
batch size: (903, 903)
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 143, accuracy: 0.4056
batch size: (898, 898)
Epoch 144, accuracy: 0.4075
Epoch 144, Train Loss: 0.0002, Val Loss: 0.9708
batch size: (902, 902)
Epoch 145, accuracy: 0.4035
batch size: (895, 895)
Epoch 146, accuracy: 0.4058
Epoch 146, Train Loss: 0.0000, Val Loss: 0.9698
batch size: (911, 911)
Epoch 147, accuracy: 0.4035
batch size: (883, 883)
Epoch 148, accuracy: 0.4028
Epoch 148, Train Loss: 0.0003, Val Loss: 0.9702
batch size: (879, 879)
Epoch 149, accuracy: 0.4095
batch size: (898, 898)
Epoch 150, accuracy: 0.4090
Epoch 150, Train Loss: 0.0001, Val Loss: 0.9706
batch size: (910, 910)
Epoch 151, accuracy: 0.4054
batch size: (895, 895)
Epoch 152, accuracy: 0.4025
Epoch 152, Train Loss: 0.0001, Val Loss: 0.9702
batch size: (910, 910)
Epoch 153, accuracy: 0.4034
batch size: (910, 910)
Epoch 154, accuracy: 0.4073
Epoch 154, Train Loss: 0.0002, Val Loss: 0.9703
batch size: (896, 896)
Epoch 155, accuracy: 0.4056
batch size: (909, 909)
Epoch 156, accuracy: 0.4061
Epoch 156, Train Loss: 0.0000, Val Loss: 0.9699
batch size: (885, 885)
Epoch 157, accuracy: 0.4046
batch size: (886, 886)
Epoch 158, accuracy: 0.4028
Epoch 158, Train Loss: 0.0001, Val Loss: 0.9699
batch size: (880, 880)
Epoch 159, accuracy: 0.4066
batch size: (887, 887)
Epoch 160, accuracy: 0.4073
Epoch 160, Train Loss: 0.0000, Val Loss: 0.9696
batch size: (890, 890)
Epoch 161, accuracy: 0.4064
batch size: (895, 895)
Epoch 162, accuracy: 0.4093
Epoch 162, Train Loss: 0.0003, Val Loss: 0.9703
batch size: (895, 895)
Epoch 163, accuracy: 0.4077
batch size: (914, 914)
Epoch 164, accuracy: 0.4036
Epoch 164, Train Loss: 0.0000, Val Loss: 0.9704
batch size: (901, 901)
Epoch 165, accuracy: 0.4096
batch size: (897, 897)
Epoch 166, accuracy: 0.4049
Epoch 166, Train Loss: 0.0001, Val Loss: 0.9705
batch size: (913, 913)
Epoch 167, accuracy: 0.4041
batch size: (901, 901)
Epoch 168, accuracy: 0.4097
Epoch 168, Train Loss: 0.0000, Val Loss: 0.9719
batch size: (918, 918)
Epoch 169, accuracy: 0.4029
batch size: (914, 914)
Epoch 170, accuracy: 0.4043
Epoch 170, Train Loss: 0.0003, Val Loss: 0.9717
batch size: (904, 904)
Epoch 171, accuracy: 0.4083
batch size: (895, 895)
Epoch 172, accuracy: 0.4061
Epoch 172, Train Loss: 0.0001, Val Loss: 0.9701
batch size: (895, 895)
Epoch 173, accuracy: 0.4090
batch size: (885, 885)
Epoch 174, accuracy: 0.4075
Epoch 174, Train Loss: 0.0001, Val Loss: 0.9713
batch size: (894, 894)
Epoch 175, accuracy: 0.4062
batch size: (890, 890)
Epoch 176, accuracy: 0.4111
Epoch 176, Train Loss: 0.0002, Val Loss: 0.9717
batch size: (915, 915)
Epoch 177, accuracy: 0.4031
batch size: (889, 889)
Epoch 178, accuracy: 0.4055
Epoch 178, Train Loss: 0.0001, Val Loss: 0.9711
batch size: (890, 890)
Epoch 179, accuracy: 0.4078
batch size: (911, 911)
Epoch 180, accuracy: 0.4043
Epoch 180, Train Loss: 0.0001, Val Loss: 0.9728
batch size: (893, 893)
Epoch 181, accuracy: 0.4067
batch size: (908, 908)
Epoch 182, accuracy: 0.4071
Epoch 182, Train Loss: 0.0002, Val Loss: 0.9722
batch size: (897, 897)
Epoch 183, accuracy: 0.4025
batch size: (890, 890)
Epoch 184, accuracy: 0.4069
Epoch 184, Train Loss: 0.0014, Val Loss: 0.9699
batch size: (905, 905)
Epoch 185, accuracy: 0.4046
batch size: (905, 905)
Epoch 186, accuracy: 0.4048
Epoch 186, Train Loss: 0.0001, Val Loss: 0.9694
batch size: (902, 902)
Epoch 187, accuracy: 0.4080
batch size: (895, 895)
Epoch 188, accuracy: 0.4048
Epoch 188, Train Loss: 0.0001, Val Loss: 0.9695
batch size: (900, 900)
Epoch 189, accuracy: 0.4066
batch size: (893, 893)
Epoch 190, accuracy: 0.4052
Epoch 190, Train Loss: 0.0049, Val Loss: 0.9703
batch size: (919, 919)
Epoch 191, accuracy: 0.4076
batch size: (903, 903)
Epoch 192, accuracy: 0.4054
Epoch 192, Train Loss: 0.0001, Val Loss: 0.9694
batch size: (896, 896)
Epoch 193, accuracy: 0.4040
batch size: (910, 910)
Epoch 194, accuracy: 0.4026
Epoch 194, Train Loss: 0.0001, Val Loss: 0.9702
batch size: (903, 903)
Epoch 195, accuracy: 0.4036
batch size: (915, 915)
Epoch 196, accuracy: 0.4012
Epoch 196, Train Loss: 0.0001, Val Loss: 0.9703
batch size: (894, 894)
Epoch 197, accuracy: 0.4050
batch size: (903, 903)
Epoch 198, accuracy: 0.4037
Epoch 198, Train Loss: 0.0003, Val Loss: 0.9687
batch size: (904, 904)
Epoch 199, accuracy: 0.4045
Loaded best model with val_loss = 0.9535018801689148
test :accuracy 0.4055, f1_macro: 0.2049, f1_micro: 0.4055, auc: 0.8344
Training JKNet with 8 layers...
可训练参数: 1184518_JKNet
不可训练参数: 0
batch size: (889, 889)
✅ Epoch 0: New best model saved with val_loss = 1.0760
Epoch 0, accuracy: 0.4029
Epoch 0, Train Loss: 1.2762, Val Loss: 1.0760
batch size: (907, 907)
Epoch 1, accuracy: 0.4310
batch size: (895, 895)
Epoch 2, accuracy: 0.1678
Epoch 2, Train Loss: 3.0387, Val Loss: 1.8537
batch size: (912, 912)
Epoch 3, accuracy: 0.1680
batch size: (904, 904)
Epoch 4, accuracy: 0.3727
Epoch 4, Train Loss: 1.0319, Val Loss: 1.4348
batch size: (892, 892)
✅ Epoch 5: New best model saved with val_loss = 1.0683
Epoch 5, accuracy: 0.4319
batch size: (896, 896)
✅ Epoch 6: New best model saved with val_loss = 1.0253
Epoch 6, accuracy: 0.4302
Epoch 6, Train Loss: 0.3284, Val Loss: 1.0253
batch size: (911, 911)
Epoch 7, accuracy: 0.4460
batch size: (898, 898)
Epoch 8, accuracy: 0.4366
Epoch 8, Train Loss: 0.3216, Val Loss: 1.0313
batch size: (916, 916)
✅ Epoch 9: New best model saved with val_loss = 1.0232
Epoch 9, accuracy: 0.4569
batch size: (910, 910)
✅ Epoch 10: New best model saved with val_loss = 1.0200
Epoch 10, accuracy: 0.4408
Epoch 10, Train Loss: 0.0444, Val Loss: 1.0200
batch size: (909, 909)
Epoch 11, accuracy: 0.4466
batch size: (903, 903)
✅ Epoch 12: New best model saved with val_loss = 1.0170
Epoch 12, accuracy: 0.4337
Epoch 12, Train Loss: 0.0246, Val Loss: 1.0170
batch size: (901, 901)
Epoch 13, accuracy: 0.4281
batch size: (892, 892)
Epoch 14, accuracy: 0.4275
Epoch 14, Train Loss: 0.0207, Val Loss: 1.0194
batch size: (896, 896)
Epoch 15, accuracy: 0.4280
batch size: (905, 905)
Epoch 16, accuracy: 0.4273
Epoch 16, Train Loss: 0.0216, Val Loss: 1.0276
batch size: (912, 912)
Epoch 17, accuracy: 0.4279
batch size: (915, 915)
Epoch 18, accuracy: 0.4318
Epoch 18, Train Loss: 0.0136, Val Loss: 1.0234
batch size: (911, 911)
Epoch 19, accuracy: 0.4271
batch size: (915, 915)
Epoch 20, accuracy: 0.4300
Epoch 20, Train Loss: 0.0069, Val Loss: 1.0329
batch size: (898, 898)
Epoch 21, accuracy: 0.4307
batch size: (905, 905)
Epoch 22, accuracy: 0.4287
Epoch 22, Train Loss: 0.0079, Val Loss: 1.0321
batch size: (907, 907)
Epoch 23, accuracy: 0.4322
batch size: (892, 892)
Epoch 24, accuracy: 0.4259
Epoch 24, Train Loss: 0.0254, Val Loss: 1.0307
batch size: (910, 910)
Epoch 25, accuracy: 0.4294
batch size: (888, 888)
Epoch 26, accuracy: 0.4288
Epoch 26, Train Loss: 0.0164, Val Loss: 1.0365
batch size: (908, 908)
Epoch 27, accuracy: 0.4313
batch size: (888, 888)
Epoch 28, accuracy: 0.4309
Epoch 28, Train Loss: 0.0138, Val Loss: 1.0392
batch size: (900, 900)
Epoch 29, accuracy: 0.4299
batch size: (901, 901)
Epoch 30, accuracy: 0.4316
Epoch 30, Train Loss: 0.0154, Val Loss: 1.0345
batch size: (904, 904)
Epoch 31, accuracy: 0.4315
batch size: (910, 910)
Epoch 32, accuracy: 0.4287
Epoch 32, Train Loss: 0.0040, Val Loss: 1.0327
batch size: (876, 876)
Epoch 33, accuracy: 0.4321
batch size: (913, 913)
Epoch 34, accuracy: 0.4303
Epoch 34, Train Loss: 0.0026, Val Loss: 1.0321
batch size: (915, 915)
Epoch 35, accuracy: 0.4291
batch size: (889, 889)
Epoch 36, accuracy: 0.4290
Epoch 36, Train Loss: 0.0066, Val Loss: 1.0313
batch size: (889, 889)
Epoch 37, accuracy: 0.4335
batch size: (904, 904)
Epoch 38, accuracy: 0.4291
Epoch 38, Train Loss: 0.0054, Val Loss: 1.0367
batch size: (900, 900)
Epoch 39, accuracy: 0.4283
batch size: (887, 887)
Epoch 40, accuracy: 0.4285
Epoch 40, Train Loss: 0.0058, Val Loss: 1.0303
batch size: (902, 902)
Epoch 41, accuracy: 0.4335
batch size: (909, 909)
Epoch 42, accuracy: 0.4255
Epoch 42, Train Loss: 0.0092, Val Loss: 1.0345
batch size: (886, 886)
Epoch 43, accuracy: 0.4307
batch size: (902, 902)
Epoch 44, accuracy: 0.4351
Epoch 44, Train Loss: 0.0118, Val Loss: 1.0325
batch size: (890, 890)
Epoch 45, accuracy: 0.4325
batch size: (906, 906)
Epoch 46, accuracy: 0.4288
Epoch 46, Train Loss: 0.0115, Val Loss: 1.0338
batch size: (899, 899)
Epoch 47, accuracy: 0.4314
batch size: (883, 883)
Epoch 48, accuracy: 0.4297
Epoch 48, Train Loss: 0.0062, Val Loss: 1.0371
batch size: (915, 915)
Epoch 49, accuracy: 0.4238
batch size: (919, 919)
Epoch 50, accuracy: 0.4331
Epoch 50, Train Loss: 0.0060, Val Loss: 1.0337
batch size: (897, 897)
Epoch 51, accuracy: 0.4280
batch size: (905, 905)
Epoch 52, accuracy: 0.4303
Epoch 52, Train Loss: 0.0043, Val Loss: 1.0347
batch size: (885, 885)
Epoch 53, accuracy: 0.4291
batch size: (912, 912)
Epoch 54, accuracy: 0.4289
Epoch 54, Train Loss: 0.0086, Val Loss: 1.0345
batch size: (898, 898)
Epoch 55, accuracy: 0.4298
batch size: (907, 907)
Epoch 56, accuracy: 0.4285
Epoch 56, Train Loss: 0.0098, Val Loss: 1.0354
batch size: (901, 901)
Epoch 57, accuracy: 0.4312
batch size: (913, 913)
Epoch 58, accuracy: 0.4348
Epoch 58, Train Loss: 0.0199, Val Loss: 1.0363
batch size: (897, 897)
Epoch 59, accuracy: 0.4263
batch size: (895, 895)
Epoch 60, accuracy: 0.4312
Epoch 60, Train Loss: 0.0049, Val Loss: 1.0343
batch size: (891, 891)
Epoch 61, accuracy: 0.4268
batch size: (896, 896)
Epoch 62, accuracy: 0.4327
Epoch 62, Train Loss: 0.0131, Val Loss: 1.0383
batch size: (896, 896)
Epoch 63, accuracy: 0.4302
batch size: (914, 914)
Epoch 64, accuracy: 0.4313
Epoch 64, Train Loss: 0.0102, Val Loss: 1.0375
batch size: (883, 883)
Epoch 65, accuracy: 0.4331
batch size: (904, 904)
Epoch 66, accuracy: 0.4303
Epoch 66, Train Loss: 0.0075, Val Loss: 1.0300
batch size: (898, 898)
Epoch 67, accuracy: 0.4314
batch size: (911, 911)
Epoch 68, accuracy: 0.4299
Epoch 68, Train Loss: 0.0088, Val Loss: 1.0396
batch size: (880, 880)
Epoch 69, accuracy: 0.4326
batch size: (903, 903)
Epoch 70, accuracy: 0.4314
Epoch 70, Train Loss: 0.0070, Val Loss: 1.0348
batch size: (896, 896)
Epoch 71, accuracy: 0.4311
batch size: (912, 912)
Epoch 72, accuracy: 0.4297
Epoch 72, Train Loss: 0.0123, Val Loss: 1.0382
batch size: (904, 904)
Epoch 73, accuracy: 0.4318
batch size: (910, 910)
Epoch 74, accuracy: 0.4298
Epoch 74, Train Loss: 0.0057, Val Loss: 1.0342
batch size: (903, 903)
Epoch 75, accuracy: 0.4324
batch size: (893, 893)
Epoch 76, accuracy: 0.4329
Epoch 76, Train Loss: 0.0146, Val Loss: 1.0355
batch size: (900, 900)
Epoch 77, accuracy: 0.4302
batch size: (889, 889)
Epoch 78, accuracy: 0.4294
Epoch 78, Train Loss: 0.0089, Val Loss: 1.0416
batch size: (903, 903)
Epoch 79, accuracy: 0.4329
batch size: (897, 897)
Epoch 80, accuracy: 0.4268
Epoch 80, Train Loss: 0.0092, Val Loss: 1.0363
batch size: (892, 892)
Epoch 81, accuracy: 0.4315
batch size: (906, 906)
Epoch 82, accuracy: 0.4315
Epoch 82, Train Loss: 0.0062, Val Loss: 1.0371
batch size: (917, 917)
Epoch 83, accuracy: 0.4299
batch size: (888, 888)
Epoch 84, accuracy: 0.4312
Epoch 84, Train Loss: 0.0038, Val Loss: 1.0376
batch size: (893, 893)
Epoch 85, accuracy: 0.4334
batch size: (907, 907)
Epoch 86, accuracy: 0.4286
Epoch 86, Train Loss: 0.0040, Val Loss: 1.0376
batch size: (892, 892)
Epoch 87, accuracy: 0.4307
batch size: (898, 898)
Epoch 88, accuracy: 0.4267
Epoch 88, Train Loss: 0.0111, Val Loss: 1.0351
batch size: (903, 903)
Epoch 89, accuracy: 0.4305
batch size: (898, 898)
Epoch 90, accuracy: 0.4325
Epoch 90, Train Loss: 0.0149, Val Loss: 1.0351
batch size: (898, 898)
Epoch 91, accuracy: 0.4301
batch size: (915, 915)
Epoch 92, accuracy: 0.4269
Epoch 92, Train Loss: 0.0067, Val Loss: 1.0306
batch size: (892, 892)
Epoch 93, accuracy: 0.4316
batch size: (877, 877)
Epoch 94, accuracy: 0.4319
Epoch 94, Train Loss: 0.0143, Val Loss: 1.0319
batch size: (887, 887)
Epoch 95, accuracy: 0.4272
batch size: (892, 892)
Epoch 96, accuracy: 0.4310
Epoch 96, Train Loss: 0.0095, Val Loss: 1.0359
batch size: (888, 888)
Epoch 97, accuracy: 0.4278
batch size: (916, 916)
Epoch 98, accuracy: 0.4323
Epoch 98, Train Loss: 0.0052, Val Loss: 1.0364
batch size: (887, 887)
Epoch 99, accuracy: 0.4308
batch size: (912, 912)
Epoch 100, accuracy: 0.4282
Epoch 100, Train Loss: 0.0091, Val Loss: 1.0382
batch size: (878, 878)
Epoch 101, accuracy: 0.4297
batch size: (896, 896)
Epoch 102, accuracy: 0.4309
Epoch 102, Train Loss: 0.0068, Val Loss: 1.0330
batch size: (880, 880)
Epoch 103, accuracy: 0.4292
batch size: (909, 909)
Epoch 104, accuracy: 0.4318
Epoch 104, Train Loss: 0.0051, Val Loss: 1.0298
batch size: (903, 903)
Epoch 105, accuracy: 0.4290
batch size: (894, 894)
Epoch 106, accuracy: 0.4296
Epoch 106, Train Loss: 0.0224, Val Loss: 1.0304
batch size: (913, 913)
Epoch 107, accuracy: 0.4300
batch size: (897, 897)
Epoch 108, accuracy: 0.4301
Epoch 108, Train Loss: 0.0050, Val Loss: 1.0336
batch size: (893, 893)
Epoch 109, accuracy: 0.4338
batch size: (878, 878)
Epoch 110, accuracy: 0.4348
Epoch 110, Train Loss: 0.0078, Val Loss: 1.0368
batch size: (913, 913)
Epoch 111, accuracy: 0.4275
batch size: (900, 900)
Epoch 112, accuracy: 0.4299
Epoch 112, Train Loss: 0.0184, Val Loss: 1.0309
batch size: (912, 912)
Epoch 113, accuracy: 0.4347
batch size: (898, 898)
Epoch 114, accuracy: 0.4251
Epoch 114, Train Loss: 0.0067, Val Loss: 1.0324
batch size: (918, 918)
Epoch 115, accuracy: 0.4286
batch size: (896, 896)
Epoch 116, accuracy: 0.4276
Epoch 116, Train Loss: 0.0055, Val Loss: 1.0303
batch size: (909, 909)
Epoch 117, accuracy: 0.4290
batch size: (908, 908)
Epoch 118, accuracy: 0.4337
Epoch 118, Train Loss: 0.0042, Val Loss: 1.0428
batch size: (898, 898)
Epoch 119, accuracy: 0.4280
batch size: (906, 906)
Epoch 120, accuracy: 0.4291
Epoch 120, Train Loss: 0.0127, Val Loss: 1.0386
batch size: (910, 910)
Epoch 121, accuracy: 0.4304
batch size: (879, 879)
Epoch 122, accuracy: 0.4309
Epoch 122, Train Loss: 0.0110, Val Loss: 1.0396
batch size: (906, 906)
Epoch 123, accuracy: 0.4315
batch size: (921, 921)
Epoch 124, accuracy: 0.4289
Epoch 124, Train Loss: 0.0079, Val Loss: 1.0413
batch size: (902, 902)
Epoch 125, accuracy: 0.4340
batch size: (899, 899)
Epoch 126, accuracy: 0.4288
Epoch 126, Train Loss: 0.0078, Val Loss: 1.0308
batch size: (891, 891)
Epoch 127, accuracy: 0.4305
batch size: (919, 919)
Epoch 128, accuracy: 0.4316
Epoch 128, Train Loss: 0.0078, Val Loss: 1.0394
batch size: (910, 910)
Epoch 129, accuracy: 0.4283
batch size: (909, 909)
Epoch 130, accuracy: 0.4302
Epoch 130, Train Loss: 0.0036, Val Loss: 1.0325
batch size: (887, 887)
Epoch 131, accuracy: 0.4312
batch size: (904, 904)
Epoch 132, accuracy: 0.4329
Epoch 132, Train Loss: 0.0050, Val Loss: 1.0388
batch size: (922, 922)
Epoch 133, accuracy: 0.4294
batch size: (902, 902)
Epoch 134, accuracy: 0.4291
Epoch 134, Train Loss: 0.0057, Val Loss: 1.0386
batch size: (897, 897)
Epoch 135, accuracy: 0.4308
batch size: (902, 902)
Epoch 136, accuracy: 0.4297
Epoch 136, Train Loss: 0.0054, Val Loss: 1.0399
batch size: (898, 898)
Epoch 137, accuracy: 0.4312
batch size: (909, 909)
Epoch 138, accuracy: 0.4286
Epoch 138, Train Loss: 0.0053, Val Loss: 1.0424
batch size: (896, 896)
Epoch 139, accuracy: 0.4273
batch size: (917, 917)
Epoch 140, accuracy: 0.4297
Epoch 140, Train Loss: 0.0117, Val Loss: 1.0372
batch size: (901, 901)
Epoch 141, accuracy: 0.4290
batch size: (899, 899)
Epoch 142, accuracy: 0.4325
Epoch 142, Train Loss: 0.0110, Val Loss: 1.0377
batch size: (911, 911)
Epoch 143, accuracy: 0.4303
batch size: (903, 903)
Epoch 144, accuracy: 0.4290
Epoch 144, Train Loss: 0.0127, Val Loss: 1.0374
batch size: (893, 893)
Epoch 145, accuracy: 0.4318
batch size: (895, 895)
Epoch 146, accuracy: 0.4288
Epoch 146, Train Loss: 0.0035, Val Loss: 1.0356
batch size: (906, 906)
Epoch 147, accuracy: 0.4340
batch size: (917, 917)
Epoch 148, accuracy: 0.4280
Epoch 148, Train Loss: 0.0447, Val Loss: 1.0317
batch size: (900, 900)
Epoch 149, accuracy: 0.4274
batch size: (897, 897)
Epoch 150, accuracy: 0.4313
Epoch 150, Train Loss: 0.0041, Val Loss: 1.0359
batch size: (904, 904)
Epoch 151, accuracy: 0.4320
batch size: (876, 876)
Epoch 152, accuracy: 0.4293
Epoch 152, Train Loss: 0.0037, Val Loss: 1.0419
batch size: (896, 896)
Epoch 153, accuracy: 0.4285
batch size: (887, 887)
Epoch 154, accuracy: 0.4288
Epoch 154, Train Loss: 0.0043, Val Loss: 1.0311
batch size: (898, 898)
Epoch 155, accuracy: 0.4290
batch size: (885, 885)
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 156, accuracy: 0.4279
Epoch 156, Train Loss: 0.0097, Val Loss: 1.0355
batch size: (895, 895)
Epoch 157, accuracy: 0.4277
batch size: (893, 893)
Epoch 158, accuracy: 0.4310
Epoch 158, Train Loss: 0.0049, Val Loss: 1.0321
batch size: (892, 892)
Epoch 159, accuracy: 0.4322
batch size: (890, 890)
Epoch 160, accuracy: 0.4328
Epoch 160, Train Loss: 0.0083, Val Loss: 1.0280
batch size: (903, 903)
Epoch 161, accuracy: 0.4294
batch size: (908, 908)
Epoch 162, accuracy: 0.4317
Epoch 162, Train Loss: 0.0102, Val Loss: 1.0372
batch size: (905, 905)
Epoch 163, accuracy: 0.4267
batch size: (889, 889)
Epoch 164, accuracy: 0.4331
Epoch 164, Train Loss: 0.0065, Val Loss: 1.0364
batch size: (901, 901)
Epoch 165, accuracy: 0.4318
batch size: (906, 906)
Epoch 166, accuracy: 0.4324
Epoch 166, Train Loss: 0.0134, Val Loss: 1.0335
batch size: (909, 909)
Epoch 167, accuracy: 0.4315
batch size: (892, 892)
Epoch 168, accuracy: 0.4267
Epoch 168, Train Loss: 0.0077, Val Loss: 1.0351
batch size: (912, 912)
Epoch 169, accuracy: 0.4282
batch size: (880, 880)
Epoch 170, accuracy: 0.4315
Epoch 170, Train Loss: 0.0060, Val Loss: 1.0372
batch size: (907, 907)
Epoch 171, accuracy: 0.4312
batch size: (893, 893)
Epoch 172, accuracy: 0.4308
Epoch 172, Train Loss: 0.0147, Val Loss: 1.0402
batch size: (886, 886)
Epoch 173, accuracy: 0.4330
batch size: (902, 902)
Epoch 174, accuracy: 0.4311
Epoch 174, Train Loss: 0.0047, Val Loss: 1.0375
batch size: (909, 909)
Epoch 175, accuracy: 0.4340
batch size: (895, 895)
Epoch 176, accuracy: 0.4346
Epoch 176, Train Loss: 0.0046, Val Loss: 1.0369
batch size: (900, 900)
Epoch 177, accuracy: 0.4323
batch size: (899, 899)
Epoch 178, accuracy: 0.4328
Epoch 178, Train Loss: 0.0040, Val Loss: 1.0280
batch size: (911, 911)
Epoch 179, accuracy: 0.4291
batch size: (899, 899)
Epoch 180, accuracy: 0.4294
Epoch 180, Train Loss: 0.0064, Val Loss: 1.0307
batch size: (901, 901)
Epoch 181, accuracy: 0.4295
batch size: (910, 910)
Epoch 182, accuracy: 0.4311
Epoch 182, Train Loss: 0.0059, Val Loss: 1.0394
batch size: (889, 889)
Epoch 183, accuracy: 0.4321
batch size: (893, 893)
Epoch 184, accuracy: 0.4289
Epoch 184, Train Loss: 0.0036, Val Loss: 1.0402
batch size: (892, 892)
Epoch 185, accuracy: 0.4294
batch size: (893, 893)
Epoch 186, accuracy: 0.4275
Epoch 186, Train Loss: 0.0183, Val Loss: 1.0344
batch size: (920, 920)
Epoch 187, accuracy: 0.4282
batch size: (907, 907)
Epoch 188, accuracy: 0.4246
Epoch 188, Train Loss: 0.0067, Val Loss: 1.0389
batch size: (903, 903)
Epoch 189, accuracy: 0.4301
batch size: (902, 902)
Epoch 190, accuracy: 0.4310
Epoch 190, Train Loss: 0.0087, Val Loss: 1.0324
batch size: (901, 901)
Epoch 191, accuracy: 0.4348
batch size: (882, 882)
Epoch 192, accuracy: 0.4268
Epoch 192, Train Loss: 0.0037, Val Loss: 1.0384
batch size: (931, 931)
Epoch 193, accuracy: 0.4318
batch size: (896, 896)
Epoch 194, accuracy: 0.4330
Epoch 194, Train Loss: 0.0176, Val Loss: 1.0342
batch size: (907, 907)
Epoch 195, accuracy: 0.4283
batch size: (885, 885)
Epoch 196, accuracy: 0.4285
Epoch 196, Train Loss: 0.0096, Val Loss: 1.0386
batch size: (908, 908)
Epoch 197, accuracy: 0.4304
batch size: (919, 919)
Epoch 198, accuracy: 0.4327
Epoch 198, Train Loss: 0.0033, Val Loss: 1.0292
batch size: (892, 892)
Epoch 199, accuracy: 0.4334
Loaded best model with val_loss = 1.0170142650604248
test :accuracy 0.4318, f1_macro: 0.2054, f1_micro: 0.4318, auc: 0.7441
Training JKNet with 32 layers...
可训练参数: 4354822_JKNet
不可训练参数: 0
batch size: (916, 916)
✅ Epoch 0: New best model saved with val_loss = 1.1580
Epoch 0, accuracy: 0.4312
Epoch 0, Train Loss: 1.1790, Val Loss: 1.1580
batch size: (893, 893)
Epoch 1, accuracy: 0.4039
batch size: (896, 896)
Epoch 2, accuracy: 0.1671
Epoch 2, Train Loss: 5.9138, Val Loss: 1.4331
batch size: (912, 912)
Epoch 3, accuracy: 0.1702
batch size: (913, 913)
✅ Epoch 4: New best model saved with val_loss = 1.1249
Epoch 4, accuracy: 0.4034
Epoch 4, Train Loss: 5.4024, Val Loss: 1.1249
batch size: (879, 879)
✅ Epoch 5: New best model saved with val_loss = 1.0931
Epoch 5, accuracy: 0.3995
batch size: (900, 900)
✅ Epoch 6: New best model saved with val_loss = 1.0849
Epoch 6, accuracy: 0.4051
Epoch 6, Train Loss: 0.7785, Val Loss: 1.0849
batch size: (903, 903)
Epoch 7, accuracy: 0.4028
batch size: (903, 903)
Epoch 8, accuracy: 0.4064
Epoch 8, Train Loss: 2.0004, Val Loss: 1.0917
batch size: (901, 901)
Epoch 9, accuracy: 0.4169
batch size: (910, 910)
Epoch 10, accuracy: 0.1677
Epoch 10, Train Loss: 0.1995, Val Loss: 1.1031
batch size: (903, 903)
Epoch 11, accuracy: 0.1680
batch size: (889, 889)
Epoch 12, accuracy: 0.1683
Epoch 12, Train Loss: 0.4758, Val Loss: 1.1002
batch size: (908, 908)
Epoch 13, accuracy: 0.1667
batch size: (911, 911)
Epoch 14, accuracy: 0.1676
Epoch 14, Train Loss: 0.4851, Val Loss: 1.1023
batch size: (907, 907)
Epoch 15, accuracy: 0.1671
batch size: (901, 901)
Epoch 16, accuracy: 0.1671
Epoch 16, Train Loss: 0.9057, Val Loss: 1.1024
batch size: (911, 911)
Epoch 17, accuracy: 0.1649
batch size: (911, 911)
Epoch 18, accuracy: 0.1658
Epoch 18, Train Loss: 0.3080, Val Loss: 1.0997
batch size: (903, 903)
Epoch 19, accuracy: 0.1670
batch size: (894, 894)
Epoch 20, accuracy: 0.1711
Epoch 20, Train Loss: 0.0753, Val Loss: 1.1001
batch size: (888, 888)
Epoch 21, accuracy: 0.1659
batch size: (909, 909)
Epoch 22, accuracy: 0.1654
Epoch 22, Train Loss: 0.0719, Val Loss: 1.1004
batch size: (900, 900)
Epoch 23, accuracy: 0.1671
batch size: (898, 898)
Epoch 24, accuracy: 0.1650
Epoch 24, Train Loss: 0.6289, Val Loss: 1.1007
batch size: (891, 891)
Epoch 25, accuracy: 0.1687
batch size: (889, 889)
Epoch 26, accuracy: 0.1679
Epoch 26, Train Loss: 0.0935, Val Loss: 1.1006
batch size: (881, 881)
Epoch 27, accuracy: 0.1686
batch size: (918, 918)
Epoch 28, accuracy: 0.1666
Epoch 28, Train Loss: 0.1259, Val Loss: 1.1001
batch size: (902, 902)
Epoch 29, accuracy: 0.1666
batch size: (891, 891)
Epoch 30, accuracy: 0.1663
Epoch 30, Train Loss: 0.0574, Val Loss: 1.0997
batch size: (894, 894)
Epoch 31, accuracy: 0.1691
batch size: (875, 875)
Epoch 32, accuracy: 0.1702
Epoch 32, Train Loss: 0.0803, Val Loss: 1.1003
batch size: (898, 898)
Epoch 33, accuracy: 0.1649
batch size: (901, 901)
Epoch 34, accuracy: 0.1694
Epoch 34, Train Loss: 0.0749, Val Loss: 1.1000
batch size: (903, 903)
Epoch 35, accuracy: 0.1622
batch size: (898, 898)
Epoch 36, accuracy: 0.1667
Epoch 36, Train Loss: 0.2469, Val Loss: 1.1000
batch size: (894, 894)
Epoch 37, accuracy: 0.1642
batch size: (899, 899)
Epoch 38, accuracy: 0.1685
Epoch 38, Train Loss: 0.1941, Val Loss: 1.1000
batch size: (912, 912)
Epoch 39, accuracy: 0.1660
batch size: (911, 911)
Epoch 40, accuracy: 0.1684
Epoch 40, Train Loss: 0.2863, Val Loss: 1.1001
batch size: (905, 905)
Epoch 41, accuracy: 0.1695
batch size: (896, 896)
Epoch 42, accuracy: 0.1671
Epoch 42, Train Loss: 0.0875, Val Loss: 1.1004
batch size: (904, 904)
Epoch 43, accuracy: 0.1677
batch size: (890, 890)
Epoch 44, accuracy: 0.1674
Epoch 44, Train Loss: 0.0821, Val Loss: 1.1002
batch size: (884, 884)
Epoch 45, accuracy: 0.1696
batch size: (889, 889)
Epoch 46, accuracy: 0.1662
Epoch 46, Train Loss: 0.0915, Val Loss: 1.1001
batch size: (887, 887)
Epoch 47, accuracy: 0.1679
batch size: (907, 907)
Epoch 48, accuracy: 0.1694
Epoch 48, Train Loss: 0.2928, Val Loss: 1.1003
batch size: (881, 881)
Epoch 49, accuracy: 0.1668
batch size: (908, 908)
Epoch 50, accuracy: 0.1680
Epoch 50, Train Loss: 0.0763, Val Loss: 1.1008
batch size: (902, 902)
Epoch 51, accuracy: 0.1638
batch size: (887, 887)
Epoch 52, accuracy: 0.1675
Epoch 52, Train Loss: 0.0860, Val Loss: 1.1012
batch size: (905, 905)
Epoch 53, accuracy: 0.1673
batch size: (907, 907)
Epoch 54, accuracy: 0.1659
Epoch 54, Train Loss: 0.0739, Val Loss: 1.1003
batch size: (908, 908)
Epoch 55, accuracy: 0.1674
batch size: (884, 884)
Epoch 56, accuracy: 0.1645
Epoch 56, Train Loss: 0.0623, Val Loss: 1.0999
batch size: (887, 887)
Epoch 57, accuracy: 0.1679
batch size: (903, 903)
Epoch 58, accuracy: 0.1683
Epoch 58, Train Loss: 0.0922, Val Loss: 1.1000
batch size: (891, 891)
Epoch 59, accuracy: 0.1686
batch size: (903, 903)
Epoch 60, accuracy: 0.1676
Epoch 60, Train Loss: 0.0822, Val Loss: 1.0994
batch size: (898, 898)
Epoch 61, accuracy: 0.1685
batch size: (893, 893)
Epoch 62, accuracy: 0.1713
Epoch 62, Train Loss: 0.0814, Val Loss: 1.1006
batch size: (888, 888)
Epoch 63, accuracy: 0.1672
batch size: (904, 904)
Epoch 64, accuracy: 0.1684
Epoch 64, Train Loss: 0.0899, Val Loss: 1.1002
batch size: (910, 910)
Epoch 65, accuracy: 0.1696
batch size: (911, 911)
Epoch 66, accuracy: 0.1675
Epoch 66, Train Loss: 0.0737, Val Loss: 1.0996
batch size: (917, 917)
Epoch 67, accuracy: 0.1658
batch size: (887, 887)
Epoch 68, accuracy: 0.1665
Epoch 68, Train Loss: 0.0845, Val Loss: 1.1001
batch size: (895, 895)
Epoch 69, accuracy: 0.1665
batch size: (877, 877)
Epoch 70, accuracy: 0.1674
Epoch 70, Train Loss: 0.0789, Val Loss: 1.1005
batch size: (897, 897)
Epoch 71, accuracy: 0.1686
batch size: (919, 919)
Epoch 72, accuracy: 0.1684
Epoch 72, Train Loss: 0.0535, Val Loss: 1.1003
batch size: (894, 894)
Epoch 73, accuracy: 0.1650
batch size: (890, 890)
Epoch 74, accuracy: 0.1673
Epoch 74, Train Loss: 0.1058, Val Loss: 1.0998
batch size: (901, 901)
Epoch 75, accuracy: 0.1683
batch size: (912, 912)
Epoch 76, accuracy: 0.1642
Epoch 76, Train Loss: 0.0775, Val Loss: 1.1002
batch size: (890, 890)
Epoch 77, accuracy: 0.1687
batch size: (898, 898)
Epoch 78, accuracy: 0.1681
Epoch 78, Train Loss: 0.0962, Val Loss: 1.1002
batch size: (906, 906)
Epoch 79, accuracy: 0.1671
batch size: (923, 923)
Epoch 80, accuracy: 0.1694
Epoch 80, Train Loss: 0.0576, Val Loss: 1.0998
batch size: (904, 904)
Epoch 81, accuracy: 0.1676
batch size: (909, 909)
Epoch 82, accuracy: 0.1660
Epoch 82, Train Loss: 0.0922, Val Loss: 1.1007
batch size: (917, 917)
Epoch 83, accuracy: 0.1696
batch size: (915, 915)
Epoch 84, accuracy: 0.1679
Epoch 84, Train Loss: 0.0602, Val Loss: 1.0991
batch size: (909, 909)
Epoch 85, accuracy: 0.1679
batch size: (884, 884)
Epoch 86, accuracy: 0.1663
Epoch 86, Train Loss: 0.0973, Val Loss: 1.1006
batch size: (884, 884)
Epoch 87, accuracy: 0.1670
batch size: (923, 923)
Epoch 88, accuracy: 0.1688
Epoch 88, Train Loss: 0.0743, Val Loss: 1.0998
batch size: (880, 880)
Epoch 89, accuracy: 0.1666
batch size: (893, 893)
Epoch 90, accuracy: 0.1660
Epoch 90, Train Loss: 0.0622, Val Loss: 1.0994
batch size: (886, 886)
Epoch 91, accuracy: 0.1676
batch size: (903, 903)
Epoch 92, accuracy: 0.1639
Epoch 92, Train Loss: 0.0879, Val Loss: 1.0993
batch size: (885, 885)
Epoch 93, accuracy: 0.1615
batch size: (910, 910)
Epoch 94, accuracy: 0.1672
Epoch 94, Train Loss: 0.2298, Val Loss: 1.0995
batch size: (920, 920)
Epoch 95, accuracy: 0.1674
batch size: (907, 907)
Epoch 96, accuracy: 0.1694
Epoch 96, Train Loss: 0.0692, Val Loss: 1.0994
batch size: (901, 901)
Epoch 97, accuracy: 0.1656
batch size: (914, 914)
Epoch 98, accuracy: 0.1674
Epoch 98, Train Loss: 0.0665, Val Loss: 1.1000
batch size: (910, 910)
Epoch 99, accuracy: 0.1673
batch size: (911, 911)
Epoch 100, accuracy: 0.1654
Epoch 100, Train Loss: 0.0627, Val Loss: 1.0994
batch size: (873, 873)
Epoch 101, accuracy: 0.1682
batch size: (910, 910)
Epoch 102, accuracy: 0.1677
Epoch 102, Train Loss: 0.0776, Val Loss: 1.1004
batch size: (882, 882)
Epoch 103, accuracy: 0.1648
batch size: (891, 891)
Epoch 104, accuracy: 0.1700
Epoch 104, Train Loss: 0.0971, Val Loss: 1.1000
batch size: (892, 892)
Epoch 105, accuracy: 0.1678
batch size: (892, 892)
Epoch 106, accuracy: 0.1690
Epoch 106, Train Loss: 0.0822, Val Loss: 1.1005
batch size: (909, 909)
Epoch 107, accuracy: 0.1663
batch size: (911, 911)
Epoch 108, accuracy: 0.1681
Epoch 108, Train Loss: 0.0729, Val Loss: 1.1006
batch size: (902, 902)
Epoch 109, accuracy: 0.1683
batch size: (890, 890)
Epoch 110, accuracy: 0.1677
Epoch 110, Train Loss: 0.0615, Val Loss: 1.0991
batch size: (902, 902)
Epoch 111, accuracy: 0.1660
batch size: (906, 906)
Epoch 112, accuracy: 0.1660
Epoch 112, Train Loss: 0.0641, Val Loss: 1.1000
batch size: (892, 892)
Epoch 113, accuracy: 0.1678
batch size: (921, 921)
Epoch 114, accuracy: 0.1678
Epoch 114, Train Loss: 0.1486, Val Loss: 1.1005
batch size: (922, 922)
Epoch 115, accuracy: 0.1688
batch size: (917, 917)
Epoch 116, accuracy: 0.1706
Epoch 116, Train Loss: 0.0730, Val Loss: 1.0991
batch size: (902, 902)
Epoch 117, accuracy: 0.1651
batch size: (911, 911)
Epoch 118, accuracy: 0.1664
Epoch 118, Train Loss: 0.0567, Val Loss: 1.0994
batch size: (897, 897)
Epoch 119, accuracy: 0.1677
batch size: (908, 908)
Epoch 120, accuracy: 0.1683
Epoch 120, Train Loss: 0.4285, Val Loss: 1.1010
batch size: (899, 899)
Epoch 121, accuracy: 0.1670
batch size: (901, 901)
Epoch 122, accuracy: 0.1690
Epoch 122, Train Loss: 0.0911, Val Loss: 1.0999
batch size: (899, 899)
Epoch 123, accuracy: 0.1649
batch size: (896, 896)
Epoch 124, accuracy: 0.1659
Epoch 124, Train Loss: 0.1011, Val Loss: 1.1000
batch size: (906, 906)
Epoch 125, accuracy: 0.1669
batch size: (911, 911)
Epoch 126, accuracy: 0.1659
Epoch 126, Train Loss: 0.4226, Val Loss: 1.1005
batch size: (900, 900)
Epoch 127, accuracy: 0.1692
batch size: (892, 892)
Epoch 128, accuracy: 0.1684
Epoch 128, Train Loss: 0.0855, Val Loss: 1.0994
batch size: (873, 873)
Epoch 129, accuracy: 0.1660
batch size: (889, 889)
Epoch 130, accuracy: 0.1667
Epoch 130, Train Loss: 0.3679, Val Loss: 1.0998
batch size: (885, 885)
Epoch 131, accuracy: 0.1667
batch size: (880, 880)
Epoch 132, accuracy: 0.1676
Epoch 132, Train Loss: 0.1181, Val Loss: 1.0988
batch size: (898, 898)
Epoch 133, accuracy: 0.1686
batch size: (908, 908)
Epoch 134, accuracy: 0.1643
Epoch 134, Train Loss: 0.1982, Val Loss: 1.0997
batch size: (902, 902)
Epoch 135, accuracy: 0.1685
batch size: (902, 902)
Epoch 136, accuracy: 0.1668
Epoch 136, Train Loss: 0.0891, Val Loss: 1.0992
batch size: (880, 880)
Epoch 137, accuracy: 0.1645
batch size: (900, 900)
Epoch 138, accuracy: 0.1662
Epoch 138, Train Loss: 0.0905, Val Loss: 1.1005
batch size: (910, 910)
Epoch 139, accuracy: 0.1663
batch size: (914, 914)
Epoch 140, accuracy: 0.1674
Epoch 140, Train Loss: 0.0735, Val Loss: 1.1000
batch size: (894, 894)
Epoch 141, accuracy: 0.1683
batch size: (919, 919)
Epoch 142, accuracy: 0.1694
Epoch 142, Train Loss: 0.0983, Val Loss: 1.1002
batch size: (901, 901)
Epoch 143, accuracy: 0.1682
batch size: (908, 908)
Epoch 144, accuracy: 0.1654
Epoch 144, Train Loss: 0.0913, Val Loss: 1.1008
batch size: (889, 889)
Epoch 145, accuracy: 0.1680
batch size: (914, 914)
Epoch 146, accuracy: 0.1692
Epoch 146, Train Loss: 0.0764, Val Loss: 1.0996
batch size: (895, 895)
Epoch 147, accuracy: 0.1639
batch size: (893, 893)
Epoch 148, accuracy: 0.1681
Epoch 148, Train Loss: 0.0819, Val Loss: 1.0997
batch size: (896, 896)
Epoch 149, accuracy: 0.1646
batch size: (892, 892)
Epoch 150, accuracy: 0.1687
Epoch 150, Train Loss: 0.0992, Val Loss: 1.1005
batch size: (892, 892)
Epoch 151, accuracy: 0.1675
batch size: (898, 898)
Epoch 152, accuracy: 0.1678
Epoch 152, Train Loss: 0.0832, Val Loss: 1.1000
batch size: (901, 901)
Epoch 153, accuracy: 0.1698
batch size: (908, 908)
Epoch 154, accuracy: 0.1661
Epoch 154, Train Loss: 0.0845, Val Loss: 1.1003
batch size: (905, 905)
Epoch 155, accuracy: 0.1689
batch size: (899, 899)
Epoch 156, accuracy: 0.1679
Epoch 156, Train Loss: 0.0707, Val Loss: 1.0999
batch size: (887, 887)
Epoch 157, accuracy: 0.1678
batch size: (887, 887)
Epoch 158, accuracy: 0.1668
Epoch 158, Train Loss: 0.1938, Val Loss: 1.0994
batch size: (908, 908)
Epoch 159, accuracy: 0.1680
batch size: (917, 917)
Epoch 160, accuracy: 0.1666
Epoch 160, Train Loss: 0.0943, Val Loss: 1.0995
batch size: (885, 885)
Epoch 161, accuracy: 0.1693
batch size: (894, 894)
Epoch 162, accuracy: 0.1669
Epoch 162, Train Loss: 0.0852, Val Loss: 1.0999
batch size: (898, 898)
Epoch 163, accuracy: 0.1665
batch size: (880, 880)
Epoch 164, accuracy: 0.1653
Epoch 164, Train Loss: 0.0697, Val Loss: 1.0999
batch size: (889, 889)
Epoch 165, accuracy: 0.1681
batch size: (901, 901)
Epoch 166, accuracy: 0.1653
Epoch 166, Train Loss: 0.0729, Val Loss: 1.1002
batch size: (917, 917)
Epoch 167, accuracy: 0.1683
batch size: (904, 904)
Epoch 168, accuracy: 0.1665
Epoch 168, Train Loss: 0.0689, Val Loss: 1.1002
batch size: (904, 904)
Epoch 169, accuracy: 0.1653
batch size: (904, 904)
Epoch 170, accuracy: 0.1687
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 170, Train Loss: 0.0813, Val Loss: 1.1010
batch size: (899, 899)
Epoch 171, accuracy: 0.1646
batch size: (904, 904)
Epoch 172, accuracy: 0.1694
Epoch 172, Train Loss: 0.0795, Val Loss: 1.1005
batch size: (901, 901)
Epoch 173, accuracy: 0.1697
batch size: (893, 893)
Epoch 174, accuracy: 0.1655
Epoch 174, Train Loss: 0.0684, Val Loss: 1.0995
batch size: (889, 889)
Epoch 175, accuracy: 0.1676
batch size: (891, 891)
Epoch 176, accuracy: 0.1681
Epoch 176, Train Loss: 0.0867, Val Loss: 1.0999
batch size: (892, 892)
Epoch 177, accuracy: 0.1669
batch size: (906, 906)
Epoch 178, accuracy: 0.1668
Epoch 178, Train Loss: 0.0799, Val Loss: 1.0999
batch size: (915, 915)
Epoch 179, accuracy: 0.1665
batch size: (896, 896)
Epoch 180, accuracy: 0.1675
Epoch 180, Train Loss: 0.0635, Val Loss: 1.0997
batch size: (889, 889)
Epoch 181, accuracy: 0.1694
batch size: (895, 895)
Epoch 182, accuracy: 0.1661
Epoch 182, Train Loss: 0.0587, Val Loss: 1.0994
batch size: (896, 896)
Epoch 183, accuracy: 0.1667
batch size: (884, 884)
Epoch 184, accuracy: 0.1628
Epoch 184, Train Loss: 0.0694, Val Loss: 1.1001
batch size: (905, 905)
Epoch 185, accuracy: 0.1678
batch size: (906, 906)
Epoch 186, accuracy: 0.1682
Epoch 186, Train Loss: 0.0740, Val Loss: 1.0999
batch size: (902, 902)
Epoch 187, accuracy: 0.1679
batch size: (898, 898)
Epoch 188, accuracy: 0.1651
Epoch 188, Train Loss: 0.0744, Val Loss: 1.0998
batch size: (898, 898)
Epoch 189, accuracy: 0.1668
batch size: (910, 910)
Epoch 190, accuracy: 0.1704
Epoch 190, Train Loss: 0.0800, Val Loss: 1.1007
batch size: (906, 906)
Epoch 191, accuracy: 0.1658
batch size: (900, 900)
Epoch 192, accuracy: 0.1693
Epoch 192, Train Loss: 0.0808, Val Loss: 1.1000
batch size: (906, 906)
Epoch 193, accuracy: 0.1676
batch size: (909, 909)
Epoch 194, accuracy: 0.1667
Epoch 194, Train Loss: 0.2066, Val Loss: 1.0998
batch size: (909, 909)
Epoch 195, accuracy: 0.1686
batch size: (919, 919)
Epoch 196, accuracy: 0.1667
Epoch 196, Train Loss: 0.0948, Val Loss: 1.0998
batch size: (906, 906)
Epoch 197, accuracy: 0.1671
batch size: (894, 894)
Epoch 198, accuracy: 0.1690
Epoch 198, Train Loss: 0.1070, Val Loss: 1.1000
batch size: (884, 884)
Epoch 199, accuracy: 0.1652
Loaded best model with val_loss = 1.0848864316940308
test :accuracy 0.4037, f1_macro: 0.1917, f1_micro: 0.4037, auc: 0.6938
Training resGCN with 2 layers...
可训练参数: 132626_resGCN
不可训练参数: 0
batch size: (892, 892)
✅ Epoch 0: New best model saved with val_loss = 1.0944
Epoch 0, accuracy: 0.4229
Epoch 0, Train Loss: 1.0983, Val Loss: 1.0944
batch size: (902, 902)
✅ Epoch 1: New best model saved with val_loss = 1.0934
Epoch 1, accuracy: 0.3641
batch size: (918, 918)
✅ Epoch 2: New best model saved with val_loss = 1.0846
Epoch 2, accuracy: 0.4646
Epoch 2, Train Loss: 1.0899, Val Loss: 1.0846
batch size: (914, 914)
✅ Epoch 3: New best model saved with val_loss = 1.0755
Epoch 3, accuracy: 0.4615
batch size: (913, 913)
✅ Epoch 4: New best model saved with val_loss = 1.0666
Epoch 4, accuracy: 0.4580
Epoch 4, Train Loss: 1.0697, Val Loss: 1.0666
batch size: (906, 906)
✅ Epoch 5: New best model saved with val_loss = 1.0574
Epoch 5, accuracy: 0.4718
batch size: (881, 881)
✅ Epoch 6: New best model saved with val_loss = 1.0479
Epoch 6, accuracy: 0.4702
Epoch 6, Train Loss: 1.0466, Val Loss: 1.0479
batch size: (900, 900)
✅ Epoch 7: New best model saved with val_loss = 1.0353
Epoch 7, accuracy: 0.4708
batch size: (921, 921)
✅ Epoch 8: New best model saved with val_loss = 1.0255
Epoch 8, accuracy: 0.4739
Epoch 8, Train Loss: 1.0316, Val Loss: 1.0255
batch size: (897, 897)
✅ Epoch 9: New best model saved with val_loss = 1.0140
Epoch 9, accuracy: 0.4778
batch size: (907, 907)
✅ Epoch 10: New best model saved with val_loss = 1.0016
Epoch 10, accuracy: 0.4838
Epoch 10, Train Loss: 0.9909, Val Loss: 1.0016
batch size: (887, 887)
✅ Epoch 11: New best model saved with val_loss = 0.9913
Epoch 11, accuracy: 0.4842
batch size: (897, 897)
✅ Epoch 12: New best model saved with val_loss = 0.9810
Epoch 12, accuracy: 0.4921
Epoch 12, Train Loss: 0.9765, Val Loss: 0.9810
batch size: (921, 921)
✅ Epoch 13: New best model saved with val_loss = 0.9703
Epoch 13, accuracy: 0.4945
batch size: (904, 904)
✅ Epoch 14: New best model saved with val_loss = 0.9604
Epoch 14, accuracy: 0.4978
Epoch 14, Train Loss: 0.9721, Val Loss: 0.9604
batch size: (914, 914)
✅ Epoch 15: New best model saved with val_loss = 0.9503
Epoch 15, accuracy: 0.5020
batch size: (886, 886)
✅ Epoch 16: New best model saved with val_loss = 0.9446
Epoch 16, accuracy: 0.5170
Epoch 16, Train Loss: 0.9237, Val Loss: 0.9446
batch size: (903, 903)
✅ Epoch 17: New best model saved with val_loss = 0.9352
Epoch 17, accuracy: 0.5293
batch size: (888, 888)
✅ Epoch 18: New best model saved with val_loss = 0.9295
Epoch 18, accuracy: 0.5403
Epoch 18, Train Loss: 0.9076, Val Loss: 0.9295
batch size: (910, 910)
✅ Epoch 19: New best model saved with val_loss = 0.9272
Epoch 19, accuracy: 0.5419
batch size: (895, 895)
✅ Epoch 20: New best model saved with val_loss = 0.9188
Epoch 20, accuracy: 0.5397
Epoch 20, Train Loss: 0.8450, Val Loss: 0.9188
batch size: (900, 900)
✅ Epoch 21: New best model saved with val_loss = 0.9156
Epoch 21, accuracy: 0.5382
batch size: (893, 893)
✅ Epoch 22: New best model saved with val_loss = 0.9139
Epoch 22, accuracy: 0.5325
Epoch 22, Train Loss: 0.8387, Val Loss: 0.9139
batch size: (879, 879)
Epoch 23, accuracy: 0.5279
batch size: (907, 907)
✅ Epoch 24: New best model saved with val_loss = 0.9100
Epoch 24, accuracy: 0.5391
Epoch 24, Train Loss: 0.8328, Val Loss: 0.9100
batch size: (904, 904)
Epoch 25, accuracy: 0.5547
batch size: (910, 910)
✅ Epoch 26: New best model saved with val_loss = 0.9051
Epoch 26, accuracy: 0.5611
Epoch 26, Train Loss: 0.8215, Val Loss: 0.9051
batch size: (909, 909)
✅ Epoch 27: New best model saved with val_loss = 0.9046
Epoch 27, accuracy: 0.5608
batch size: (891, 891)
✅ Epoch 28: New best model saved with val_loss = 0.8993
Epoch 28, accuracy: 0.5576
Epoch 28, Train Loss: 0.7256, Val Loss: 0.8993
batch size: (923, 923)
✅ Epoch 29: New best model saved with val_loss = 0.8972
Epoch 29, accuracy: 0.5498
batch size: (899, 899)
Epoch 30, accuracy: 0.5374
Epoch 30, Train Loss: 0.7531, Val Loss: 0.9086
batch size: (881, 881)
✅ Epoch 31: New best model saved with val_loss = 0.8959
Epoch 31, accuracy: 0.5479
batch size: (898, 898)
Epoch 32, accuracy: 0.5579
Epoch 32, Train Loss: 0.7951, Val Loss: 0.8984
batch size: (913, 913)
✅ Epoch 33: New best model saved with val_loss = 0.8871
Epoch 33, accuracy: 0.5733
batch size: (919, 919)
✅ Epoch 34: New best model saved with val_loss = 0.8869
Epoch 34, accuracy: 0.5773
Epoch 34, Train Loss: 0.7239, Val Loss: 0.8869
batch size: (900, 900)
✅ Epoch 35: New best model saved with val_loss = 0.8855
Epoch 35, accuracy: 0.5409
batch size: (890, 890)
Epoch 36, accuracy: 0.5384
Epoch 36, Train Loss: 0.7510, Val Loss: 0.8913
batch size: (894, 894)
Epoch 37, accuracy: 0.5351
batch size: (915, 915)
Epoch 38, accuracy: 0.5339
Epoch 38, Train Loss: 0.7162, Val Loss: 0.8980
batch size: (900, 900)
Epoch 39, accuracy: 0.5288
batch size: (906, 906)
Epoch 40, accuracy: 0.5283
Epoch 40, Train Loss: 0.6880, Val Loss: 0.9005
batch size: (903, 903)
Epoch 41, accuracy: 0.5304
batch size: (896, 896)
Epoch 42, accuracy: 0.5252
Epoch 42, Train Loss: 0.7056, Val Loss: 0.8927
batch size: (884, 884)
Epoch 43, accuracy: 0.5260
batch size: (906, 906)
Epoch 44, accuracy: 0.5280
Epoch 44, Train Loss: 0.7043, Val Loss: 0.8932
batch size: (889, 889)
Epoch 45, accuracy: 0.5267
batch size: (887, 887)
Epoch 46, accuracy: 0.5293
Epoch 46, Train Loss: 0.7370, Val Loss: 0.8994
batch size: (905, 905)
Epoch 47, accuracy: 0.5282
batch size: (907, 907)
Epoch 48, accuracy: 0.5255
Epoch 48, Train Loss: 0.7068, Val Loss: 0.8961
batch size: (887, 887)
Epoch 49, accuracy: 0.5257
batch size: (889, 889)
Epoch 50, accuracy: 0.5243
Epoch 50, Train Loss: 0.7296, Val Loss: 0.9014
batch size: (886, 886)
Epoch 51, accuracy: 0.5281
batch size: (896, 896)
Epoch 52, accuracy: 0.5269
Epoch 52, Train Loss: 0.6764, Val Loss: 0.8995
batch size: (878, 878)
Epoch 53, accuracy: 0.5285
batch size: (900, 900)
Epoch 54, accuracy: 0.5276
Epoch 54, Train Loss: 0.6785, Val Loss: 0.9010
batch size: (908, 908)
Epoch 55, accuracy: 0.5242
batch size: (897, 897)
Epoch 56, accuracy: 0.5238
Epoch 56, Train Loss: 0.6911, Val Loss: 0.9029
batch size: (907, 907)
Epoch 57, accuracy: 0.5234
batch size: (890, 890)
Epoch 58, accuracy: 0.5259
Epoch 58, Train Loss: 0.7177, Val Loss: 0.8878
batch size: (942, 942)
Epoch 59, accuracy: 0.5292
batch size: (882, 882)
Epoch 60, accuracy: 0.5251
Epoch 60, Train Loss: 0.6813, Val Loss: 0.9031
batch size: (906, 906)
Epoch 61, accuracy: 0.5251
batch size: (899, 899)
Epoch 62, accuracy: 0.5284
Epoch 62, Train Loss: 0.7091, Val Loss: 0.8893
batch size: (909, 909)
Epoch 63, accuracy: 0.5262
batch size: (883, 883)
Epoch 64, accuracy: 0.5247
Epoch 64, Train Loss: 0.6973, Val Loss: 0.9038
batch size: (894, 894)
Epoch 65, accuracy: 0.5289
batch size: (908, 908)
Epoch 66, accuracy: 0.5236
Epoch 66, Train Loss: 0.6967, Val Loss: 0.9020
batch size: (905, 905)
Epoch 67, accuracy: 0.5299
batch size: (887, 887)
Epoch 68, accuracy: 0.5263
Epoch 68, Train Loss: 0.7246, Val Loss: 0.8883
batch size: (891, 891)
Epoch 69, accuracy: 0.5283
batch size: (903, 903)
Epoch 70, accuracy: 0.5272
Epoch 70, Train Loss: 0.7105, Val Loss: 0.9005
batch size: (905, 905)
Epoch 71, accuracy: 0.5267
batch size: (900, 900)
Epoch 72, accuracy: 0.5265
Epoch 72, Train Loss: 0.6930, Val Loss: 0.8905
batch size: (890, 890)
Epoch 73, accuracy: 0.5269
batch size: (911, 911)
Epoch 74, accuracy: 0.5239
Epoch 74, Train Loss: 0.7324, Val Loss: 0.9136
batch size: (898, 898)
Epoch 75, accuracy: 0.5269
batch size: (908, 908)
Epoch 76, accuracy: 0.5239
Epoch 76, Train Loss: 0.7307, Val Loss: 0.8989
batch size: (893, 893)
Epoch 77, accuracy: 0.5267
batch size: (920, 920)
Epoch 78, accuracy: 0.5268
Epoch 78, Train Loss: 0.7107, Val Loss: 0.8975
batch size: (900, 900)
Epoch 79, accuracy: 0.5280
batch size: (911, 911)
Epoch 80, accuracy: 0.5282
Epoch 80, Train Loss: 0.6943, Val Loss: 0.8892
batch size: (891, 891)
Epoch 81, accuracy: 0.5300
batch size: (906, 906)
Epoch 82, accuracy: 0.5295
Epoch 82, Train Loss: 0.7171, Val Loss: 0.9014
batch size: (886, 886)
Epoch 83, accuracy: 0.5234
batch size: (889, 889)
Epoch 84, accuracy: 0.5233
Epoch 84, Train Loss: 0.7056, Val Loss: 0.8976
batch size: (900, 900)
Epoch 85, accuracy: 0.5274
batch size: (906, 906)
Epoch 86, accuracy: 0.5274
Epoch 86, Train Loss: 0.7036, Val Loss: 0.9009
batch size: (912, 912)
Epoch 87, accuracy: 0.5254
batch size: (906, 906)
Epoch 88, accuracy: 0.5242
Epoch 88, Train Loss: 0.7154, Val Loss: 0.9040
batch size: (885, 885)
Epoch 89, accuracy: 0.5279
batch size: (901, 901)
Epoch 90, accuracy: 0.5267
Epoch 90, Train Loss: 0.7062, Val Loss: 0.8968
batch size: (887, 887)
Epoch 91, accuracy: 0.5270
batch size: (905, 905)
Epoch 92, accuracy: 0.5228
Epoch 92, Train Loss: 0.7206, Val Loss: 0.8916
batch size: (902, 902)
Epoch 93, accuracy: 0.5261
batch size: (902, 902)
Epoch 94, accuracy: 0.5284
Epoch 94, Train Loss: 0.7068, Val Loss: 0.9063
batch size: (896, 896)
Epoch 95, accuracy: 0.5257
batch size: (897, 897)
Epoch 96, accuracy: 0.5274
Epoch 96, Train Loss: 0.6845, Val Loss: 0.9034
batch size: (886, 886)
Epoch 97, accuracy: 0.5290
batch size: (911, 911)
Epoch 98, accuracy: 0.5277
Epoch 98, Train Loss: 0.7023, Val Loss: 0.8911
batch size: (900, 900)
Epoch 99, accuracy: 0.5291
batch size: (885, 885)
Epoch 100, accuracy: 0.5264
Epoch 100, Train Loss: 0.7602, Val Loss: 0.8962
batch size: (877, 877)
Epoch 101, accuracy: 0.5277
batch size: (902, 902)
Epoch 102, accuracy: 0.5284
Epoch 102, Train Loss: 0.7244, Val Loss: 0.8996
batch size: (906, 906)
Epoch 103, accuracy: 0.5262
batch size: (892, 892)
Epoch 104, accuracy: 0.5266
Epoch 104, Train Loss: 0.6992, Val Loss: 0.9013
batch size: (890, 890)
Epoch 105, accuracy: 0.5248
batch size: (893, 893)
Epoch 106, accuracy: 0.5279
Epoch 106, Train Loss: 0.7037, Val Loss: 0.8979
batch size: (893, 893)
Epoch 107, accuracy: 0.5245
batch size: (911, 911)
Epoch 108, accuracy: 0.5269
Epoch 108, Train Loss: 0.6978, Val Loss: 0.9000
batch size: (904, 904)
Epoch 109, accuracy: 0.5267
batch size: (890, 890)
Epoch 110, accuracy: 0.5248
Epoch 110, Train Loss: 0.6854, Val Loss: 0.9016
batch size: (898, 898)
Epoch 111, accuracy: 0.5256
batch size: (909, 909)
Epoch 112, accuracy: 0.5254
Epoch 112, Train Loss: 0.7204, Val Loss: 0.9078
batch size: (887, 887)
Epoch 113, accuracy: 0.5278
batch size: (889, 889)
Epoch 114, accuracy: 0.5300
Epoch 114, Train Loss: 0.6978, Val Loss: 0.8976
batch size: (898, 898)
Epoch 115, accuracy: 0.5281
batch size: (891, 891)
Epoch 116, accuracy: 0.5262
Epoch 116, Train Loss: 0.7075, Val Loss: 0.9007
batch size: (887, 887)
Epoch 117, accuracy: 0.5328
batch size: (894, 894)
Epoch 118, accuracy: 0.5251
Epoch 118, Train Loss: 0.7043, Val Loss: 0.8963
batch size: (876, 876)
Epoch 119, accuracy: 0.5282
batch size: (900, 900)
Epoch 120, accuracy: 0.5260
Epoch 120, Train Loss: 0.6924, Val Loss: 0.9080
batch size: (905, 905)
Epoch 121, accuracy: 0.5247
batch size: (901, 901)
Epoch 122, accuracy: 0.5283
Epoch 122, Train Loss: 0.7138, Val Loss: 0.9003
batch size: (901, 901)
Epoch 123, accuracy: 0.5247
batch size: (908, 908)
Epoch 124, accuracy: 0.5278
Epoch 124, Train Loss: 0.7079, Val Loss: 0.8982
batch size: (918, 918)
Epoch 125, accuracy: 0.5259
batch size: (898, 898)
Epoch 126, accuracy: 0.5253
Epoch 126, Train Loss: 0.6756, Val Loss: 0.9076
batch size: (902, 902)
Epoch 127, accuracy: 0.5263
batch size: (915, 915)
Epoch 128, accuracy: 0.5271
Epoch 128, Train Loss: 0.6879, Val Loss: 0.9001
batch size: (904, 904)
Epoch 129, accuracy: 0.5275
batch size: (896, 896)
Epoch 130, accuracy: 0.5295
Epoch 130, Train Loss: 0.7004, Val Loss: 0.9033
batch size: (898, 898)
Epoch 131, accuracy: 0.5235
batch size: (914, 914)
Epoch 132, accuracy: 0.5282
Epoch 132, Train Loss: 0.6995, Val Loss: 0.9036
batch size: (885, 885)
Epoch 133, accuracy: 0.5291
batch size: (907, 907)
Epoch 134, accuracy: 0.5224
Epoch 134, Train Loss: 0.7392, Val Loss: 0.8979
batch size: (900, 900)
Epoch 135, accuracy: 0.5280
batch size: (895, 895)
Epoch 136, accuracy: 0.5272
Epoch 136, Train Loss: 0.7081, Val Loss: 0.8981
batch size: (909, 909)
Epoch 137, accuracy: 0.5227
batch size: (905, 905)
Epoch 138, accuracy: 0.5275
Epoch 138, Train Loss: 0.7309, Val Loss: 0.8949
batch size: (899, 899)
Epoch 139, accuracy: 0.5264
batch size: (889, 889)
Epoch 140, accuracy: 0.5258
Epoch 140, Train Loss: 0.7115, Val Loss: 0.8935
batch size: (885, 885)
Epoch 141, accuracy: 0.5245
batch size: (894, 894)
Epoch 142, accuracy: 0.5219
Epoch 142, Train Loss: 0.7224, Val Loss: 0.9010
batch size: (902, 902)
Epoch 143, accuracy: 0.5264
batch size: (878, 878)
Epoch 144, accuracy: 0.5278
Epoch 144, Train Loss: 0.7072, Val Loss: 0.8904
batch size: (887, 887)
Epoch 145, accuracy: 0.5254
batch size: (877, 877)
Epoch 146, accuracy: 0.5240
Epoch 146, Train Loss: 0.7270, Val Loss: 0.9008
batch size: (888, 888)
Epoch 147, accuracy: 0.5279
batch size: (918, 918)
Epoch 148, accuracy: 0.5272
Epoch 148, Train Loss: 0.7402, Val Loss: 0.9053
batch size: (899, 899)
Epoch 149, accuracy: 0.5263
batch size: (871, 871)
Epoch 150, accuracy: 0.5260
Epoch 150, Train Loss: 0.7143, Val Loss: 0.8950
batch size: (880, 880)
Epoch 151, accuracy: 0.5243
batch size: (918, 918)
Epoch 152, accuracy: 0.5257
Epoch 152, Train Loss: 0.7270, Val Loss: 0.8978
batch size: (909, 909)
Epoch 153, accuracy: 0.5250
batch size: (918, 918)
Epoch 154, accuracy: 0.5278
Epoch 154, Train Loss: 0.7151, Val Loss: 0.9025
batch size: (888, 888)
Epoch 155, accuracy: 0.5308
batch size: (892, 892)
Epoch 156, accuracy: 0.5237
Epoch 156, Train Loss: 0.7096, Val Loss: 0.8969
batch size: (908, 908)
Epoch 157, accuracy: 0.5273
batch size: (888, 888)
Epoch 158, accuracy: 0.5282
Epoch 158, Train Loss: 0.6821, Val Loss: 0.9052
batch size: (895, 895)
Epoch 159, accuracy: 0.5283
batch size: (912, 912)
Epoch 160, accuracy: 0.5246
Epoch 160, Train Loss: 0.7487, Val Loss: 0.8977
batch size: (909, 909)
Epoch 161, accuracy: 0.5287
batch size: (887, 887)
Epoch 162, accuracy: 0.5289
Epoch 162, Train Loss: 0.7111, Val Loss: 0.8956
batch size: (883, 883)
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 163, accuracy: 0.5262
batch size: (880, 880)
Epoch 164, accuracy: 0.5257
Epoch 164, Train Loss: 0.6830, Val Loss: 0.9059
batch size: (884, 884)
Epoch 165, accuracy: 0.5258
batch size: (901, 901)
Epoch 166, accuracy: 0.5265
Epoch 166, Train Loss: 0.7018, Val Loss: 0.9014
batch size: (874, 874)
Epoch 167, accuracy: 0.5272
batch size: (904, 904)
Epoch 168, accuracy: 0.5241
Epoch 168, Train Loss: 0.7418, Val Loss: 0.9000
batch size: (911, 911)
Epoch 169, accuracy: 0.5281
batch size: (914, 914)
Epoch 170, accuracy: 0.5286
Epoch 170, Train Loss: 0.7090, Val Loss: 0.8932
batch size: (902, 902)
Epoch 171, accuracy: 0.5229
batch size: (922, 922)
Epoch 172, accuracy: 0.5242
Epoch 172, Train Loss: 0.6851, Val Loss: 0.9050
batch size: (910, 910)
Epoch 173, accuracy: 0.5261
batch size: (902, 902)
Epoch 174, accuracy: 0.5251
Epoch 174, Train Loss: 0.7243, Val Loss: 0.9005
batch size: (903, 903)
Epoch 175, accuracy: 0.5280
batch size: (897, 897)
Epoch 176, accuracy: 0.5267
Epoch 176, Train Loss: 0.7285, Val Loss: 0.8903
batch size: (903, 903)
Epoch 177, accuracy: 0.5228
batch size: (912, 912)
Epoch 178, accuracy: 0.5256
Epoch 178, Train Loss: 0.7323, Val Loss: 0.9022
batch size: (903, 903)
Epoch 179, accuracy: 0.5246
batch size: (915, 915)
Epoch 180, accuracy: 0.5268
Epoch 180, Train Loss: 0.7135, Val Loss: 0.8937
batch size: (920, 920)
Epoch 181, accuracy: 0.5286
batch size: (891, 891)
Epoch 182, accuracy: 0.5288
Epoch 182, Train Loss: 0.7103, Val Loss: 0.9003
batch size: (903, 903)
Epoch 183, accuracy: 0.5240
batch size: (902, 902)
Epoch 184, accuracy: 0.5264
Epoch 184, Train Loss: 0.6867, Val Loss: 0.8963
batch size: (882, 882)
Epoch 185, accuracy: 0.5250
batch size: (906, 906)
Epoch 186, accuracy: 0.5268
Epoch 186, Train Loss: 0.6997, Val Loss: 0.9004
batch size: (901, 901)
Epoch 187, accuracy: 0.5263
batch size: (897, 897)
Epoch 188, accuracy: 0.5260
Epoch 188, Train Loss: 0.6659, Val Loss: 0.9003
batch size: (890, 890)
Epoch 189, accuracy: 0.5272
batch size: (887, 887)
Epoch 190, accuracy: 0.5279
Epoch 190, Train Loss: 0.7067, Val Loss: 0.8941
batch size: (900, 900)
Epoch 191, accuracy: 0.5279
batch size: (909, 909)
Epoch 192, accuracy: 0.5252
Epoch 192, Train Loss: 0.6943, Val Loss: 0.9003
batch size: (907, 907)
Epoch 193, accuracy: 0.5252
batch size: (912, 912)
Epoch 194, accuracy: 0.5291
Epoch 194, Train Loss: 0.7153, Val Loss: 0.9014
batch size: (895, 895)
Epoch 195, accuracy: 0.5260
batch size: (895, 895)
Epoch 196, accuracy: 0.5259
Epoch 196, Train Loss: 0.7122, Val Loss: 0.9015
batch size: (901, 901)
Epoch 197, accuracy: 0.5244
batch size: (897, 897)
Epoch 198, accuracy: 0.5254
Epoch 198, Train Loss: 0.6765, Val Loss: 0.9109
batch size: (884, 884)
Epoch 199, accuracy: 0.5248
Loaded best model with val_loss = 0.8855235576629639
test :accuracy 0.5395, f1_macro: 0.4810, f1_micro: 0.5395, auc: 0.7447
Training resGCN with 8 layers...
可训练参数: 1850386_resGCN
不可训练参数: 0
batch size: (899, 899)
✅ Epoch 0: New best model saved with val_loss = 1.0980
Epoch 0, accuracy: 0.3897
Epoch 0, Train Loss: 1.1211, Val Loss: 1.0980
batch size: (903, 903)
✅ Epoch 1: New best model saved with val_loss = 1.0968
Epoch 1, accuracy: 0.4317
batch size: (894, 894)
✅ Epoch 2: New best model saved with val_loss = 1.0961
Epoch 2, accuracy: 0.4297
Epoch 2, Train Loss: 1.0986, Val Loss: 1.0961
batch size: (904, 904)
✅ Epoch 3: New best model saved with val_loss = 1.0955
Epoch 3, accuracy: 0.4292
batch size: (905, 905)
✅ Epoch 4: New best model saved with val_loss = 1.0951
Epoch 4, accuracy: 0.4296
Epoch 4, Train Loss: 1.0987, Val Loss: 1.0951
batch size: (904, 904)
✅ Epoch 5: New best model saved with val_loss = 1.0947
Epoch 5, accuracy: 0.4314
batch size: (912, 912)
✅ Epoch 6: New best model saved with val_loss = 1.0944
Epoch 6, accuracy: 0.4282
Epoch 6, Train Loss: 1.0987, Val Loss: 1.0944
batch size: (901, 901)
✅ Epoch 7: New best model saved with val_loss = 1.0942
Epoch 7, accuracy: 0.4308
batch size: (879, 879)
✅ Epoch 8: New best model saved with val_loss = 1.0941
Epoch 8, accuracy: 0.4263
Epoch 8, Train Loss: 1.0988, Val Loss: 1.0941
batch size: (911, 911)
✅ Epoch 9: New best model saved with val_loss = 1.0940
Epoch 9, accuracy: 0.4288
batch size: (894, 894)
✅ Epoch 10: New best model saved with val_loss = 1.0940
Epoch 10, accuracy: 0.4316
Epoch 10, Train Loss: 1.0988, Val Loss: 1.0940
batch size: (892, 892)
Epoch 11, accuracy: 0.4306
batch size: (887, 887)
Epoch 12, accuracy: 0.4279
Epoch 12, Train Loss: 1.0988, Val Loss: 1.0941
batch size: (907, 907)
Epoch 13, accuracy: 0.4298
batch size: (899, 899)
Epoch 14, accuracy: 0.4291
Epoch 14, Train Loss: 1.0988, Val Loss: 1.0943
batch size: (905, 905)
Epoch 15, accuracy: 0.4308
batch size: (884, 884)
Epoch 16, accuracy: 0.4300
Epoch 16, Train Loss: 1.0987, Val Loss: 1.0946
batch size: (893, 893)
Epoch 17, accuracy: 0.4295
batch size: (916, 916)
Epoch 18, accuracy: 0.4304
Epoch 18, Train Loss: 1.0987, Val Loss: 1.0946
batch size: (909, 909)
Epoch 19, accuracy: 0.4296
batch size: (894, 894)
Epoch 20, accuracy: 0.4323
Epoch 20, Train Loss: 1.0987, Val Loss: 1.0946
batch size: (900, 900)
Epoch 21, accuracy: 0.4302
batch size: (901, 901)
Epoch 22, accuracy: 0.4303
Epoch 22, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (904, 904)
Epoch 23, accuracy: 0.4299
batch size: (889, 889)
Epoch 24, accuracy: 0.4310
Epoch 24, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (914, 914)
Epoch 25, accuracy: 0.4299
batch size: (892, 892)
Epoch 26, accuracy: 0.4305
Epoch 26, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (905, 905)
Epoch 27, accuracy: 0.4339
batch size: (896, 896)
Epoch 28, accuracy: 0.4285
Epoch 28, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (898, 898)
Epoch 29, accuracy: 0.4299
batch size: (904, 904)
Epoch 30, accuracy: 0.4295
Epoch 30, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (908, 908)
Epoch 31, accuracy: 0.4300
batch size: (909, 909)
Epoch 32, accuracy: 0.4325
Epoch 32, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (898, 898)
Epoch 33, accuracy: 0.4291
batch size: (884, 884)
Epoch 34, accuracy: 0.4287
Epoch 34, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (896, 896)
Epoch 35, accuracy: 0.4274
batch size: (886, 886)
Epoch 36, accuracy: 0.4261
Epoch 36, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (898, 898)
Epoch 37, accuracy: 0.4282
batch size: (898, 898)
Epoch 38, accuracy: 0.4271
Epoch 38, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (886, 886)
Epoch 39, accuracy: 0.4308
batch size: (893, 893)
Epoch 40, accuracy: 0.4272
Epoch 40, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (885, 885)
Epoch 41, accuracy: 0.4264
batch size: (901, 901)
Epoch 42, accuracy: 0.4314
Epoch 42, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (891, 891)
Epoch 43, accuracy: 0.4296
batch size: (897, 897)
Epoch 44, accuracy: 0.4310
Epoch 44, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (896, 896)
Epoch 45, accuracy: 0.4306
batch size: (903, 903)
Epoch 46, accuracy: 0.4307
Epoch 46, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (871, 871)
Epoch 47, accuracy: 0.4267
batch size: (892, 892)
Epoch 48, accuracy: 0.4313
Epoch 48, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (905, 905)
Epoch 49, accuracy: 0.4292
batch size: (902, 902)
Epoch 50, accuracy: 0.4319
Epoch 50, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (877, 877)
Epoch 51, accuracy: 0.4340
batch size: (885, 885)
Epoch 52, accuracy: 0.4300
Epoch 52, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (895, 895)
Epoch 53, accuracy: 0.4313
batch size: (898, 898)
Epoch 54, accuracy: 0.4308
Epoch 54, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (926, 926)
Epoch 55, accuracy: 0.4294
batch size: (902, 902)
Epoch 56, accuracy: 0.4301
Epoch 56, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (897, 897)
Epoch 57, accuracy: 0.4275
batch size: (895, 895)
Epoch 58, accuracy: 0.4300
Epoch 58, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (900, 900)
Epoch 59, accuracy: 0.4308
batch size: (897, 897)
Epoch 60, accuracy: 0.4293
Epoch 60, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (894, 894)
Epoch 61, accuracy: 0.4281
batch size: (912, 912)
Epoch 62, accuracy: 0.4281
Epoch 62, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (890, 890)
Epoch 63, accuracy: 0.4283
batch size: (892, 892)
Epoch 64, accuracy: 0.4261
Epoch 64, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (896, 896)
Epoch 65, accuracy: 0.4289
batch size: (897, 897)
Epoch 66, accuracy: 0.4295
Epoch 66, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (884, 884)
Epoch 67, accuracy: 0.4307
batch size: (899, 899)
Epoch 68, accuracy: 0.4266
Epoch 68, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (886, 886)
Epoch 69, accuracy: 0.4281
batch size: (887, 887)
Epoch 70, accuracy: 0.4283
Epoch 70, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (906, 906)
Epoch 71, accuracy: 0.4360
batch size: (891, 891)
Epoch 72, accuracy: 0.4265
Epoch 72, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (901, 901)
Epoch 73, accuracy: 0.4276
batch size: (903, 903)
Epoch 74, accuracy: 0.4288
Epoch 74, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (921, 921)
Epoch 75, accuracy: 0.4268
batch size: (901, 901)
Epoch 76, accuracy: 0.4306
Epoch 76, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (893, 893)
Epoch 77, accuracy: 0.4330
batch size: (919, 919)
Epoch 78, accuracy: 0.4287
Epoch 78, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (898, 898)
Epoch 79, accuracy: 0.4310
batch size: (912, 912)
Epoch 80, accuracy: 0.4326
Epoch 80, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (890, 890)
Epoch 81, accuracy: 0.4313
batch size: (900, 900)
Epoch 82, accuracy: 0.4240
Epoch 82, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (897, 897)
Epoch 83, accuracy: 0.4294
batch size: (902, 902)
Epoch 84, accuracy: 0.4262
Epoch 84, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (891, 891)
Epoch 85, accuracy: 0.4261
batch size: (917, 917)
Epoch 86, accuracy: 0.4311
Epoch 86, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (902, 902)
Epoch 87, accuracy: 0.4306
batch size: (904, 904)
Epoch 88, accuracy: 0.4271
Epoch 88, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (898, 898)
Epoch 89, accuracy: 0.4260
batch size: (893, 893)
Epoch 90, accuracy: 0.4228
Epoch 90, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (905, 905)
Epoch 91, accuracy: 0.4273
batch size: (913, 913)
Epoch 92, accuracy: 0.4280
Epoch 92, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (907, 907)
Epoch 93, accuracy: 0.4330
batch size: (900, 900)
Epoch 94, accuracy: 0.4317
Epoch 94, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (887, 887)
Epoch 95, accuracy: 0.4287
batch size: (883, 883)
Epoch 96, accuracy: 0.4309
Epoch 96, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (902, 902)
Epoch 97, accuracy: 0.4297
batch size: (900, 900)
Epoch 98, accuracy: 0.4317
Epoch 98, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (900, 900)
Epoch 99, accuracy: 0.4338
batch size: (890, 890)
Epoch 100, accuracy: 0.4298
Epoch 100, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (906, 906)
Epoch 101, accuracy: 0.4279
batch size: (915, 915)
Epoch 102, accuracy: 0.4281
Epoch 102, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (907, 907)
Epoch 103, accuracy: 0.4316
batch size: (937, 937)
Epoch 104, accuracy: 0.4340
Epoch 104, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (886, 886)
Epoch 105, accuracy: 0.4304
batch size: (897, 897)
Epoch 106, accuracy: 0.4290
Epoch 106, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (912, 912)
Epoch 107, accuracy: 0.4298
batch size: (913, 913)
Epoch 108, accuracy: 0.4335
Epoch 108, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (891, 891)
Epoch 109, accuracy: 0.4314
batch size: (891, 891)
Epoch 110, accuracy: 0.4309
Epoch 110, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (921, 921)
Epoch 111, accuracy: 0.4318
batch size: (873, 873)
Epoch 112, accuracy: 0.4298
Epoch 112, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (896, 896)
Epoch 113, accuracy: 0.4341
batch size: (885, 885)
Epoch 114, accuracy: 0.4325
Epoch 114, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (886, 886)
Epoch 115, accuracy: 0.4316
batch size: (886, 886)
Epoch 116, accuracy: 0.4279
Epoch 116, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (895, 895)
Epoch 117, accuracy: 0.4290
batch size: (907, 907)
Epoch 118, accuracy: 0.4319
Epoch 118, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (923, 923)
Epoch 119, accuracy: 0.4350
batch size: (893, 893)
Epoch 120, accuracy: 0.4328
Epoch 120, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (910, 910)
Epoch 121, accuracy: 0.4305
batch size: (904, 904)
Epoch 122, accuracy: 0.4309
Epoch 122, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (895, 895)
Epoch 123, accuracy: 0.4280
batch size: (894, 894)
Epoch 124, accuracy: 0.4287
Epoch 124, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (908, 908)
Epoch 125, accuracy: 0.4314
batch size: (909, 909)
Epoch 126, accuracy: 0.4312
Epoch 126, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (910, 910)
Epoch 127, accuracy: 0.4307
batch size: (894, 894)
Epoch 128, accuracy: 0.4326
Epoch 128, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (913, 913)
Epoch 129, accuracy: 0.4280
batch size: (900, 900)
Epoch 130, accuracy: 0.4307
Epoch 130, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (892, 892)
Epoch 131, accuracy: 0.4310
batch size: (893, 893)
Epoch 132, accuracy: 0.4339
Epoch 132, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (897, 897)
Epoch 133, accuracy: 0.4336
batch size: (889, 889)
Epoch 134, accuracy: 0.4330
Epoch 134, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (923, 923)
Epoch 135, accuracy: 0.4321
batch size: (909, 909)
Epoch 136, accuracy: 0.4301
Epoch 136, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (883, 883)
Epoch 137, accuracy: 0.4317
batch size: (892, 892)
Epoch 138, accuracy: 0.4286
Epoch 138, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (929, 929)
Epoch 139, accuracy: 0.4300
batch size: (896, 896)
Epoch 140, accuracy: 0.4284
Epoch 140, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (902, 902)
Epoch 141, accuracy: 0.4282
batch size: (908, 908)
Epoch 142, accuracy: 0.4286
Epoch 142, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (906, 906)
Epoch 143, accuracy: 0.4244
batch size: (900, 900)
Epoch 144, accuracy: 0.4293
Epoch 144, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (903, 903)
Epoch 145, accuracy: 0.4272
batch size: (916, 916)
Epoch 146, accuracy: 0.4274
Epoch 146, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (905, 905)
Epoch 147, accuracy: 0.4311
batch size: (902, 902)
Epoch 148, accuracy: 0.4303
Epoch 148, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (902, 902)
Epoch 149, accuracy: 0.4300
batch size: (894, 894)
Epoch 150, accuracy: 0.4315
Epoch 150, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (887, 887)
Epoch 151, accuracy: 0.4292
batch size: (909, 909)
Epoch 152, accuracy: 0.4277
Epoch 152, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (897, 897)
Epoch 153, accuracy: 0.4327
batch size: (893, 893)
Epoch 154, accuracy: 0.4293
Epoch 154, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (920, 920)
Epoch 155, accuracy: 0.4323
batch size: (907, 907)
Epoch 156, accuracy: 0.4314
Epoch 156, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (892, 892)
Epoch 157, accuracy: 0.4302
batch size: (904, 904)
Epoch 158, accuracy: 0.4306
Epoch 158, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (902, 902)
Epoch 159, accuracy: 0.4325
batch size: (913, 913)
Epoch 160, accuracy: 0.4279
Epoch 160, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (904, 904)
Epoch 161, accuracy: 0.4358
batch size: (888, 888)
Epoch 162, accuracy: 0.4358
Epoch 162, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (906, 906)
Epoch 163, accuracy: 0.4301
batch size: (906, 906)
Epoch 164, accuracy: 0.4246
Epoch 164, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (887, 887)
Epoch 165, accuracy: 0.4302
batch size: (878, 878)
Epoch 166, accuracy: 0.4288
Epoch 166, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (893, 893)
Epoch 167, accuracy: 0.4292
batch size: (898, 898)
Epoch 168, accuracy: 0.4292
Epoch 168, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (908, 908)
Epoch 169, accuracy: 0.4276
batch size: (919, 919)
Epoch 170, accuracy: 0.4303
Epoch 170, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (891, 891)
Epoch 171, accuracy: 0.4288
batch size: (882, 882)
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 172, accuracy: 0.4297
Epoch 172, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (893, 893)
Epoch 173, accuracy: 0.4358
batch size: (900, 900)
Epoch 174, accuracy: 0.4305
Epoch 174, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (895, 895)
Epoch 175, accuracy: 0.4296
batch size: (911, 911)
Epoch 176, accuracy: 0.4296
Epoch 176, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (899, 899)
Epoch 177, accuracy: 0.4316
batch size: (894, 894)
Epoch 178, accuracy: 0.4321
Epoch 178, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (912, 912)
Epoch 179, accuracy: 0.4301
batch size: (906, 906)
Epoch 180, accuracy: 0.4274
Epoch 180, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (902, 902)
Epoch 181, accuracy: 0.4304
batch size: (910, 910)
Epoch 182, accuracy: 0.4254
Epoch 182, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (864, 864)
Epoch 183, accuracy: 0.4278
batch size: (904, 904)
Epoch 184, accuracy: 0.4311
Epoch 184, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (893, 893)
Epoch 185, accuracy: 0.4326
batch size: (893, 893)
Epoch 186, accuracy: 0.4286
Epoch 186, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (903, 903)
Epoch 187, accuracy: 0.4314
batch size: (902, 902)
Epoch 188, accuracy: 0.4299
Epoch 188, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (898, 898)
Epoch 189, accuracy: 0.4319
batch size: (892, 892)
Epoch 190, accuracy: 0.4314
Epoch 190, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (893, 893)
Epoch 191, accuracy: 0.4300
batch size: (897, 897)
Epoch 192, accuracy: 0.4322
Epoch 192, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (902, 902)
Epoch 193, accuracy: 0.4294
batch size: (900, 900)
Epoch 194, accuracy: 0.4323
Epoch 194, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (896, 896)
Epoch 195, accuracy: 0.4320
batch size: (886, 886)
Epoch 196, accuracy: 0.4277
Epoch 196, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (891, 891)
Epoch 197, accuracy: 0.4329
batch size: (886, 886)
Epoch 198, accuracy: 0.4281
Epoch 198, Train Loss: 1.0987, Val Loss: 1.0947
batch size: (887, 887)
Epoch 199, accuracy: 0.4322
Loaded best model with val_loss = 1.0939942598342896
test :accuracy 0.4294, f1_macro: 0.2003, f1_micro: 0.4294, auc: 0.5000
Training resGCN with 32 layers...
可训练参数: 573202_resGCN
不可训练参数: 0
batch size: (896, 896)
✅ Epoch 0: New best model saved with val_loss = 1.0995
Epoch 0, accuracy: 0.1631
Epoch 0, Train Loss: 253170.9531, Val Loss: 1.0995
batch size: (897, 897)
Epoch 1, accuracy: 0.1651
batch size: (901, 901)
Epoch 2, accuracy: 0.1675
Epoch 2, Train Loss: 7.2237, Val Loss: 1.1008
batch size: (897, 897)
Epoch 3, accuracy: 0.1682
batch size: (914, 914)
Epoch 4, accuracy: 0.1684
Epoch 4, Train Loss: 1.0987, Val Loss: 1.1017
batch size: (888, 888)
Epoch 5, accuracy: 0.1680
batch size: (896, 896)
Epoch 6, accuracy: 0.1654
Epoch 6, Train Loss: 1.0988, Val Loss: 1.1024
batch size: (885, 885)
Epoch 7, accuracy: 0.1656
batch size: (903, 903)
Epoch 8, accuracy: 0.1661
Epoch 8, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (909, 909)
Epoch 9, accuracy: 0.1672
batch size: (888, 888)
Epoch 10, accuracy: 0.1660
Epoch 10, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (903, 903)
Epoch 11, accuracy: 0.1687
batch size: (924, 924)
Epoch 12, accuracy: 0.1680
Epoch 12, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (898, 898)
Epoch 13, accuracy: 0.1689
batch size: (891, 891)
Epoch 14, accuracy: 0.1692
Epoch 14, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (891, 891)
Epoch 15, accuracy: 0.1662
batch size: (890, 890)
Epoch 16, accuracy: 0.1702
Epoch 16, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (907, 907)
Epoch 17, accuracy: 0.1670
batch size: (911, 911)
Epoch 18, accuracy: 0.1667
Epoch 18, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (899, 899)
Epoch 19, accuracy: 0.1688
batch size: (900, 900)
Epoch 20, accuracy: 0.1649
Epoch 20, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (897, 897)
Epoch 21, accuracy: 0.1664
batch size: (904, 904)
Epoch 22, accuracy: 0.1655
Epoch 22, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (895, 895)
Epoch 23, accuracy: 0.1659
batch size: (903, 903)
Epoch 24, accuracy: 0.1683
Epoch 24, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (894, 894)
Epoch 25, accuracy: 0.1680
batch size: (914, 914)
Epoch 26, accuracy: 0.1671
Epoch 26, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (903, 903)
Epoch 27, accuracy: 0.1698
batch size: (889, 889)
Epoch 28, accuracy: 0.1681
Epoch 28, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (896, 896)
Epoch 29, accuracy: 0.1675
batch size: (901, 901)
Epoch 30, accuracy: 0.1697
Epoch 30, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (880, 880)
Epoch 31, accuracy: 0.1695
batch size: (902, 902)
Epoch 32, accuracy: 0.1693
Epoch 32, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (900, 900)
Epoch 33, accuracy: 0.1647
batch size: (896, 896)
Epoch 34, accuracy: 0.1687
Epoch 34, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (884, 884)
Epoch 35, accuracy: 0.1679
batch size: (903, 903)
Epoch 36, accuracy: 0.1665
Epoch 36, Train Loss: 4.3560, Val Loss: 1.1025
batch size: (907, 907)
Epoch 37, accuracy: 0.1693
batch size: (880, 880)
Epoch 38, accuracy: 0.1659
Epoch 38, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (910, 910)
Epoch 39, accuracy: 0.1707
batch size: (900, 900)
Epoch 40, accuracy: 0.1694
Epoch 40, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (870, 870)
Epoch 41, accuracy: 0.1680
batch size: (887, 887)
Epoch 42, accuracy: 0.1682
Epoch 42, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (908, 908)
Epoch 43, accuracy: 0.1699
batch size: (896, 896)
Epoch 44, accuracy: 0.1695
Epoch 44, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (890, 890)
Epoch 45, accuracy: 0.1675
batch size: (902, 902)
Epoch 46, accuracy: 0.1651
Epoch 46, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (907, 907)
Epoch 47, accuracy: 0.1679
batch size: (898, 898)
Epoch 48, accuracy: 0.1668
Epoch 48, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (893, 893)
Epoch 49, accuracy: 0.1694
batch size: (913, 913)
Epoch 50, accuracy: 0.1661
Epoch 50, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (898, 898)
Epoch 51, accuracy: 0.1683
batch size: (899, 899)
Epoch 52, accuracy: 0.1668
Epoch 52, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (911, 911)
Epoch 53, accuracy: 0.1653
batch size: (899, 899)
Epoch 54, accuracy: 0.1689
Epoch 54, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (898, 898)
Epoch 55, accuracy: 0.1665
batch size: (897, 897)
Epoch 56, accuracy: 0.1663
Epoch 56, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (915, 915)
Epoch 57, accuracy: 0.1684
batch size: (905, 905)
Epoch 58, accuracy: 0.1696
Epoch 58, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (919, 919)
Epoch 59, accuracy: 0.1688
batch size: (896, 896)
Epoch 60, accuracy: 0.1659
Epoch 60, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (883, 883)
Epoch 61, accuracy: 0.1673
batch size: (916, 916)
Epoch 62, accuracy: 0.1710
Epoch 62, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (918, 918)
Epoch 63, accuracy: 0.1680
batch size: (893, 893)
Epoch 64, accuracy: 0.1667
Epoch 64, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (907, 907)
Epoch 65, accuracy: 0.1639
batch size: (918, 918)
Epoch 66, accuracy: 0.1659
Epoch 66, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (904, 904)
Epoch 67, accuracy: 0.1704
batch size: (904, 904)
Epoch 68, accuracy: 0.1639
Epoch 68, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (909, 909)
Epoch 69, accuracy: 0.1684
batch size: (908, 908)
Epoch 70, accuracy: 0.1667
Epoch 70, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (883, 883)
Epoch 71, accuracy: 0.1682
batch size: (894, 894)
Epoch 72, accuracy: 0.1685
Epoch 72, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (904, 904)
Epoch 73, accuracy: 0.1657
batch size: (902, 902)
Epoch 74, accuracy: 0.1656
Epoch 74, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (884, 884)
Epoch 75, accuracy: 0.1674
batch size: (886, 886)
Epoch 76, accuracy: 0.1671
Epoch 76, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (909, 909)
Epoch 77, accuracy: 0.1648
batch size: (896, 896)
Epoch 78, accuracy: 0.1713
Epoch 78, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (890, 890)
Epoch 79, accuracy: 0.1657
batch size: (905, 905)
Epoch 80, accuracy: 0.1678
Epoch 80, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (894, 894)
Epoch 81, accuracy: 0.1632
batch size: (896, 896)
Epoch 82, accuracy: 0.1683
Epoch 82, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (913, 913)
Epoch 83, accuracy: 0.1675
batch size: (881, 881)
Epoch 84, accuracy: 0.1730
Epoch 84, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (876, 876)
Epoch 85, accuracy: 0.1650
batch size: (885, 885)
Epoch 86, accuracy: 0.1684
Epoch 86, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (910, 910)
Epoch 87, accuracy: 0.1650
batch size: (891, 891)
Epoch 88, accuracy: 0.1650
Epoch 88, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (897, 897)
Epoch 89, accuracy: 0.1707
batch size: (911, 911)
Epoch 90, accuracy: 0.1671
Epoch 90, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (894, 894)
Epoch 91, accuracy: 0.1693
batch size: (893, 893)
Epoch 92, accuracy: 0.1687
Epoch 92, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (887, 887)
Epoch 93, accuracy: 0.1717
batch size: (900, 900)
Epoch 94, accuracy: 0.1676
Epoch 94, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (896, 896)
Epoch 95, accuracy: 0.1669
batch size: (919, 919)
Epoch 96, accuracy: 0.1678
Epoch 96, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (892, 892)
Epoch 97, accuracy: 0.1674
batch size: (903, 903)
Epoch 98, accuracy: 0.1673
Epoch 98, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (902, 902)
Epoch 99, accuracy: 0.1687
batch size: (896, 896)
Epoch 100, accuracy: 0.1682
Epoch 100, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (890, 890)
Epoch 101, accuracy: 0.1658
batch size: (869, 869)
Epoch 102, accuracy: 0.1667
Epoch 102, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (911, 911)
Epoch 103, accuracy: 0.1681
batch size: (890, 890)
Epoch 104, accuracy: 0.1643
Epoch 104, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (894, 894)
Epoch 105, accuracy: 0.1686
batch size: (894, 894)
Epoch 106, accuracy: 0.1664
Epoch 106, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (898, 898)
Epoch 107, accuracy: 0.1698
batch size: (905, 905)
Epoch 108, accuracy: 0.1694
Epoch 108, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (896, 896)
Epoch 109, accuracy: 0.1686
batch size: (898, 898)
Epoch 110, accuracy: 0.1673
Epoch 110, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (896, 896)
Epoch 111, accuracy: 0.1665
batch size: (890, 890)
Epoch 112, accuracy: 0.1700
Epoch 112, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (897, 897)
Epoch 113, accuracy: 0.1670
batch size: (899, 899)
Epoch 114, accuracy: 0.1654
Epoch 114, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (907, 907)
Epoch 115, accuracy: 0.1658
batch size: (911, 911)
Epoch 116, accuracy: 0.1674
Epoch 116, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (881, 881)
Epoch 117, accuracy: 0.1662
batch size: (910, 910)
Epoch 118, accuracy: 0.1668
Epoch 118, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (896, 896)
Epoch 119, accuracy: 0.1690
batch size: (904, 904)
Epoch 120, accuracy: 0.1694
Epoch 120, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (892, 892)
Epoch 121, accuracy: 0.1656
batch size: (881, 881)
Epoch 122, accuracy: 0.1689
Epoch 122, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (881, 881)
Epoch 123, accuracy: 0.1640
batch size: (907, 907)
Epoch 124, accuracy: 0.1664
Epoch 124, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (914, 914)
Epoch 125, accuracy: 0.1665
batch size: (895, 895)
Epoch 126, accuracy: 0.1665
Epoch 126, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (900, 900)
Epoch 127, accuracy: 0.1654
batch size: (911, 911)
Epoch 128, accuracy: 0.1685
Epoch 128, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (890, 890)
Epoch 129, accuracy: 0.1660
batch size: (886, 886)
Epoch 130, accuracy: 0.1648
Epoch 130, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (885, 885)
Epoch 131, accuracy: 0.1684
batch size: (901, 901)
Epoch 132, accuracy: 0.1636
Epoch 132, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (891, 891)
Epoch 133, accuracy: 0.1659
batch size: (895, 895)
Epoch 134, accuracy: 0.1667
Epoch 134, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (887, 887)
Epoch 135, accuracy: 0.1672
batch size: (902, 902)
Epoch 136, accuracy: 0.1666
Epoch 136, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (898, 898)
Epoch 137, accuracy: 0.1689
batch size: (924, 924)
Epoch 138, accuracy: 0.1683
Epoch 138, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (909, 909)
Epoch 139, accuracy: 0.1680
batch size: (919, 919)
Epoch 140, accuracy: 0.1660
Epoch 140, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (907, 907)
Epoch 141, accuracy: 0.1656
batch size: (895, 895)
Epoch 142, accuracy: 0.1656
Epoch 142, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (912, 912)
Epoch 143, accuracy: 0.1652
batch size: (906, 906)
Epoch 144, accuracy: 0.1702
Epoch 144, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (903, 903)
Epoch 145, accuracy: 0.1672
batch size: (887, 887)
Epoch 146, accuracy: 0.1694
Epoch 146, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (901, 901)
Epoch 147, accuracy: 0.1662
batch size: (899, 899)
Epoch 148, accuracy: 0.1690
Epoch 148, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (904, 904)
Epoch 149, accuracy: 0.1700
batch size: (908, 908)
Epoch 150, accuracy: 0.1659
Epoch 150, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (888, 888)
Epoch 151, accuracy: 0.1670
batch size: (896, 896)
Epoch 152, accuracy: 0.1694
Epoch 152, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (910, 910)
Epoch 153, accuracy: 0.1693
batch size: (888, 888)
Epoch 154, accuracy: 0.1670
Epoch 154, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (896, 896)
Epoch 155, accuracy: 0.1670
batch size: (900, 900)
Epoch 156, accuracy: 0.1679
Epoch 156, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (885, 885)
Epoch 157, accuracy: 0.1694
batch size: (911, 911)
Epoch 158, accuracy: 0.1686
Epoch 158, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (896, 896)
Epoch 159, accuracy: 0.1659
batch size: (906, 906)
Epoch 160, accuracy: 0.1675
Epoch 160, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (898, 898)
Epoch 161, accuracy: 0.1673
batch size: (876, 876)
Epoch 162, accuracy: 0.1705
Epoch 162, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (888, 888)
Epoch 163, accuracy: 0.1682
batch size: (921, 921)
Epoch 164, accuracy: 0.1683
Epoch 164, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (882, 882)
Epoch 165, accuracy: 0.1692
batch size: (881, 881)
Epoch 166, accuracy: 0.1661
Epoch 166, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (904, 904)
Epoch 167, accuracy: 0.1677
batch size: (911, 911)
Epoch 168, accuracy: 0.1664
Epoch 168, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (904, 904)
Epoch 169, accuracy: 0.1683
batch size: (898, 898)
Epoch 170, accuracy: 0.1678
Epoch 170, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (892, 892)
Epoch 171, accuracy: 0.1677
batch size: (892, 892)
Epoch 172, accuracy: 0.1679
Epoch 172, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (924, 924)
Epoch 173, accuracy: 0.1697
batch size: (903, 903)
Epoch 174, accuracy: 0.1713
Epoch 174, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (891, 891)
Epoch 175, accuracy: 0.1648
batch size: (908, 908)
Epoch 176, accuracy: 0.1682
Epoch 176, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (883, 883)
Epoch 177, accuracy: 0.1685
batch size: (908, 908)
Epoch 178, accuracy: 0.1659
Epoch 178, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (903, 903)
Epoch 179, accuracy: 0.1649
batch size: (899, 899)
Epoch 180, accuracy: 0.1690
Epoch 180, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (907, 907)
Epoch 181, accuracy: 0.1623
batch size: (893, 893)
Epoch 182, accuracy: 0.1665
Epoch 182, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (898, 898)
Epoch 183, accuracy: 0.1666
batch size: (891, 891)
Epoch 184, accuracy: 0.1658
Epoch 184, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (898, 898)
Epoch 185, accuracy: 0.1668
batch size: (900, 900)
Epoch 186, accuracy: 0.1673
Epoch 186, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (888, 888)
Epoch 187, accuracy: 0.1682
batch size: (905, 905)
Epoch 188, accuracy: 0.1676
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 188, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (904, 904)
Epoch 189, accuracy: 0.1667
batch size: (889, 889)
Epoch 190, accuracy: 0.1694
Epoch 190, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (906, 906)
Epoch 191, accuracy: 0.1688
batch size: (900, 900)
Epoch 192, accuracy: 0.1676
Epoch 192, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (894, 894)
Epoch 193, accuracy: 0.1653
batch size: (911, 911)
Epoch 194, accuracy: 0.1675
Epoch 194, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (903, 903)
Epoch 195, accuracy: 0.1678
batch size: (916, 916)
Epoch 196, accuracy: 0.1674
Epoch 196, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (896, 896)
Epoch 197, accuracy: 0.1665
batch size: (896, 896)
Epoch 198, accuracy: 0.1671
Epoch 198, Train Loss: 1.0988, Val Loss: 1.1025
batch size: (902, 902)
Epoch 199, accuracy: 0.1717
Loaded best model with val_loss = 1.0994501113891602
test :accuracy 0.1656, f1_macro: 0.0947, f1_micro: 0.1656, auc: 0.5000
Training GINConv with 2 layers...
可训练参数: 265483_GINConv
不可训练参数: 0
batch size: (897, 897)
✅ Epoch 0: New best model saved with val_loss = 1.1285
Epoch 0, accuracy: 0.1874
Epoch 0, Train Loss: 1.7369, Val Loss: 1.1285
batch size: (898, 898)
✅ Epoch 1: New best model saved with val_loss = 1.0514
Epoch 1, accuracy: 0.4319
batch size: (898, 898)
✅ Epoch 2: New best model saved with val_loss = 1.0334
Epoch 2, accuracy: 0.4741
Epoch 2, Train Loss: 1.5214, Val Loss: 1.0334
batch size: (901, 901)
Epoch 3, accuracy: 0.4556
batch size: (893, 893)
Epoch 4, accuracy: 0.4235
Epoch 4, Train Loss: 0.4209, Val Loss: 1.0337
batch size: (907, 907)
✅ Epoch 5: New best model saved with val_loss = 1.0120
Epoch 5, accuracy: 0.4482
batch size: (903, 903)
✅ Epoch 6: New best model saved with val_loss = 0.9718
Epoch 6, accuracy: 0.4793
Epoch 6, Train Loss: 0.2766, Val Loss: 0.9718
batch size: (909, 909)
✅ Epoch 7: New best model saved with val_loss = 0.9327
Epoch 7, accuracy: 0.5192
batch size: (880, 880)
✅ Epoch 8: New best model saved with val_loss = 0.9011
Epoch 8, accuracy: 0.5544
Epoch 8, Train Loss: 0.1551, Val Loss: 0.9011
batch size: (913, 913)
✅ Epoch 9: New best model saved with val_loss = 0.8751
Epoch 9, accuracy: 0.5695
batch size: (897, 897)
✅ Epoch 10: New best model saved with val_loss = 0.8507
Epoch 10, accuracy: 0.5698
Epoch 10, Train Loss: 0.1309, Val Loss: 0.8507
batch size: (897, 897)
✅ Epoch 11: New best model saved with val_loss = 0.8307
Epoch 11, accuracy: 0.5627
batch size: (894, 894)
✅ Epoch 12: New best model saved with val_loss = 0.8242
Epoch 12, accuracy: 0.5548
Epoch 12, Train Loss: 0.0597, Val Loss: 0.8242
batch size: (909, 909)
✅ Epoch 13: New best model saved with val_loss = 0.8124
Epoch 13, accuracy: 0.5542
batch size: (898, 898)
Epoch 14, accuracy: 0.5552
Epoch 14, Train Loss: 0.0468, Val Loss: 0.8278
batch size: (893, 893)
Epoch 15, accuracy: 0.5610
batch size: (889, 889)
Epoch 16, accuracy: 0.5815
Epoch 16, Train Loss: 0.0269, Val Loss: 0.8261
batch size: (900, 900)
Epoch 17, accuracy: 0.5969
batch size: (900, 900)
✅ Epoch 18: New best model saved with val_loss = 0.7699
Epoch 18, accuracy: 0.6262
Epoch 18, Train Loss: 0.0428, Val Loss: 0.7699
batch size: (898, 898)
✅ Epoch 19: New best model saved with val_loss = 0.7349
Epoch 19, accuracy: 0.6489
batch size: (891, 891)
Epoch 20, accuracy: 0.6644
Epoch 20, Train Loss: 0.0081, Val Loss: 0.7451
batch size: (897, 897)
Epoch 21, accuracy: 0.6734
batch size: (903, 903)
Epoch 22, accuracy: 0.6806
Epoch 22, Train Loss: 0.0029, Val Loss: 0.7702
batch size: (903, 903)
Epoch 23, accuracy: 0.6894
batch size: (902, 902)
Epoch 24, accuracy: 0.6912
Epoch 24, Train Loss: 0.0043, Val Loss: 0.7909
batch size: (889, 889)
Epoch 25, accuracy: 0.6909
batch size: (897, 897)
Epoch 26, accuracy: 0.6876
Epoch 26, Train Loss: 0.0052, Val Loss: 0.8934
batch size: (886, 886)
Epoch 27, accuracy: 0.6833
batch size: (895, 895)
Epoch 28, accuracy: 0.6721
Epoch 28, Train Loss: 0.0021, Val Loss: 0.9240
batch size: (899, 899)
Epoch 29, accuracy: 0.6704
batch size: (898, 898)
Epoch 30, accuracy: 0.6609
Epoch 30, Train Loss: 0.0056, Val Loss: 0.9531
batch size: (901, 901)
Epoch 31, accuracy: 0.6563
batch size: (878, 878)
Epoch 32, accuracy: 0.6530
Epoch 32, Train Loss: 0.0462, Val Loss: 1.0449
batch size: (918, 918)
Epoch 33, accuracy: 0.6472
batch size: (913, 913)
Epoch 34, accuracy: 0.6436
Epoch 34, Train Loss: 0.0029, Val Loss: 1.1307
batch size: (916, 916)
Epoch 35, accuracy: 0.6406
batch size: (874, 874)
Epoch 36, accuracy: 0.6354
Epoch 36, Train Loss: 0.0038, Val Loss: 1.1245
batch size: (894, 894)
Epoch 37, accuracy: 0.6361
batch size: (913, 913)
Epoch 38, accuracy: 0.6275
Epoch 38, Train Loss: 0.0034, Val Loss: 1.2344
batch size: (901, 901)
Epoch 39, accuracy: 0.6307
batch size: (893, 893)
Epoch 40, accuracy: 0.6227
Epoch 40, Train Loss: 0.0018, Val Loss: 1.2640
batch size: (895, 895)
Epoch 41, accuracy: 0.6252
batch size: (884, 884)
Epoch 42, accuracy: 0.6244
Epoch 42, Train Loss: 0.0013, Val Loss: 1.4291
batch size: (906, 906)
Epoch 43, accuracy: 0.6231
batch size: (881, 881)
Epoch 44, accuracy: 0.6219
Epoch 44, Train Loss: 0.0019, Val Loss: 1.5215
batch size: (882, 882)
Epoch 45, accuracy: 0.6187
batch size: (896, 896)
Epoch 46, accuracy: 0.6189
Epoch 46, Train Loss: 0.0038, Val Loss: 1.6020
batch size: (894, 894)
Epoch 47, accuracy: 0.6208
batch size: (896, 896)
Epoch 48, accuracy: 0.6223
Epoch 48, Train Loss: 0.0022, Val Loss: 1.5738
batch size: (886, 886)
Epoch 49, accuracy: 0.6182
batch size: (895, 895)
Epoch 50, accuracy: 0.6183
Epoch 50, Train Loss: 0.0014, Val Loss: 1.6058
batch size: (893, 893)
Epoch 51, accuracy: 0.6238
batch size: (896, 896)
Epoch 52, accuracy: 0.6206
Epoch 52, Train Loss: 0.0027, Val Loss: 1.7826
batch size: (908, 908)
Epoch 53, accuracy: 0.6190
batch size: (900, 900)
Epoch 54, accuracy: 0.6190
Epoch 54, Train Loss: 0.0016, Val Loss: 1.8751
batch size: (908, 908)
Epoch 55, accuracy: 0.6218
batch size: (890, 890)
Epoch 56, accuracy: 0.6223
Epoch 56, Train Loss: 0.0022, Val Loss: 1.9189
batch size: (910, 910)
Epoch 57, accuracy: 0.6223
batch size: (895, 895)
Epoch 58, accuracy: 0.6226
Epoch 58, Train Loss: 0.0033, Val Loss: 1.8096
batch size: (916, 916)
Epoch 59, accuracy: 0.6202
batch size: (906, 906)
Epoch 60, accuracy: 0.6240
Epoch 60, Train Loss: 0.0015, Val Loss: 1.9792
batch size: (890, 890)
Epoch 61, accuracy: 0.6207
batch size: (894, 894)
Epoch 62, accuracy: 0.6199
Epoch 62, Train Loss: 0.0032, Val Loss: 2.0147
batch size: (887, 887)
Epoch 63, accuracy: 0.6206
batch size: (905, 905)
Epoch 64, accuracy: 0.6203
Epoch 64, Train Loss: 0.0481, Val Loss: 2.0277
batch size: (906, 906)
Epoch 65, accuracy: 0.6224
batch size: (906, 906)
Epoch 66, accuracy: 0.6253
Epoch 66, Train Loss: 0.0016, Val Loss: 2.0906
batch size: (902, 902)
Epoch 67, accuracy: 0.6240
batch size: (895, 895)
Epoch 68, accuracy: 0.6241
Epoch 68, Train Loss: 0.0048, Val Loss: 2.0765
batch size: (905, 905)
Epoch 69, accuracy: 0.6190
batch size: (905, 905)
Epoch 70, accuracy: 0.6235
Epoch 70, Train Loss: 0.0025, Val Loss: 2.1138
batch size: (907, 907)
Epoch 71, accuracy: 0.6245
batch size: (900, 900)
Epoch 72, accuracy: 0.6209
Epoch 72, Train Loss: 0.0023, Val Loss: 2.1086
batch size: (908, 908)
Epoch 73, accuracy: 0.6207
batch size: (913, 913)
Epoch 74, accuracy: 0.6245
Epoch 74, Train Loss: 0.0012, Val Loss: 2.1195
batch size: (907, 907)
Epoch 75, accuracy: 0.6198
batch size: (910, 910)
Epoch 76, accuracy: 0.6258
Epoch 76, Train Loss: 0.0012, Val Loss: 2.0695
batch size: (892, 892)
Epoch 77, accuracy: 0.6252
batch size: (910, 910)
Epoch 78, accuracy: 0.6249
Epoch 78, Train Loss: 0.0024, Val Loss: 2.1467
batch size: (916, 916)
Epoch 79, accuracy: 0.6272
batch size: (904, 904)
Epoch 80, accuracy: 0.6211
Epoch 80, Train Loss: 0.0015, Val Loss: 2.1222
batch size: (903, 903)
Epoch 81, accuracy: 0.6232
batch size: (896, 896)
Epoch 82, accuracy: 0.6231
Epoch 82, Train Loss: 0.0011, Val Loss: 2.1133
batch size: (891, 891)
Epoch 83, accuracy: 0.6248
batch size: (914, 914)
Epoch 84, accuracy: 0.6263
Epoch 84, Train Loss: 0.0030, Val Loss: 2.2217
batch size: (902, 902)
Epoch 85, accuracy: 0.6247
batch size: (898, 898)
Epoch 86, accuracy: 0.6272
Epoch 86, Train Loss: 0.0043, Val Loss: 2.2602
batch size: (901, 901)
Epoch 87, accuracy: 0.6236
batch size: (897, 897)
Epoch 88, accuracy: 0.6231
Epoch 88, Train Loss: 0.0015, Val Loss: 2.0783
batch size: (898, 898)
Epoch 89, accuracy: 0.6238
batch size: (886, 886)
Epoch 90, accuracy: 0.6243
Epoch 90, Train Loss: 0.1017, Val Loss: 2.3039
batch size: (895, 895)
Epoch 91, accuracy: 0.6222
batch size: (891, 891)
Epoch 92, accuracy: 0.6226
Epoch 92, Train Loss: 0.0088, Val Loss: 2.2297
batch size: (908, 908)
Epoch 93, accuracy: 0.6288
batch size: (911, 911)
Epoch 94, accuracy: 0.6270
Epoch 94, Train Loss: 0.0216, Val Loss: 2.2338
batch size: (908, 908)
Epoch 95, accuracy: 0.6213
batch size: (906, 906)
Epoch 96, accuracy: 0.6235
Epoch 96, Train Loss: 0.0011, Val Loss: 2.1720
batch size: (907, 907)
Epoch 97, accuracy: 0.6246
batch size: (906, 906)
Epoch 98, accuracy: 0.6279
Epoch 98, Train Loss: 0.0010, Val Loss: 2.2704
batch size: (926, 926)
Epoch 99, accuracy: 0.6249
batch size: (917, 917)
Epoch 100, accuracy: 0.6279
Epoch 100, Train Loss: 0.0446, Val Loss: 2.1783
batch size: (892, 892)
Epoch 101, accuracy: 0.6263
batch size: (905, 905)
Epoch 102, accuracy: 0.6237
Epoch 102, Train Loss: 0.0015, Val Loss: 2.1186
batch size: (895, 895)
Epoch 103, accuracy: 0.6222
batch size: (887, 887)
Epoch 104, accuracy: 0.6246
Epoch 104, Train Loss: 0.0008, Val Loss: 2.2081
batch size: (896, 896)
Epoch 105, accuracy: 0.6261
batch size: (898, 898)
Epoch 106, accuracy: 0.6273
Epoch 106, Train Loss: 0.0025, Val Loss: 2.1849
batch size: (900, 900)
Epoch 107, accuracy: 0.6217
batch size: (881, 881)
Epoch 108, accuracy: 0.6251
Epoch 108, Train Loss: 0.0036, Val Loss: 2.1965
batch size: (924, 924)
Epoch 109, accuracy: 0.6239
batch size: (890, 890)
Epoch 110, accuracy: 0.6244
Epoch 110, Train Loss: 0.0291, Val Loss: 2.1335
batch size: (900, 900)
Epoch 111, accuracy: 0.6237
batch size: (913, 913)
Epoch 112, accuracy: 0.6253
Epoch 112, Train Loss: 0.0020, Val Loss: 2.2013
batch size: (882, 882)
Epoch 113, accuracy: 0.6282
batch size: (918, 918)
Epoch 114, accuracy: 0.6265
Epoch 114, Train Loss: 0.0200, Val Loss: 2.2520
batch size: (920, 920)
Epoch 115, accuracy: 0.6274
batch size: (914, 914)
Epoch 116, accuracy: 0.6276
Epoch 116, Train Loss: 0.0065, Val Loss: 2.2310
batch size: (898, 898)
Epoch 117, accuracy: 0.6249
batch size: (887, 887)
Epoch 118, accuracy: 0.6260
Epoch 118, Train Loss: 0.0026, Val Loss: 2.1598
batch size: (904, 904)
Epoch 119, accuracy: 0.6252
batch size: (892, 892)
Epoch 120, accuracy: 0.6259
Epoch 120, Train Loss: 0.0379, Val Loss: 2.1452
batch size: (903, 903)
Epoch 121, accuracy: 0.6257
batch size: (899, 899)
Epoch 122, accuracy: 0.6250
Epoch 122, Train Loss: 0.0017, Val Loss: 2.1118
batch size: (912, 912)
Epoch 123, accuracy: 0.6250
batch size: (905, 905)
Epoch 124, accuracy: 0.6239
Epoch 124, Train Loss: 0.0026, Val Loss: 2.1602
batch size: (896, 896)
Epoch 125, accuracy: 0.6226
batch size: (895, 895)
Epoch 126, accuracy: 0.6241
Epoch 126, Train Loss: 0.0208, Val Loss: 2.2464
batch size: (900, 900)
Epoch 127, accuracy: 0.6252
batch size: (902, 902)
Epoch 128, accuracy: 0.6261
Epoch 128, Train Loss: 0.0022, Val Loss: 2.2385
batch size: (900, 900)
Epoch 129, accuracy: 0.6264
batch size: (904, 904)
Epoch 130, accuracy: 0.6215
Epoch 130, Train Loss: 0.0043, Val Loss: 2.1204
batch size: (899, 899)
Epoch 131, accuracy: 0.6242
batch size: (893, 893)
Epoch 132, accuracy: 0.6233
Epoch 132, Train Loss: 0.0020, Val Loss: 2.2028
batch size: (910, 910)
Epoch 133, accuracy: 0.6287
batch size: (898, 898)
Epoch 134, accuracy: 0.6228
Epoch 134, Train Loss: 0.0328, Val Loss: 2.1501
batch size: (907, 907)
Epoch 135, accuracy: 0.6284
batch size: (891, 891)
Epoch 136, accuracy: 0.6238
Epoch 136, Train Loss: 0.0014, Val Loss: 2.2045
batch size: (885, 885)
Epoch 137, accuracy: 0.6249
batch size: (891, 891)
Epoch 138, accuracy: 0.6259
Epoch 138, Train Loss: 0.1035, Val Loss: 2.2235
batch size: (900, 900)
Epoch 139, accuracy: 0.6278
batch size: (910, 910)
Epoch 140, accuracy: 0.6259
Epoch 140, Train Loss: 0.1436, Val Loss: 2.2669
batch size: (911, 911)
Epoch 141, accuracy: 0.6257
batch size: (890, 890)
Epoch 142, accuracy: 0.6236
Epoch 142, Train Loss: 0.0021, Val Loss: 2.2508
batch size: (911, 911)
Epoch 143, accuracy: 0.6279
batch size: (898, 898)
Epoch 144, accuracy: 0.6248
Epoch 144, Train Loss: 0.0022, Val Loss: 2.2076
batch size: (902, 902)
Epoch 145, accuracy: 0.6261
batch size: (901, 901)
Epoch 146, accuracy: 0.6246
Epoch 146, Train Loss: 0.0097, Val Loss: 2.2700
batch size: (885, 885)
Epoch 147, accuracy: 0.6258
batch size: (896, 896)
Epoch 148, accuracy: 0.6223
Epoch 148, Train Loss: 0.0021, Val Loss: 2.2122
batch size: (897, 897)
Epoch 149, accuracy: 0.6224
batch size: (888, 888)
Epoch 150, accuracy: 0.6252
Epoch 150, Train Loss: 0.2124, Val Loss: 2.2112
batch size: (897, 897)
Epoch 151, accuracy: 0.6214
batch size: (885, 885)
Epoch 152, accuracy: 0.6240
Epoch 152, Train Loss: 0.0014, Val Loss: 2.1828
batch size: (892, 892)
Epoch 153, accuracy: 0.6279
batch size: (913, 913)
Epoch 154, accuracy: 0.6246
Epoch 154, Train Loss: 0.0017, Val Loss: 2.2306
batch size: (915, 915)
Epoch 155, accuracy: 0.6278
batch size: (906, 906)
Epoch 156, accuracy: 0.6260
Epoch 156, Train Loss: 0.0013, Val Loss: 2.1896
batch size: (916, 916)
Epoch 157, accuracy: 0.6254
batch size: (907, 907)
Epoch 158, accuracy: 0.6271
Epoch 158, Train Loss: 0.0012, Val Loss: 2.2233
batch size: (905, 905)
Epoch 159, accuracy: 0.6281
batch size: (902, 902)
Epoch 160, accuracy: 0.6281
Epoch 160, Train Loss: 0.0022, Val Loss: 2.1147
batch size: (896, 896)
Epoch 161, accuracy: 0.6263
batch size: (891, 891)
Epoch 162, accuracy: 0.6293
Epoch 162, Train Loss: 0.0011, Val Loss: 2.1875
batch size: (896, 896)
Epoch 163, accuracy: 0.6253
batch size: (901, 901)
Epoch 164, accuracy: 0.6259
Epoch 164, Train Loss: 0.0009, Val Loss: 2.2063
batch size: (908, 908)
Epoch 165, accuracy: 0.6257
batch size: (909, 909)
Epoch 166, accuracy: 0.6240
Epoch 166, Train Loss: 0.0057, Val Loss: 2.2096
batch size: (891, 891)
Epoch 167, accuracy: 0.6275
batch size: (895, 895)
Epoch 168, accuracy: 0.6242
Epoch 168, Train Loss: 0.0050, Val Loss: 2.0807
batch size: (887, 887)
Epoch 169, accuracy: 0.6229
batch size: (897, 897)
Epoch 170, accuracy: 0.6258
Epoch 170, Train Loss: 0.0057, Val Loss: 2.2114
batch size: (876, 876)
Epoch 171, accuracy: 0.6247
batch size: (898, 898)
Epoch 172, accuracy: 0.6257
Epoch 172, Train Loss: 0.0028, Val Loss: 2.1939
batch size: (892, 892)
Epoch 173, accuracy: 0.6260
batch size: (904, 904)
Epoch 174, accuracy: 0.6271
Epoch 174, Train Loss: 0.0019, Val Loss: 2.1864
batch size: (890, 890)
Epoch 175, accuracy: 0.6229
batch size: (902, 902)
Epoch 176, accuracy: 0.6257
Epoch 176, Train Loss: 0.0066, Val Loss: 2.0293
batch size: (909, 909)
Epoch 177, accuracy: 0.6249
batch size: (903, 903)
Epoch 178, accuracy: 0.6244
Epoch 178, Train Loss: 0.0217, Val Loss: 2.1176
batch size: (900, 900)
Epoch 179, accuracy: 0.6230
batch size: (927, 927)
Epoch 180, accuracy: 0.6249
Epoch 180, Train Loss: 0.0026, Val Loss: 2.1910
batch size: (888, 888)
Epoch 181, accuracy: 0.6214
batch size: (894, 894)
Epoch 182, accuracy: 0.6256
Epoch 182, Train Loss: 0.2130, Val Loss: 2.1410
batch size: (890, 890)
Epoch 183, accuracy: 0.6271
batch size: (896, 896)
Epoch 184, accuracy: 0.6231
Epoch 184, Train Loss: 0.0015, Val Loss: 2.1890
batch size: (908, 908)
Epoch 185, accuracy: 0.6274
batch size: (890, 890)
Epoch 186, accuracy: 0.6279
Epoch 186, Train Loss: 0.0026, Val Loss: 2.3010
batch size: (888, 888)
Epoch 187, accuracy: 0.6256
batch size: (901, 901)
Epoch 188, accuracy: 0.6276
Epoch 188, Train Loss: 0.0013, Val Loss: 2.1154
batch size: (917, 917)
Epoch 189, accuracy: 0.6286
batch size: (896, 896)
Epoch 190, accuracy: 0.6265
Epoch 190, Train Loss: 0.0065, Val Loss: 2.2007
batch size: (900, 900)
Epoch 191, accuracy: 0.6265
batch size: (913, 913)
Epoch 192, accuracy: 0.6272
Epoch 192, Train Loss: 0.0024, Val Loss: 2.2730
batch size: (904, 904)
Epoch 193, accuracy: 0.6286
batch size: (923, 923)
Epoch 194, accuracy: 0.6241
Epoch 194, Train Loss: 0.0026, Val Loss: 2.2491
batch size: (895, 895)
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 195, accuracy: 0.6235
batch size: (898, 898)
Epoch 196, accuracy: 0.6250
Epoch 196, Train Loss: 0.0336, Val Loss: 2.2180
batch size: (897, 897)
Epoch 197, accuracy: 0.6266
batch size: (913, 913)
Epoch 198, accuracy: 0.6221
Epoch 198, Train Loss: 0.0026, Val Loss: 2.1046
batch size: (903, 903)
Epoch 199, accuracy: 0.6219
Loaded best model with val_loss = 0.7349396347999573
test :accuracy 0.6484, f1_macro: 0.5993, f1_micro: 0.6484, auc: 0.8632
Training GINConv with 8 layers...
可训练参数: 1059595_GINConv
不可训练参数: 0
batch size: (914, 914)
✅ Epoch 0: New best model saved with val_loss = 4.4382
Epoch 0, accuracy: 0.4005
Epoch 0, Train Loss: 1.5539, Val Loss: 4.4382
batch size: (884, 884)
Epoch 1, accuracy: 0.4123
batch size: (907, 907)
✅ Epoch 2: New best model saved with val_loss = 1.9208
Epoch 2, accuracy: 0.4033
Epoch 2, Train Loss: 2.0288, Val Loss: 1.9208
batch size: (892, 892)
Epoch 3, accuracy: 0.4012
batch size: (901, 901)
✅ Epoch 4: New best model saved with val_loss = 1.4742
Epoch 4, accuracy: 0.4044
Epoch 4, Train Loss: 1.0511, Val Loss: 1.4742
batch size: (891, 891)
✅ Epoch 5: New best model saved with val_loss = 1.4066
Epoch 5, accuracy: 0.3988
batch size: (897, 897)
✅ Epoch 6: New best model saved with val_loss = 1.2962
Epoch 6, accuracy: 0.4017
Epoch 6, Train Loss: 1.0429, Val Loss: 1.2962
batch size: (900, 900)
Epoch 7, accuracy: 0.4048
batch size: (889, 889)
Epoch 8, accuracy: 0.4014
Epoch 8, Train Loss: 1.0170, Val Loss: 1.3347
batch size: (891, 891)
Epoch 9, accuracy: 0.4026
batch size: (894, 894)
Epoch 10, accuracy: 0.4031
Epoch 10, Train Loss: 0.9600, Val Loss: 1.3426
batch size: (880, 880)
Epoch 11, accuracy: 0.4047
batch size: (922, 922)
Epoch 12, accuracy: 0.3999
Epoch 12, Train Loss: 1.0732, Val Loss: 1.3756
batch size: (882, 882)
✅ Epoch 13: New best model saved with val_loss = 1.2702
Epoch 13, accuracy: 0.4036
batch size: (891, 891)
✅ Epoch 14: New best model saved with val_loss = 1.2336
Epoch 14, accuracy: 0.4030
Epoch 14, Train Loss: 0.9933, Val Loss: 1.2336
batch size: (891, 891)
Epoch 15, accuracy: 0.3996
batch size: (886, 886)
✅ Epoch 16: New best model saved with val_loss = 1.2226
Epoch 16, accuracy: 0.4009
Epoch 16, Train Loss: 0.9820, Val Loss: 1.2226
batch size: (914, 914)
Epoch 17, accuracy: 0.4034
batch size: (914, 914)
Epoch 18, accuracy: 0.4029
Epoch 18, Train Loss: 0.9325, Val Loss: 1.2626
batch size: (903, 903)
Epoch 19, accuracy: 0.4042
batch size: (912, 912)
Epoch 20, accuracy: 0.4013
Epoch 20, Train Loss: 0.9816, Val Loss: 1.2513
batch size: (908, 908)
Epoch 21, accuracy: 0.4006
batch size: (907, 907)
Epoch 22, accuracy: 0.4001
Epoch 22, Train Loss: 0.9949, Val Loss: 1.2347
batch size: (895, 895)
Epoch 23, accuracy: 0.4031
batch size: (899, 899)
Epoch 24, accuracy: 0.4024
Epoch 24, Train Loss: 1.0457, Val Loss: 1.2236
batch size: (903, 903)
✅ Epoch 25: New best model saved with val_loss = 1.2162
Epoch 25, accuracy: 0.4015
batch size: (902, 902)
Epoch 26, accuracy: 0.4031
Epoch 26, Train Loss: 0.9479, Val Loss: 1.2270
batch size: (917, 917)
Epoch 27, accuracy: 0.4014
batch size: (892, 892)
Epoch 28, accuracy: 0.4002
Epoch 28, Train Loss: 0.9664, Val Loss: 1.2834
batch size: (893, 893)
Epoch 29, accuracy: 0.4001
batch size: (911, 911)
Epoch 30, accuracy: 0.4016
Epoch 30, Train Loss: 0.9761, Val Loss: 1.2907
batch size: (905, 905)
Epoch 31, accuracy: 0.4056
batch size: (904, 904)
Epoch 32, accuracy: 0.3988
Epoch 32, Train Loss: 0.9380, Val Loss: 1.2837
batch size: (893, 893)
Epoch 33, accuracy: 0.3992
batch size: (872, 872)
Epoch 34, accuracy: 0.4003
Epoch 34, Train Loss: 0.9392, Val Loss: 1.2370
batch size: (899, 899)
Epoch 35, accuracy: 0.4035
batch size: (883, 883)
Epoch 36, accuracy: 0.3997
Epoch 36, Train Loss: 0.9700, Val Loss: 1.3247
batch size: (901, 901)
Epoch 37, accuracy: 0.4012
batch size: (898, 898)
Epoch 38, accuracy: 0.4029
Epoch 38, Train Loss: 0.9456, Val Loss: 1.3406
batch size: (915, 915)
Epoch 39, accuracy: 0.4028
batch size: (904, 904)
Epoch 40, accuracy: 0.4023
Epoch 40, Train Loss: 0.9942, Val Loss: 1.2356
batch size: (917, 917)
Epoch 41, accuracy: 0.3989
batch size: (912, 912)
Epoch 42, accuracy: 0.4012
Epoch 42, Train Loss: 0.9854, Val Loss: 1.2771
batch size: (900, 900)
Epoch 43, accuracy: 0.4058
batch size: (903, 903)
Epoch 44, accuracy: 0.3988
Epoch 44, Train Loss: 0.9354, Val Loss: 1.2995
batch size: (893, 893)
Epoch 45, accuracy: 0.4035
batch size: (883, 883)
Epoch 46, accuracy: 0.4085
Epoch 46, Train Loss: 0.9722, Val Loss: 1.2487
batch size: (912, 912)
Epoch 47, accuracy: 0.4045
batch size: (897, 897)
Epoch 48, accuracy: 0.3991
Epoch 48, Train Loss: 0.9472, Val Loss: 1.2434
batch size: (897, 897)
Epoch 49, accuracy: 0.4041
batch size: (901, 901)
Epoch 50, accuracy: 0.4047
Epoch 50, Train Loss: 0.9525, Val Loss: 1.2675
batch size: (895, 895)
Epoch 51, accuracy: 0.4019
batch size: (905, 905)
Epoch 52, accuracy: 0.3998
Epoch 52, Train Loss: 0.9855, Val Loss: 1.2741
batch size: (892, 892)
Epoch 53, accuracy: 0.4020
batch size: (905, 905)
Epoch 54, accuracy: 0.4014
Epoch 54, Train Loss: 0.9374, Val Loss: 1.2868
batch size: (885, 885)
Epoch 55, accuracy: 0.3984
batch size: (889, 889)
Epoch 56, accuracy: 0.4000
Epoch 56, Train Loss: 0.9410, Val Loss: 1.2631
batch size: (894, 894)
Epoch 57, accuracy: 0.4001
batch size: (894, 894)
Epoch 58, accuracy: 0.4023
Epoch 58, Train Loss: 0.9689, Val Loss: 1.2889
batch size: (902, 902)
Epoch 59, accuracy: 0.4016
batch size: (916, 916)
Epoch 60, accuracy: 0.3991
Epoch 60, Train Loss: 0.9396, Val Loss: 1.2750
batch size: (910, 910)
Epoch 61, accuracy: 0.4042
batch size: (910, 910)
Epoch 62, accuracy: 0.4046
Epoch 62, Train Loss: 0.9819, Val Loss: 1.2929
batch size: (895, 895)
✅ Epoch 63: New best model saved with val_loss = 1.2127
Epoch 63, accuracy: 0.3974
batch size: (897, 897)
Epoch 64, accuracy: 0.4031
Epoch 64, Train Loss: 0.9724, Val Loss: 1.2658
batch size: (901, 901)
Epoch 65, accuracy: 0.4013
batch size: (910, 910)
Epoch 66, accuracy: 0.4035
Epoch 66, Train Loss: 0.9780, Val Loss: 1.2453
batch size: (915, 915)
Epoch 67, accuracy: 0.4035
batch size: (885, 885)
Epoch 68, accuracy: 0.4111
Epoch 68, Train Loss: 0.9473, Val Loss: 1.2151
batch size: (890, 890)
Epoch 69, accuracy: 0.4010
batch size: (892, 892)
Epoch 70, accuracy: 0.4076
Epoch 70, Train Loss: 0.9278, Val Loss: 1.3196
batch size: (895, 895)
Epoch 71, accuracy: 0.3987
batch size: (896, 896)
Epoch 72, accuracy: 0.3996
Epoch 72, Train Loss: 0.9614, Val Loss: 1.2558
batch size: (911, 911)
Epoch 73, accuracy: 0.4006
batch size: (895, 895)
Epoch 74, accuracy: 0.4021
Epoch 74, Train Loss: 0.9825, Val Loss: 1.2333
batch size: (916, 916)
Epoch 75, accuracy: 0.4045
batch size: (887, 887)
Epoch 76, accuracy: 0.4036
Epoch 76, Train Loss: 0.9796, Val Loss: 1.2294
batch size: (898, 898)
Epoch 77, accuracy: 0.4009
batch size: (910, 910)
Epoch 78, accuracy: 0.4023
Epoch 78, Train Loss: 0.9510, Val Loss: 1.2555
batch size: (919, 919)
Epoch 79, accuracy: 0.4015
batch size: (923, 923)
Epoch 80, accuracy: 0.4029
Epoch 80, Train Loss: 0.9566, Val Loss: 1.2376
batch size: (907, 907)
Epoch 81, accuracy: 0.4033
batch size: (878, 878)
Epoch 82, accuracy: 0.3982
Epoch 82, Train Loss: 0.9409, Val Loss: 1.2708
batch size: (894, 894)
Epoch 83, accuracy: 0.4072
batch size: (911, 911)
Epoch 84, accuracy: 0.4029
Epoch 84, Train Loss: 0.9601, Val Loss: 1.2661
batch size: (903, 903)
Epoch 85, accuracy: 0.4028
batch size: (912, 912)
Epoch 86, accuracy: 0.4010
Epoch 86, Train Loss: 0.9600, Val Loss: 1.2554
batch size: (898, 898)
Epoch 87, accuracy: 0.4000
batch size: (885, 885)
Epoch 88, accuracy: 0.3994
Epoch 88, Train Loss: 0.9782, Val Loss: 1.2516
batch size: (893, 893)
Epoch 89, accuracy: 0.4036
batch size: (896, 896)
Epoch 90, accuracy: 0.3998
Epoch 90, Train Loss: 0.9723, Val Loss: 1.2557
batch size: (887, 887)
Epoch 91, accuracy: 0.4036
batch size: (889, 889)
Epoch 92, accuracy: 0.3986
Epoch 92, Train Loss: 0.9661, Val Loss: 1.2898
batch size: (899, 899)
Epoch 93, accuracy: 0.4024
batch size: (881, 881)
Epoch 94, accuracy: 0.4058
Epoch 94, Train Loss: 0.9888, Val Loss: 1.2676
batch size: (896, 896)
Epoch 95, accuracy: 0.4035
batch size: (895, 895)
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 96, accuracy: 0.4010
Epoch 96, Train Loss: 0.9387, Val Loss: 1.2341
batch size: (917, 917)
Epoch 97, accuracy: 0.4013
batch size: (914, 914)
✅ Epoch 98: New best model saved with val_loss = 1.1944
Epoch 98, accuracy: 0.4040
Epoch 98, Train Loss: 0.9843, Val Loss: 1.1944
batch size: (892, 892)
Epoch 99, accuracy: 0.3989
batch size: (887, 887)
Epoch 100, accuracy: 0.4012
Epoch 100, Train Loss: 0.9565, Val Loss: 1.2272
batch size: (877, 877)
Epoch 101, accuracy: 0.4074
batch size: (907, 907)
Epoch 102, accuracy: 0.4033
Epoch 102, Train Loss: 1.0460, Val Loss: 1.2507
batch size: (890, 890)
Epoch 103, accuracy: 0.4025
batch size: (902, 902)
Epoch 104, accuracy: 0.4024
Epoch 104, Train Loss: 0.9314, Val Loss: 1.2400
batch size: (886, 886)
Epoch 105, accuracy: 0.3993
batch size: (906, 906)
Epoch 106, accuracy: 0.4026
Epoch 106, Train Loss: 0.9647, Val Loss: 1.2668
batch size: (907, 907)
Epoch 107, accuracy: 0.3993
batch size: (907, 907)
Epoch 108, accuracy: 0.3992
Epoch 108, Train Loss: 0.9519, Val Loss: 1.2416
batch size: (901, 901)
Epoch 109, accuracy: 0.4024
batch size: (898, 898)
Epoch 110, accuracy: 0.4029
Epoch 110, Train Loss: 0.9884, Val Loss: 1.2168
batch size: (898, 898)
Epoch 111, accuracy: 0.4016
batch size: (895, 895)
Epoch 112, accuracy: 0.3978
Epoch 112, Train Loss: 1.0233, Val Loss: 1.3006
batch size: (904, 904)
Epoch 113, accuracy: 0.4000
batch size: (900, 900)
Epoch 114, accuracy: 0.4003
Epoch 114, Train Loss: 0.9710, Val Loss: 1.2953
batch size: (897, 897)
Epoch 115, accuracy: 0.3996
batch size: (906, 906)
Epoch 116, accuracy: 0.3983
Epoch 116, Train Loss: 0.9528, Val Loss: 1.2721
batch size: (900, 900)
Epoch 117, accuracy: 0.4017
batch size: (912, 912)
Epoch 118, accuracy: 0.4027
Epoch 118, Train Loss: 0.9637, Val Loss: 1.2226
batch size: (908, 908)
Epoch 119, accuracy: 0.4047
batch size: (896, 896)
Epoch 120, accuracy: 0.4020
Epoch 120, Train Loss: 0.9647, Val Loss: 1.2563
batch size: (907, 907)
Epoch 121, accuracy: 0.4053
batch size: (897, 897)
Epoch 122, accuracy: 0.4065
Epoch 122, Train Loss: 1.0132, Val Loss: 1.1983
batch size: (925, 925)
Epoch 123, accuracy: 0.4029
batch size: (895, 895)
Epoch 124, accuracy: 0.4016
Epoch 124, Train Loss: 0.9583, Val Loss: 1.2578
batch size: (901, 901)
Epoch 125, accuracy: 0.4008
batch size: (889, 889)
Epoch 126, accuracy: 0.3988
Epoch 126, Train Loss: 0.9372, Val Loss: 1.2309
batch size: (902, 902)
Epoch 127, accuracy: 0.4007
batch size: (893, 893)
Epoch 128, accuracy: 0.4055
Epoch 128, Train Loss: 0.9663, Val Loss: 1.2638
batch size: (873, 873)
Epoch 129, accuracy: 0.3998
batch size: (883, 883)
Epoch 130, accuracy: 0.4029
Epoch 130, Train Loss: 0.9467, Val Loss: 1.2991
batch size: (878, 878)
Epoch 131, accuracy: 0.4018
batch size: (886, 886)
Epoch 132, accuracy: 0.4016
Epoch 132, Train Loss: 0.9655, Val Loss: 1.2823
batch size: (921, 921)
Epoch 133, accuracy: 0.4003
batch size: (919, 919)
Epoch 134, accuracy: 0.4006
Epoch 134, Train Loss: 0.9676, Val Loss: 1.2919
batch size: (911, 911)
Epoch 135, accuracy: 0.4021
batch size: (911, 911)
Epoch 136, accuracy: 0.4045
Epoch 136, Train Loss: 0.9685, Val Loss: 1.2666
batch size: (870, 870)
Epoch 137, accuracy: 0.4037
batch size: (893, 893)
Epoch 138, accuracy: 0.4043
Epoch 138, Train Loss: 0.9844, Val Loss: 1.2205
batch size: (910, 910)
Epoch 139, accuracy: 0.4013
batch size: (905, 905)
Epoch 140, accuracy: 0.4031
Epoch 140, Train Loss: 0.9464, Val Loss: 1.3306
batch size: (891, 891)
Epoch 141, accuracy: 0.4033
batch size: (904, 904)
Epoch 142, accuracy: 0.4026
Epoch 142, Train Loss: 0.9910, Val Loss: 1.2427
batch size: (897, 897)
Epoch 143, accuracy: 0.4023
batch size: (909, 909)
Epoch 144, accuracy: 0.4015
Epoch 144, Train Loss: 0.9420, Val Loss: 1.2361
batch size: (907, 907)
Epoch 145, accuracy: 0.4023
batch size: (869, 869)
Epoch 146, accuracy: 0.4029
Epoch 146, Train Loss: 0.9871, Val Loss: 1.2613
batch size: (895, 895)
Epoch 147, accuracy: 0.4019
batch size: (894, 894)
Epoch 148, accuracy: 0.4009
Epoch 148, Train Loss: 0.9550, Val Loss: 1.2177
batch size: (895, 895)
Epoch 149, accuracy: 0.3994
batch size: (914, 914)
Epoch 150, accuracy: 0.4015
Epoch 150, Train Loss: 0.9597, Val Loss: 1.3071
batch size: (882, 882)
Epoch 151, accuracy: 0.4029
batch size: (897, 897)
Epoch 152, accuracy: 0.4014
Epoch 152, Train Loss: 0.9446, Val Loss: 1.2539
batch size: (894, 894)
Epoch 153, accuracy: 0.3948
batch size: (902, 902)
Epoch 154, accuracy: 0.4003
Epoch 154, Train Loss: 0.9563, Val Loss: 1.2748
batch size: (895, 895)
Epoch 155, accuracy: 0.4026
batch size: (890, 890)
Epoch 156, accuracy: 0.4004
Epoch 156, Train Loss: 0.9360, Val Loss: 1.2474
batch size: (901, 901)
Epoch 157, accuracy: 0.3981
batch size: (893, 893)
Epoch 158, accuracy: 0.4039
Epoch 158, Train Loss: 0.9480, Val Loss: 1.2360
batch size: (918, 918)
Epoch 159, accuracy: 0.4069
batch size: (910, 910)
Epoch 160, accuracy: 0.4005
Epoch 160, Train Loss: 0.9905, Val Loss: 1.2518
batch size: (903, 903)
Epoch 161, accuracy: 0.4007
batch size: (900, 900)
Epoch 162, accuracy: 0.4012
Epoch 162, Train Loss: 0.9421, Val Loss: 1.2421
batch size: (882, 882)
Epoch 163, accuracy: 0.4021
batch size: (897, 897)
Epoch 164, accuracy: 0.4022
Epoch 164, Train Loss: 0.9440, Val Loss: 1.2372
batch size: (914, 914)
Epoch 165, accuracy: 0.4046
batch size: (901, 901)
Epoch 166, accuracy: 0.4003
Epoch 166, Train Loss: 1.0251, Val Loss: 1.2191
batch size: (887, 887)
Epoch 167, accuracy: 0.4027
batch size: (911, 911)
Epoch 168, accuracy: 0.3990
Epoch 168, Train Loss: 0.9723, Val Loss: 1.2767
batch size: (874, 874)
Epoch 169, accuracy: 0.4015
batch size: (911, 911)
Epoch 170, accuracy: 0.4040
Epoch 170, Train Loss: 0.9589, Val Loss: 1.2415
batch size: (891, 891)
Epoch 171, accuracy: 0.3995
batch size: (912, 912)
Epoch 172, accuracy: 0.4000
Epoch 172, Train Loss: 0.9792, Val Loss: 1.2304
batch size: (904, 904)
Epoch 173, accuracy: 0.4035
batch size: (875, 875)
Epoch 174, accuracy: 0.4018
Epoch 174, Train Loss: 0.9729, Val Loss: 1.2367
batch size: (920, 920)
Epoch 175, accuracy: 0.3988
batch size: (885, 885)
Epoch 176, accuracy: 0.4026
Epoch 176, Train Loss: 0.9521, Val Loss: 1.2188
batch size: (925, 925)
Epoch 177, accuracy: 0.4062
batch size: (920, 920)
Epoch 178, accuracy: 0.4005
Epoch 178, Train Loss: 0.9691, Val Loss: 1.2271
batch size: (894, 894)
Epoch 179, accuracy: 0.4018
batch size: (903, 903)
Epoch 180, accuracy: 0.4008
Epoch 180, Train Loss: 0.9379, Val Loss: 1.2751
batch size: (888, 888)
Epoch 181, accuracy: 0.4039
batch size: (895, 895)
Epoch 182, accuracy: 0.4007
Epoch 182, Train Loss: 0.9838, Val Loss: 1.3577
batch size: (900, 900)
Epoch 183, accuracy: 0.4053
batch size: (903, 903)
Epoch 184, accuracy: 0.4043
Epoch 184, Train Loss: 0.9737, Val Loss: 1.3381
batch size: (881, 881)
Epoch 185, accuracy: 0.3995
batch size: (890, 890)
Epoch 186, accuracy: 0.4019
Epoch 186, Train Loss: 1.0504, Val Loss: 1.2862
batch size: (897, 897)
Epoch 187, accuracy: 0.4035
batch size: (893, 893)
Epoch 188, accuracy: 0.4043
Epoch 188, Train Loss: 0.9723, Val Loss: 1.2899
batch size: (911, 911)
Epoch 189, accuracy: 0.3988
batch size: (883, 883)
Epoch 190, accuracy: 0.4052
Epoch 190, Train Loss: 0.9919, Val Loss: 1.2695
batch size: (883, 883)
Epoch 191, accuracy: 0.4003
batch size: (870, 870)
Epoch 192, accuracy: 0.4015
Epoch 192, Train Loss: 0.9460, Val Loss: 1.3058
batch size: (880, 880)
Epoch 193, accuracy: 0.3989
batch size: (905, 905)
Epoch 194, accuracy: 0.4071
Epoch 194, Train Loss: 0.9648, Val Loss: 1.2551
batch size: (911, 911)
Epoch 195, accuracy: 0.4041
batch size: (898, 898)
Epoch 196, accuracy: 0.3990
Epoch 196, Train Loss: 0.9706, Val Loss: 1.2468
batch size: (890, 890)
Epoch 197, accuracy: 0.4003
batch size: (900, 900)
Epoch 198, accuracy: 0.4027
Epoch 198, Train Loss: 0.9562, Val Loss: 1.2262
batch size: (895, 895)
Epoch 199, accuracy: 0.4009
Loaded best model with val_loss = 1.1943848133087158
test :accuracy 0.4033, f1_macro: 0.2022, f1_micro: 0.4033, auc: 0.5058
Training GINConv with 32 layers...
可训练参数: 4236043_GINConv
不可训练参数: 0
batch size: (900, 900)
✅ Epoch 0: New best model saved with val_loss = 19716210.0000
Epoch 0, accuracy: 0.4333
Epoch 0, Train Loss: 1.4130, Val Loss: 19716210.0000
batch size: (889, 889)
✅ Epoch 1: New best model saved with val_loss = 110529.8750
Epoch 1, accuracy: 0.1680
batch size: (892, 892)
✅ Epoch 2: New best model saved with val_loss = 114.9858
Epoch 2, accuracy: 0.1685
Epoch 2, Train Loss: 1.3758, Val Loss: 114.9858
batch size: (900, 900)
✅ Epoch 3: New best model saved with val_loss = 5.1735
Epoch 3, accuracy: 0.2112
batch size: (886, 886)
✅ Epoch 4: New best model saved with val_loss = 2.4087
Epoch 4, accuracy: 0.4028
Epoch 4, Train Loss: 1.1928, Val Loss: 2.4087
batch size: (894, 894)
✅ Epoch 5: New best model saved with val_loss = 1.4275
Epoch 5, accuracy: 0.4057
batch size: (883, 883)
Epoch 6, accuracy: 0.4047
Epoch 6, Train Loss: 2.2234, Val Loss: 1.8940
batch size: (894, 894)
✅ Epoch 7: New best model saved with val_loss = 1.3771
Epoch 7, accuracy: 0.4023
batch size: (901, 901)
Epoch 8, accuracy: 0.4050
Epoch 8, Train Loss: 1.1150, Val Loss: 1.4163
batch size: (892, 892)
Epoch 9, accuracy: 0.4007
batch size: (906, 906)
Epoch 10, accuracy: 0.3549
Epoch 10, Train Loss: 1.3349, Val Loss: 1.5792
batch size: (908, 908)
✅ Epoch 11: New best model saved with val_loss = 1.2927
Epoch 11, accuracy: 0.3545
batch size: (903, 903)
✅ Epoch 12: New best model saved with val_loss = 1.2819
Epoch 12, accuracy: 0.4017
Epoch 12, Train Loss: 1.5549, Val Loss: 1.2819
batch size: (886, 886)
Epoch 13, accuracy: 0.4036
batch size: (897, 897)
Epoch 14, accuracy: 0.4047
Epoch 14, Train Loss: 1.0988, Val Loss: 1.3677
batch size: (892, 892)
✅ Epoch 15: New best model saved with val_loss = 1.2679
Epoch 15, accuracy: 0.4044
batch size: (907, 907)
Epoch 16, accuracy: 0.4016
Epoch 16, Train Loss: 1.7867, Val Loss: 1.4248
batch size: (911, 911)
Epoch 17, accuracy: 0.4047
batch size: (904, 904)
Epoch 18, accuracy: 0.3563
Epoch 18, Train Loss: 1.0774, Val Loss: 1.2775
batch size: (892, 892)
Epoch 19, accuracy: 0.3571
batch size: (901, 901)
Epoch 20, accuracy: 0.4025
Epoch 20, Train Loss: 1.2779, Val Loss: 1.3183
batch size: (897, 897)
Epoch 21, accuracy: 0.4042
batch size: (900, 900)
✅ Epoch 22: New best model saved with val_loss = 1.2221
Epoch 22, accuracy: 0.4060
Epoch 22, Train Loss: 1.0619, Val Loss: 1.2221
batch size: (906, 906)
Epoch 23, accuracy: 0.3994
batch size: (901, 901)
Epoch 24, accuracy: 0.4014
Epoch 24, Train Loss: 1.0736, Val Loss: 1.2552
batch size: (901, 901)
Epoch 25, accuracy: 0.4010
batch size: (888, 888)
Epoch 26, accuracy: 0.4019
Epoch 26, Train Loss: 1.1077, Val Loss: 1.3041
batch size: (902, 902)
Epoch 27, accuracy: 0.4012
batch size: (916, 916)
Epoch 28, accuracy: 0.4030
Epoch 28, Train Loss: 1.0720, Val Loss: 1.2473
batch size: (917, 917)
Epoch 29, accuracy: 0.3997
batch size: (906, 906)
Epoch 30, accuracy: 0.3985
Epoch 30, Train Loss: 1.1059, Val Loss: 1.3674
batch size: (909, 909)
Epoch 31, accuracy: 0.3999
batch size: (912, 912)
Epoch 32, accuracy: 0.4028
Epoch 32, Train Loss: 1.4887, Val Loss: 1.3787
batch size: (896, 896)
Epoch 33, accuracy: 0.4048
batch size: (901, 901)
✅ Epoch 34: New best model saved with val_loss = 1.2179
Epoch 34, accuracy: 0.4029
Epoch 34, Train Loss: 1.0712, Val Loss: 1.2179
batch size: (899, 899)
Epoch 35, accuracy: 0.3999
batch size: (911, 911)
Epoch 36, accuracy: 0.4025
Epoch 36, Train Loss: 1.0711, Val Loss: 1.2771
batch size: (883, 883)
Epoch 37, accuracy: 0.4028
batch size: (901, 901)
Epoch 38, accuracy: 0.4046
Epoch 38, Train Loss: 1.0868, Val Loss: 1.2989
batch size: (914, 914)
Epoch 39, accuracy: 0.4042
batch size: (899, 899)
Epoch 40, accuracy: 0.3986
Epoch 40, Train Loss: 1.0550, Val Loss: 1.3325
batch size: (909, 909)
Epoch 41, accuracy: 0.4045
batch size: (904, 904)
Epoch 42, accuracy: 0.4026
Epoch 42, Train Loss: 1.0785, Val Loss: 1.2784
batch size: (896, 896)
Epoch 43, accuracy: 0.4007
batch size: (893, 893)
Epoch 44, accuracy: 0.4018
Epoch 44, Train Loss: 1.0575, Val Loss: 1.2601
batch size: (882, 882)
Epoch 45, accuracy: 0.4030
batch size: (898, 898)
✅ Epoch 46: New best model saved with val_loss = 1.2160
Epoch 46, accuracy: 0.4034
Epoch 46, Train Loss: 1.0653, Val Loss: 1.2160
batch size: (906, 906)
Epoch 47, accuracy: 0.4052
batch size: (909, 909)
Epoch 48, accuracy: 0.4021
Epoch 48, Train Loss: 1.0725, Val Loss: 1.2252
batch size: (903, 903)
Epoch 49, accuracy: 0.4043
batch size: (903, 903)
Epoch 50, accuracy: 0.3987
Epoch 50, Train Loss: 1.0888, Val Loss: 1.2932
batch size: (898, 898)
Epoch 51, accuracy: 0.4002
batch size: (911, 911)
Epoch 52, accuracy: 0.3997
Epoch 52, Train Loss: 1.1031, Val Loss: 1.3493
batch size: (891, 891)
Epoch 53, accuracy: 0.3999
batch size: (903, 903)
Epoch 54, accuracy: 0.4036
Epoch 54, Train Loss: 1.0611, Val Loss: 1.3017
batch size: (893, 893)
Epoch 55, accuracy: 0.4034
batch size: (916, 916)
Epoch 56, accuracy: 0.4007
Epoch 56, Train Loss: 1.0874, Val Loss: 1.3699
batch size: (891, 891)
Epoch 57, accuracy: 0.3987
batch size: (883, 883)
Epoch 58, accuracy: 0.4013
Epoch 58, Train Loss: 1.5032, Val Loss: 1.2948
batch size: (904, 904)
Epoch 59, accuracy: 0.4034
batch size: (906, 906)
Epoch 60, accuracy: 0.4039
Epoch 60, Train Loss: 1.1014, Val Loss: 1.3505
batch size: (905, 905)
Epoch 61, accuracy: 0.4060
batch size: (884, 884)
Epoch 62, accuracy: 0.4049
Epoch 62, Train Loss: 1.0751, Val Loss: 1.2753
batch size: (908, 908)
Epoch 63, accuracy: 0.4022
batch size: (886, 886)
✅ Epoch 64: New best model saved with val_loss = 1.2066
Epoch 64, accuracy: 0.4022
Epoch 64, Train Loss: 1.0620, Val Loss: 1.2066
batch size: (910, 910)
Epoch 65, accuracy: 0.4015
batch size: (896, 896)
Epoch 66, accuracy: 0.4032
Epoch 66, Train Loss: 1.0659, Val Loss: 1.2470
batch size: (886, 886)
Epoch 67, accuracy: 0.3998
batch size: (917, 917)
Epoch 68, accuracy: 0.4039
Epoch 68, Train Loss: 1.0675, Val Loss: 1.2809
batch size: (894, 894)
Epoch 69, accuracy: 0.4065
batch size: (889, 889)
Epoch 70, accuracy: 0.3990
Epoch 70, Train Loss: 1.0707, Val Loss: 1.2745
batch size: (903, 903)
Epoch 71, accuracy: 0.4031
batch size: (893, 893)
Epoch 72, accuracy: 0.4012
Epoch 72, Train Loss: 1.2141, Val Loss: 1.3551
batch size: (908, 908)
Epoch 73, accuracy: 0.4032
batch size: (871, 871)
Epoch 74, accuracy: 0.4028
Epoch 74, Train Loss: 1.2131, Val Loss: 1.3792
batch size: (905, 905)
Epoch 75, accuracy: 0.4054
batch size: (918, 918)
Epoch 76, accuracy: 0.4028
Epoch 76, Train Loss: 1.0746, Val Loss: 1.2727
batch size: (901, 901)
Epoch 77, accuracy: 0.4000
batch size: (917, 917)
Epoch 78, accuracy: 0.4036
Epoch 78, Train Loss: 1.0718, Val Loss: 1.3071
batch size: (922, 922)
✅ Epoch 79: New best model saved with val_loss = 1.2054
Epoch 79, accuracy: 0.4011
batch size: (897, 897)
Epoch 80, accuracy: 0.4031
Epoch 80, Train Loss: 1.2077, Val Loss: 1.4450
batch size: (899, 899)
Epoch 81, accuracy: 0.4042
batch size: (904, 904)
Epoch 82, accuracy: 0.4005
Epoch 82, Train Loss: 1.3931, Val Loss: 1.4162
batch size: (910, 910)
Epoch 83, accuracy: 0.3997
batch size: (898, 898)
Epoch 84, accuracy: 0.4027
Epoch 84, Train Loss: 1.0702, Val Loss: 1.2628
batch size: (906, 906)
Epoch 85, accuracy: 0.4030
batch size: (910, 910)
Epoch 86, accuracy: 0.4032
Epoch 86, Train Loss: 1.1675, Val Loss: 1.3447
batch size: (890, 890)
Epoch 87, accuracy: 0.4031
batch size: (900, 900)
Epoch 88, accuracy: 0.4013
Epoch 88, Train Loss: 1.1713, Val Loss: 1.3314
batch size: (895, 895)
Epoch 89, accuracy: 0.4034
batch size: (900, 900)
Epoch 90, accuracy: 0.4019
Epoch 90, Train Loss: 1.0798, Val Loss: 1.3014
batch size: (888, 888)
Epoch 91, accuracy: 0.4022
batch size: (892, 892)
Epoch 92, accuracy: 0.4038
Epoch 92, Train Loss: 1.0725, Val Loss: 1.2273
batch size: (902, 902)
Epoch 93, accuracy: 0.4083
batch size: (910, 910)
Epoch 94, accuracy: 0.4048
Epoch 94, Train Loss: 1.0734, Val Loss: 1.2507
batch size: (916, 916)
Epoch 95, accuracy: 0.4006
batch size: (894, 894)
Epoch 96, accuracy: 0.4023
Epoch 96, Train Loss: 1.3798, Val Loss: 1.4218
batch size: (895, 895)
Epoch 97, accuracy: 0.4040
batch size: (905, 905)
Epoch 98, accuracy: 0.4034
Epoch 98, Train Loss: 1.4828, Val Loss: 1.3200
batch size: (900, 900)
Epoch 99, accuracy: 0.4040
batch size: (896, 896)
Epoch 100, accuracy: 0.4014
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 100, Train Loss: 1.0747, Val Loss: 1.2516
batch size: (911, 911)
✅ Epoch 101: New best model saved with val_loss = 1.1950
Epoch 101, accuracy: 0.4060
batch size: (911, 911)
Epoch 102, accuracy: 0.4016
Epoch 102, Train Loss: 1.0616, Val Loss: 1.2233
batch size: (906, 906)
Epoch 103, accuracy: 0.4013
batch size: (903, 903)
Epoch 104, accuracy: 0.4025
Epoch 104, Train Loss: 1.0728, Val Loss: 1.2429
batch size: (901, 901)
Epoch 105, accuracy: 0.4058
batch size: (907, 907)
Epoch 106, accuracy: 0.4016
Epoch 106, Train Loss: 1.0593, Val Loss: 1.4001
batch size: (918, 918)
Epoch 107, accuracy: 0.4038
batch size: (914, 914)
Epoch 108, accuracy: 0.4007
Epoch 108, Train Loss: 1.0842, Val Loss: 1.2713
batch size: (901, 901)
Epoch 109, accuracy: 0.4086
batch size: (893, 893)
Epoch 110, accuracy: 0.4066
Epoch 110, Train Loss: 1.0694, Val Loss: 1.2547
batch size: (904, 904)
Epoch 111, accuracy: 0.3983
batch size: (894, 894)
Epoch 112, accuracy: 0.4001
Epoch 112, Train Loss: 1.0745, Val Loss: 1.2533
batch size: (917, 917)
Epoch 113, accuracy: 0.4003
batch size: (883, 883)
Epoch 114, accuracy: 0.4016
Epoch 114, Train Loss: 1.0742, Val Loss: 1.2672
batch size: (900, 900)
Epoch 115, accuracy: 0.4012
batch size: (911, 911)
Epoch 116, accuracy: 0.3987
Epoch 116, Train Loss: 1.0736, Val Loss: 1.2559
batch size: (894, 894)
Epoch 117, accuracy: 0.3996
batch size: (901, 901)
Epoch 118, accuracy: 0.4053
Epoch 118, Train Loss: 1.0576, Val Loss: 1.2173
batch size: (895, 895)
Epoch 119, accuracy: 0.4015
batch size: (914, 914)
Epoch 120, accuracy: 0.4004
Epoch 120, Train Loss: 1.2401, Val Loss: 1.4000
batch size: (907, 907)
Epoch 121, accuracy: 0.4049
batch size: (912, 912)
Epoch 122, accuracy: 0.4037
Epoch 122, Train Loss: 1.0583, Val Loss: 1.3178
batch size: (924, 924)
Epoch 123, accuracy: 0.4060
batch size: (906, 906)
Epoch 124, accuracy: 0.4040
Epoch 124, Train Loss: 1.4613, Val Loss: 1.4090
batch size: (907, 907)
Epoch 125, accuracy: 0.4016
batch size: (893, 893)
Epoch 126, accuracy: 0.4023
Epoch 126, Train Loss: 1.0622, Val Loss: 1.2563
batch size: (894, 894)
Epoch 127, accuracy: 0.4002
batch size: (911, 911)
Epoch 128, accuracy: 0.4011
Epoch 128, Train Loss: 1.0589, Val Loss: 1.2351
batch size: (911, 911)
Epoch 129, accuracy: 0.3996
batch size: (888, 888)
Epoch 130, accuracy: 0.4031
Epoch 130, Train Loss: 1.1348, Val Loss: 1.3148
batch size: (918, 918)
Epoch 131, accuracy: 0.3996
batch size: (890, 890)
Epoch 132, accuracy: 0.4046
Epoch 132, Train Loss: 1.0695, Val Loss: 1.2314
batch size: (900, 900)
Epoch 133, accuracy: 0.4015
batch size: (922, 922)
Epoch 134, accuracy: 0.4030
Epoch 134, Train Loss: 1.0779, Val Loss: 1.2647
batch size: (902, 902)
Epoch 135, accuracy: 0.4031
batch size: (894, 894)
Epoch 136, accuracy: 0.4046
Epoch 136, Train Loss: 1.3443, Val Loss: 1.4613
batch size: (896, 896)
Epoch 137, accuracy: 0.4021
batch size: (887, 887)
Epoch 138, accuracy: 0.4038
Epoch 138, Train Loss: 1.1795, Val Loss: 1.3264
batch size: (909, 909)
Epoch 139, accuracy: 0.4011
batch size: (884, 884)
Epoch 140, accuracy: 0.4017
Epoch 140, Train Loss: 1.3467, Val Loss: 1.3464
batch size: (901, 901)
Epoch 141, accuracy: 0.4017
batch size: (907, 907)
Epoch 142, accuracy: 0.4030
Epoch 142, Train Loss: 1.0691, Val Loss: 1.2344
batch size: (914, 914)
Epoch 143, accuracy: 0.4052
batch size: (890, 890)
Epoch 144, accuracy: 0.4027
Epoch 144, Train Loss: 1.4369, Val Loss: 1.3471
batch size: (900, 900)
Epoch 145, accuracy: 0.4026
batch size: (884, 884)
Epoch 146, accuracy: 0.3983
Epoch 146, Train Loss: 1.0744, Val Loss: 1.2135
batch size: (919, 919)
Epoch 147, accuracy: 0.4029
batch size: (885, 885)
Epoch 148, accuracy: 0.3994
Epoch 148, Train Loss: 1.0707, Val Loss: 1.2629
batch size: (904, 904)
Epoch 149, accuracy: 0.4014
batch size: (897, 897)
Epoch 150, accuracy: 0.4017
Epoch 150, Train Loss: 1.0818, Val Loss: 1.2601
batch size: (885, 885)
Epoch 151, accuracy: 0.4041
batch size: (899, 899)
Epoch 152, accuracy: 0.4009
Epoch 152, Train Loss: 1.2951, Val Loss: 1.3498
batch size: (894, 894)
Epoch 153, accuracy: 0.3998
batch size: (898, 898)
Epoch 154, accuracy: 0.4045
Epoch 154, Train Loss: 1.0734, Val Loss: 1.2180
batch size: (907, 907)
Epoch 155, accuracy: 0.4049
batch size: (884, 884)
Epoch 156, accuracy: 0.4015
Epoch 156, Train Loss: 1.0732, Val Loss: 1.2424
batch size: (900, 900)
Epoch 157, accuracy: 0.4021
batch size: (876, 876)
Epoch 158, accuracy: 0.4036
Epoch 158, Train Loss: 1.0728, Val Loss: 1.2679
batch size: (869, 869)
Epoch 159, accuracy: 0.4042
batch size: (900, 900)
Epoch 160, accuracy: 0.4061
Epoch 160, Train Loss: 1.0765, Val Loss: 1.2253
batch size: (917, 917)
Epoch 161, accuracy: 0.4002
batch size: (888, 888)
Epoch 162, accuracy: 0.3982
Epoch 162, Train Loss: 1.1018, Val Loss: 1.3081
batch size: (913, 913)
Epoch 163, accuracy: 0.4031
batch size: (921, 921)
Epoch 164, accuracy: 0.3996
Epoch 164, Train Loss: 1.2018, Val Loss: 1.4415
batch size: (900, 900)
Epoch 165, accuracy: 0.4043
batch size: (897, 897)
Epoch 166, accuracy: 0.4014
Epoch 166, Train Loss: 1.4128, Val Loss: 1.3797
batch size: (908, 908)
Epoch 167, accuracy: 0.4069
batch size: (894, 894)
Epoch 168, accuracy: 0.3976
Epoch 168, Train Loss: 1.2960, Val Loss: 1.3939
batch size: (881, 881)
Epoch 169, accuracy: 0.4028
batch size: (897, 897)
Epoch 170, accuracy: 0.4025
Epoch 170, Train Loss: 1.0571, Val Loss: 1.2337
batch size: (895, 895)
Epoch 171, accuracy: 0.4073
batch size: (924, 924)
Epoch 172, accuracy: 0.4017
Epoch 172, Train Loss: 1.0633, Val Loss: 1.2604
batch size: (901, 901)
Epoch 173, accuracy: 0.4018
batch size: (887, 887)
Epoch 174, accuracy: 0.4038
Epoch 174, Train Loss: 1.1610, Val Loss: 1.3567
batch size: (904, 904)
Epoch 175, accuracy: 0.4015
batch size: (912, 912)
Epoch 176, accuracy: 0.4015
Epoch 176, Train Loss: 1.3675, Val Loss: 1.3416
batch size: (892, 892)
Epoch 177, accuracy: 0.4033
batch size: (905, 905)
Epoch 178, accuracy: 0.3988
Epoch 178, Train Loss: 1.0616, Val Loss: 1.2712
batch size: (901, 901)
Epoch 179, accuracy: 0.4023
batch size: (929, 929)
Epoch 180, accuracy: 0.4006
Epoch 180, Train Loss: 1.0561, Val Loss: 1.2391
batch size: (891, 891)
Epoch 181, accuracy: 0.3985
batch size: (926, 926)
Epoch 182, accuracy: 0.4048
Epoch 182, Train Loss: 1.0559, Val Loss: 1.2316
batch size: (909, 909)
Epoch 183, accuracy: 0.4016
batch size: (918, 918)
Epoch 184, accuracy: 0.4038
Epoch 184, Train Loss: 1.0762, Val Loss: 1.3221
batch size: (913, 913)
Epoch 185, accuracy: 0.4043
batch size: (887, 887)
Epoch 186, accuracy: 0.4007
Epoch 186, Train Loss: 1.4753, Val Loss: 1.3706
batch size: (879, 879)
Epoch 187, accuracy: 0.4005
batch size: (907, 907)
Epoch 188, accuracy: 0.4014
Epoch 188, Train Loss: 1.4503, Val Loss: 1.3539
batch size: (900, 900)
Epoch 189, accuracy: 0.4003
batch size: (896, 896)
Epoch 190, accuracy: 0.4059
Epoch 190, Train Loss: 1.0714, Val Loss: 1.2517
batch size: (900, 900)
Epoch 191, accuracy: 0.4010
batch size: (901, 901)
Epoch 192, accuracy: 0.4034
Epoch 192, Train Loss: 1.0717, Val Loss: 1.2730
batch size: (886, 886)
Epoch 193, accuracy: 0.4042
batch size: (890, 890)
Epoch 194, accuracy: 0.4032
Epoch 194, Train Loss: 1.0927, Val Loss: 1.2930
batch size: (904, 904)
Epoch 195, accuracy: 0.4006
batch size: (907, 907)
Epoch 196, accuracy: 0.4038
Epoch 196, Train Loss: 1.0726, Val Loss: 1.2690
batch size: (882, 882)
Epoch 197, accuracy: 0.4056
batch size: (894, 894)
Epoch 198, accuracy: 0.4004
Epoch 198, Train Loss: 1.0626, Val Loss: 1.2491
batch size: (890, 890)
Epoch 199, accuracy: 0.4020
Loaded best model with val_loss = 1.195007562637329
test :accuracy 0.4017, f1_macro: 0.1911, f1_micro: 0.4017, auc: 0.5026
Training mlp with 2 layers...
可训练参数: 83465_mlp
不可训练参数: 0
batch size: (898, 898)
✅ Epoch 0: New best model saved with val_loss = 1.0940
Epoch 0, accuracy: 0.4338
Epoch 0, Train Loss: 1.1243, Val Loss: 1.0940
batch size: (887, 887)
✅ Epoch 1: New best model saved with val_loss = 1.0913
Epoch 1, accuracy: 0.4292
batch size: (899, 899)
✅ Epoch 2: New best model saved with val_loss = 1.0877
Epoch 2, accuracy: 0.4293
Epoch 2, Train Loss: 0.1781, Val Loss: 1.0877
batch size: (899, 899)
✅ Epoch 3: New best model saved with val_loss = 1.0830
Epoch 3, accuracy: 0.4510
batch size: (895, 895)
✅ Epoch 4: New best model saved with val_loss = 1.0773
Epoch 4, accuracy: 0.5122
Epoch 4, Train Loss: 0.0182, Val Loss: 1.0773
batch size: (899, 899)
✅ Epoch 5: New best model saved with val_loss = 1.0710
Epoch 5, accuracy: 0.5560
batch size: (916, 916)
✅ Epoch 6: New best model saved with val_loss = 1.0645
Epoch 6, accuracy: 0.5916
Epoch 6, Train Loss: 0.0028, Val Loss: 1.0645
batch size: (915, 915)
✅ Epoch 7: New best model saved with val_loss = 1.0582
Epoch 7, accuracy: 0.6162
batch size: (893, 893)
✅ Epoch 8: New best model saved with val_loss = 1.0522
Epoch 8, accuracy: 0.6290
Epoch 8, Train Loss: 0.0007, Val Loss: 1.0522
batch size: (893, 893)
✅ Epoch 9: New best model saved with val_loss = 1.0464
Epoch 9, accuracy: 0.6408
batch size: (884, 884)
✅ Epoch 10: New best model saved with val_loss = 1.0407
Epoch 10, accuracy: 0.6455
Epoch 10, Train Loss: 0.0002, Val Loss: 1.0407
batch size: (895, 895)
✅ Epoch 11: New best model saved with val_loss = 1.0350
Epoch 11, accuracy: 0.6493
batch size: (906, 906)
✅ Epoch 12: New best model saved with val_loss = 1.0291
Epoch 12, accuracy: 0.6496
Epoch 12, Train Loss: 0.0001, Val Loss: 1.0291
batch size: (908, 908)
✅ Epoch 13: New best model saved with val_loss = 1.0229
Epoch 13, accuracy: 0.6550
batch size: (887, 887)
✅ Epoch 14: New best model saved with val_loss = 1.0163
Epoch 14, accuracy: 0.6589
Epoch 14, Train Loss: 0.0001, Val Loss: 1.0163
batch size: (899, 899)
✅ Epoch 15: New best model saved with val_loss = 1.0092
Epoch 15, accuracy: 0.6584
batch size: (900, 900)
✅ Epoch 16: New best model saved with val_loss = 1.0017
Epoch 16, accuracy: 0.6614
Epoch 16, Train Loss: 0.0001, Val Loss: 1.0017
batch size: (906, 906)
✅ Epoch 17: New best model saved with val_loss = 0.9938
Epoch 17, accuracy: 0.6605
batch size: (897, 897)
✅ Epoch 18: New best model saved with val_loss = 0.9856
Epoch 18, accuracy: 0.6616
Epoch 18, Train Loss: 0.0000, Val Loss: 0.9856
batch size: (909, 909)
✅ Epoch 19: New best model saved with val_loss = 0.9770
Epoch 19, accuracy: 0.6613
batch size: (901, 901)
✅ Epoch 20: New best model saved with val_loss = 0.9684
Epoch 20, accuracy: 0.6646
Epoch 20, Train Loss: 0.0000, Val Loss: 0.9684
batch size: (879, 879)
✅ Epoch 21: New best model saved with val_loss = 0.9597
Epoch 21, accuracy: 0.6682
batch size: (914, 914)
✅ Epoch 22: New best model saved with val_loss = 0.9509
Epoch 22, accuracy: 0.6666
Epoch 22, Train Loss: 0.0000, Val Loss: 0.9509
batch size: (904, 904)
✅ Epoch 23: New best model saved with val_loss = 0.9419
Epoch 23, accuracy: 0.6674
batch size: (891, 891)
✅ Epoch 24: New best model saved with val_loss = 0.9330
Epoch 24, accuracy: 0.6688
Epoch 24, Train Loss: 0.0000, Val Loss: 0.9330
batch size: (898, 898)
✅ Epoch 25: New best model saved with val_loss = 0.9238
Epoch 25, accuracy: 0.6698
batch size: (898, 898)
✅ Epoch 26: New best model saved with val_loss = 0.9149
Epoch 26, accuracy: 0.6702
Epoch 26, Train Loss: 0.0000, Val Loss: 0.9149
batch size: (910, 910)
✅ Epoch 27: New best model saved with val_loss = 0.9059
Epoch 27, accuracy: 0.6708
batch size: (904, 904)
✅ Epoch 28: New best model saved with val_loss = 0.8974
Epoch 28, accuracy: 0.6687
Epoch 28, Train Loss: 0.0000, Val Loss: 0.8974
batch size: (905, 905)
✅ Epoch 29: New best model saved with val_loss = 0.8890
Epoch 29, accuracy: 0.6701
batch size: (906, 906)
✅ Epoch 30: New best model saved with val_loss = 0.8811
Epoch 30, accuracy: 0.6688
Epoch 30, Train Loss: 0.0000, Val Loss: 0.8811
batch size: (917, 917)
✅ Epoch 31: New best model saved with val_loss = 0.8736
Epoch 31, accuracy: 0.6706
batch size: (897, 897)
✅ Epoch 32: New best model saved with val_loss = 0.8668
Epoch 32, accuracy: 0.6684
Epoch 32, Train Loss: 0.0000, Val Loss: 0.8668
batch size: (895, 895)
✅ Epoch 33: New best model saved with val_loss = 0.8602
Epoch 33, accuracy: 0.6669
batch size: (893, 893)
✅ Epoch 34: New best model saved with val_loss = 0.8540
Epoch 34, accuracy: 0.6649
Epoch 34, Train Loss: 0.0000, Val Loss: 0.8540
batch size: (888, 888)
✅ Epoch 35: New best model saved with val_loss = 0.8484
Epoch 35, accuracy: 0.6683
batch size: (912, 912)
✅ Epoch 36: New best model saved with val_loss = 0.8432
Epoch 36, accuracy: 0.6668
Epoch 36, Train Loss: 0.0000, Val Loss: 0.8432
batch size: (874, 874)
✅ Epoch 37: New best model saved with val_loss = 0.8382
Epoch 37, accuracy: 0.6646
batch size: (905, 905)
✅ Epoch 38: New best model saved with val_loss = 0.8334
Epoch 38, accuracy: 0.6649
Epoch 38, Train Loss: 0.0000, Val Loss: 0.8334
batch size: (902, 902)
✅ Epoch 39: New best model saved with val_loss = 0.8290
Epoch 39, accuracy: 0.6645
batch size: (902, 902)
✅ Epoch 40: New best model saved with val_loss = 0.8246
Epoch 40, accuracy: 0.6638
Epoch 40, Train Loss: 0.0000, Val Loss: 0.8246
batch size: (906, 906)
✅ Epoch 41: New best model saved with val_loss = 0.8201
Epoch 41, accuracy: 0.6652
batch size: (892, 892)
✅ Epoch 42: New best model saved with val_loss = 0.8158
Epoch 42, accuracy: 0.6652
Epoch 42, Train Loss: 0.0000, Val Loss: 0.8158
batch size: (899, 899)
✅ Epoch 43: New best model saved with val_loss = 0.8119
Epoch 43, accuracy: 0.6686
batch size: (903, 903)
✅ Epoch 44: New best model saved with val_loss = 0.8081
Epoch 44, accuracy: 0.6696
Epoch 44, Train Loss: 0.0000, Val Loss: 0.8081
batch size: (904, 904)
✅ Epoch 45: New best model saved with val_loss = 0.8045
Epoch 45, accuracy: 0.6692
batch size: (882, 882)
✅ Epoch 46: New best model saved with val_loss = 0.8016
Epoch 46, accuracy: 0.6690
Epoch 46, Train Loss: 0.0000, Val Loss: 0.8016
batch size: (885, 885)
✅ Epoch 47: New best model saved with val_loss = 0.7992
Epoch 47, accuracy: 0.6732
batch size: (908, 908)
✅ Epoch 48: New best model saved with val_loss = 0.7971
Epoch 48, accuracy: 0.6735
Epoch 48, Train Loss: 0.0001, Val Loss: 0.7971
batch size: (892, 892)
✅ Epoch 49: New best model saved with val_loss = 0.7949
Epoch 49, accuracy: 0.6744
batch size: (903, 903)
✅ Epoch 50: New best model saved with val_loss = 0.7937
Epoch 50, accuracy: 0.6734
Epoch 50, Train Loss: 0.0001, Val Loss: 0.7937
batch size: (892, 892)
✅ Epoch 51: New best model saved with val_loss = 0.7926
Epoch 51, accuracy: 0.6790
batch size: (903, 903)
✅ Epoch 52: New best model saved with val_loss = 0.7924
Epoch 52, accuracy: 0.6788
Epoch 52, Train Loss: 0.0001, Val Loss: 0.7924
batch size: (885, 885)
Epoch 53, accuracy: 0.6807
batch size: (901, 901)
Epoch 54, accuracy: 0.6816
Epoch 54, Train Loss: 0.0001, Val Loss: 0.7935
batch size: (921, 921)
Epoch 55, accuracy: 0.6811
batch size: (879, 879)
Epoch 56, accuracy: 0.6832
Epoch 56, Train Loss: 0.0001, Val Loss: 0.7972
batch size: (891, 891)
Epoch 57, accuracy: 0.6809
batch size: (899, 899)
Epoch 58, accuracy: 0.6806
Epoch 58, Train Loss: 0.0001, Val Loss: 0.8028
batch size: (907, 907)
Epoch 59, accuracy: 0.6809
batch size: (891, 891)
Epoch 60, accuracy: 0.6827
Epoch 60, Train Loss: 0.0001, Val Loss: 0.8109
batch size: (894, 894)
Epoch 61, accuracy: 0.6834
batch size: (871, 871)
Epoch 62, accuracy: 0.6808
Epoch 62, Train Loss: 0.0001, Val Loss: 0.8216
batch size: (883, 883)
Epoch 63, accuracy: 0.6825
batch size: (910, 910)
Epoch 64, accuracy: 0.6836
Epoch 64, Train Loss: 0.0001, Val Loss: 0.8347
batch size: (924, 924)
Epoch 65, accuracy: 0.6834
batch size: (877, 877)
Epoch 66, accuracy: 0.6824
Epoch 66, Train Loss: 0.0001, Val Loss: 0.8512
batch size: (902, 902)
Epoch 67, accuracy: 0.6844
batch size: (901, 901)
Epoch 68, accuracy: 0.6844
Epoch 68, Train Loss: 0.0001, Val Loss: 0.8683
batch size: (936, 936)
Epoch 69, accuracy: 0.6861
batch size: (891, 891)
Epoch 70, accuracy: 0.6862
Epoch 70, Train Loss: 0.0001, Val Loss: 0.8869
batch size: (885, 885)
Epoch 71, accuracy: 0.6858
batch size: (915, 915)
Epoch 72, accuracy: 0.6810
Epoch 72, Train Loss: 0.0001, Val Loss: 0.9068
batch size: (905, 905)
Epoch 73, accuracy: 0.6860
batch size: (878, 878)
Epoch 74, accuracy: 0.6852
Epoch 74, Train Loss: 0.0001, Val Loss: 0.9271
batch size: (895, 895)
Epoch 75, accuracy: 0.6846
batch size: (909, 909)
Epoch 76, accuracy: 0.6846
Epoch 76, Train Loss: 0.0001, Val Loss: 0.9475
batch size: (874, 874)
Epoch 77, accuracy: 0.6829
batch size: (902, 902)
Epoch 78, accuracy: 0.6859
Epoch 78, Train Loss: 0.0001, Val Loss: 0.9663
batch size: (908, 908)
Epoch 79, accuracy: 0.6835
batch size: (889, 889)
Epoch 80, accuracy: 0.6849
Epoch 80, Train Loss: 0.0001, Val Loss: 0.9861
batch size: (903, 903)
Epoch 81, accuracy: 0.6863
batch size: (902, 902)
Epoch 82, accuracy: 0.6867
Epoch 82, Train Loss: 0.0001, Val Loss: 1.0022
batch size: (894, 894)
Epoch 83, accuracy: 0.6840
batch size: (907, 907)
Epoch 84, accuracy: 0.6855
Epoch 84, Train Loss: 0.0001, Val Loss: 1.0176
batch size: (898, 898)
Epoch 85, accuracy: 0.6833
batch size: (898, 898)
Epoch 86, accuracy: 0.6864
Epoch 86, Train Loss: 0.0001, Val Loss: 1.0305
batch size: (906, 906)
Epoch 87, accuracy: 0.6866
batch size: (915, 915)
Epoch 88, accuracy: 0.6840
Epoch 88, Train Loss: 0.0001, Val Loss: 1.0435
batch size: (905, 905)
Epoch 89, accuracy: 0.6870
batch size: (891, 891)
Epoch 90, accuracy: 0.6858
Epoch 90, Train Loss: 0.0001, Val Loss: 1.0547
batch size: (884, 884)
Epoch 91, accuracy: 0.6861
batch size: (896, 896)
Epoch 92, accuracy: 0.6842
Epoch 92, Train Loss: 0.0001, Val Loss: 1.0631
batch size: (900, 900)
Epoch 93, accuracy: 0.6841
batch size: (909, 909)
Epoch 94, accuracy: 0.6868
Epoch 94, Train Loss: 0.0001, Val Loss: 1.0716
batch size: (888, 888)
Epoch 95, accuracy: 0.6850
batch size: (908, 908)
Epoch 96, accuracy: 0.6864
Epoch 96, Train Loss: 0.0001, Val Loss: 1.0777
batch size: (893, 893)
Epoch 97, accuracy: 0.6839
batch size: (892, 892)
Epoch 98, accuracy: 0.6865
Epoch 98, Train Loss: 0.0001, Val Loss: 1.0828
batch size: (910, 910)
Epoch 99, accuracy: 0.6868
batch size: (908, 908)
Epoch 100, accuracy: 0.6849
Epoch 100, Train Loss: 0.0001, Val Loss: 1.0887
batch size: (924, 924)
Epoch 101, accuracy: 0.6856
batch size: (899, 899)
Epoch 102, accuracy: 0.6837
Epoch 102, Train Loss: 0.0001, Val Loss: 1.0917
batch size: (879, 879)
Epoch 103, accuracy: 0.6841
batch size: (915, 915)
Epoch 104, accuracy: 0.6879
Epoch 104, Train Loss: 0.0001, Val Loss: 1.0947
batch size: (905, 905)
Epoch 105, accuracy: 0.6863
batch size: (909, 909)
Epoch 106, accuracy: 0.6867
Epoch 106, Train Loss: 0.0001, Val Loss: 1.0980
batch size: (886, 886)
Epoch 107, accuracy: 0.6868
batch size: (892, 892)
Epoch 108, accuracy: 0.6840
Epoch 108, Train Loss: 0.0001, Val Loss: 1.1006
batch size: (909, 909)
Epoch 109, accuracy: 0.6844
batch size: (911, 911)
Epoch 110, accuracy: 0.6846
Epoch 110, Train Loss: 0.0001, Val Loss: 1.1046
batch size: (890, 890)
Epoch 111, accuracy: 0.6835
batch size: (912, 912)
Epoch 112, accuracy: 0.6856
Epoch 112, Train Loss: 0.0001, Val Loss: 1.1072
batch size: (897, 897)
Epoch 113, accuracy: 0.6856
batch size: (878, 878)
Epoch 114, accuracy: 0.6854
Epoch 114, Train Loss: 0.0001, Val Loss: 1.1079
batch size: (922, 922)
Epoch 115, accuracy: 0.6839
batch size: (903, 903)
Epoch 116, accuracy: 0.6855
Epoch 116, Train Loss: 0.0001, Val Loss: 1.1091
batch size: (906, 906)
Epoch 117, accuracy: 0.6850
batch size: (906, 906)
Epoch 118, accuracy: 0.6854
Epoch 118, Train Loss: 0.0001, Val Loss: 1.1086
batch size: (908, 908)
Epoch 119, accuracy: 0.6852
batch size: (901, 901)
Epoch 120, accuracy: 0.6865
Epoch 120, Train Loss: 0.0001, Val Loss: 1.1096
batch size: (886, 886)
Epoch 121, accuracy: 0.6840
batch size: (914, 914)
Epoch 122, accuracy: 0.6855
Epoch 122, Train Loss: 0.0001, Val Loss: 1.1099
batch size: (899, 899)
Epoch 123, accuracy: 0.6862
batch size: (897, 897)
Epoch 124, accuracy: 0.6876
Epoch 124, Train Loss: 0.0001, Val Loss: 1.1098
batch size: (915, 915)
Epoch 125, accuracy: 0.6843
batch size: (910, 910)
Epoch 126, accuracy: 0.6850
Epoch 126, Train Loss: 0.0001, Val Loss: 1.1104
batch size: (908, 908)
Epoch 127, accuracy: 0.6865
batch size: (906, 906)
Epoch 128, accuracy: 0.6863
Epoch 128, Train Loss: 0.0001, Val Loss: 1.1121
batch size: (872, 872)
Epoch 129, accuracy: 0.6860
batch size: (914, 914)
Epoch 130, accuracy: 0.6863
Epoch 130, Train Loss: 0.0001, Val Loss: 1.1107
batch size: (909, 909)
Epoch 131, accuracy: 0.6844
batch size: (908, 908)
Epoch 132, accuracy: 0.6864
Epoch 132, Train Loss: 0.0001, Val Loss: 1.1120
batch size: (901, 901)
Epoch 133, accuracy: 0.6856
batch size: (912, 912)
Epoch 134, accuracy: 0.6838
Epoch 134, Train Loss: 0.0001, Val Loss: 1.1119
batch size: (906, 906)
Epoch 135, accuracy: 0.6866
batch size: (879, 879)
Epoch 136, accuracy: 0.6865
Epoch 136, Train Loss: 0.0001, Val Loss: 1.1111
batch size: (903, 903)
Epoch 137, accuracy: 0.6853
batch size: (916, 916)
Epoch 138, accuracy: 0.6847
Epoch 138, Train Loss: 0.0001, Val Loss: 1.1104
batch size: (893, 893)
Epoch 139, accuracy: 0.6835
batch size: (912, 912)
Epoch 140, accuracy: 0.6863
Epoch 140, Train Loss: 0.0001, Val Loss: 1.1108
batch size: (901, 901)
Epoch 141, accuracy: 0.6869
batch size: (909, 909)
Epoch 142, accuracy: 0.6849
Epoch 142, Train Loss: 0.0001, Val Loss: 1.1107
batch size: (901, 901)
Epoch 143, accuracy: 0.6842
batch size: (903, 903)
Epoch 144, accuracy: 0.6874
Epoch 144, Train Loss: 0.0001, Val Loss: 1.1093
batch size: (903, 903)
Epoch 145, accuracy: 0.6818
batch size: (901, 901)
Epoch 146, accuracy: 0.6868
Epoch 146, Train Loss: 0.0001, Val Loss: 1.1098
batch size: (894, 894)
Epoch 147, accuracy: 0.6862
batch size: (913, 913)
Epoch 148, accuracy: 0.6851
Epoch 148, Train Loss: 0.0001, Val Loss: 1.1104
batch size: (901, 901)
Epoch 149, accuracy: 0.6855
batch size: (895, 895)
Epoch 150, accuracy: 0.6861
Epoch 150, Train Loss: 0.0001, Val Loss: 1.1105
batch size: (887, 887)
Epoch 151, accuracy: 0.6880
batch size: (911, 911)
Epoch 152, accuracy: 0.6870
Epoch 152, Train Loss: 0.0001, Val Loss: 1.1092
batch size: (908, 908)
Epoch 153, accuracy: 0.6844
batch size: (901, 901)
Epoch 154, accuracy: 0.6844
Epoch 154, Train Loss: 0.0001, Val Loss: 1.1099
batch size: (907, 907)
Epoch 155, accuracy: 0.6854
batch size: (912, 912)
Epoch 156, accuracy: 0.6857
Epoch 156, Train Loss: 0.0001, Val Loss: 1.1089
batch size: (883, 883)
Epoch 157, accuracy: 0.6858
batch size: (901, 901)
Epoch 158, accuracy: 0.6863
Epoch 158, Train Loss: 0.0001, Val Loss: 1.1088
batch size: (902, 902)
Epoch 159, accuracy: 0.6866
batch size: (910, 910)
Epoch 160, accuracy: 0.6859
Epoch 160, Train Loss: 0.0001, Val Loss: 1.1089
batch size: (897, 897)
Epoch 161, accuracy: 0.6853
batch size: (881, 881)
Epoch 162, accuracy: 0.6856
Epoch 162, Train Loss: 0.0001, Val Loss: 1.1102
batch size: (897, 897)
Epoch 163, accuracy: 0.6862
batch size: (890, 890)
Epoch 164, accuracy: 0.6869
Epoch 164, Train Loss: 0.0001, Val Loss: 1.1106
batch size: (906, 906)
Epoch 165, accuracy: 0.6853
batch size: (895, 895)
Epoch 166, accuracy: 0.6843
Epoch 166, Train Loss: 0.0001, Val Loss: 1.1103
batch size: (903, 903)
Epoch 167, accuracy: 0.6841
batch size: (893, 893)
Epoch 168, accuracy: 0.6858
Epoch 168, Train Loss: 0.0001, Val Loss: 1.1089
batch size: (905, 905)
Epoch 169, accuracy: 0.6870
batch size: (899, 899)
Epoch 170, accuracy: 0.6873
Epoch 170, Train Loss: 0.0001, Val Loss: 1.1083
batch size: (896, 896)
Epoch 171, accuracy: 0.6854
batch size: (895, 895)
Epoch 172, accuracy: 0.6842
Epoch 172, Train Loss: 0.0001, Val Loss: 1.1082
batch size: (894, 894)
Epoch 173, accuracy: 0.6849
batch size: (895, 895)
Epoch 174, accuracy: 0.6863
Epoch 174, Train Loss: 0.0001, Val Loss: 1.1084
batch size: (919, 919)
Epoch 175, accuracy: 0.6860
batch size: (903, 903)
Epoch 176, accuracy: 0.6863
Epoch 176, Train Loss: 0.0001, Val Loss: 1.1075
batch size: (886, 886)
Epoch 177, accuracy: 0.6843
batch size: (909, 909)
Epoch 178, accuracy: 0.6851
Epoch 178, Train Loss: 0.0001, Val Loss: 1.1076
batch size: (894, 894)
Epoch 179, accuracy: 0.6846
batch size: (923, 923)
Epoch 180, accuracy: 0.6881
Epoch 180, Train Loss: 0.0001, Val Loss: 1.1070
batch size: (915, 915)
Epoch 181, accuracy: 0.6856
batch size: (913, 913)
Epoch 182, accuracy: 0.6841
Epoch 182, Train Loss: 0.0001, Val Loss: 1.1082
batch size: (905, 905)
Epoch 183, accuracy: 0.6879
batch size: (911, 911)
Epoch 184, accuracy: 0.6857
Epoch 184, Train Loss: 0.0001, Val Loss: 1.1091
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
batch size: (893, 893)
Epoch 185, accuracy: 0.6840
batch size: (892, 892)
Epoch 186, accuracy: 0.6870
Epoch 186, Train Loss: 0.0001, Val Loss: 1.1094
batch size: (905, 905)
Epoch 187, accuracy: 0.6852
batch size: (909, 909)
Epoch 188, accuracy: 0.6875
Epoch 188, Train Loss: 0.0001, Val Loss: 1.1092
batch size: (907, 907)
Epoch 189, accuracy: 0.6859
batch size: (890, 890)
Epoch 190, accuracy: 0.6855
Epoch 190, Train Loss: 0.0001, Val Loss: 1.1094
batch size: (905, 905)
Epoch 191, accuracy: 0.6852
batch size: (907, 907)
Epoch 192, accuracy: 0.6876
Epoch 192, Train Loss: 0.0001, Val Loss: 1.1098
batch size: (887, 887)
Epoch 193, accuracy: 0.6865
batch size: (905, 905)
Epoch 194, accuracy: 0.6866
Epoch 194, Train Loss: 0.0001, Val Loss: 1.1100
batch size: (898, 898)
Epoch 195, accuracy: 0.6862
batch size: (884, 884)
Epoch 196, accuracy: 0.6846
Epoch 196, Train Loss: 0.0001, Val Loss: 1.1100
batch size: (892, 892)
Epoch 197, accuracy: 0.6844
batch size: (894, 894)
Epoch 198, accuracy: 0.6844
Epoch 198, Train Loss: 0.0001, Val Loss: 1.1106
batch size: (901, 901)
Epoch 199, accuracy: 0.6868
Loaded best model with val_loss = 0.7924004197120667
test :accuracy 0.6787, f1_macro: 0.6755, f1_micro: 0.6787, auc: 0.8386
Training mlp with 8 layers...
可训练参数: 186377_mlp
不可训练参数: 0
batch size: (892, 892)
✅ Epoch 0: New best model saved with val_loss = 1.1080
Epoch 0, accuracy: 0.1682
Epoch 0, Train Loss: 1.1506, Val Loss: 1.1080
batch size: (898, 898)
✅ Epoch 1: New best model saved with val_loss = 1.1060
Epoch 1, accuracy: 0.1677
batch size: (910, 910)
✅ Epoch 2: New best model saved with val_loss = 1.1020
Epoch 2, accuracy: 0.4311
Epoch 2, Train Loss: 0.2054, Val Loss: 1.1020
batch size: (882, 882)
✅ Epoch 3: New best model saved with val_loss = 1.0970
Epoch 3, accuracy: 0.4271
batch size: (910, 910)
✅ Epoch 4: New best model saved with val_loss = 1.0921
Epoch 4, accuracy: 0.4303
Epoch 4, Train Loss: 0.0103, Val Loss: 1.0921
batch size: (916, 916)
✅ Epoch 5: New best model saved with val_loss = 1.0875
Epoch 5, accuracy: 0.4316
batch size: (896, 896)
✅ Epoch 6: New best model saved with val_loss = 1.0843
Epoch 6, accuracy: 0.4331
Epoch 6, Train Loss: 0.0013, Val Loss: 1.0843
batch size: (894, 894)
✅ Epoch 7: New best model saved with val_loss = 1.0824
Epoch 7, accuracy: 0.4341
batch size: (918, 918)
✅ Epoch 8: New best model saved with val_loss = 1.0806
Epoch 8, accuracy: 0.4296
Epoch 8, Train Loss: 0.0005, Val Loss: 1.0806
batch size: (917, 917)
✅ Epoch 9: New best model saved with val_loss = 1.0797
Epoch 9, accuracy: 0.4287
batch size: (896, 896)
✅ Epoch 10: New best model saved with val_loss = 1.0789
Epoch 10, accuracy: 0.4312
Epoch 10, Train Loss: 0.0002, Val Loss: 1.0789
batch size: (908, 908)
Epoch 11, accuracy: 0.4310
batch size: (890, 890)
Epoch 12, accuracy: 0.4334
Epoch 12, Train Loss: 0.0001, Val Loss: 1.0797
batch size: (891, 891)
Epoch 13, accuracy: 0.4297
batch size: (910, 910)
Epoch 14, accuracy: 0.4394
Epoch 14, Train Loss: 0.0001, Val Loss: 1.0799
batch size: (902, 902)
Epoch 15, accuracy: 0.5316
batch size: (885, 885)
Epoch 16, accuracy: 0.4397
Epoch 16, Train Loss: 0.0000, Val Loss: 1.0791
batch size: (898, 898)
Epoch 17, accuracy: 0.4185
batch size: (884, 884)
Epoch 18, accuracy: 0.4078
Epoch 18, Train Loss: 0.0000, Val Loss: 1.0796
batch size: (900, 900)
Epoch 19, accuracy: 0.4024
batch size: (891, 891)
Epoch 20, accuracy: 0.4043
Epoch 20, Train Loss: 0.0000, Val Loss: 1.0827
batch size: (911, 911)
Epoch 21, accuracy: 0.4088
batch size: (905, 905)
Epoch 22, accuracy: 0.4059
Epoch 22, Train Loss: 0.0000, Val Loss: 1.0865
batch size: (892, 892)
Epoch 23, accuracy: 0.4036
batch size: (889, 889)
Epoch 24, accuracy: 0.4086
Epoch 24, Train Loss: 0.0000, Val Loss: 1.0892
batch size: (886, 886)
Epoch 25, accuracy: 0.4041
batch size: (892, 892)
Epoch 26, accuracy: 0.4096
Epoch 26, Train Loss: 0.0000, Val Loss: 1.0899
batch size: (901, 901)
Epoch 27, accuracy: 0.4095
batch size: (888, 888)
Epoch 28, accuracy: 0.4065
Epoch 28, Train Loss: 0.0000, Val Loss: 1.0884
batch size: (884, 884)
Epoch 29, accuracy: 0.4123
batch size: (906, 906)
Epoch 30, accuracy: 0.4084
Epoch 30, Train Loss: 0.0000, Val Loss: 1.0867
batch size: (880, 880)
Epoch 31, accuracy: 0.4102
batch size: (886, 886)
Epoch 32, accuracy: 0.4143
Epoch 32, Train Loss: 0.0000, Val Loss: 1.0840
batch size: (887, 887)
Epoch 33, accuracy: 0.4169
batch size: (903, 903)
Epoch 34, accuracy: 0.4121
Epoch 34, Train Loss: 0.0000, Val Loss: 1.0804
batch size: (908, 908)
✅ Epoch 35: New best model saved with val_loss = 1.0787
Epoch 35, accuracy: 0.4178
batch size: (892, 892)
✅ Epoch 36: New best model saved with val_loss = 1.0762
Epoch 36, accuracy: 0.4197
Epoch 36, Train Loss: 0.0000, Val Loss: 1.0762
batch size: (899, 899)
✅ Epoch 37: New best model saved with val_loss = 1.0746
Epoch 37, accuracy: 0.4204
batch size: (896, 896)
✅ Epoch 38: New best model saved with val_loss = 1.0724
Epoch 38, accuracy: 0.4221
Epoch 38, Train Loss: 0.0000, Val Loss: 1.0724
batch size: (914, 914)
✅ Epoch 39: New best model saved with val_loss = 1.0708
Epoch 39, accuracy: 0.4289
batch size: (888, 888)
✅ Epoch 40: New best model saved with val_loss = 1.0687
Epoch 40, accuracy: 0.4283
Epoch 40, Train Loss: 0.0000, Val Loss: 1.0687
batch size: (891, 891)
✅ Epoch 41: New best model saved with val_loss = 1.0656
Epoch 41, accuracy: 0.4315
batch size: (900, 900)
✅ Epoch 42: New best model saved with val_loss = 1.0631
Epoch 42, accuracy: 0.4348
Epoch 42, Train Loss: 0.0000, Val Loss: 1.0631
batch size: (896, 896)
✅ Epoch 43: New best model saved with val_loss = 1.0609
Epoch 43, accuracy: 0.4398
batch size: (894, 894)
✅ Epoch 44: New best model saved with val_loss = 1.0580
Epoch 44, accuracy: 0.4488
Epoch 44, Train Loss: 0.0000, Val Loss: 1.0580
batch size: (905, 905)
✅ Epoch 45: New best model saved with val_loss = 1.0551
Epoch 45, accuracy: 0.4550
batch size: (898, 898)
✅ Epoch 46: New best model saved with val_loss = 1.0517
Epoch 46, accuracy: 0.4633
Epoch 46, Train Loss: 0.0000, Val Loss: 1.0517
batch size: (898, 898)
✅ Epoch 47: New best model saved with val_loss = 1.0486
Epoch 47, accuracy: 0.4669
batch size: (887, 887)
✅ Epoch 48: New best model saved with val_loss = 1.0460
Epoch 48, accuracy: 0.4738
Epoch 48, Train Loss: 0.0000, Val Loss: 1.0460
batch size: (899, 899)
✅ Epoch 49: New best model saved with val_loss = 1.0428
Epoch 49, accuracy: 0.4856
batch size: (896, 896)
✅ Epoch 50: New best model saved with val_loss = 1.0402
Epoch 50, accuracy: 0.4869
Epoch 50, Train Loss: 0.0000, Val Loss: 1.0402
batch size: (910, 910)
✅ Epoch 51: New best model saved with val_loss = 1.0372
Epoch 51, accuracy: 0.4988
batch size: (903, 903)
✅ Epoch 52: New best model saved with val_loss = 1.0350
Epoch 52, accuracy: 0.5060
Epoch 52, Train Loss: 0.0000, Val Loss: 1.0350
batch size: (893, 893)
✅ Epoch 53: New best model saved with val_loss = 1.0324
Epoch 53, accuracy: 0.5137
batch size: (903, 903)
✅ Epoch 54: New best model saved with val_loss = 1.0287
Epoch 54, accuracy: 0.5204
Epoch 54, Train Loss: 0.0000, Val Loss: 1.0287
batch size: (887, 887)
✅ Epoch 55: New best model saved with val_loss = 1.0254
Epoch 55, accuracy: 0.5284
batch size: (893, 893)
✅ Epoch 56: New best model saved with val_loss = 1.0231
Epoch 56, accuracy: 0.5346
Epoch 56, Train Loss: 0.0000, Val Loss: 1.0231
batch size: (911, 911)
✅ Epoch 57: New best model saved with val_loss = 1.0201
Epoch 57, accuracy: 0.5405
batch size: (904, 904)
✅ Epoch 58: New best model saved with val_loss = 1.0180
Epoch 58, accuracy: 0.5425
Epoch 58, Train Loss: 0.0000, Val Loss: 1.0180
batch size: (898, 898)
✅ Epoch 59: New best model saved with val_loss = 1.0174
Epoch 59, accuracy: 0.5435
batch size: (906, 906)
✅ Epoch 60: New best model saved with val_loss = 1.0167
Epoch 60, accuracy: 0.5447
Epoch 60, Train Loss: 0.0000, Val Loss: 1.0167
batch size: (899, 899)
✅ Epoch 61: New best model saved with val_loss = 1.0161
Epoch 61, accuracy: 0.5475
batch size: (889, 889)
Epoch 62, accuracy: 0.5477
Epoch 62, Train Loss: 0.0000, Val Loss: 1.0170
batch size: (909, 909)
Epoch 63, accuracy: 0.5500
batch size: (921, 921)
Epoch 64, accuracy: 0.5509
Epoch 64, Train Loss: 0.0000, Val Loss: 1.0185
batch size: (908, 908)
Epoch 65, accuracy: 0.5501
batch size: (896, 896)
Epoch 66, accuracy: 0.5490
Epoch 66, Train Loss: 0.0000, Val Loss: 1.0220
batch size: (892, 892)
Epoch 67, accuracy: 0.5523
batch size: (890, 890)
Epoch 68, accuracy: 0.5483
Epoch 68, Train Loss: 0.0000, Val Loss: 1.0288
batch size: (915, 915)
Epoch 69, accuracy: 0.5473
batch size: (907, 907)
Epoch 70, accuracy: 0.5464
Epoch 70, Train Loss: 0.0000, Val Loss: 1.0397
batch size: (894, 894)
Epoch 71, accuracy: 0.5436
batch size: (903, 903)
Epoch 72, accuracy: 0.5404
Epoch 72, Train Loss: 0.0000, Val Loss: 1.0549
batch size: (901, 901)
Epoch 73, accuracy: 0.5390
batch size: (905, 905)
Epoch 74, accuracy: 0.5405
Epoch 74, Train Loss: 0.0000, Val Loss: 1.0742
batch size: (905, 905)
Epoch 75, accuracy: 0.5395
batch size: (910, 910)
Epoch 76, accuracy: 0.5416
Epoch 76, Train Loss: 0.0000, Val Loss: 1.0966
batch size: (897, 897)
Epoch 77, accuracy: 0.5386
batch size: (917, 917)
Epoch 78, accuracy: 0.5375
Epoch 78, Train Loss: 0.0000, Val Loss: 1.1219
batch size: (903, 903)
Epoch 79, accuracy: 0.5353
batch size: (897, 897)
Epoch 80, accuracy: 0.5354
Epoch 80, Train Loss: 0.0000, Val Loss: 1.1500
batch size: (907, 907)
Epoch 81, accuracy: 0.5353
batch size: (907, 907)
Epoch 82, accuracy: 0.5342
Epoch 82, Train Loss: 0.0000, Val Loss: 1.1784
batch size: (893, 893)
Epoch 83, accuracy: 0.5349
batch size: (890, 890)
Epoch 84, accuracy: 0.5312
Epoch 84, Train Loss: 0.0000, Val Loss: 1.2047
batch size: (884, 884)
Epoch 85, accuracy: 0.5336
batch size: (912, 912)
Epoch 86, accuracy: 0.5325
Epoch 86, Train Loss: 0.0000, Val Loss: 1.2288
batch size: (922, 922)
Epoch 87, accuracy: 0.5338
batch size: (904, 904)
Epoch 88, accuracy: 0.5348
Epoch 88, Train Loss: 0.0000, Val Loss: 1.2514
batch size: (891, 891)
Epoch 89, accuracy: 0.5343
batch size: (915, 915)
Epoch 90, accuracy: 0.5337
Epoch 90, Train Loss: 0.0000, Val Loss: 1.2733
batch size: (893, 893)
Epoch 91, accuracy: 0.5335
batch size: (907, 907)
Epoch 92, accuracy: 0.5311
Epoch 92, Train Loss: 0.0000, Val Loss: 1.2954
batch size: (905, 905)
Epoch 93, accuracy: 0.5330
batch size: (901, 901)
Epoch 94, accuracy: 0.5332
Epoch 94, Train Loss: 0.0000, Val Loss: 1.3117
batch size: (914, 914)
Epoch 95, accuracy: 0.5352
batch size: (888, 888)
Epoch 96, accuracy: 0.5334
Epoch 96, Train Loss: 0.0000, Val Loss: 1.3290
batch size: (896, 896)
Epoch 97, accuracy: 0.5357
batch size: (879, 879)
Epoch 98, accuracy: 0.5338
Epoch 98, Train Loss: 0.0000, Val Loss: 1.3420
batch size: (896, 896)
Epoch 99, accuracy: 0.5338
batch size: (890, 890)
Epoch 100, accuracy: 0.5347
Epoch 100, Train Loss: 0.0000, Val Loss: 1.3537
batch size: (906, 906)
Epoch 101, accuracy: 0.5373
batch size: (883, 883)
Epoch 102, accuracy: 0.5355
Epoch 102, Train Loss: 0.0000, Val Loss: 1.3658
batch size: (898, 898)
Epoch 103, accuracy: 0.5344
batch size: (920, 920)
Epoch 104, accuracy: 0.5329
Epoch 104, Train Loss: 0.0000, Val Loss: 1.3810
batch size: (906, 906)
Epoch 105, accuracy: 0.5327
batch size: (918, 918)
Epoch 106, accuracy: 0.5352
Epoch 106, Train Loss: 0.0000, Val Loss: 1.3881
batch size: (884, 884)
Epoch 107, accuracy: 0.5328
batch size: (902, 902)
Epoch 108, accuracy: 0.5373
Epoch 108, Train Loss: 0.0000, Val Loss: 1.3957
batch size: (879, 879)
Epoch 109, accuracy: 0.5364
batch size: (899, 899)
Epoch 110, accuracy: 0.5371
Epoch 110, Train Loss: 0.0000, Val Loss: 1.4027
batch size: (910, 910)
Epoch 111, accuracy: 0.5355
batch size: (882, 882)
Epoch 112, accuracy: 0.5351
Epoch 112, Train Loss: 0.0000, Val Loss: 1.4115
batch size: (902, 902)
Epoch 113, accuracy: 0.5317
batch size: (904, 904)
Epoch 114, accuracy: 0.5344
Epoch 114, Train Loss: 0.0000, Val Loss: 1.4175
batch size: (890, 890)
Epoch 115, accuracy: 0.5354
batch size: (924, 924)
Epoch 116, accuracy: 0.5351
Epoch 116, Train Loss: 0.0000, Val Loss: 1.4242
batch size: (887, 887)
Epoch 117, accuracy: 0.5357
batch size: (902, 902)
Epoch 118, accuracy: 0.5352
Epoch 118, Train Loss: 0.0000, Val Loss: 1.4259
batch size: (897, 897)
Epoch 119, accuracy: 0.5358
batch size: (909, 909)
Epoch 120, accuracy: 0.5376
Epoch 120, Train Loss: 0.0000, Val Loss: 1.4286
batch size: (900, 900)
Epoch 121, accuracy: 0.5362
batch size: (908, 908)
Epoch 122, accuracy: 0.5356
Epoch 122, Train Loss: 0.0000, Val Loss: 1.4308
batch size: (913, 913)
Epoch 123, accuracy: 0.5353
batch size: (901, 901)
Epoch 124, accuracy: 0.5378
Epoch 124, Train Loss: 0.0000, Val Loss: 1.4315
batch size: (897, 897)
Epoch 125, accuracy: 0.5388
batch size: (908, 908)
Epoch 126, accuracy: 0.5382
Epoch 126, Train Loss: 0.0000, Val Loss: 1.4329
batch size: (899, 899)
Epoch 127, accuracy: 0.5372
batch size: (903, 903)
Epoch 128, accuracy: 0.5383
Epoch 128, Train Loss: 0.0000, Val Loss: 1.4358
batch size: (896, 896)
Epoch 129, accuracy: 0.5354
batch size: (895, 895)
Epoch 130, accuracy: 0.5354
Epoch 130, Train Loss: 0.0000, Val Loss: 1.4374
batch size: (921, 921)
Epoch 131, accuracy: 0.5359
batch size: (914, 914)
Epoch 132, accuracy: 0.5373
Epoch 132, Train Loss: 0.0000, Val Loss: 1.4398
batch size: (911, 911)
Epoch 133, accuracy: 0.5382
batch size: (909, 909)
Epoch 134, accuracy: 0.5368
Epoch 134, Train Loss: 0.0000, Val Loss: 1.4404
batch size: (885, 885)
Epoch 135, accuracy: 0.5359
batch size: (887, 887)
Epoch 136, accuracy: 0.5363
Epoch 136, Train Loss: 0.0000, Val Loss: 1.4413
batch size: (888, 888)
Epoch 137, accuracy: 0.5377
batch size: (921, 921)
Epoch 138, accuracy: 0.5365
Epoch 138, Train Loss: 0.0000, Val Loss: 1.4400
batch size: (905, 905)
Epoch 139, accuracy: 0.5367
batch size: (889, 889)
Epoch 140, accuracy: 0.5375
Epoch 140, Train Loss: 0.0000, Val Loss: 1.4379
batch size: (889, 889)
Epoch 141, accuracy: 0.5360
batch size: (896, 896)
Epoch 142, accuracy: 0.5354
Epoch 142, Train Loss: 0.0000, Val Loss: 1.4401
batch size: (912, 912)
Epoch 143, accuracy: 0.5376
batch size: (868, 868)
Epoch 144, accuracy: 0.5337
Epoch 144, Train Loss: 0.0000, Val Loss: 1.4397
batch size: (912, 912)
Epoch 145, accuracy: 0.5368
batch size: (895, 895)
Epoch 146, accuracy: 0.5386
Epoch 146, Train Loss: 0.0000, Val Loss: 1.4390
batch size: (909, 909)
Epoch 147, accuracy: 0.5365
batch size: (881, 881)
Epoch 148, accuracy: 0.5379
Epoch 148, Train Loss: 0.0000, Val Loss: 1.4423
batch size: (907, 907)
Epoch 149, accuracy: 0.5389
batch size: (887, 887)
Epoch 150, accuracy: 0.5375
Epoch 150, Train Loss: 0.0000, Val Loss: 1.4429
batch size: (896, 896)
Epoch 151, accuracy: 0.5386
batch size: (896, 896)
Epoch 152, accuracy: 0.5387
Epoch 152, Train Loss: 0.0000, Val Loss: 1.4430
batch size: (894, 894)
Epoch 153, accuracy: 0.5375
batch size: (887, 887)
Epoch 154, accuracy: 0.5370
Epoch 154, Train Loss: 0.0000, Val Loss: 1.4468
batch size: (910, 910)
Epoch 155, accuracy: 0.5352
batch size: (902, 902)
Epoch 156, accuracy: 0.5361
Epoch 156, Train Loss: 0.0000, Val Loss: 1.4465
batch size: (890, 890)
Epoch 157, accuracy: 0.5359
batch size: (897, 897)
Epoch 158, accuracy: 0.5353
Epoch 158, Train Loss: 0.0000, Val Loss: 1.4426
batch size: (898, 898)
Epoch 159, accuracy: 0.5369
batch size: (873, 873)
Epoch 160, accuracy: 0.5380
Epoch 160, Train Loss: 0.0000, Val Loss: 1.4411
batch size: (899, 899)
Epoch 161, accuracy: 0.5374
batch size: (914, 914)
Epoch 162, accuracy: 0.5377
Epoch 162, Train Loss: 0.0000, Val Loss: 1.4362
batch size: (897, 897)
Epoch 163, accuracy: 0.5376
batch size: (892, 892)
Epoch 164, accuracy: 0.5361
Epoch 164, Train Loss: 0.0000, Val Loss: 1.4383
batch size: (892, 892)
Epoch 165, accuracy: 0.5376
batch size: (889, 889)
Epoch 166, accuracy: 0.5384
Epoch 166, Train Loss: 0.0000, Val Loss: 1.4376
batch size: (898, 898)
Epoch 167, accuracy: 0.5383
batch size: (900, 900)
Epoch 168, accuracy: 0.5391
Epoch 168, Train Loss: 0.0000, Val Loss: 1.4362
batch size: (884, 884)
Epoch 169, accuracy: 0.5392
batch size: (892, 892)
Epoch 170, accuracy: 0.5375
Epoch 170, Train Loss: 0.0000, Val Loss: 1.4388
batch size: (913, 913)
Epoch 171, accuracy: 0.5378
batch size: (892, 892)
Epoch 172, accuracy: 0.5383
Epoch 172, Train Loss: 0.0000, Val Loss: 1.4394
batch size: (902, 902)
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 173, accuracy: 0.5397
batch size: (916, 916)
Epoch 174, accuracy: 0.5368
Epoch 174, Train Loss: 0.0000, Val Loss: 1.4388
batch size: (917, 917)
Epoch 175, accuracy: 0.5399
batch size: (892, 892)
Epoch 176, accuracy: 0.5387
Epoch 176, Train Loss: 0.0000, Val Loss: 1.4403
batch size: (900, 900)
Epoch 177, accuracy: 0.5372
batch size: (906, 906)
Epoch 178, accuracy: 0.5380
Epoch 178, Train Loss: 0.0000, Val Loss: 1.4428
batch size: (907, 907)
Epoch 179, accuracy: 0.5373
batch size: (888, 888)
Epoch 180, accuracy: 0.5386
Epoch 180, Train Loss: 0.0000, Val Loss: 1.4430
batch size: (895, 895)
Epoch 181, accuracy: 0.5384
batch size: (905, 905)
Epoch 182, accuracy: 0.5384
Epoch 182, Train Loss: 0.0000, Val Loss: 1.4408
batch size: (888, 888)
Epoch 183, accuracy: 0.5382
batch size: (884, 884)
Epoch 184, accuracy: 0.5377
Epoch 184, Train Loss: 0.0000, Val Loss: 1.4437
batch size: (903, 903)
Epoch 185, accuracy: 0.5393
batch size: (895, 895)
Epoch 186, accuracy: 0.5370
Epoch 186, Train Loss: 0.0000, Val Loss: 1.4404
batch size: (908, 908)
Epoch 187, accuracy: 0.5383
batch size: (881, 881)
Epoch 188, accuracy: 0.5387
Epoch 188, Train Loss: 0.0000, Val Loss: 1.4370
batch size: (906, 906)
Epoch 189, accuracy: 0.5388
batch size: (896, 896)
Epoch 190, accuracy: 0.5384
Epoch 190, Train Loss: 0.0000, Val Loss: 1.4375
batch size: (905, 905)
Epoch 191, accuracy: 0.5390
batch size: (893, 893)
Epoch 192, accuracy: 0.5363
Epoch 192, Train Loss: 0.0000, Val Loss: 1.4375
batch size: (901, 901)
Epoch 193, accuracy: 0.5364
batch size: (902, 902)
Epoch 194, accuracy: 0.5378
Epoch 194, Train Loss: 0.0000, Val Loss: 1.4361
batch size: (900, 900)
Epoch 195, accuracy: 0.5371
batch size: (903, 903)
Epoch 196, accuracy: 0.5380
Epoch 196, Train Loss: 0.0000, Val Loss: 1.4366
batch size: (900, 900)
Epoch 197, accuracy: 0.5384
batch size: (915, 915)
Epoch 198, accuracy: 0.5381
Epoch 198, Train Loss: 0.0000, Val Loss: 1.4385
batch size: (903, 903)
Epoch 199, accuracy: 0.5377
Loaded best model with val_loss = 1.0161420106887817
test :accuracy 0.5471, f1_macro: 0.5137, f1_micro: 0.5471, auc: 0.6949
Training mlp with 32 layers...
可训练参数: 598025_mlp
不可训练参数: 0
batch size: (890, 890)
✅ Epoch 0: New best model saved with val_loss = 1.0955
Epoch 0, accuracy: 0.4047
Epoch 0, Train Loss: 1.0974, Val Loss: 1.0955
batch size: (892, 892)
Epoch 1, accuracy: 0.4007
batch size: (893, 893)
Epoch 2, accuracy: 0.3964
Epoch 2, Train Loss: 1.7255, Val Loss: 1.1016
batch size: (904, 904)
Epoch 3, accuracy: 0.1665
batch size: (912, 912)
Epoch 4, accuracy: 0.1674
Epoch 4, Train Loss: 1.2124, Val Loss: 1.1067
batch size: (888, 888)
Epoch 5, accuracy: 0.1687
batch size: (903, 903)
Epoch 6, accuracy: 0.1671
Epoch 6, Train Loss: 1.0423, Val Loss: 1.1080
batch size: (887, 887)
Epoch 7, accuracy: 0.1650
batch size: (895, 895)
Epoch 8, accuracy: 0.1699
Epoch 8, Train Loss: 0.9179, Val Loss: 1.1079
batch size: (916, 916)
Epoch 9, accuracy: 0.1666
batch size: (885, 885)
Epoch 10, accuracy: 0.1704
Epoch 10, Train Loss: 0.8565, Val Loss: 1.1079
batch size: (888, 888)
Epoch 11, accuracy: 0.1652
batch size: (905, 905)
Epoch 12, accuracy: 0.1675
Epoch 12, Train Loss: 0.7783, Val Loss: 1.1083
batch size: (890, 890)
Epoch 13, accuracy: 0.1680
batch size: (911, 911)
Epoch 14, accuracy: 0.1688
Epoch 14, Train Loss: 0.7488, Val Loss: 1.1083
batch size: (891, 891)
Epoch 15, accuracy: 0.1687
batch size: (914, 914)
Epoch 16, accuracy: 0.1692
Epoch 16, Train Loss: 0.7473, Val Loss: 1.1082
batch size: (907, 907)
Epoch 17, accuracy: 0.1684
batch size: (909, 909)
Epoch 18, accuracy: 0.1701
Epoch 18, Train Loss: 0.7478, Val Loss: 1.1079
batch size: (890, 890)
Epoch 19, accuracy: 0.1685
batch size: (902, 902)
Epoch 20, accuracy: 0.1692
Epoch 20, Train Loss: 0.7323, Val Loss: 1.1075
batch size: (884, 884)
Epoch 21, accuracy: 0.1661
batch size: (904, 904)
Epoch 22, accuracy: 0.1677
Epoch 22, Train Loss: 0.7352, Val Loss: 1.1073
batch size: (898, 898)
Epoch 23, accuracy: 0.1650
batch size: (901, 901)
Epoch 24, accuracy: 0.1685
Epoch 24, Train Loss: 0.7356, Val Loss: 1.1072
batch size: (901, 901)
Epoch 25, accuracy: 0.1657
batch size: (907, 907)
Epoch 26, accuracy: 0.1674
Epoch 26, Train Loss: 0.7381, Val Loss: 1.1069
batch size: (896, 896)
Epoch 27, accuracy: 0.1684
batch size: (893, 893)
Epoch 28, accuracy: 0.1699
Epoch 28, Train Loss: 0.7259, Val Loss: 1.1065
batch size: (897, 897)
Epoch 29, accuracy: 0.1681
batch size: (921, 921)
Epoch 30, accuracy: 0.1673
Epoch 30, Train Loss: 0.7260, Val Loss: 1.1060
batch size: (886, 886)
Epoch 31, accuracy: 0.1677
batch size: (909, 909)
Epoch 32, accuracy: 0.1674
Epoch 32, Train Loss: 0.7342, Val Loss: 1.1058
batch size: (893, 893)
Epoch 33, accuracy: 0.1688
batch size: (906, 906)
Epoch 34, accuracy: 0.1673
Epoch 34, Train Loss: 0.7339, Val Loss: 1.1054
batch size: (907, 907)
Epoch 35, accuracy: 0.1665
batch size: (902, 902)
Epoch 36, accuracy: 0.1681
Epoch 36, Train Loss: 0.7294, Val Loss: 1.1050
batch size: (913, 913)
Epoch 37, accuracy: 0.1681
batch size: (871, 871)
Epoch 38, accuracy: 0.1674
Epoch 38, Train Loss: 0.7377, Val Loss: 1.1048
batch size: (900, 900)
Epoch 39, accuracy: 0.1671
batch size: (904, 904)
Epoch 40, accuracy: 0.1665
Epoch 40, Train Loss: 0.7374, Val Loss: 1.1043
batch size: (903, 903)
Epoch 41, accuracy: 0.1647
batch size: (893, 893)
Epoch 42, accuracy: 0.1666
Epoch 42, Train Loss: 0.7261, Val Loss: 1.1039
batch size: (895, 895)
Epoch 43, accuracy: 0.1672
batch size: (906, 906)
Epoch 44, accuracy: 0.1682
Epoch 44, Train Loss: 0.7286, Val Loss: 1.1035
batch size: (908, 908)
Epoch 45, accuracy: 0.1671
batch size: (894, 894)
Epoch 46, accuracy: 0.1676
Epoch 46, Train Loss: 0.7250, Val Loss: 1.1032
batch size: (901, 901)
Epoch 47, accuracy: 0.1688
batch size: (882, 882)
Epoch 48, accuracy: 0.1679
Epoch 48, Train Loss: 0.7249, Val Loss: 1.1028
batch size: (915, 915)
Epoch 49, accuracy: 0.1691
batch size: (889, 889)
Epoch 50, accuracy: 0.1658
Epoch 50, Train Loss: 0.7427, Val Loss: 1.1027
batch size: (908, 908)
Epoch 51, accuracy: 0.1667
batch size: (902, 902)
Epoch 52, accuracy: 0.1653
Epoch 52, Train Loss: 0.7407, Val Loss: 1.1026
batch size: (880, 880)
Epoch 53, accuracy: 0.1686
batch size: (905, 905)
Epoch 54, accuracy: 0.1677
Epoch 54, Train Loss: 0.7235, Val Loss: 1.1025
batch size: (897, 897)
Epoch 55, accuracy: 0.1683
batch size: (898, 898)
Epoch 56, accuracy: 0.1676
Epoch 56, Train Loss: 0.7313, Val Loss: 1.1025
batch size: (918, 918)
Epoch 57, accuracy: 0.1656
batch size: (895, 895)
Epoch 58, accuracy: 0.1695
Epoch 58, Train Loss: 0.7364, Val Loss: 1.1024
batch size: (894, 894)
Epoch 59, accuracy: 0.1736
batch size: (912, 912)
Epoch 60, accuracy: 0.1745
Epoch 60, Train Loss: 0.7394, Val Loss: 1.1025
batch size: (914, 914)
Epoch 61, accuracy: 0.1715
batch size: (917, 917)
Epoch 62, accuracy: 0.1727
Epoch 62, Train Loss: 0.7253, Val Loss: 1.1025
batch size: (890, 890)
Epoch 63, accuracy: 0.1747
batch size: (882, 882)
Epoch 64, accuracy: 0.1759
Epoch 64, Train Loss: 0.7255, Val Loss: 1.1025
batch size: (910, 910)
Epoch 65, accuracy: 0.1778
batch size: (909, 909)
Epoch 66, accuracy: 0.1781
Epoch 66, Train Loss: 0.7233, Val Loss: 1.1026
batch size: (912, 912)
Epoch 67, accuracy: 0.1788
batch size: (911, 911)
Epoch 68, accuracy: 0.1820
Epoch 68, Train Loss: 0.7329, Val Loss: 1.1025
batch size: (892, 892)
Epoch 69, accuracy: 0.1846
batch size: (914, 914)
Epoch 70, accuracy: 0.1847
Epoch 70, Train Loss: 0.7321, Val Loss: 1.1025
batch size: (889, 889)
Epoch 71, accuracy: 0.1861
batch size: (907, 907)
Epoch 72, accuracy: 0.1895
Epoch 72, Train Loss: 0.7342, Val Loss: 1.1025
batch size: (891, 891)
Epoch 73, accuracy: 0.1865
batch size: (905, 905)
Epoch 74, accuracy: 0.1927
Epoch 74, Train Loss: 0.7273, Val Loss: 1.1024
batch size: (905, 905)
Epoch 75, accuracy: 0.1921
batch size: (902, 902)
Epoch 76, accuracy: 0.1953
Epoch 76, Train Loss: 0.7360, Val Loss: 1.1023
batch size: (888, 888)
Epoch 77, accuracy: 0.1950
batch size: (889, 889)
Epoch 78, accuracy: 0.1960
Epoch 78, Train Loss: 0.7302, Val Loss: 1.1025
batch size: (897, 897)
Epoch 79, accuracy: 0.1945
batch size: (923, 923)
Epoch 80, accuracy: 0.1932
Epoch 80, Train Loss: 0.7261, Val Loss: 1.1028
batch size: (901, 901)
Epoch 81, accuracy: 0.1971
batch size: (895, 895)
Epoch 82, accuracy: 0.2020
Epoch 82, Train Loss: 0.7355, Val Loss: 1.1031
batch size: (888, 888)
Epoch 83, accuracy: 0.2000
batch size: (900, 900)
Epoch 84, accuracy: 0.2013
Epoch 84, Train Loss: 0.7311, Val Loss: 1.1024
batch size: (908, 908)
Epoch 85, accuracy: 0.1991
batch size: (886, 886)
Epoch 86, accuracy: 0.1976
Epoch 86, Train Loss: 0.7485, Val Loss: 1.1028
batch size: (907, 907)
Epoch 87, accuracy: 0.2000
batch size: (923, 923)
Epoch 88, accuracy: 0.1989
Epoch 88, Train Loss: 0.7363, Val Loss: 1.1039
batch size: (907, 907)
Epoch 89, accuracy: 0.2010
batch size: (905, 905)
Epoch 90, accuracy: 0.1986
Epoch 90, Train Loss: 0.7286, Val Loss: 1.1036
batch size: (904, 904)
Epoch 91, accuracy: 0.2018
batch size: (913, 913)
Epoch 92, accuracy: 0.2061
Epoch 92, Train Loss: 0.7292, Val Loss: 1.1030
batch size: (879, 879)
Epoch 93, accuracy: 0.2057
batch size: (914, 914)
Epoch 94, accuracy: 0.2070
Epoch 94, Train Loss: 0.7299, Val Loss: 1.1035
batch size: (883, 883)
Epoch 95, accuracy: 0.2092
batch size: (917, 917)
Epoch 96, accuracy: 0.2104
Epoch 96, Train Loss: 0.7385, Val Loss: 1.1046
batch size: (922, 922)
Epoch 97, accuracy: 0.2106
batch size: (902, 902)
Epoch 98, accuracy: 0.2136
Epoch 98, Train Loss: 0.7215, Val Loss: 1.1053
batch size: (908, 908)
Epoch 99, accuracy: 0.2093
batch size: (886, 886)
Epoch 100, accuracy: 0.2149
Epoch 100, Train Loss: 0.7397, Val Loss: 1.1071
batch size: (900, 900)
Epoch 101, accuracy: 0.2155
batch size: (901, 901)
Epoch 102, accuracy: 0.2108
Epoch 102, Train Loss: 0.7317, Val Loss: 1.1087
batch size: (913, 913)
Epoch 103, accuracy: 0.2170
batch size: (894, 894)
Epoch 104, accuracy: 0.2156
Epoch 104, Train Loss: 0.7309, Val Loss: 1.1097
batch size: (888, 888)
Epoch 105, accuracy: 0.2153
batch size: (890, 890)
Epoch 106, accuracy: 0.2167
Epoch 106, Train Loss: 0.7310, Val Loss: 1.1121
batch size: (899, 899)
Epoch 107, accuracy: 0.2155
batch size: (906, 906)
Epoch 108, accuracy: 0.2188
Epoch 108, Train Loss: 0.7348, Val Loss: 1.1129
batch size: (909, 909)
Epoch 109, accuracy: 0.2181
batch size: (900, 900)
Epoch 110, accuracy: 0.2191
Epoch 110, Train Loss: 0.7294, Val Loss: 1.1134
batch size: (897, 897)
Epoch 111, accuracy: 0.2197
batch size: (903, 903)
Epoch 112, accuracy: 0.2198
Epoch 112, Train Loss: 0.7332, Val Loss: 1.1137
batch size: (905, 905)
Epoch 113, accuracy: 0.2231
batch size: (913, 913)
Epoch 114, accuracy: 0.2255
Epoch 114, Train Loss: 0.7316, Val Loss: 1.1140
batch size: (898, 898)
Epoch 115, accuracy: 0.2302
batch size: (904, 904)
Epoch 116, accuracy: 0.2289
Epoch 116, Train Loss: 0.7263, Val Loss: 1.1140
batch size: (889, 889)
Epoch 117, accuracy: 0.2304
batch size: (905, 905)
Epoch 118, accuracy: 0.2302
Epoch 118, Train Loss: 0.7321, Val Loss: 1.1149
batch size: (900, 900)
Epoch 119, accuracy: 0.2317
batch size: (896, 896)
Epoch 120, accuracy: 0.2289
Epoch 120, Train Loss: 0.7284, Val Loss: 1.1157
batch size: (908, 908)
Epoch 121, accuracy: 0.2297
batch size: (903, 903)
Epoch 122, accuracy: 0.2324
Epoch 122, Train Loss: 0.7304, Val Loss: 1.1159
batch size: (888, 888)
Epoch 123, accuracy: 0.2347
batch size: (918, 918)
Epoch 124, accuracy: 0.2321
Epoch 124, Train Loss: 0.7280, Val Loss: 1.1162
batch size: (905, 905)
Epoch 125, accuracy: 0.2315
batch size: (898, 898)
Epoch 126, accuracy: 0.2315
Epoch 126, Train Loss: 0.7315, Val Loss: 1.1169
batch size: (900, 900)
Epoch 127, accuracy: 0.2292
batch size: (886, 886)
Epoch 128, accuracy: 0.2321
Epoch 128, Train Loss: 0.7305, Val Loss: 1.1173
batch size: (882, 882)
Epoch 129, accuracy: 0.2307
batch size: (905, 905)
Epoch 130, accuracy: 0.2302
Epoch 130, Train Loss: 0.7260, Val Loss: 1.1172
batch size: (893, 893)
Epoch 131, accuracy: 0.2294
batch size: (901, 901)
Epoch 132, accuracy: 0.2319
Epoch 132, Train Loss: 0.7378, Val Loss: 1.1183
batch size: (898, 898)
Epoch 133, accuracy: 0.2348
batch size: (899, 899)
Epoch 134, accuracy: 0.2346
Epoch 134, Train Loss: 0.7352, Val Loss: 1.1180
batch size: (897, 897)
Epoch 135, accuracy: 0.2314
batch size: (907, 907)
Epoch 136, accuracy: 0.2341
Epoch 136, Train Loss: 0.7331, Val Loss: 1.1181
batch size: (892, 892)
Epoch 137, accuracy: 0.2334
batch size: (900, 900)
Epoch 138, accuracy: 0.2341
Epoch 138, Train Loss: 0.7222, Val Loss: 1.1185
batch size: (910, 910)
Epoch 139, accuracy: 0.2363
batch size: (904, 904)
Epoch 140, accuracy: 0.2339
Epoch 140, Train Loss: 0.7294, Val Loss: 1.1188
batch size: (897, 897)
Epoch 141, accuracy: 0.2342
batch size: (902, 902)
Epoch 142, accuracy: 0.2353
Epoch 142, Train Loss: 0.7344, Val Loss: 1.1187
batch size: (903, 903)
Epoch 143, accuracy: 0.2329
batch size: (898, 898)
Epoch 144, accuracy: 0.2314
Epoch 144, Train Loss: 0.7351, Val Loss: 1.1184
batch size: (892, 892)
Epoch 145, accuracy: 0.2316
batch size: (883, 883)
Epoch 146, accuracy: 0.2320
Epoch 146, Train Loss: 0.7360, Val Loss: 1.1186
batch size: (898, 898)
Epoch 147, accuracy: 0.2321
batch size: (911, 911)
Epoch 148, accuracy: 0.2322
Epoch 148, Train Loss: 0.7254, Val Loss: 1.1186
batch size: (916, 916)
Epoch 149, accuracy: 0.2289
batch size: (915, 915)
Epoch 150, accuracy: 0.2324
Epoch 150, Train Loss: 0.7373, Val Loss: 1.1188
batch size: (902, 902)
Epoch 151, accuracy: 0.2316
batch size: (913, 913)
Epoch 152, accuracy: 0.2328
Epoch 152, Train Loss: 0.7391, Val Loss: 1.1187
batch size: (900, 900)
Epoch 153, accuracy: 0.2330
batch size: (909, 909)
Epoch 154, accuracy: 0.2334
Epoch 154, Train Loss: 0.7300, Val Loss: 1.1182
batch size: (924, 924)
Epoch 155, accuracy: 0.2340
batch size: (904, 904)
Epoch 156, accuracy: 0.2298
Epoch 156, Train Loss: 0.7268, Val Loss: 1.1185
batch size: (885, 885)
Epoch 157, accuracy: 0.2328
batch size: (883, 883)
Epoch 158, accuracy: 0.2328
Epoch 158, Train Loss: 0.7383, Val Loss: 1.1183
batch size: (899, 899)
Epoch 159, accuracy: 0.2328
batch size: (882, 882)
Epoch 160, accuracy: 0.2304
Epoch 160, Train Loss: 0.7424, Val Loss: 1.1190
batch size: (892, 892)
Epoch 161, accuracy: 0.2299
batch size: (903, 903)
Epoch 162, accuracy: 0.2305
Epoch 162, Train Loss: 0.7385, Val Loss: 1.1189
batch size: (899, 899)
Epoch 163, accuracy: 0.2313
batch size: (888, 888)
Epoch 164, accuracy: 0.2309
Epoch 164, Train Loss: 0.7308, Val Loss: 1.1184
batch size: (911, 911)
Epoch 165, accuracy: 0.2285
batch size: (895, 895)
Epoch 166, accuracy: 0.2287
Epoch 166, Train Loss: 0.7264, Val Loss: 1.1182
batch size: (897, 897)
Epoch 167, accuracy: 0.2321
batch size: (894, 894)
Epoch 168, accuracy: 0.2321
Epoch 168, Train Loss: 0.7336, Val Loss: 1.1181
batch size: (907, 907)
Epoch 169, accuracy: 0.2325
batch size: (905, 905)
Epoch 170, accuracy: 0.2304
Epoch 170, Train Loss: 0.7285, Val Loss: 1.1187
batch size: (896, 896)
Epoch 171, accuracy: 0.2295
batch size: (906, 906)
Epoch 172, accuracy: 0.2339
Epoch 172, Train Loss: 0.7288, Val Loss: 1.1182
batch size: (906, 906)
Epoch 173, accuracy: 0.2277
batch size: (878, 878)
Epoch 174, accuracy: 0.2313
Epoch 174, Train Loss: 0.7289, Val Loss: 1.1178
batch size: (895, 895)
Epoch 175, accuracy: 0.2315
batch size: (894, 894)
Epoch 176, accuracy: 0.2335
Epoch 176, Train Loss: 0.7272, Val Loss: 1.1175
batch size: (914, 914)
Epoch 177, accuracy: 0.2334
batch size: (902, 902)
Epoch 178, accuracy: 0.2311
Epoch 178, Train Loss: 0.7241, Val Loss: 1.1178
batch size: (894, 894)
Epoch 179, accuracy: 0.2340
batch size: (897, 897)
Epoch 180, accuracy: 0.2298
Epoch 180, Train Loss: 0.7292, Val Loss: 1.1183
batch size: (919, 919)
Epoch 181, accuracy: 0.2329
batch size: (897, 897)
Epoch 182, accuracy: 0.2321
Epoch 182, Train Loss: 0.7278, Val Loss: 1.1187
batch size: (895, 895)
Epoch 183, accuracy: 0.2298
batch size: (897, 897)
Epoch 184, accuracy: 0.2310
Epoch 184, Train Loss: 0.7360, Val Loss: 1.1188
batch size: (899, 899)
Epoch 185, accuracy: 0.2334
batch size: (922, 922)
Epoch 186, accuracy: 0.2297
Epoch 186, Train Loss: 0.7363, Val Loss: 1.1187
batch size: (903, 903)
Epoch 187, accuracy: 0.2287
batch size: (870, 870)
Epoch 188, accuracy: 0.2302
Epoch 188, Train Loss: 0.7424, Val Loss: 1.1184
batch size: (889, 889)
Epoch 189, accuracy: 0.2287
batch size: (894, 894)/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
/root/miniconda3/lib/python3.12/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling
  warnings.warn(f"Using '{self.__class__.__name__}' without a "

Epoch 190, accuracy: 0.2273
Epoch 190, Train Loss: 0.7312, Val Loss: 1.1181
batch size: (901, 901)
Epoch 191, accuracy: 0.2267
batch size: (900, 900)
Epoch 192, accuracy: 0.2291
Epoch 192, Train Loss: 0.7298, Val Loss: 1.1178
batch size: (890, 890)
Epoch 193, accuracy: 0.2270
batch size: (908, 908)
Epoch 194, accuracy: 0.2292
Epoch 194, Train Loss: 0.7352, Val Loss: 1.1182
batch size: (914, 914)
Epoch 195, accuracy: 0.2278
batch size: (900, 900)
Epoch 196, accuracy: 0.2301
Epoch 196, Train Loss: 0.7380, Val Loss: 1.1172
batch size: (886, 886)
Epoch 197, accuracy: 0.2311
batch size: (890, 890)
Epoch 198, accuracy: 0.2319
Epoch 198, Train Loss: 0.7279, Val Loss: 1.1169
batch size: (883, 883)
Epoch 199, accuracy: 0.2298
Loaded best model with val_loss = 1.0955466032028198
test :accuracy 0.4019, f1_macro: 0.1911, f1_micro: 0.4019, auc: 0.5000
Final Results: {'GCN_2_Pubmed': np.float64(0.7238901946871258), 'GCN_8_Pubmed': np.float64(0.16725994482513126), 'GCN_32_Pubmed': np.float64(0.17649691219905128), 'GraphSAGE_2_Pubmed': np.float64(0.7305731637040649), 'GraphSAGE_8_Pubmed': np.float64(0.1673787271918113), 'GraphSAGE_32_Pubmed': np.float64(0.18815048434332057), 'GAT_2_Pubmed': np.float64(0.7467908718131574), 'GAT_8_Pubmed': np.float64(0.5306021938661294), 'GAT_32_Pubmed': np.float64(0.4299452384132496), 'JKNet_2_Pubmed': np.float64(0.4054814087399009), 'JKNet_8_Pubmed': np.float64(0.4318473026112314), 'JKNet_32_Pubmed': np.float64(0.4036627751537296), 'resGCN_2_Pubmed': np.float64(0.5394637378273823), 'resGCN_8_Pubmed': np.float64(0.42944185012230374), 'resGCN_32_Pubmed': np.float64(0.1655960028551035), 'GINConv_2_Pubmed': np.float64(0.6483585293854549), 'GINConv_8_Pubmed': np.float64(0.40333451578573964), 'GINConv_32_Pubmed': np.float64(0.4017235220575103), 'mlp_2_Pubmed': np.float64(0.6786691179767723), 'mlp_8_Pubmed': np.float64(0.5471003099591213), 'mlp_32_Pubmed': np.float64(0.4018882141749036)} ['133385_GCN_0', '532745_GCN_0', '2130185_GCN_0', '262153_GraphSAGE_0', '45833_GraphSAGE_0', '98057_GraphSAGE_0', '196495_GAT_0', '1090447_GAT_0', '4666255_GAT_0', '391942_JKNet_0', '1184518_JKNet_0', '4354822_JKNet_0', '132626_resGCN_0', '1850386_resGCN_0', '573202_resGCN_0', '265483_GINConv_0', '1059595_GINConv_0', '4236043_GINConv_0', '83465_mlp_0', '186377_mlp_0', '598025_mlp_0']
========== Running baseline 3/3 ==========
Training GCN with 2 layers...
可训练参数: 133385_GCN
不可训练参数: 0
batch size: (881, 881)
✅ Epoch 0: New best model saved with val_loss = 1.0754
Epoch 0, accuracy: 0.6611
Epoch 0, Train Loss: 1.4217, Val Loss: 1.0754
batch size: (900, 900)
✅ Epoch 1: New best model saved with val_loss = 1.0558
Epoch 1, accuracy: 0.6268
batch size: (905, 905)
✅ Epoch 2: New best model saved with val_loss = 1.0387
Epoch 2, accuracy: 0.5916
Epoch 2, Train Loss: 0.2281, Val Loss: 1.0387
batch size: (903, 903)
✅ Epoch 3: New best model saved with val_loss = 1.0236
Epoch 3, accuracy: 0.5887
batch size: (890, 890)
✅ Epoch 4: New best model saved with val_loss = 1.0107
Epoch 4, accuracy: 0.5996
Epoch 4, Train Loss: 0.0911, Val Loss: 1.0107
batch size: (884, 884)
✅ Epoch 5: New best model saved with val_loss = 0.9952
Epoch 5, accuracy: 0.6150
batch size: (895, 895)
✅ Epoch 6: New best model saved with val_loss = 0.9831
Epoch 6, accuracy: 0.6310
Epoch 6, Train Loss: 0.0368, Val Loss: 0.9831
batch size: (904, 904)
✅ Epoch 7: New best model saved with val_loss = 0.9687
Epoch 7, accuracy: 0.6484
batch size: (892, 892)
✅ Epoch 8: New best model saved with val_loss = 0.9545
Epoch 8, accuracy: 0.6633
Epoch 8, Train Loss: 0.0127, Val Loss: 0.9545
batch size: (901, 901)
✅ Epoch 9: New best model saved with val_loss = 0.9410
Epoch 9, accuracy: 0.6746
batch size: (890, 890)
✅ Epoch 10: New best model saved with val_loss = 0.9267
Epoch 10, accuracy: 0.6843
Epoch 10, Train Loss: 0.0088, Val Loss: 0.9267
batch size: (903, 903)
✅ Epoch 11: New best model saved with val_loss = 0.9123
Epoch 11, accuracy: 0.6914
batch size: (899, 899)
✅ Epoch 12: New best model saved with val_loss = 0.8972
Epoch 12, accuracy: 0.6994
Epoch 12, Train Loss: 0.0082, Val Loss: 0.8972
batch size: (894, 894)
✅ Epoch 13: New best model saved with val_loss = 0.8829
Epoch 13, accuracy: 0.7014
batch size: (886, 886)
✅ Epoch 14: New best model saved with val_loss = 0.8647
Epoch 14, accuracy: 0.7046
Epoch 14, Train Loss: 0.0046, Val Loss: 0.8647
batch size: (910, 910)
✅ Epoch 15: New best model saved with val_loss = 0.8514
Epoch 15, accuracy: 0.7053
batch size: (900, 900)
✅ Epoch 16: New best model saved with val_loss = 0.8373
Epoch 16, accuracy: 0.7086
Epoch 16, Train Loss: 0.0020, Val Loss: 0.8373
batch size: (901, 901)
✅ Epoch 17: New best model saved with val_loss = 0.8267
Epoch 17, accuracy: 0.7089
batch size: (911, 911)
✅ Epoch 18: New best model saved with val_loss = 0.8159
Epoch 18, accuracy: 0.7064
Epoch 18, Train Loss: 0.0134, Val Loss: 0.8159
batch size: (895, 895)
✅ Epoch 19: New best model saved with val_loss = 0.8033
Epoch 19, accuracy: 0.7085
batch size: (906, 906)
✅ Epoch 20: New best model saved with val_loss = 0.7936
Epoch 20, accuracy: 0.7055
Epoch 20, Train Loss: 0.0026, Val Loss: 0.7936
batch size: (897, 897)
✅ Epoch 21: New best model saved with val_loss = 0.7819
Epoch 21, accuracy: 0.7046
batch size: (891, 891)
✅ Epoch 22: New best model saved with val_loss = 0.7700
Epoch 22, accuracy: 0.7017
Epoch 22, Train Loss: 0.0013, Val Loss: 0.7700
batch size: (869, 869)
✅ Epoch 23: New best model saved with val_loss = 0.7600
Epoch 23, accuracy: 0.6997
batch size: (890, 890)
✅ Epoch 24: New best model saved with val_loss = 0.7526
Epoch 24, accuracy: 0.6970
Epoch 24, Train Loss: 0.0012, Val Loss: 0.7526
batch size: (885, 885)
✅ Epoch 25: New best model saved with val_loss = 0.7391
Epoch 25, accuracy: 0.6975
batch size: (908, 908)
✅ Epoch 26: New best model saved with val_loss = 0.7279
Epoch 26, accuracy: 0.6968
Epoch 26, Train Loss: 0.0030, Val Loss: 0.7279
batch size: (883, 883)
✅ Epoch 27: New best model saved with val_loss = 0.7168
Epoch 27, accuracy: 0.6984
batch size: (899, 899)
✅ Epoch 28: New best model saved with val_loss = 0.7089
Epoch 28, accuracy: 0.6994
Epoch 28, Train Loss: 0.0012, Val Loss: 0.7089
batch size: (921, 921)
✅ Epoch 29: New best model saved with val_loss = 0.6947
Epoch 29, accuracy: 0.7030
batch size: (902, 902)
✅ Epoch 30: New best model saved with val_loss = 0.6822
Epoch 30, accuracy: 0.7048
Epoch 30, Train Loss: 0.0085, Val Loss: 0.6822
batch size: (892, 892)
✅ Epoch 31: New best model saved with val_loss = 0.6712
Epoch 31, accuracy: 0.7050
batch size: (901, 901)
✅ Epoch 32: New best model saved with val_loss = 0.6512
Epoch 32, accuracy: 0.7079
Epoch 32, Train Loss: 0.0005, Val Loss: 0.6512
batch size: (898, 898)
✅ Epoch 33: New best model saved with val_loss = 0.6438
Epoch 33, accuracy: 0.7082
batch size: (904, 904)
✅ Epoch 34: New best model saved with val_loss = 0.6323
Epoch 34, accuracy: 0.7080
Epoch 34, Train Loss: 0.0006, Val Loss: 0.6323
batch size: (890, 890)
✅ Epoch 35: New best model saved with val_loss = 0.6262
Epoch 35, accuracy: 0.7100
batch size: (935, 935)
✅ Epoch 36: New best model saved with val_loss = 0.6192
Epoch 36, accuracy: 0.7109
Epoch 36, Train Loss: 0.0006, Val Loss: 0.6192
batch size: (901, 901)
✅ Epoch 37: New best model saved with val_loss = 0.6120
Epoch 37, accuracy: 0.7107
batch size: (901, 901)
✅ Epoch 38: New best model saved with val_loss = 0.6031
Epoch 38, accuracy: 0.7122
Epoch 38, Train Loss: 0.0024, Val Loss: 0.6031
batch size: (893, 893)
✅ Epoch 39: New best model saved with val_loss = 0.5907
Epoch 39, accuracy: 0.7112
batch size: (883, 883)
Epoch 40, accuracy: 0.7122
Epoch 40, Train Loss: 0.0024, Val Loss: 0.5938
batch size: (895, 895)
Epoch 41, accuracy: 0.7108
batch size: (893, 893)
✅ Epoch 42: New best model saved with val_loss = 0.5834
Epoch 42, accuracy: 0.7086
Epoch 42, Train Loss: 0.0008, Val Loss: 0.5834
batch size: (889, 889)
Epoch 43, accuracy: 0.7042
batch size: (890, 890)
Epoch 44, accuracy: 0.7057
Epoch 44, Train Loss: 0.0005, Val Loss: 0.5903
batch size: (879, 879)
Epoch 45, accuracy: 0.7013
batch size: (898, 898)
Epoch 46, accuracy: 0.6963
Epoch 46, Train Loss: 0.0011, Val Loss: 0.6025
batch size: (889, 889)
Epoch 47, accuracy: 0.6979
batch size: (890, 890)
Epoch 48, accuracy: 0.6966
Epoch 48, Train Loss: 0.0072, Val Loss: 0.6081
batch size: (908, 908)
Epoch 49, accuracy: 0.7002
batch size: (913, 913)
Epoch 50, accuracy: 0.6996
Epoch 50, Train Loss: 0.0019, Val Loss: 0.6118
batch size: (883, 883)
Epoch 51, accuracy: 0.6997
batch size: (907, 907)
Epoch 52, accuracy: 0.7012
Epoch 52, Train Loss: 0.0008, Val Loss: 0.6039
batch size: (909, 909)
Epoch 53, accuracy: 0.7003
batch size: (906, 906)
Epoch 54, accuracy: 0.7006
Epoch 54, Train Loss: 0.0011, Val Loss: 0.6364
batch size: (904, 904)
Epoch 55, accuracy: 0.7009
batch size: (893, 893)
Epoch 56, accuracy: 0.7003
Epoch 56, Train Loss: 0.0005, Val Loss: 0.6343
batch size: (898, 898)
Epoch 57, accuracy: 0.7006
batch size: (887, 887)
Epoch 58, accuracy: 0.6991
Epoch 58, Train Loss: 0.0008, Val Loss: 0.6592
batch size: (885, 885)
Epoch 59, accuracy: 0.6995
batch size: (903, 903)
Epoch 60, accuracy: 0.6984
Epoch 60, Train Loss: 0.0015, Val Loss: 0.6680
batch size: (892, 892)
Epoch 61, accuracy: 0.7008
batch size: (897, 897)
Epoch 62, accuracy: 0.7009
Epoch 62, Train Loss: 0.0006, Val Loss: 0.6979
batch size: (907, 907)
Epoch 63, accuracy: 0.6997
batch size: (893, 893)
Epoch 64, accuracy: 0.6973
Epoch 64, Train Loss: 0.0007, Val Loss: 0.7181
batch size: (895, 895)
Epoch 65, accuracy: 0.7016
batch size: (904, 904)
Epoch 66, accuracy: 0.6999
Epoch 66, Train Loss: 0.0034, Val Loss: 0.7103
batch size: (884, 884)
Epoch 67, accuracy: 0.7026
batch size: (890, 890)
Epoch 68, accuracy: 0.7006
Epoch 68, Train Loss: 0.0010, Val Loss: 0.7529
batch size: (905, 905)
Epoch 69, accuracy: 0.7020
batch size: (905, 905)
Epoch 70, accuracy: 0.7003
Epoch 70, Train Loss: 0.0005, Val Loss: 0.7898
batch size: (896, 896)
Epoch 71, accuracy: 0.6985
batch size: (888, 888)
Epoch 72, accuracy: 0.7017
Epoch 72, Train Loss: 0.0036, Val Loss: 0.7724
batch size: (902, 902)
Epoch 73, accuracy: 0.7016
batch size: (897, 897)
Epoch 74, accuracy: 0.6982
Epoch 74, Train Loss: 0.0012, Val Loss: 0.7703
batch size: (888, 888)
Epoch 75, accuracy: 0.7001
batch size: (887, 887)
Epoch 76, accuracy: 0.7030
Epoch 76, Train Loss: 0.0007, Val Loss: 0.8242
batch size: (894, 894)
Epoch 77, accuracy: 0.7023
batch size: (906, 906)
Epoch 78, accuracy: 0.7027
Epoch 78, Train Loss: 0.0009, Val Loss: 0.8694
batch size: (915, 915)
Epoch 79, accuracy: 0.7005
batch size: (915, 915)
Epoch 80, accuracy: 0.7024
Epoch 80, Train Loss: 0.0014, Val Loss: 0.8230
batch size: (902, 902)
Epoch 81, accuracy: 0.6997
batch size: (893, 893)
Epoch 82, accuracy: 0.7031
Epoch 82, Train Loss: 0.0008, Val Loss: 0.8489
batch size: (908, 908)
Epoch 83, accuracy: 0.7036
batch size: (903, 903)
Epoch 84, accuracy: 0.7020
Epoch 84, Train Loss: 0.0009, Val Loss: 0.8522
batch size: (884, 884)
Epoch 85, accuracy: 0.7012
batch size: (907, 907)
Epoch 86, accuracy: 0.7028
Epoch 86, Train Loss: 0.0021, Val Loss: 0.8709
batch size: (899, 899)
Epoch 87, accuracy: 0.7009
batch size: (897, 897)
Epoch 88, accuracy: 0.7021
Epoch 88, Train Loss: 0.0010, Val Loss: 0.8415
batch size: (927, 927)
Epoch 89, accuracy: 0.7004
batch size: (920, 920)
Epoch 90, accuracy: 0.7031
Epoch 90, Train Loss: 0.0011, Val Loss: 0.9001
batch size: (889, 889)
Epoch 91, accuracy: 0.7016
batch size: (912, 912)
Epoch 92, accuracy: 0.7010
Epoch 92, Train Loss: 0.0011, Val Loss: 0.8946
batch size: (899, 899)
Epoch 93, accuracy: 0.7035
batch size: (912, 912)
Epoch 94, accuracy: 0.7028
Epoch 94, Train Loss: 0.0009, Val Loss: 0.8955
batch size: (890, 890)
Epoch 95, accuracy: 0.7016
batch size: (886, 886)
Epoch 96, accuracy: 0.7023
Epoch 96, Train Loss: 0.0013, Val Loss: 0.8862
batch size: (906, 906)
Epoch 97, accuracy: 0.7004
batch size: (884, 884)
Epoch 98, accuracy: 0.7062
Epoch 98, Train Loss: 0.0010, Val Loss: 0.8838
batch size: (895, 895)
Epoch 99, accuracy: 0.7016
batch size: (919, 919)
Epoch 100, accuracy: 0.7030
Epoch 100, Train Loss: 0.0011, Val Loss: 0.9128
batch size: (897, 897)
Epoch 101, accuracy: 0.7024
batch size: (899, 899)
Epoch 102, accuracy: 0.7017
Epoch 102, Train Loss: 0.0006, Val Loss: 0.9174
batch size: (898, 898)
Epoch 103, accuracy: 0.7006
batch size: (903, 903)
Epoch 104, accuracy: 0.7022
Epoch 104, Train Loss: 0.0011, Val Loss: 0.9258
batch size: (898, 898)
Epoch 105, accuracy: 0.7033
batch size: (888, 888)
Epoch 106, accuracy: 0.7009
Epoch 106, Train Loss: 0.0008, Val Loss: 0.9019
batch size: (902, 902)
Epoch 107, accuracy: 0.7031
batch size: (907, 907)
Epoch 108, accuracy: 0.7002
Epoch 108, Train Loss: 0.0020, Val Loss: 0.9278
batch size: (896, 896)
Epoch 109, accuracy: 0.7032
batch size: (891, 891)
Epoch 110, accuracy: 0.7030
Epoch 110, Train Loss: 0.0019, Val Loss: 0.9100
batch size: (888, 888)
Epoch 111, accuracy: 0.7021
batch size: (898, 898)
Epoch 112, accuracy: 0.6981
Epoch 112, Train Loss: 0.0018, Val Loss: 0.8796
batch size: (885, 885)
Epoch 113, accuracy: 0.7023
batch size: (895, 895)
Epoch 114, accuracy: 0.7034
Epoch 114, Train Loss: 0.0027, Val Loss: 0.9376
batch size: (899, 899)
Epoch 115, accuracy: 0.7026
batch size: (916, 916)
Epoch 116, accuracy: 0.7030
Epoch 116, Train Loss: 0.0019, Val Loss: 0.9389
batch size: (906, 906)
Epoch 117, accuracy: 0.7015
batch size: (906, 906)
Epoch 118, accuracy: 0.7012
Epoch 118, Train Loss: 0.0042, Val Loss: 0.8970
batch size: (892, 892)
Epoch 119, accuracy: 0.7012
batch size: (905, 905)
Epoch 120, accuracy: 0.7024
Epoch 120, Train Loss: 0.0012, Val Loss: 0.9170
batch size: (908, 908)
Epoch 121, accuracy: 0.7008
batch size: (893, 893)
Epoch 122, accuracy: 0.7014
Epoch 122, Train Loss: 0.0008, Val Loss: 0.9311
batch size: (900, 900)
Epoch 123, accuracy: 0.6998
batch size: (912, 912)
Epoch 124, accuracy: 0.7012
Epoch 124, Train Loss: 0.0007, Val Loss: 0.9502
batch size: (930, 930)
Epoch 125, accuracy: 0.7030
batch size: (905, 905)
Epoch 126, accuracy: 0.7009
Epoch 126, Train Loss: 0.0017, Val Loss: 0.9388
batch size: (907, 907)
Epoch 127, accuracy: 0.7009
batch size: (905, 905)
Epoch 128, accuracy: 0.7032
Epoch 128, Train Loss: 0.0015, Val Loss: 0.9403
batch size: (912, 912)
Epoch 129, accuracy: 0.7017
batch size: (907, 907)
Epoch 130, accuracy: 0.7010
Epoch 130, Train Loss: 0.0011, Val Loss: 0.9039
batch size: (885, 885)
Epoch 131, accuracy: 0.7030
batch size: (914, 914)
Epoch 132, accuracy: 0.7035
Epoch 132, Train Loss: 0.0018, Val Loss: 0.9159
batch size: (887, 887)
Epoch 133, accuracy: 0.7044
batch size: (922, 922)
Epoch 134, accuracy: 0.7041
Epoch 134, Train Loss: 0.0006, Val Loss: 0.8651
batch size: (896, 896)
Epoch 135, accuracy: 0.7019
batch size: (886, 886)
Epoch 136, accuracy: 0.7041
Epoch 136, Train Loss: 0.0008, Val Loss: 0.9484
batch size: (901, 901)
Epoch 137, accuracy: 0.7010
batch size: (884, 884)
Epoch 138, accuracy: 0.7017
Epoch 138, Train Loss: 0.0050, Val Loss: 0.9116
batch size: (902, 902)
Epoch 139, accuracy: 0.7012
batch size: (904, 904)
Epoch 140, accuracy: 0.7025
Epoch 140, Train Loss: 0.0008, Val Loss: 0.9169
batch size: (903, 903)
Epoch 141, accuracy: 0.7018
batch size: (892, 892)
Epoch 142, accuracy: 0.7027
Epoch 142, Train Loss: 0.0017, Val Loss: 0.9239
batch size: (900, 900)
Epoch 143, accuracy: 0.7025
batch size: (917, 917)
Epoch 144, accuracy: 0.7041
Epoch 144, Train Loss: 0.0007, Val Loss: 0.9121
batch size: (895, 895)
Epoch 145, accuracy: 0.7015
batch size: (899, 899)
Epoch 146, accuracy: 0.7031
Epoch 146, Train Loss: 0.0012, Val Loss: 0.9030
batch size: (902, 902)
Epoch 147, accuracy: 0.7054
batch size: (891, 891)
Epoch 148, accuracy: 0.7015
Epoch 148, Train Loss: 0.0013, Val Loss: 0.9125
batch size: (896, 896)
Epoch 149, accuracy: 0.6993
batch size: (898, 898)
Epoch 150, accuracy: 0.7018
Epoch 150, Train Loss: 0.0018, Val Loss: 0.9414
batch size: (898, 898)
Epoch 151, accuracy: 0.7023
batch size: (896, 896)
Epoch 152, accuracy: 0.7019
Epoch 152, Train Loss: 0.0014, Val Loss: 0.9236
batch size: (903, 903)
Epoch 153, accuracy: 0.7021
batch size: (911, 911)
Epoch 154, accuracy: 0.6997
Epoch 154, Train Loss: 0.0013, Val Loss: 0.9263
batch size: (910, 910)
Epoch 155, accuracy: 0.7038
batch size: (910, 910)
Epoch 156, accuracy: 0.6993
Epoch 156, Train Loss: 0.0005, Val Loss: 0.9501
batch size: (888, 888)
Epoch 157, accuracy: 0.7024
batch size: (908, 908)
Epoch 158, accuracy: 0.7045
Epoch 158, Train Loss: 0.0018, Val Loss: 0.9752
batch size: (899, 899)
Epoch 159, accuracy: 0.7031
batch size: (895, 895)
Epoch 160, accuracy: 0.7018
Epoch 160, Train Loss: 0.0007, Val Loss: 0.9169
batch size: (901, 901)
Epoch 161, accuracy: 0.7041
batch size: (906, 906)
Epoch 162, accuracy: 0.7016
Epoch 162, Train Loss: 0.0010, Val Loss: 0.9084
batch size: (916, 916)
Epoch 163, accuracy: 0.7020
batch size: (895, 895)
Epoch 164, accuracy: 0.7017
Epoch 164, Train Loss: 0.0010, Val Loss: 0.9688
batch size: (891, 891)
Epoch 165, accuracy: 0.7015
batch size: (910, 910)
Epoch 166, accuracy: 0.7050
Epoch 166, Train Loss: 0.0022, Val Loss: 0.9188
batch size: (889, 889)
Epoch 167, accuracy: 0.7010
batch size: (885, 885)
Epoch 168, accuracy: 0.7016
Epoch 168, Train Loss: 0.0020, Val Loss: 0.8937
batch size: (905, 905)
Epoch 169, accuracy: 0.6993
batch size: (907, 907)
Epoch 170, accuracy: 0.6997
Epoch 170, Train Loss: 0.0007, Val Loss: 0.9644
batch size: (882, 882)
Epoch 171, accuracy: 0.7026
batch size: (905, 905)
Epoch 172, accuracy: 0.7013
Epoch 172, Train Loss: 0.0009, Val Loss: 0.9130
batch size: (904, 904)
Epoch 173, accuracy: 0.7032
batch size: (889, 889)
Epoch 174, accuracy: 0.7030
Epoch 174, Train Loss: 0.0006, Val Loss: 0.9274
batch size: (913, 913)
Epoch 175, accuracy: 0.7034
batch size: (916, 916)
Epoch 176, accuracy: 0.7031
Epoch 176, Train Loss: 0.0007, Val Loss: 0.9414
batch size: (893, 893)
Epoch 177, accuracy: 0.7032
batch size: (906, 906)
Epoch 178, accuracy: 0.7015
Epoch 178, Train Loss: 0.0007, Val Loss: 0.9046
batch size: (900, 900)
Epoch 179, accuracy: 0.7007
batch size: (895, 895)
Epoch 180, accuracy: 0.7017
Epoch 180, Train Loss: 0.0011, Val Loss: 0.9195
batch size: (902, 902)
Epoch 181, accuracy: 0.7010
batch size: (866, 866)
Epoch 182, accuracy: 0.7016
Epoch 182, Train Loss: 0.0007, Val Loss: 0.9191
batch size: (911, 911)
Epoch 183, accuracy: 0.7021
batch size: (896, 896)
Epoch 184, accuracy: 0.7014
Epoch 184, Train Loss: 0.0008, Val Loss: 0.9689
batch size: (894, 894)
Epoch 185, accuracy: 0.7025
batch size: (892, 892)
Epoch 186, accuracy: 0.7014
Epoch 186, Train Loss: 0.0013, Val Loss: 0.9017
batch size: (912, 912)
Epoch 187, accuracy: 0.7026
batch size: (880, 880)
Epoch 188, accuracy: 0.7017
Epoch 188, Train Loss: 0.0017, Val Loss: 0.9190
batch size: (886, 886)
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 189, accuracy: 0.7040
batch size: (894, 894)
Epoch 190, accuracy: 0.7023
Epoch 190, Train Loss: 0.0009, Val Loss: 0.8850
batch size: (902, 902)
Epoch 191, accuracy: 0.7023
batch size: (912, 912)
Epoch 192, accuracy: 0.7026
Epoch 192, Train Loss: 0.0018, Val Loss: 0.9171
batch size: (896, 896)
Epoch 193, accuracy: 0.7030
batch size: (911, 911)
Epoch 194, accuracy: 0.7011
Epoch 194, Train Loss: 0.0006, Val Loss: 0.9264
batch size: (903, 903)
Epoch 195, accuracy: 0.7041
batch size: (906, 906)
Epoch 196, accuracy: 0.7023
Epoch 196, Train Loss: 0.0015, Val Loss: 0.8915
batch size: (895, 895)
Epoch 197, accuracy: 0.7027
batch size: (901, 901)
Epoch 198, accuracy: 0.7026
Epoch 198, Train Loss: 0.0007, Val Loss: 0.8680
batch size: (902, 902)
Epoch 199, accuracy: 0.7013
Loaded best model with val_loss = 0.5833867788314819
test :accuracy 0.7085, f1_macro: 0.7049, f1_micro: 0.7085, auc: 0.8732
Training GCN with 8 layers...
可训练参数: 532745_GCN
不可训练参数: 0
batch size: (905, 905)
✅ Epoch 0: New best model saved with val_loss = 1.0890
Epoch 0, accuracy: 0.4310
Epoch 0, Train Loss: 1.2286, Val Loss: 1.0890
batch size: (910, 910)
Epoch 1, accuracy: 0.4346
batch size: (916, 916)
Epoch 2, accuracy: 0.1674
Epoch 2, Train Loss: 1.0915, Val Loss: 1.3792
batch size: (907, 907)
Epoch 3, accuracy: 0.3730
batch size: (903, 903)
Epoch 4, accuracy: 0.3790
Epoch 4, Train Loss: 0.8763, Val Loss: 1.7690
batch size: (894, 894)
Epoch 5, accuracy: 0.3879
batch size: (902, 902)
Epoch 6, accuracy: 0.3801
Epoch 6, Train Loss: 0.6267, Val Loss: 2.0482
batch size: (894, 894)
Epoch 7, accuracy: 0.3961
batch size: (893, 893)
Epoch 8, accuracy: 0.4020
Epoch 8, Train Loss: 0.5823, Val Loss: 1.6427
batch size: (921, 921)
Epoch 9, accuracy: 0.4058
batch size: (906, 906)
Epoch 10, accuracy: 0.4091
Epoch 10, Train Loss: 0.6110, Val Loss: 1.5878
batch size: (888, 888)
Epoch 11, accuracy: 0.4027
batch size: (894, 894)
Epoch 12, accuracy: 0.4061
Epoch 12, Train Loss: 0.7705, Val Loss: 1.5453
batch size: (884, 884)
Epoch 13, accuracy: 0.4052
batch size: (911, 911)
Epoch 14, accuracy: 0.4008
Epoch 14, Train Loss: 0.5946, Val Loss: 1.6241
batch size: (913, 913)
Epoch 15, accuracy: 0.4030
batch size: (915, 915)
Epoch 16, accuracy: 0.4057
Epoch 16, Train Loss: 0.5808, Val Loss: 1.5571
batch size: (886, 886)
Epoch 17, accuracy: 0.4043
batch size: (877, 877)
Epoch 18, accuracy: 0.4088
Epoch 18, Train Loss: 0.6132, Val Loss: 1.5667
batch size: (897, 897)
Epoch 19, accuracy: 0.4076
batch size: (888, 888)
Epoch 20, accuracy: 0.3996
Epoch 20, Train Loss: 0.6316, Val Loss: 1.5614
batch size: (896, 896)
Epoch 21, accuracy: 0.4050
batch size: (888, 888)
Epoch 22, accuracy: 0.4118
Epoch 22, Train Loss: 0.5996, Val Loss: 1.5614
batch size: (887, 887)
Epoch 23, accuracy: 0.4008
batch size: (888, 888)
Epoch 24, accuracy: 0.3987
Epoch 24, Train Loss: 0.5858, Val Loss: 1.5863
batch size: (911, 911)
Epoch 25, accuracy: 0.3969
batch size: (898, 898)
Epoch 26, accuracy: 0.4004
Epoch 26, Train Loss: 0.4926, Val Loss: 1.5950
batch size: (896, 896)
Epoch 27, accuracy: 0.3988
batch size: (905, 905)
Epoch 28, accuracy: 0.4034
Epoch 28, Train Loss: 0.6542, Val Loss: 1.5660
batch size: (903, 903)
Epoch 29, accuracy: 0.4072
batch size: (883, 883)
Epoch 30, accuracy: 0.3988
Epoch 30, Train Loss: 0.6360, Val Loss: 1.5708
batch size: (887, 887)
Epoch 31, accuracy: 0.3944
batch size: (902, 902)
Epoch 32, accuracy: 0.4078
Epoch 32, Train Loss: 0.6799, Val Loss: 1.5811
batch size: (904, 904)
Epoch 33, accuracy: 0.4063
batch size: (873, 873)
Epoch 34, accuracy: 0.4067
Epoch 34, Train Loss: 0.5792, Val Loss: 1.5349
batch size: (920, 920)
Epoch 35, accuracy: 0.3989
batch size: (900, 900)
Epoch 36, accuracy: 0.4022
Epoch 36, Train Loss: 0.6634, Val Loss: 1.5731
batch size: (881, 881)
Epoch 37, accuracy: 0.4051
batch size: (881, 881)
Epoch 38, accuracy: 0.3942
Epoch 38, Train Loss: 0.6184, Val Loss: 1.6146
batch size: (894, 894)
Epoch 39, accuracy: 0.4009
batch size: (916, 916)
Epoch 40, accuracy: 0.4008
Epoch 40, Train Loss: 0.4826, Val Loss: 1.6271
batch size: (914, 914)
Epoch 41, accuracy: 0.3991
batch size: (907, 907)
Epoch 42, accuracy: 0.4019
Epoch 42, Train Loss: 0.7238, Val Loss: 1.5924
batch size: (896, 896)
Epoch 43, accuracy: 0.3953
batch size: (900, 900)
Epoch 44, accuracy: 0.4007
Epoch 44, Train Loss: 0.6292, Val Loss: 1.6105
batch size: (879, 879)
Epoch 45, accuracy: 0.3998
batch size: (894, 894)
Epoch 46, accuracy: 0.4041
Epoch 46, Train Loss: 0.7153, Val Loss: 1.6143
batch size: (926, 926)
Epoch 47, accuracy: 0.3996
batch size: (903, 903)
Epoch 48, accuracy: 0.3992
Epoch 48, Train Loss: 0.6097, Val Loss: 1.5774
batch size: (902, 902)
Epoch 49, accuracy: 0.3945
batch size: (908, 908)
Epoch 50, accuracy: 0.3979
Epoch 50, Train Loss: 0.6625, Val Loss: 1.6353
batch size: (899, 899)
Epoch 51, accuracy: 0.4024
batch size: (894, 894)
Epoch 52, accuracy: 0.4047
Epoch 52, Train Loss: 0.4989, Val Loss: 1.5503
batch size: (882, 882)
Epoch 53, accuracy: 0.4050
batch size: (913, 913)
Epoch 54, accuracy: 0.4025
Epoch 54, Train Loss: 0.4329, Val Loss: 1.5618
batch size: (893, 893)
Epoch 55, accuracy: 0.4106
batch size: (897, 897)
Epoch 56, accuracy: 0.3968
Epoch 56, Train Loss: 0.6546, Val Loss: 1.6142
batch size: (879, 879)
Epoch 57, accuracy: 0.4012
batch size: (892, 892)
Epoch 58, accuracy: 0.4014
Epoch 58, Train Loss: 0.5173, Val Loss: 1.5746
batch size: (906, 906)
Epoch 59, accuracy: 0.4006
batch size: (898, 898)
Epoch 60, accuracy: 0.3994
Epoch 60, Train Loss: 0.7063, Val Loss: 1.5919
batch size: (885, 885)
Epoch 61, accuracy: 0.3974
batch size: (879, 879)
Epoch 62, accuracy: 0.4029
Epoch 62, Train Loss: 0.7266, Val Loss: 1.5902
batch size: (898, 898)
Epoch 63, accuracy: 0.4007
batch size: (905, 905)
Epoch 64, accuracy: 0.4035
Epoch 64, Train Loss: 0.6620, Val Loss: 1.6059
batch size: (897, 897)
Epoch 65, accuracy: 0.4014
batch size: (895, 895)
Epoch 66, accuracy: 0.4074
Epoch 66, Train Loss: 0.5555, Val Loss: 1.5169
batch size: (914, 914)
Epoch 67, accuracy: 0.4137
batch size: (898, 898)
Epoch 68, accuracy: 0.4000
Epoch 68, Train Loss: 0.6829, Val Loss: 1.5569
batch size: (884, 884)
Epoch 69, accuracy: 0.4042
batch size: (923, 923)
Epoch 70, accuracy: 0.3962
Epoch 70, Train Loss: 0.5936, Val Loss: 1.5803
batch size: (904, 904)
Epoch 71, accuracy: 0.4081
batch size: (891, 891)
Epoch 72, accuracy: 0.3934
Epoch 72, Train Loss: 0.6579, Val Loss: 1.5795
batch size: (877, 877)
Epoch 73, accuracy: 0.4048
batch size: (880, 880)
Epoch 74, accuracy: 0.3965
Epoch 74, Train Loss: 0.7024, Val Loss: 1.5650
batch size: (889, 889)
Epoch 75, accuracy: 0.4032
batch size: (892, 892)
Epoch 76, accuracy: 0.4042
Epoch 76, Train Loss: 0.6348, Val Loss: 1.5851
batch size: (908, 908)
Epoch 77, accuracy: 0.4013
batch size: (899, 899)
Epoch 78, accuracy: 0.4099
Epoch 78, Train Loss: 0.5946, Val Loss: 1.4945
batch size: (913, 913)
Epoch 79, accuracy: 0.4066
batch size: (898, 898)
Epoch 80, accuracy: 0.4077
Epoch 80, Train Loss: 0.6703, Val Loss: 1.5583
batch size: (913, 913)
Epoch 81, accuracy: 0.4069
batch size: (910, 910)
Epoch 82, accuracy: 0.4056
Epoch 82, Train Loss: 0.6706, Val Loss: 1.5497
batch size: (892, 892)
Epoch 83, accuracy: 0.4025
batch size: (895, 895)
Epoch 84, accuracy: 0.4007
Epoch 84, Train Loss: 0.5856, Val Loss: 1.5609
batch size: (918, 918)
Epoch 85, accuracy: 0.4062
batch size: (890, 890)
Epoch 86, accuracy: 0.4067
Epoch 86, Train Loss: 0.6082, Val Loss: 1.5361
batch size: (896, 896)
Epoch 87, accuracy: 0.4111
batch size: (909, 909)
Epoch 88, accuracy: 0.4010
Epoch 88, Train Loss: 0.5959, Val Loss: 1.5422
batch size: (884, 884)
Epoch 89, accuracy: 0.4019
batch size: (886, 886)
Epoch 90, accuracy: 0.4025
Epoch 90, Train Loss: 0.5738, Val Loss: 1.5437
batch size: (911, 911)
Epoch 91, accuracy: 0.4027
batch size: (909, 909)
Epoch 92, accuracy: 0.4001
Epoch 92, Train Loss: 0.6436, Val Loss: 1.5800
batch size: (884, 884)
Epoch 93, accuracy: 0.3997
batch size: (888, 888)
Epoch 94, accuracy: 0.4010
Epoch 94, Train Loss: 0.6278, Val Loss: 1.5623
batch size: (911, 911)
Epoch 95, accuracy: 0.4022
batch size: (909, 909)
Epoch 96, accuracy: 0.4053
Epoch 96, Train Loss: 0.5859, Val Loss: 1.5664
batch size: /root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
(903, 903)
Epoch 97, accuracy: 0.4009
batch size: (898, 898)
Epoch 98, accuracy: 0.3965
Epoch 98, Train Loss: 0.6455, Val Loss: 1.5976
batch size: (897, 897)
Epoch 99, accuracy: 0.4019
batch size: (893, 893)
Epoch 100, accuracy: 0.3949
Epoch 100, Train Loss: 0.6351, Val Loss: 1.5837
batch size: (901, 901)
Epoch 101, accuracy: 0.3975
batch size: (899, 899)
Epoch 102, accuracy: 0.4005
Epoch 102, Train Loss: 0.6718, Val Loss: 1.5950
batch size: (905, 905)
Epoch 103, accuracy: 0.3974
batch size: (895, 895)
Epoch 104, accuracy: 0.4036
Epoch 104, Train Loss: 0.5082, Val Loss: 1.5747
batch size: (894, 894)
Epoch 105, accuracy: 0.3977
batch size: (912, 912)
Epoch 106, accuracy: 0.3956
Epoch 106, Train Loss: 0.5438, Val Loss: 1.6291
batch size: (882, 882)
Epoch 107, accuracy: 0.4051
batch size: (914, 914)
Epoch 108, accuracy: 0.4065
Epoch 108, Train Loss: 0.6193, Val Loss: 1.5535
batch size: (922, 922)
Epoch 109, accuracy: 0.4084
batch size: (903, 903)
Epoch 110, accuracy: 0.4019
Epoch 110, Train Loss: 0.6624, Val Loss: 1.6302
batch size: (911, 911)
Epoch 111, accuracy: 0.3991
batch size: (891, 891)
Epoch 112, accuracy: 0.3986
Epoch 112, Train Loss: 0.5824, Val Loss: 1.5947
batch size: (901, 901)
Epoch 113, accuracy: 0.4062
batch size: (916, 916)
Epoch 114, accuracy: 0.4013
Epoch 114, Train Loss: 0.5552, Val Loss: 1.6087
batch size: (886, 886)
Epoch 115, accuracy: 0.4003
batch size: (897, 897)
Epoch 116, accuracy: 0.3981
Epoch 116, Train Loss: 0.5947, Val Loss: 1.5985
batch size: (899, 899)
Epoch 117, accuracy: 0.4060
batch size: (897, 897)
Epoch 118, accuracy: 0.4066
Epoch 118, Train Loss: 0.5194, Val Loss: 1.5538
batch size: (905, 905)
Epoch 119, accuracy: 0.4036
batch size: (901, 901)
Epoch 120, accuracy: 0.4023
Epoch 120, Train Loss: 0.6667, Val Loss: 1.5860
batch size: (913, 913)
Epoch 121, accuracy: 0.4023
batch size: (894, 894)
Epoch 122, accuracy: 0.4011
Epoch 122, Train Loss: 0.7093, Val Loss: 1.5730
batch size: (917, 917)
Epoch 123, accuracy: 0.4043
batch size: (903, 903)
Epoch 124, accuracy: 0.4091
Epoch 124, Train Loss: 0.5565, Val Loss: 1.5310
batch size: (884, 884)
Epoch 125, accuracy: 0.4002
batch size: (889, 889)
Epoch 126, accuracy: 0.3938
Epoch 126, Train Loss: 0.5754, Val Loss: 1.5940
batch size: (895, 895)
Epoch 127, accuracy: 0.3924
batch size: (889, 889)
Epoch 128, accuracy: 0.4006
Epoch 128, Train Loss: 0.6392, Val Loss: 1.5869
batch size: (914, 914)
Epoch 129, accuracy: 0.3986
batch size: (899, 899)
Epoch 130, accuracy: 0.4041
Epoch 130, Train Loss: 0.6416, Val Loss: 1.5352
batch size: (885, 885)
Epoch 131, accuracy: 0.4063
batch size: (892, 892)
Epoch 132, accuracy: 0.4051
Epoch 132, Train Loss: 0.5977, Val Loss: 1.5324
batch size: (883, 883)
Epoch 133, accuracy: 0.4007
batch size: (896, 896)
Epoch 134, accuracy: 0.4025
Epoch 134, Train Loss: 0.5609, Val Loss: 1.5309
batch size: (917, 917)
Epoch 135, accuracy: 0.4003
batch size: (900, 900)
Epoch 136, accuracy: 0.3946
Epoch 136, Train Loss: 0.7315, Val Loss: 1.6224
batch size: (899, 899)
Epoch 137, accuracy: 0.4080
batch size: (891, 891)
Epoch 138, accuracy: 0.4044
Epoch 138, Train Loss: 0.6254, Val Loss: 1.5678
batch size: (884, 884)
Epoch 139, accuracy: 0.4014
batch size: (909, 909)
Epoch 140, accuracy: 0.4032
Epoch 140, Train Loss: 0.6632, Val Loss: 1.6168
batch size: (900, 900)
Epoch 141, accuracy: 0.3927
batch size: (912, 912)
Epoch 142, accuracy: 0.4028
Epoch 142, Train Loss: 0.6793, Val Loss: 1.5369
batch size: (912, 912)
Epoch 143, accuracy: 0.4033
batch size: (900, 900)
Epoch 144, accuracy: 0.3952
Epoch 144, Train Loss: 0.5891, Val Loss: 1.6246
batch size: (903, 903)
Epoch 145, accuracy: 0.4052
batch size: (907, 907)
Epoch 146, accuracy: 0.3972
Epoch 146, Train Loss: 0.5634, Val Loss: 1.5966
batch size: (903, 903)
Epoch 147, accuracy: 0.4093
batch size: (900, 900)
Epoch 148, accuracy: 0.4008
Epoch 148, Train Loss: 0.5346, Val Loss: 1.5739
batch size: (915, 915)
Epoch 149, accuracy: 0.4035
batch size: (906, 906)
Epoch 150, accuracy: 0.4040
Epoch 150, Train Loss: 0.6090, Val Loss: 1.5854
batch size: (900, 900)
Epoch 151, accuracy: 0.4041
batch size: (897, 897)
Epoch 152, accuracy: 0.4004
Epoch 152, Train Loss: 0.6354, Val Loss: 1.5712
batch size: (911, 911)
Epoch 153, accuracy: 0.3939
batch size: (877, 877)
Epoch 154, accuracy: 0.4000
Epoch 154, Train Loss: 0.7957, Val Loss: 1.5959
batch size: (891, 891)
Epoch 155, accuracy: 0.3991
batch size: (886, 886)
Epoch 156, accuracy: 0.3964
Epoch 156, Train Loss: 0.7447, Val Loss: 1.6517
batch size: (895, 895)
Epoch 157, accuracy: 0.3969
batch size: (907, 907)
Epoch 158, accuracy: 0.3980
Epoch 158, Train Loss: 0.6543, Val Loss: 1.6064
batch size: (897, 897)
Epoch 159, accuracy: 0.3985
batch size: (891, 891)
Epoch 160, accuracy: 0.3980
Epoch 160, Train Loss: 0.8089, Val Loss: 1.6222
batch size: (912, 912)
Epoch 161, accuracy: 0.4026
batch size: (901, 901)
Epoch 162, accuracy: 0.4041
Epoch 162, Train Loss: 0.7202, Val Loss: 1.5469
batch size: (906, 906)
Epoch 163, accuracy: 0.4025
batch size: (888, 888)
Epoch 164, accuracy: 0.4051
Epoch 164, Train Loss: 0.5936, Val Loss: 1.5592
batch size: (901, 901)
Epoch 165, accuracy: 0.3951
batch size: (897, 897)
Epoch 166, accuracy: 0.3964
Epoch 166, Train Loss: 0.6630, Val Loss: 1.5602
batch size: (897, 897)
Epoch 167, accuracy: 0.4100
batch size: (903, 903)
Epoch 168, accuracy: 0.3995
Epoch 168, Train Loss: 0.5099, Val Loss: 1.5847
batch size: (901, 901)
Epoch 169, accuracy: 0.4032
batch size: (896, 896)
Epoch 170, accuracy: 0.4028
Epoch 170, Train Loss: 0.7355, Val Loss: 1.5701
batch size: (889, 889)
Epoch 171, accuracy: 0.4019
batch size: (912, 912)
Epoch 172, accuracy: 0.3983
Epoch 172, Train Loss: 0.6555, Val Loss: 1.5763
batch size: (926, 926)
Epoch 173, accuracy: 0.4013
batch size: (887, 887)
Epoch 174, accuracy: 0.4018
Epoch 174, Train Loss: 0.6036, Val Loss: 1.5622
batch size: (887, 887)
Epoch 175, accuracy: 0.4010
batch size: (913, 913)
Epoch 176, accuracy: 0.4007
Epoch 176, Train Loss: 0.6632, Val Loss: 1.6049
batch size: (889, 889)
Epoch 177, accuracy: 0.4056
batch size: (898, 898)
Epoch 178, accuracy: 0.4054
Epoch 178, Train Loss: 0.5645, Val Loss: 1.5856
batch size: (905, 905)
Epoch 179, accuracy: 0.3991
batch size: (910, 910)
Epoch 180, accuracy: 0.3969
Epoch 180, Train Loss: 0.5273, Val Loss: 1.6146
batch size: (911, 911)
Epoch 181, accuracy: 0.3972
batch size: (901, 901)
Epoch 182, accuracy: 0.4025
Epoch 182, Train Loss: 0.6445, Val Loss: 1.6016
batch size: (906, 906)
Epoch 183, accuracy: 0.3980
batch size: (913, 913)
Epoch 184, accuracy: 0.3974
Epoch 184, Train Loss: 0.5578, Val Loss: 1.6035
batch size: (893, 893)
Epoch 185, accuracy: 0.4003
batch size: (881, 881)
Epoch 186, accuracy: 0.4077
Epoch 186, Train Loss: 0.6315, Val Loss: 1.5580
batch size: (900, 900)
Epoch 187, accuracy: 0.4007
batch size: (901, 901)
Epoch 188, accuracy: 0.4048
Epoch 188, Train Loss: 0.6676, Val Loss: 1.5562
batch size: (909, 909)
Epoch 189, accuracy: 0.3986
batch size: (899, 899)
Epoch 190, accuracy: 0.4012
Epoch 190, Train Loss: 0.6529, Val Loss: 1.6138
batch size: (909, 909)
Epoch 191, accuracy: 0.3961
batch size: (909, 909)
Epoch 192, accuracy: 0.3961
Epoch 192, Train Loss: 0.6431, Val Loss: 1.6425
batch size: (894, 894)
Epoch 193, accuracy: 0.4021
batch size: (904, 904)
Epoch 194, accuracy: 0.4020
Epoch 194, Train Loss: 0.5527, Val Loss: 1.5999
batch size: (906, 906)
Epoch 195, accuracy: 0.4013
batch size: (888, 888)
Epoch 196, accuracy: 0.4040
Epoch 196, Train Loss: 0.6080, Val Loss: 1.5802
batch size: (896, 896)
Epoch 197, accuracy: 0.4056
batch size: (900, 900)
Epoch 198, accuracy: 0.4067
Epoch 198, Train Loss: 0.7065, Val Loss: 1.5586
batch size: (900, 900)
Epoch 199, accuracy: 0.4003
Loaded best model with val_loss = 1.0889866352081299
test :accuracy 0.4290, f1_macro: 0.2001, f1_micro: 0.4290, auc: 0.5428
Training GCN with 32 layers...
可训练参数: 2130185_GCN
不可训练参数: 0
batch size: (911, 911)
✅ Epoch 0: New best model saved with val_loss = 1.1272
Epoch 0, accuracy: 0.1651
Epoch 0, Train Loss: 1.1409, Val Loss: 1.1272
batch size: (901, 901)
Epoch 1, accuracy: 0.1663
batch size: (926, 926)
Epoch 2, accuracy: 0.1671
Epoch 2, Train Loss: 1.2243, Val Loss: 1.2056
batch size: (904, 904)
Epoch 3, accuracy: 0.1659
batch size: (902, 902)
Epoch 4, accuracy: 0.1713
Epoch 4, Train Loss: 1.1783, Val Loss: 1.2717
batch size: (901, 901)
Epoch 5, accuracy: 0.1692
batch size: (898, 898)
Epoch 6, accuracy: 0.4303
Epoch 6, Train Loss: 1.2066, Val Loss: 1.2334
batch size: (884, 884)
Epoch 7, accuracy: 0.4293
batch size: (902, 902)
Epoch 8, accuracy: 0.4290
Epoch 8, Train Loss: 1.2867, Val Loss: 1.1764
batch size: (911, 911)
Epoch 9, accuracy: 0.4294
batch size: (893, 893)
Epoch 10, accuracy: 0.4282
Epoch 10, Train Loss: 1.2009, Val Loss: 1.1674
batch size: (890, 890)
Epoch 11, accuracy: 0.4268
batch size: (889, 889)
Epoch 12, accuracy: 0.4293
Epoch 12, Train Loss: 1.2087, Val Loss: 1.1648
batch size: (904, 904)
Epoch 13, accuracy: 0.4294
batch size: (899, 899)
Epoch 14, accuracy: 0.4296
Epoch 14, Train Loss: 1.1877, Val Loss: 1.1602
batch size: (903, 903)
Epoch 15, accuracy: 0.4327
batch size: (898, 898)
Epoch 16, accuracy: 0.4288
Epoch 16, Train Loss: 1.2351, Val Loss: 1.1767
batch size: (918, 918)
Epoch 17, accuracy: 0.4310
batch size: (892, 892)
Epoch 18, accuracy: 0.4310
Epoch 18, Train Loss: 1.2389, Val Loss: 1.1691
batch size: (882, 882)
Epoch 19, accuracy: 0.4276
batch size: (895, 895)
Epoch 20, accuracy: 0.4286
Epoch 20, Train Loss: 1.1441, Val Loss: 1.1494
batch size: (896, 896)
Epoch 21, accuracy: 0.4313
batch size: (907, 907)
Epoch 22, accuracy: 0.4349
Epoch 22, Train Loss: 1.0759, Val Loss: 1.1697
batch size: (899, 899)
Epoch 23, accuracy: 0.4286
batch size: (915, 915)
Epoch 24, accuracy: 0.4249
Epoch 24, Train Loss: 1.1810, Val Loss: 1.1973
batch size: (911, 911)
Epoch 25, accuracy: 0.4283
batch size: (904, 904)
Epoch 26, accuracy: 0.4282
Epoch 26, Train Loss: 1.1580, Val Loss: 1.1486
batch size: (904, 904)
Epoch 27, accuracy: 0.4342
batch size: (894, 894)
Epoch 28, accuracy: 0.4336
Epoch 28, Train Loss: 1.0496, Val Loss: 1.1582
batch size: (899, 899)
Epoch 29, accuracy: 0.4322
batch size: (921, 921)
Epoch 30, accuracy: 0.4263
Epoch 30, Train Loss: 1.1125, Val Loss: 1.1791
batch size: (916, 916)
Epoch 31, accuracy: 0.4255
batch size: (906, 906)
Epoch 32, accuracy: 0.4309
Epoch 32, Train Loss: 1.0230, Val Loss: 1.1671
batch size: (901, 901)
Epoch 33, accuracy: 0.4307
batch size: (919, 919)
Epoch 34, accuracy: 0.4347
Epoch 34, Train Loss: 1.1029, Val Loss: 1.1539
batch size: (897, 897)
Epoch 35, accuracy: 0.4318
batch size: (906, 906)
Epoch 36, accuracy: 0.4324
Epoch 36, Train Loss: 1.3054, Val Loss: 1.1512
batch size: (896, 896)
Epoch 37, accuracy: 0.4317
batch size: (888, 888)
Epoch 38, accuracy: 0.4277
Epoch 38, Train Loss: 1.1986, Val Loss: 1.1776
batch size: (896, 896)
Epoch 39, accuracy: 0.4328
batch size: (902, 902)
Epoch 40, accuracy: 0.4295
Epoch 40, Train Loss: 1.0997, Val Loss: 1.1693
batch size: (911, 911)
Epoch 41, accuracy: 0.4305
batch size: (898, 898)
Epoch 42, accuracy: 0.4297
Epoch 42, Train Loss: 1.1379, Val Loss: 1.1633
batch size: (894, 894)
Epoch 43, accuracy: 0.4323
batch size: (884, 884)
Epoch 44, accuracy: 0.4263
Epoch 44, Train Loss: 1.2154, Val Loss: 1.1695
batch size: (911, 911)
Epoch 45, accuracy: 0.4308
batch size: (898, 898)
Epoch 46, accuracy: 0.4336
Epoch 46, Train Loss: 1.1207, Val Loss: 1.1606
batch size: (887, 887)
Epoch 47, accuracy: 0.4322
batch size: (923, 923)
Epoch 48, accuracy: 0.4296
Epoch 48, Train Loss: 1.1332, Val Loss: 1.1493
batch size: (893, 893)
Epoch 49, accuracy: 0.4296
batch size: (896, 896)
Epoch 50, accuracy: 0.4295
Epoch 50, Train Loss: 1.2413, Val Loss: 1.1499
batch size: (893, 893)
Epoch 51, accuracy: 0.4297
batch size: (897, 897)
Epoch 52, accuracy: 0.4285
Epoch 52, Train Loss: 1.3078, Val Loss: 1.1726
batch size: (915, 915)
Epoch 53, accuracy: 0.4329
batch size: (892, 892)
Epoch 54, accuracy: 0.4272
Epoch 54, Train Loss: 1.2879, Val Loss: 1.1714
batch size: (903, 903)
Epoch 55, accuracy: 0.4338
batch size: (913, 913)
Epoch 56, accuracy: 0.4287
Epoch 56, Train Loss: 1.0711, Val Loss: 1.1675
batch size: (901, 901)
Epoch 57, accuracy: 0.4305
batch size: (897, 897)
Epoch 58, accuracy: 0.4308
Epoch 58, Train Loss: 1.3056, Val Loss: 1.1446
batch size: (922, 922)
Epoch 59, accuracy: 0.4293
batch size: (907, 907)
Epoch 60, accuracy: 0.4317
Epoch 60, Train Loss: 1.1935, Val Loss: 1.1484
batch size: (903, 903)
Epoch 61, accuracy: 0.4274
batch size: (890, 890)
Epoch 62, accuracy: 0.4292
Epoch 62, Train Loss: 1.2386, Val Loss: 1.1962
batch size: (891, 891)
Epoch 63, accuracy: 0.4291
batch size: (890, 890)
Epoch 64, accuracy: 0.4325
Epoch 64, Train Loss: 1.2605, Val Loss: 1.1609
batch size: (915, 915)
Epoch 65, accuracy: 0.4336
batch size: (914, 914)
Epoch 66, accuracy: 0.4295
Epoch 66, Train Loss: 1.2604, Val Loss: 1.1646
batch size: (872, 872)
Epoch 67, accuracy: 0.4299
batch size: (899, 899)
Epoch 68, accuracy: 0.4318
Epoch 68, Train Loss: 1.2783, Val Loss: 1.1706
batch size: (897, 897)
Epoch 69, accuracy: 0.4332
batch size: (881, 881)
Epoch 70, accuracy: 0.4298
Epoch 70, Train Loss: 1.3226, Val Loss: 1.1856
batch size: (892, 892)
Epoch 71, accuracy: 0.4302
batch size: (901, 901)
Epoch 72, accuracy: 0.4310
Epoch 72, Train Loss: 1.1747, Val Loss: 1.1568
batch size: (896, 896)
Epoch 73, accuracy: 0.4333
batch size: (904, 904)
Epoch 74, accuracy: 0.4298
Epoch 74, Train Loss: 1.1359, Val Loss: 1.1646
batch size: (905, 905)
Epoch 75, accuracy: 0.4300
batch size: (893, 893)
Epoch 76, accuracy: 0.4277
Epoch 76, Train Loss: 1.1166, Val Loss: 1.1693
batch size: (869, 869)
Epoch 77, accuracy: 0.4313
batch size: (918, 918)
Epoch 78, accuracy: 0.4273
Epoch 78, Train Loss: 1.1350, Val Loss: 1.1514
batch size: (876, 876)
Epoch 79, accuracy: 0.4338
batch size: (915, 915)
Epoch 80, accuracy: 0.4310
Epoch 80, Train Loss: 1.1368, Val Loss: 1.1667
batch size: (926, 926)
Epoch 81, accuracy: 0.4347
batch size: (897, 897)
Epoch 82, accuracy: 0.4311
Epoch 82, Train Loss: 1.1230, Val Loss: 1.1783
batch size: (891, 891)
Epoch 83, accuracy: 0.4290
batch size: (895, 895)
Epoch 84, accuracy: 0.4336
Epoch 84, Train Loss: 1.1766, Val Loss: 1.1596
batch size: (920, 920)
Epoch 85, accuracy: 0.4279
batch size: (904, 904)
Epoch 86, accuracy: 0.4277
Epoch 86, Train Loss: 1.2373, Val Loss: 1.1537
batch size: (905, 905)
Epoch 87, accuracy: 0.4285
batch size: (895, 895)
Epoch 88, accuracy: 0.4285
Epoch 88, Train Loss: 1.1638, Val Loss: 1.1765
batch size: (909, 909)
Epoch 89, accuracy: 0.4308
batch size: (901, 901)
Epoch 90, accuracy: 0.4295
Epoch 90, Train Loss: 1.0624, Val Loss: 1.1680
batch size: (896, 896)
Epoch 91, accuracy: 0.4292
batch size: (892, 892)
Epoch 92, accuracy: 0.4268
Epoch 92, Train Loss: 1.0762, Val Loss: 1.1692
batch size: (895, 895)
Epoch 93, accuracy: 0.4297
batch size: (884, 884)
Epoch 94, accuracy: 0.4266
Epoch 94, Train Loss: 1.1645, Val Loss: 1.1662
batch size: (899, 899)
Epoch 95, accuracy: 0.4294
batch size: (907, 907)
Epoch 96, accuracy: 0.4320
Epoch 96, Train Loss: 1.1001, Val Loss: 1.1789
batch size: (896, 896)
Epoch 97, accuracy: 0.4294
batch size: (907, 907)
Epoch 98, accuracy: 0.4334
Epoch 98, Train Loss: 1.2804, Val Loss: 1.1546
batch size: (912, 912)
Epoch 99, accuracy: 0.4274
batch size: (908, 908)
Epoch 100, accuracy: 0.4314
Epoch 100, Train Loss: 1.1744, Val Loss: 1.1843
batch size: (908, 908)
Epoch 101, accuracy: 0.4324
batch size: (904, 904)
Epoch 102, accuracy: 0.4298
Epoch 102, Train Loss: 1.2572, Val Loss: 1.1521
batch size: (901, 901)
Epoch 103, accuracy: 0.4289
batch size: (897, 897)
Epoch 104, accuracy: 0.4274
Epoch 104, Train Loss: 1.1126, Val Loss: 1.1462
batch size: (902, 902)
Epoch 105, accuracy: 0.4356
batch size: (881, 881)
Epoch 106, accuracy: 0.4307
Epoch 106, Train Loss: 1.1381, Val Loss: 1.1742
batch size: (898, 898)
Epoch 107, accuracy: 0.4299
batch size: (891, 891)
Epoch 108, accuracy: 0.4296
Epoch 108, Train Loss: 1.2950, Val Loss: 1.1603
batch size: (906, 906)
Epoch 109, accuracy: 0.4286
batch size: (912, 912)
Epoch 110, accuracy: 0.4294
Epoch 110, Train Loss: 1.1639, Val Loss: 1.1464
batch size: (907, 907)
Epoch 111, accuracy: 0.4305
batch size: (912, 912)
Epoch 112, accuracy: 0.4284
Epoch 112, Train Loss: 1.0270, Val Loss: 1.1777
batch size: (900, 900)
Epoch 113, accuracy: 0.4289
batch size: (902, 902)
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 114, accuracy: 0.4332
Epoch 114, Train Loss: 1.0761, Val Loss: 1.1485
batch size: (893, 893)
Epoch 115, accuracy: 0.4294
batch size: (887, 887)
Epoch 116, accuracy: 0.4293
Epoch 116, Train Loss: 1.0925, Val Loss: 1.1480
batch size: (901, 901)
Epoch 117, accuracy: 0.4281
batch size: (904, 904)
Epoch 118, accuracy: 0.4297
Epoch 118, Train Loss: 1.1148, Val Loss: 1.1623
batch size: (904, 904)
Epoch 119, accuracy: 0.4291
batch size: (887, 887)
Epoch 120, accuracy: 0.4326
Epoch 120, Train Loss: 1.2663, Val Loss: 1.1636
batch size: (895, 895)
Epoch 121, accuracy: 0.4290
batch size: (895, 895)
Epoch 122, accuracy: 0.4307
Epoch 122, Train Loss: 1.0918, Val Loss: 1.1780
batch size: (888, 888)
Epoch 123, accuracy: 0.4320
batch size: (904, 904)
Epoch 124, accuracy: 0.4307
Epoch 124, Train Loss: 1.0655, Val Loss: 1.1434
batch size: (896, 896)
Epoch 125, accuracy: 0.4308
batch size: (899, 899)
Epoch 126, accuracy: 0.4297
Epoch 126, Train Loss: 1.1159, Val Loss: 1.1417
batch size: (884, 884)
Epoch 127, accuracy: 0.4287
batch size: (899, 899)
Epoch 128, accuracy: 0.4320
Epoch 128, Train Loss: 1.3660, Val Loss: 1.1825
batch size: (887, 887)
Epoch 129, accuracy: 0.4339
batch size: (896, 896)
Epoch 130, accuracy: 0.4332
Epoch 130, Train Loss: 1.2179, Val Loss: 1.1457
batch size: (887, 887)
Epoch 131, accuracy: 0.4322
batch size: (908, 908)
Epoch 132, accuracy: 0.4260
Epoch 132, Train Loss: 1.1215, Val Loss: 1.1553
batch size: (882, 882)
Epoch 133, accuracy: 0.4282
batch size: (898, 898)
Epoch 134, accuracy: 0.4304
Epoch 134, Train Loss: 1.0882, Val Loss: 1.1732
batch size: (910, 910)
Epoch 135, accuracy: 0.4320
batch size: (900, 900)
Epoch 136, accuracy: 0.4304
Epoch 136, Train Loss: 1.1282, Val Loss: 1.1458
batch size: (903, 903)
Epoch 137, accuracy: 0.4253
batch size: (896, 896)
Epoch 138, accuracy: 0.4292
Epoch 138, Train Loss: 1.1853, Val Loss: 1.1445
batch size: (900, 900)
Epoch 139, accuracy: 0.4320
batch size: (907, 907)
Epoch 140, accuracy: 0.4336
Epoch 140, Train Loss: 1.1370, Val Loss: 1.1604
batch size: (915, 915)
Epoch 141, accuracy: 0.4311
batch size: (892, 892)
Epoch 142, accuracy: 0.4268
Epoch 142, Train Loss: 1.0780, Val Loss: 1.1817
batch size: (895, 895)
Epoch 143, accuracy: 0.4330
batch size: (897, 897)
Epoch 144, accuracy: 0.4276
Epoch 144, Train Loss: 1.1057, Val Loss: 1.1723
batch size: (904, 904)
Epoch 145, accuracy: 0.4278
batch size: (885, 885)
Epoch 146, accuracy: 0.4318
Epoch 146, Train Loss: 1.0674, Val Loss: 1.1596
batch size: (915, 915)
Epoch 147, accuracy: 0.4305
batch size: (911, 911)
Epoch 148, accuracy: 0.4306
Epoch 148, Train Loss: 1.1361, Val Loss: 1.1622
batch size: (917, 917)
Epoch 149, accuracy: 0.4272
batch size: (910, 910)
Epoch 150, accuracy: 0.4280
Epoch 150, Train Loss: 1.0861, Val Loss: 1.1750
batch size: (908, 908)
Epoch 151, accuracy: 0.4272
batch size: (920, 920)
Epoch 152, accuracy: 0.4324
Epoch 152, Train Loss: 1.1002, Val Loss: 1.1510
batch size: (913, 913)
Epoch 153, accuracy: 0.4284
batch size: (892, 892)
Epoch 154, accuracy: 0.4294
Epoch 154, Train Loss: 1.1460, Val Loss: 1.1561
batch size: (904, 904)
Epoch 155, accuracy: 0.4290
batch size: (906, 906)
Epoch 156, accuracy: 0.4318
Epoch 156, Train Loss: 1.1490, Val Loss: 1.1694
batch size: (914, 914)
Epoch 157, accuracy: 0.4329
batch size: (900, 900)
Epoch 158, accuracy: 0.4289
Epoch 158, Train Loss: 1.0464, Val Loss: 1.1786
batch size: (899, 899)
Epoch 159, accuracy: 0.4336
batch size: (900, 900)
Epoch 160, accuracy: 0.4358
Epoch 160, Train Loss: 1.0785, Val Loss: 1.1474
batch size: (895, 895)
Epoch 161, accuracy: 0.4314
batch size: (903, 903)
Epoch 162, accuracy: 0.4338
Epoch 162, Train Loss: 1.1269, Val Loss: 1.1567
batch size: (892, 892)
Epoch 163, accuracy: 0.4312
batch size: (924, 924)
Epoch 164, accuracy: 0.4276
Epoch 164, Train Loss: 1.0899, Val Loss: 1.1717
batch size: (885, 885)
Epoch 165, accuracy: 0.4296
batch size: (907, 907)
Epoch 166, accuracy: 0.4307
Epoch 166, Train Loss: 1.1285, Val Loss: 1.1619
batch size: (904, 904)
Epoch 167, accuracy: 0.4316
batch size: (898, 898)
Epoch 168, accuracy: 0.4273
Epoch 168, Train Loss: 1.2920, Val Loss: 1.1784
batch size: (904, 904)
Epoch 169, accuracy: 0.4295
batch size: (915, 915)
Epoch 170, accuracy: 0.4317
Epoch 170, Train Loss: 1.0430, Val Loss: 1.1674
batch size: (910, 910)
Epoch 171, accuracy: 0.4336
batch size: (910, 910)
Epoch 172, accuracy: 0.4280
Epoch 172, Train Loss: 1.2077, Val Loss: 1.1441
batch size: (924, 924)
Epoch 173, accuracy: 0.4259
batch size: (897, 897)
Epoch 174, accuracy: 0.4327
Epoch 174, Train Loss: 1.1460, Val Loss: 1.1615
batch size: (890, 890)
Epoch 175, accuracy: 0.4282
batch size: (895, 895)
Epoch 176, accuracy: 0.4297
Epoch 176, Train Loss: 1.0752, Val Loss: 1.1586
batch size: (892, 892)
Epoch 177, accuracy: 0.4313
batch size: (883, 883)
Epoch 178, accuracy: 0.4320
Epoch 178, Train Loss: 1.2946, Val Loss: 1.1721
batch size: (899, 899)
Epoch 179, accuracy: 0.4237
batch size: (915, 915)
Epoch 180, accuracy: 0.4282
Epoch 180, Train Loss: 1.1729, Val Loss: 1.1365
batch size: (888, 888)
Epoch 181, accuracy: 0.4325
batch size: (896, 896)
Epoch 182, accuracy: 0.4331
Epoch 182, Train Loss: 1.1902, Val Loss: 1.1581
batch size: (887, 887)
Epoch 183, accuracy: 0.4308
batch size: (898, 898)
Epoch 184, accuracy: 0.4291
Epoch 184, Train Loss: 1.2587, Val Loss: 1.1887
batch size: (930, 930)
Epoch 185, accuracy: 0.4325
batch size: (899, 899)
Epoch 186, accuracy: 0.4246
Epoch 186, Train Loss: 1.3235, Val Loss: 1.1664
batch size: (899, 899)
Epoch 187, accuracy: 0.4320
batch size: (925, 925)
Epoch 188, accuracy: 0.4295
Epoch 188, Train Loss: 1.0426, Val Loss: 1.1614
batch size: (893, 893)
Epoch 189, accuracy: 0.4298
batch size: (888, 888)
Epoch 190, accuracy: 0.4282
Epoch 190, Train Loss: 1.1451, Val Loss: 1.1430
batch size: (898, 898)
Epoch 191, accuracy: 0.4313
batch size: (900, 900)
Epoch 192, accuracy: 0.4297
Epoch 192, Train Loss: 1.1685, Val Loss: 1.1495
batch size: (895, 895)
Epoch 193, accuracy: 0.4304
batch size: (892, 892)
Epoch 194, accuracy: 0.4312
Epoch 194, Train Loss: 1.1430, Val Loss: 1.1560
batch size: (904, 904)
Epoch 195, accuracy: 0.4282
batch size: (895, 895)
Epoch 196, accuracy: 0.4295
Epoch 196, Train Loss: 1.1870, Val Loss: 1.1767
batch size: (922, 922)
Epoch 197, accuracy: 0.4291
batch size: (905, 905)
Epoch 198, accuracy: 0.4342
Epoch 198, Train Loss: 1.1450, Val Loss: 1.1572
batch size: (899, 899)
Epoch 199, accuracy: 0.4272
Loaded best model with val_loss = 1.1272106170654297
test :accuracy 0.1671, f1_macro: 0.0955, f1_micro: 0.1671, auc: 0.5206
Training GraphSAGE with 2 layers...
可训练参数: 262153_GraphSAGE
不可训练参数: 0
batch size: (907, 907)
✅ Epoch 0: New best model saved with val_loss = 1.0967
Epoch 0, accuracy: 0.1715
Epoch 0, Train Loss: 1.2154, Val Loss: 1.0967
batch size: (904, 904)
✅ Epoch 1: New best model saved with val_loss = 1.0934
Epoch 1, accuracy: 0.1953
batch size: (908, 908)
✅ Epoch 2: New best model saved with val_loss = 1.0871
Epoch 2, accuracy: 0.2477
Epoch 2, Train Loss: 0.5523, Val Loss: 1.0871
batch size: (904, 904)
✅ Epoch 3: New best model saved with val_loss = 1.0789
Epoch 3, accuracy: 0.3473
batch size: (893, 893)
✅ Epoch 4: New best model saved with val_loss = 1.0689
Epoch 4, accuracy: 0.4571
Epoch 4, Train Loss: 0.2337, Val Loss: 1.0689
batch size: (877, 877)
✅ Epoch 5: New best model saved with val_loss = 1.0576
Epoch 5, accuracy: 0.5353
batch size: (897, 897)
✅ Epoch 6: New best model saved with val_loss = 1.0444
Epoch 6, accuracy: 0.5870
Epoch 6, Train Loss: 0.0852, Val Loss: 1.0444
batch size: (912, 912)
✅ Epoch 7: New best model saved with val_loss = 1.0293
Epoch 7, accuracy: 0.6253
batch size: (902, 902)
✅ Epoch 8: New best model saved with val_loss = 1.0132
Epoch 8, accuracy: 0.6435
Epoch 8, Train Loss: 0.0202, Val Loss: 1.0132
batch size: (892, 892)
✅ Epoch 9: New best model saved with val_loss = 0.9979
Epoch 9, accuracy: 0.6519
batch size: (905, 905)
✅ Epoch 10: New best model saved with val_loss = 0.9812
Epoch 10, accuracy: 0.6524
Epoch 10, Train Loss: 0.0031, Val Loss: 0.9812
batch size: (870, 870)
✅ Epoch 11: New best model saved with val_loss = 0.9654
Epoch 11, accuracy: 0.6518
batch size: (898, 898)
✅ Epoch 12: New best model saved with val_loss = 0.9496
Epoch 12, accuracy: 0.6557
Epoch 12, Train Loss: 0.0010, Val Loss: 0.9496
batch size: (888, 888)
✅ Epoch 13: New best model saved with val_loss = 0.9347
Epoch 13, accuracy: 0.6562
batch size: (904, 904)
✅ Epoch 14: New best model saved with val_loss = 0.9191
Epoch 14, accuracy: 0.6547
Epoch 14, Train Loss: 0.0008, Val Loss: 0.9191
batch size: (903, 903)
✅ Epoch 15: New best model saved with val_loss = 0.9029
Epoch 15, accuracy: 0.6579
batch size: (901, 901)
✅ Epoch 16: New best model saved with val_loss = 0.8889
Epoch 16, accuracy: 0.6597
Epoch 16, Train Loss: 0.0008, Val Loss: 0.8889
batch size: (898, 898)
✅ Epoch 17: New best model saved with val_loss = 0.8753
Epoch 17, accuracy: 0.6641
batch size: (904, 904)
✅ Epoch 18: New best model saved with val_loss = 0.8599
Epoch 18, accuracy: 0.6689
Epoch 18, Train Loss: 0.0022, Val Loss: 0.8599
batch size: (912, 912)
✅ Epoch 19: New best model saved with val_loss = 0.8450
Epoch 19, accuracy: 0.6707
batch size: (889, 889)
✅ Epoch 20: New best model saved with val_loss = 0.8320
Epoch 20, accuracy: 0.6720
Epoch 20, Train Loss: 0.0003, Val Loss: 0.8320
batch size: (907, 907)
✅ Epoch 21: New best model saved with val_loss = 0.8214
Epoch 21, accuracy: 0.6734
batch size: (894, 894)
✅ Epoch 22: New best model saved with val_loss = 0.8081
Epoch 22, accuracy: 0.6742
Epoch 22, Train Loss: 0.0015, Val Loss: 0.8081
batch size: (902, 902)
✅ Epoch 23: New best model saved with val_loss = 0.7971
Epoch 23, accuracy: 0.6733
batch size: (932, 932)
✅ Epoch 24: New best model saved with val_loss = 0.7871
Epoch 24, accuracy: 0.6741
Epoch 24, Train Loss: 0.0001, Val Loss: 0.7871
batch size: (896, 896)
✅ Epoch 25: New best model saved with val_loss = 0.7721
Epoch 25, accuracy: 0.6742
batch size: (887, 887)
✅ Epoch 26: New best model saved with val_loss = 0.7623
Epoch 26, accuracy: 0.6759
Epoch 26, Train Loss: 0.0004, Val Loss: 0.7623
batch size: (912, 912)
✅ Epoch 27: New best model saved with val_loss = 0.7497
Epoch 27, accuracy: 0.6759
batch size: (901, 901)
✅ Epoch 28: New best model saved with val_loss = 0.7384
Epoch 28, accuracy: 0.6758
Epoch 28, Train Loss: 0.0001, Val Loss: 0.7384
batch size: (899, 899)
✅ Epoch 29: New best model saved with val_loss = 0.7299
Epoch 29, accuracy: 0.6811
batch size: (912, 912)
✅ Epoch 30: New best model saved with val_loss = 0.7177
Epoch 30, accuracy: 0.6802
Epoch 30, Train Loss: 0.0002, Val Loss: 0.7177
batch size: (884, 884)
✅ Epoch 31: New best model saved with val_loss = 0.7059
Epoch 31, accuracy: 0.6816
batch size: (906, 906)
✅ Epoch 32: New best model saved with val_loss = 0.7003
Epoch 32, accuracy: 0.6817
Epoch 32, Train Loss: 0.0004, Val Loss: 0.7003
batch size: (857, 857)
✅ Epoch 33: New best model saved with val_loss = 0.6866
Epoch 33, accuracy: 0.6832
batch size: (900, 900)
✅ Epoch 34: New best model saved with val_loss = 0.6749
Epoch 34, accuracy: 0.6836
Epoch 34, Train Loss: 0.0001, Val Loss: 0.6749
batch size: (889, 889)
✅ Epoch 35: New best model saved with val_loss = 0.6717
Epoch 35, accuracy: 0.6833
batch size: (904, 904)
✅ Epoch 36: New best model saved with val_loss = 0.6606
Epoch 36, accuracy: 0.6826
Epoch 36, Train Loss: 0.0004, Val Loss: 0.6606
batch size: (887, 887)
✅ Epoch 37: New best model saved with val_loss = 0.6569
Epoch 37, accuracy: 0.6837
batch size: (905, 905)
✅ Epoch 38: New best model saved with val_loss = 0.6483
Epoch 38, accuracy: 0.6840
Epoch 38, Train Loss: 0.0001, Val Loss: 0.6483
batch size: (903, 903)
✅ Epoch 39: New best model saved with val_loss = 0.6313
Epoch 39, accuracy: 0.6849
batch size: (907, 907)
Epoch 40, accuracy: 0.6890
Epoch 40, Train Loss: 0.0002, Val Loss: 0.6327
batch size: (909, 909)
✅ Epoch 41: New best model saved with val_loss = 0.6299
Epoch 41, accuracy: 0.6881
batch size: (892, 892)
✅ Epoch 42: New best model saved with val_loss = 0.6201
Epoch 42, accuracy: 0.6885
Epoch 42, Train Loss: 0.0001, Val Loss: 0.6201
batch size: (900, 900)
Epoch 43, accuracy: 0.6901
batch size: (917, 917)
✅ Epoch 44: New best model saved with val_loss = 0.6075
Epoch 44, accuracy: 0.6879
Epoch 44, Train Loss: 0.0001, Val Loss: 0.6075
batch size: (917, 917)
Epoch 45, accuracy: 0.6907
batch size: (889, 889)
✅ Epoch 46: New best model saved with val_loss = 0.6069
Epoch 46, accuracy: 0.6920
Epoch 46, Train Loss: 0.0001, Val Loss: 0.6069
batch size: (887, 887)
✅ Epoch 47: New best model saved with val_loss = 0.6059
Epoch 47, accuracy: 0.6916
batch size: (900, 900)
Epoch 48, accuracy: 0.6913
Epoch 48, Train Loss: 0.0001, Val Loss: 0.6128
batch size: (873, 873)
✅ Epoch 49: New best model saved with val_loss = 0.6038
Epoch 49, accuracy: 0.6932
batch size: (917, 917)
Epoch 50, accuracy: 0.6939
Epoch 50, Train Loss: 0.0001, Val Loss: 0.6087
batch size: (910, 910)
Epoch 51, accuracy: 0.6946
batch size: (906, 906)
Epoch 52, accuracy: 0.6953
Epoch 52, Train Loss: 0.0002, Val Loss: 0.6114
batch size: (874, 874)
Epoch 53, accuracy: 0.6956
batch size: (915, 915)
Epoch 54, accuracy: 0.6991
Epoch 54, Train Loss: 0.0001, Val Loss: 0.6153
batch size: (893, 893)
Epoch 55, accuracy: 0.6968
batch size: (877, 877)
Epoch 56, accuracy: 0.6987
Epoch 56, Train Loss: 0.0001, Val Loss: 0.6276
batch size: (892, 892)
Epoch 57, accuracy: 0.6969
batch size: (897, 897)
Epoch 58, accuracy: 0.6972
Epoch 58, Train Loss: 0.0002, Val Loss: 0.6413
batch size: (914, 914)
Epoch 59, accuracy: 0.6987
batch size: (893, 893)
Epoch 60, accuracy: 0.7014
Epoch 60, Train Loss: 0.0001, Val Loss: 0.6634
batch size: (909, 909)
Epoch 61, accuracy: 0.6995
batch size: (880, 880)
Epoch 62, accuracy: 0.7010
Epoch 62, Train Loss: 0.0004, Val Loss: 0.6851
batch size: (903, 903)
Epoch 63, accuracy: 0.6998
batch size: (908, 908)
Epoch 64, accuracy: 0.7006
Epoch 64, Train Loss: 0.0001, Val Loss: 0.7129
batch size: (897, 897)
Epoch 65, accuracy: 0.7014
batch size: (877, 877)
Epoch 66, accuracy: 0.7003
Epoch 66, Train Loss: 0.0002, Val Loss: 0.7214
batch size: (898, 898)
Epoch 67, accuracy: 0.7009
batch size: (886, 886)
Epoch 68, accuracy: 0.7021
Epoch 68, Train Loss: 0.0001, Val Loss: 0.7395
batch size: (918, 918)
Epoch 69, accuracy: 0.7013
batch size: (913, 913)
Epoch 70, accuracy: 0.7021
Epoch 70, Train Loss: 0.0002, Val Loss: 0.7547
batch size: (893, 893)
Epoch 71, accuracy: 0.7019
batch size: (895, 895)
Epoch 72, accuracy: 0.7015
Epoch 72, Train Loss: 0.0001, Val Loss: 0.7889
batch size: (912, 912)
Epoch 73, accuracy: 0.7023
batch size: (881, 881)
Epoch 74, accuracy: 0.7036
Epoch 74, Train Loss: 0.0002, Val Loss: 0.7980
batch size: (909, 909)
Epoch 75, accuracy: 0.7041
batch size: (886, 886)
Epoch 76, accuracy: 0.7039
Epoch 76, Train Loss: 0.0001, Val Loss: 0.8276
batch size: (906, 906)
Epoch 77, accuracy: 0.7036
batch size: (906, 906)
Epoch 78, accuracy: 0.7026
Epoch 78, Train Loss: 0.0001, Val Loss: 0.8508
batch size: (904, 904)
Epoch 79, accuracy: 0.7061
batch size: (900, 900)
Epoch 80, accuracy: 0.7030
Epoch 80, Train Loss: 0.0001, Val Loss: 0.8512
batch size: (912, 912)
Epoch 81, accuracy: 0.7075
batch size: (885, 885)
Epoch 82, accuracy: 0.7037
Epoch 82, Train Loss: 0.0001, Val Loss: 0.8642
batch size: (910, 910)
Epoch 83, accuracy: 0.7060
batch size: (899, 899)
Epoch 84, accuracy: 0.7048
Epoch 84, Train Loss: 0.0001, Val Loss: 0.8898
batch size: (887, 887)
Epoch 85, accuracy: 0.7053
batch size: (888, 888)
Epoch 86, accuracy: 0.7052
Epoch 86, Train Loss: 0.0001, Val Loss: 0.8840
batch size: (907, 907)
Epoch 87, accuracy: 0.7033
batch size: (907, 907)
Epoch 88, accuracy: 0.7038
Epoch 88, Train Loss: 0.0001, Val Loss: 0.9008
batch size: (876, 876)
Epoch 89, accuracy: 0.7053
batch size: (881, 881)
Epoch 90, accuracy: 0.7051
Epoch 90, Train Loss: 0.0001, Val Loss: 0.9095
batch size: (911, 911)
Epoch 91, accuracy: 0.7053
batch size: (904, 904)
Epoch 92, accuracy: 0.7067
Epoch 92, Train Loss: 0.0001, Val Loss: 0.9143
batch size: (907, 907)
Epoch 93, accuracy: 0.7072
batch size: (898, 898)
Epoch 94, accuracy: 0.7055
Epoch 94, Train Loss: 0.0000, Val Loss: 0.9117
batch size: (900, 900)
Epoch 95, accuracy: 0.7045
batch size: /root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
(892, 892)
Epoch 96, accuracy: 0.7077
Epoch 96, Train Loss: 0.0001, Val Loss: 0.9290
batch size: (903, 903)
Epoch 97, accuracy: 0.7078
batch size: (901, 901)
Epoch 98, accuracy: 0.7063
Epoch 98, Train Loss: 0.0002, Val Loss: 0.9170
batch size: (914, 914)
Epoch 99, accuracy: 0.7067
batch size: (900, 900)
Epoch 100, accuracy: 0.7045
Epoch 100, Train Loss: 0.0003, Val Loss: 0.9255
batch size: (904, 904)
Epoch 101, accuracy: 0.7044
batch size: (903, 903)
Epoch 102, accuracy: 0.7052
Epoch 102, Train Loss: 0.0001, Val Loss: 0.9167
batch size: (912, 912)
Epoch 103, accuracy: 0.7050
batch size: (917, 917)
Epoch 104, accuracy: 0.7032
Epoch 104, Train Loss: 0.0001, Val Loss: 0.9246
batch size: (910, 910)
Epoch 105, accuracy: 0.7060
batch size: (905, 905)
Epoch 106, accuracy: 0.7056
Epoch 106, Train Loss: 0.0002, Val Loss: 0.9435
batch size: (916, 916)
Epoch 107, accuracy: 0.7041
batch size: (895, 895)
Epoch 108, accuracy: 0.7042
Epoch 108, Train Loss: 0.0001, Val Loss: 0.9325
batch size: (922, 922)
Epoch 109, accuracy: 0.7030
batch size: (915, 915)
Epoch 110, accuracy: 0.7078
Epoch 110, Train Loss: 0.0001, Val Loss: 0.9722
batch size: (905, 905)
Epoch 111, accuracy: 0.7044
batch size: (903, 903)
Epoch 112, accuracy: 0.7041
Epoch 112, Train Loss: 0.0001, Val Loss: 0.9368
batch size: (891, 891)
Epoch 113, accuracy: 0.7057
batch size: (890, 890)
Epoch 114, accuracy: 0.7037
Epoch 114, Train Loss: 0.0001, Val Loss: 0.9506
batch size: (888, 888)
Epoch 115, accuracy: 0.7038
batch size: (901, 901)
Epoch 116, accuracy: 0.7063
Epoch 116, Train Loss: 0.0001, Val Loss: 0.9618
batch size: (900, 900)
Epoch 117, accuracy: 0.7050
batch size: (891, 891)
Epoch 118, accuracy: 0.7057
Epoch 118, Train Loss: 0.0001, Val Loss: 0.9405
batch size: (918, 918)
Epoch 119, accuracy: 0.7048
batch size: (893, 893)
Epoch 120, accuracy: 0.7050
Epoch 120, Train Loss: 0.0001, Val Loss: 0.9294
batch size: (904, 904)
Epoch 121, accuracy: 0.7041
batch size: (922, 922)
Epoch 122, accuracy: 0.7042
Epoch 122, Train Loss: 0.0002, Val Loss: 0.9484
batch size: (909, 909)
Epoch 123, accuracy: 0.7054
batch size: (903, 903)
Epoch 124, accuracy: 0.7035
Epoch 124, Train Loss: 0.0001, Val Loss: 0.9394
batch size: (907, 907)
Epoch 125, accuracy: 0.7066
batch size: (877, 877)
Epoch 126, accuracy: 0.7073
Epoch 126, Train Loss: 0.0002, Val Loss: 0.9438
batch size: (888, 888)
Epoch 127, accuracy: 0.7034
batch size: (903, 903)
Epoch 128, accuracy: 0.7043
Epoch 128, Train Loss: 0.0003, Val Loss: 0.9464
batch size: (893, 893)
Epoch 129, accuracy: 0.7055
batch size: (885, 885)
Epoch 130, accuracy: 0.7043
Epoch 130, Train Loss: 0.0001, Val Loss: 0.9264
batch size: (885, 885)
Epoch 131, accuracy: 0.7022
batch size: (890, 890)
Epoch 132, accuracy: 0.7051
Epoch 132, Train Loss: 0.0001, Val Loss: 0.9380
batch size: (914, 914)
Epoch 133, accuracy: 0.7067
batch size: (899, 899)
Epoch 134, accuracy: 0.7031
Epoch 134, Train Loss: 0.0001, Val Loss: 0.9477
batch size: (902, 902)
Epoch 135, accuracy: 0.7049
batch size: (898, 898)
Epoch 136, accuracy: 0.7044
Epoch 136, Train Loss: 0.0003, Val Loss: 0.9302
batch size: (917, 917)
Epoch 137, accuracy: 0.7045
batch size: (907, 907)
Epoch 138, accuracy: 0.7036
Epoch 138, Train Loss: 0.0001, Val Loss: 0.9239
batch size: (882, 882)
Epoch 139, accuracy: 0.7044
batch size: (901, 901)
Epoch 140, accuracy: 0.7048
Epoch 140, Train Loss: 0.0001, Val Loss: 0.9218
batch size: (893, 893)
Epoch 141, accuracy: 0.7040
batch size: (900, 900)
Epoch 142, accuracy: 0.7038
Epoch 142, Train Loss: 0.0003, Val Loss: 0.9354
batch size: (894, 894)
Epoch 143, accuracy: 0.7055
batch size: (890, 890)
Epoch 144, accuracy: 0.7058
Epoch 144, Train Loss: 0.0001, Val Loss: 0.9428
batch size: (903, 903)
Epoch 145, accuracy: 0.7031
batch size: (910, 910)
Epoch 146, accuracy: 0.7023
Epoch 146, Train Loss: 0.0001, Val Loss: 0.9817
batch size: (899, 899)
Epoch 147, accuracy: 0.7020
batch size: (897, 897)
Epoch 148, accuracy: 0.7035
Epoch 148, Train Loss: 0.0002, Val Loss: 0.9383
batch size: (900, 900)
Epoch 149, accuracy: 0.7017
batch size: (897, 897)
Epoch 150, accuracy: 0.7064
Epoch 150, Train Loss: 0.0002, Val Loss: 0.9353
batch size: (891, 891)
Epoch 151, accuracy: 0.7052
batch size: (894, 894)
Epoch 152, accuracy: 0.7053
Epoch 152, Train Loss: 0.0002, Val Loss: 0.9407
batch size: (891, 891)
Epoch 153, accuracy: 0.7036
batch size: (895, 895)
Epoch 154, accuracy: 0.7051
Epoch 154, Train Loss: 0.0001, Val Loss: 0.9312
batch size: (919, 919)
Epoch 155, accuracy: 0.7040
batch size: (906, 906)
Epoch 156, accuracy: 0.7063
Epoch 156, Train Loss: 0.0001, Val Loss: 0.9579
batch size: (908, 908)
Epoch 157, accuracy: 0.7048
batch size: (897, 897)
Epoch 158, accuracy: 0.7047
Epoch 158, Train Loss: 0.0001, Val Loss: 0.9408
batch size: (907, 907)
Epoch 159, accuracy: 0.7044
batch size: (904, 904)
Epoch 160, accuracy: 0.7045
Epoch 160, Train Loss: 0.0001, Val Loss: 0.9458
batch size: (907, 907)
Epoch 161, accuracy: 0.7035
batch size: (901, 901)
Epoch 162, accuracy: 0.7041
Epoch 162, Train Loss: 0.0002, Val Loss: 0.9389
batch size: (893, 893)
Epoch 163, accuracy: 0.7044
batch size: (904, 904)
Epoch 164, accuracy: 0.7038
Epoch 164, Train Loss: 0.0001, Val Loss: 0.9409
batch size: (903, 903)
Epoch 165, accuracy: 0.7034
batch size: (903, 903)
Epoch 166, accuracy: 0.7028
Epoch 166, Train Loss: 0.0001, Val Loss: 0.9661
batch size: (909, 909)
Epoch 167, accuracy: 0.7052
batch size: (889, 889)
Epoch 168, accuracy: 0.7054
Epoch 168, Train Loss: 0.0002, Val Loss: 0.9358
batch size: (888, 888)
Epoch 169, accuracy: 0.7057
batch size: (909, 909)
Epoch 170, accuracy: 0.7053
Epoch 170, Train Loss: 0.0001, Val Loss: 0.9426
batch size: (899, 899)
Epoch 171, accuracy: 0.7045
batch size: (907, 907)
Epoch 172, accuracy: 0.7055
Epoch 172, Train Loss: 0.0001, Val Loss: 0.9468
batch size: (885, 885)
Epoch 173, accuracy: 0.7054
batch size: (915, 915)
Epoch 174, accuracy: 0.7050
Epoch 174, Train Loss: 0.0001, Val Loss: 0.9466
batch size: (895, 895)
Epoch 175, accuracy: 0.7059
batch size: (890, 890)
Epoch 176, accuracy: 0.7045
Epoch 176, Train Loss: 0.0002, Val Loss: 0.9514
batch size: (888, 888)
Epoch 177, accuracy: 0.7040
batch size: (911, 911)
Epoch 178, accuracy: 0.7053
Epoch 178, Train Loss: 0.0001, Val Loss: 0.9433
batch size: (905, 905)
Epoch 179, accuracy: 0.7014
batch size: (893, 893)
Epoch 180, accuracy: 0.7038
Epoch 180, Train Loss: 0.0016, Val Loss: 0.9401
batch size: (902, 902)
Epoch 181, accuracy: 0.7060
batch size: (895, 895)
Epoch 182, accuracy: 0.7036
Epoch 182, Train Loss: 0.0001, Val Loss: 0.9401
batch size: (905, 905)
Epoch 183, accuracy: 0.7061
batch size: (898, 898)
Epoch 184, accuracy: 0.7049
Epoch 184, Train Loss: 0.0001, Val Loss: 0.9408
batch size: (912, 912)
Epoch 185, accuracy: 0.7057
batch size: (902, 902)
Epoch 186, accuracy: 0.7037
Epoch 186, Train Loss: 0.0001, Val Loss: 0.9593
batch size: (905, 905)
Epoch 187, accuracy: 0.7041
batch size: (881, 881)
Epoch 188, accuracy: 0.7035
Epoch 188, Train Loss: 0.0001, Val Loss: 0.9402
batch size: (888, 888)
Epoch 189, accuracy: 0.7070
batch size: (878, 878)
Epoch 190, accuracy: 0.7048
Epoch 190, Train Loss: 0.0001, Val Loss: 0.9275
batch size: (897, 897)
Epoch 191, accuracy: 0.7038
batch size: (913, 913)
Epoch 192, accuracy: 0.7059
Epoch 192, Train Loss: 0.0002, Val Loss: 0.9507
batch size: (903, 903)
Epoch 193, accuracy: 0.7045
batch size: (901, 901)
Epoch 194, accuracy: 0.7060
Epoch 194, Train Loss: 0.0001, Val Loss: 0.9583
batch size: (896, 896)
Epoch 195, accuracy: 0.7041
batch size: (893, 893)
Epoch 196, accuracy: 0.7079
Epoch 196, Train Loss: 0.0001, Val Loss: 0.9499
batch size: (903, 903)
Epoch 197, accuracy: 0.7047
batch size: (900, 900)
Epoch 198, accuracy: 0.7039
Epoch 198, Train Loss: 0.0001, Val Loss: 0.9251
batch size: (900, 900)
Epoch 199, accuracy: 0.7081
Loaded best model with val_loss = 0.603751003742218
test :accuracy 0.6919, f1_macro: 0.6875, f1_micro: 0.6919, auc: 0.8569
Training GraphSAGE with 8 layers...
可训练参数: 45833_GraphSAGE
不可训练参数: 0
batch size: (898, 898)
✅ Epoch 0: New best model saved with val_loss = 1.0899
Epoch 0, accuracy: 0.4303
Epoch 0, Train Loss: 1.1295, Val Loss: 1.0899
batch size: (918, 918)
Epoch 1, accuracy: 0.4278
batch size: (896, 896)
Epoch 2, accuracy: 0.2267
Epoch 2, Train Loss: 1.1129, Val Loss: 1.0920
batch size: (906, 906)
Epoch 3, accuracy: 0.2237
batch size: (902, 902)
Epoch 4, accuracy: 0.2216
Epoch 4, Train Loss: 1.2899, Val Loss: 1.0950
batch size: (911, 911)
Epoch 5, accuracy: 0.2246
batch size: (870, 870)
Epoch 6, accuracy: 0.2235
Epoch 6, Train Loss: 1.1921, Val Loss: 1.0974
batch size: (918, 918)
Epoch 7, accuracy: 0.2215
batch size: (913, 913)
Epoch 8, accuracy: 0.2226
Epoch 8, Train Loss: 1.1046, Val Loss: 1.0973
batch size: (890, 890)
Epoch 9, accuracy: 0.2278
batch size: (907, 907)
Epoch 10, accuracy: 0.2227
Epoch 10, Train Loss: 1.1759, Val Loss: 1.0981
batch size: (908, 908)
Epoch 11, accuracy: 0.2263
batch size: (888, 888)
Epoch 12, accuracy: 0.2249
Epoch 12, Train Loss: 1.1196, Val Loss: 1.0970
batch size: (894, 894)
Epoch 13, accuracy: 0.2229
batch size: (908, 908)
Epoch 14, accuracy: 0.2237
Epoch 14, Train Loss: 1.1276, Val Loss: 1.0968
batch size: (903, 903)
Epoch 15, accuracy: 0.2219
batch size: (881, 881)
Epoch 16, accuracy: 0.2254
Epoch 16, Train Loss: 1.1356, Val Loss: 1.0973
batch size: (901, 901)
Epoch 17, accuracy: 0.2240
batch size: (904, 904)
Epoch 18, accuracy: 0.2215
Epoch 18, Train Loss: 1.1561, Val Loss: 1.0972
batch size: (906, 906)
Epoch 19, accuracy: 0.2232
batch size: (903, 903)
Epoch 20, accuracy: 0.2247
Epoch 20, Train Loss: 1.1685, Val Loss: 1.0965
batch size: (905, 905)
Epoch 21, accuracy: 0.2237
batch size: (910, 910)
Epoch 22, accuracy: 0.2248
Epoch 22, Train Loss: 1.1145, Val Loss: 1.0954
batch size: (897, 897)
Epoch 23, accuracy: 0.2258
batch size: (909, 909)
Epoch 24, accuracy: 0.2255
Epoch 24, Train Loss: 1.1246, Val Loss: 1.0972
batch size: (900, 900)
Epoch 25, accuracy: 0.2235
batch size: (899, 899)
Epoch 26, accuracy: 0.2259
Epoch 26, Train Loss: 1.1470, Val Loss: 1.0966
batch size: (907, 907)
Epoch 27, accuracy: 0.2254
batch size: (877, 877)
Epoch 28, accuracy: 0.2243
Epoch 28, Train Loss: 1.0566, Val Loss: 1.0965
batch size: (895, 895)
Epoch 29, accuracy: 0.2246
batch size: (904, 904)
Epoch 30, accuracy: 0.2251
Epoch 30, Train Loss: 1.0542, Val Loss: 1.0964
batch size: (891, 891)
Epoch 31, accuracy: 0.2223
batch size: (901, 901)
Epoch 32, accuracy: 0.2202
Epoch 32, Train Loss: 1.1107, Val Loss: 1.0963
batch size: (891, 891)
Epoch 33, accuracy: 0.2226
batch size: (886, 886)
Epoch 34, accuracy: 0.2229
Epoch 34, Train Loss: 1.1058, Val Loss: 1.0969
batch size: (912, 912)
Epoch 35, accuracy: 0.2231
batch size: (898, 898)
Epoch 36, accuracy: 0.2225
Epoch 36, Train Loss: 1.1163, Val Loss: 1.0968
batch size: (906, 906)
Epoch 37, accuracy: 0.2260
batch size: (906, 906)
Epoch 38, accuracy: 0.2264
Epoch 38, Train Loss: 1.1666, Val Loss: 1.0975
batch size: (895, 895)
Epoch 39, accuracy: 0.2231
batch size: (886, 886)
Epoch 40, accuracy: 0.2241
Epoch 40, Train Loss: 1.1071, Val Loss: 1.0971
batch size: (900, 900)
Epoch 41, accuracy: 0.2234
batch size: (919, 919)
Epoch 42, accuracy: 0.2231
Epoch 42, Train Loss: 1.1486, Val Loss: 1.0973
batch size: (901, 901)
Epoch 43, accuracy: 0.2249
batch size: (894, 894)
Epoch 44, accuracy: 0.2252
Epoch 44, Train Loss: 1.1384, Val Loss: 1.0979
batch size: (912, 912)
Epoch 45, accuracy: 0.2278
batch size: (903, 903)
Epoch 46, accuracy: 0.2228
Epoch 46, Train Loss: 1.0622, Val Loss: 1.0981
batch size: (891, 891)
Epoch 47, accuracy: 0.2238
batch size: (907, 907)
Epoch 48, accuracy: 0.2148
Epoch 48, Train Loss: 1.1430, Val Loss: 1.0986
batch size: (893, 893)
Epoch 49, accuracy: 0.2254
batch size: (899, 899)
Epoch 50, accuracy: 0.2239
Epoch 50, Train Loss: 1.1149, Val Loss: 1.0973
batch size: (906, 906)
Epoch 51, accuracy: 0.2214
batch size: (893, 893)
Epoch 52, accuracy: 0.2265
Epoch 52, Train Loss: 1.1004, Val Loss: 1.0979
batch size: (921, 921)
Epoch 53, accuracy: 0.2235
batch size: (892, 892)
Epoch 54, accuracy: 0.2248
Epoch 54, Train Loss: 1.1456, Val Loss: 1.0974
batch size: (887, 887)
Epoch 55, accuracy: 0.2240
batch size: (914, 914)
Epoch 56, accuracy: 0.2276
Epoch 56, Train Loss: 1.1078, Val Loss: 1.0978
batch size: (891, 891)
Epoch 57, accuracy: 0.2242
batch size: (902, 902)
Epoch 58, accuracy: 0.2260
Epoch 58, Train Loss: 1.0935, Val Loss: 1.0976
batch size: (913, 913)
Epoch 59, accuracy: 0.2257
batch size: (910, 910)
Epoch 60, accuracy: 0.2269
Epoch 60, Train Loss: 1.2082, Val Loss: 1.0974
batch size: (887, 887)
Epoch 61, accuracy: 0.2235
batch size: (887, 887)
Epoch 62, accuracy: 0.2251
Epoch 62, Train Loss: 1.0974, Val Loss: 1.0977
batch size: (890, 890)
Epoch 63, accuracy: 0.2272
batch size: (891, 891)
Epoch 64, accuracy: 0.2258
Epoch 64, Train Loss: 1.0872, Val Loss: 1.0975
batch size: (901, 901)
Epoch 65, accuracy: 0.2240
batch size: (893, 893)
Epoch 66, accuracy: 0.2209
Epoch 66, Train Loss: 1.1977, Val Loss: 1.0977
batch size: (906, 906)
Epoch 67, accuracy: 0.2247
batch size: (921, 921)
Epoch 68, accuracy: 0.2258
Epoch 68, Train Loss: 1.0966, Val Loss: 1.0964
batch size: (889, 889)
Epoch 69, accuracy: 0.2200
batch size: (886, 886)
Epoch 70, accuracy: 0.2233
Epoch 70, Train Loss: 1.1907, Val Loss: 1.0973
batch size: (901, 901)
Epoch 71, accuracy: 0.2234
batch size: (895, 895)
Epoch 72, accuracy: 0.2223
Epoch 72, Train Loss: 1.1641, Val Loss: 1.0973
batch size: (920, 920)
Epoch 73, accuracy: 0.2227
batch size: (912, 912)
Epoch 74, accuracy: 0.2259
Epoch 74, Train Loss: 1.1897, Val Loss: 1.0969
batch size: (893, 893)
Epoch 75, accuracy: 0.2238
batch size: (901, 901)
Epoch 76, accuracy: 0.2237
Epoch 76, Train Loss: 1.1215, Val Loss: 1.0966
batch size: (893, 893)
Epoch 77, accuracy: 0.2266
batch size: (912, 912)
Epoch 78, accuracy: 0.2247
Epoch 78, Train Loss: 1.1591, Val Loss: 1.0966
batch size: (903, 903)
Epoch 79, accuracy: 0.2226
batch size: (887, 887)
Epoch 80, accuracy: 0.2256
Epoch 80, Train Loss: 1.1779, Val Loss: 1.0972
batch size: (889, 889)
Epoch 81, accuracy: 0.2248
batch size: (884, 884)
Epoch 82, accuracy: 0.2260
Epoch 82, Train Loss: 1.0623, Val Loss: 1.0976
batch size: (887, 887)
Epoch 83, accuracy: 0.2244
batch size: (909, 909)
Epoch 84, accuracy: 0.2251
Epoch 84, Train Loss: 1.1539, Val Loss: 1.0959
batch size: (897, 897)
Epoch 85, accuracy: 0.2268
batch size: (902, 902)
Epoch 86, accuracy: 0.2225
Epoch 86, Train Loss: 1.0947, Val Loss: 1.0966
batch size: (885, 885)
Epoch 87, accuracy: 0.2233
batch size: (896, 896)
Epoch 88, accuracy: 0.2244
Epoch 88, Train Loss: 1.1056, Val Loss: 1.0976
batch size: (896, 896)
Epoch 89, accuracy: 0.2245
batch size: (877, 877)
Epoch 90, accuracy: 0.2241
Epoch 90, Train Loss: 1.0906, Val Loss: 1.0978
batch size: (901, 901)
Epoch 91, accuracy: 0.2237
batch size: (914, 914)
Epoch 92, accuracy: 0.2216
Epoch 92, Train Loss: 1.0614, Val Loss: 1.0977
batch size: (891, 891)
Epoch 93, accuracy: 0.2233
batch size: (911, 911)
Epoch 94, accuracy: 0.2245
Epoch 94, Train Loss: 1.1093, Val Loss: 1.0980
batch size: (895, 895)
Epoch 95, accuracy: 0.2228
batch size: (905, 905)
Epoch 96, accuracy: 0.2249
Epoch 96, Train Loss: 1.2821, Val Loss: 1.0961
batch size: (892, 892)
Epoch 97, accuracy: 0.2213
batch size: (892, 892)
Epoch 98, accuracy: 0.2236
Epoch 98, Train Loss: 1.1044, Val Loss: 1.0962
batch size: (901, 901)
Epoch 99, accuracy: 0.2231
batch size: (898, 898)
Epoch 100, accuracy: 0.2236
Epoch 100, Train Loss: 1.1337, Val Loss: 1.0964
batch size: (891, 891)
Epoch 101, accuracy: 0.2254
batch size: (898, 898)
Epoch 102, accuracy: 0.2237
Epoch 102, Train Loss: 1.2195, Val Loss: 1.0971
batch size: (916, 916)
Epoch 103, accuracy: 0.2254
batch size: (906, 906)
Epoch 104, accuracy: 0.2227
Epoch 104, Train Loss: 1.1659, Val Loss: 1.0972
batch size: (882, 882)
Epoch 105, accuracy: 0.2265
batch size: (886, 886)
Epoch 106, accuracy: 0.2252
Epoch 106, Train Loss: 1.2155, Val Loss: 1.0969
batch size: (903, 903)
Epoch 107, accuracy: 0.2234
batch size: (893, 893)
Epoch 108, accuracy: 0.2260
Epoch 108, Train Loss: 1.1435, Val Loss: 1.0974
batch size: (908, 908)
Epoch 109, accuracy: 0.2259
batch size: (905, 905)
Epoch 110, accuracy: 0.2229
Epoch 110, Train Loss: 1.1141, Val Loss: 1.0967
batch size: (895, 895)
Epoch 111, accuracy: 0.2260
batch size: (901, 901)
Epoch 112, accuracy: 0.2243
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 112, Train Loss: 1.1366, Val Loss: 1.0974
batch size: (911, 911)
Epoch 113, accuracy: 0.2211
batch size: (908, 908)
Epoch 114, accuracy: 0.2282
Epoch 114, Train Loss: 1.1101, Val Loss: 1.0976
batch size: (895, 895)
Epoch 115, accuracy: 0.2263
batch size: (898, 898)
Epoch 116, accuracy: 0.2281
Epoch 116, Train Loss: 1.1689, Val Loss: 1.0971
batch size: (886, 886)
Epoch 117, accuracy: 0.2248
batch size: (902, 902)
Epoch 118, accuracy: 0.2229
Epoch 118, Train Loss: 1.1895, Val Loss: 1.0977
batch size: (897, 897)
Epoch 119, accuracy: 0.2239
batch size: (904, 904)
Epoch 120, accuracy: 0.2258
Epoch 120, Train Loss: 1.0608, Val Loss: 1.0982
batch size: (901, 901)
Epoch 121, accuracy: 0.2170
batch size: (901, 901)
Epoch 122, accuracy: 0.2223
Epoch 122, Train Loss: 1.1687, Val Loss: 1.0985
batch size: (895, 895)
Epoch 123, accuracy: 0.2238
batch size: (918, 918)
Epoch 124, accuracy: 0.2238
Epoch 124, Train Loss: 1.1121, Val Loss: 1.0977
batch size: (909, 909)
Epoch 125, accuracy: 0.2249
batch size: (899, 899)
Epoch 126, accuracy: 0.2254
Epoch 126, Train Loss: 1.0400, Val Loss: 1.0963
batch size: (886, 886)
Epoch 127, accuracy: 0.2238
batch size: (887, 887)
Epoch 128, accuracy: 0.2222
Epoch 128, Train Loss: 1.0836, Val Loss: 1.0969
batch size: (899, 899)
Epoch 129, accuracy: 0.2251
batch size: (907, 907)
Epoch 130, accuracy: 0.2244
Epoch 130, Train Loss: 1.1382, Val Loss: 1.0967
batch size: (894, 894)
Epoch 131, accuracy: 0.2230
batch size: (899, 899)
Epoch 132, accuracy: 0.2244
Epoch 132, Train Loss: 1.2004, Val Loss: 1.0964
batch size: (925, 925)
Epoch 133, accuracy: 0.2236
batch size: (915, 915)
Epoch 134, accuracy: 0.2229
Epoch 134, Train Loss: 1.1377, Val Loss: 1.0974
batch size: (904, 904)
Epoch 135, accuracy: 0.2246
batch size: (907, 907)
Epoch 136, accuracy: 0.2200
Epoch 136, Train Loss: 1.1380, Val Loss: 1.0977
batch size: (894, 894)
Epoch 137, accuracy: 0.2225
batch size: (902, 902)
Epoch 138, accuracy: 0.2250
Epoch 138, Train Loss: 1.1181, Val Loss: 1.0980
batch size: (903, 903)
Epoch 139, accuracy: 0.2231
batch size: (875, 875)
Epoch 140, accuracy: 0.2245
Epoch 140, Train Loss: 1.0775, Val Loss: 1.0971
batch size: (905, 905)
Epoch 141, accuracy: 0.2225
batch size: (915, 915)
Epoch 142, accuracy: 0.2223
Epoch 142, Train Loss: 1.0647, Val Loss: 1.0969
batch size: (898, 898)
Epoch 143, accuracy: 0.2254
batch size: (903, 903)
Epoch 144, accuracy: 0.2240
Epoch 144, Train Loss: 1.1252, Val Loss: 1.0964
batch size: (893, 893)
Epoch 145, accuracy: 0.2235
batch size: (883, 883)
Epoch 146, accuracy: 0.2262
Epoch 146, Train Loss: 1.1128, Val Loss: 1.0969
batch size: (905, 905)
Epoch 147, accuracy: 0.2229
batch size: (901, 901)
Epoch 148, accuracy: 0.2238
Epoch 148, Train Loss: 1.1281, Val Loss: 1.0972
batch size: (918, 918)
Epoch 149, accuracy: 0.2225
batch size: (896, 896)
Epoch 150, accuracy: 0.2239
Epoch 150, Train Loss: 1.1522, Val Loss: 1.0967
batch size: (904, 904)
Epoch 151, accuracy: 0.2248
batch size: (905, 905)
Epoch 152, accuracy: 0.2246
Epoch 152, Train Loss: 1.0819, Val Loss: 1.0960
batch size: (901, 901)
Epoch 153, accuracy: 0.2220
batch size: (911, 911)
Epoch 154, accuracy: 0.2246
Epoch 154, Train Loss: 1.1214, Val Loss: 1.0956
batch size: (899, 899)
Epoch 155, accuracy: 0.2237
batch size: (899, 899)
Epoch 156, accuracy: 0.2235
Epoch 156, Train Loss: 1.1515, Val Loss: 1.0967
batch size: (892, 892)
Epoch 157, accuracy: 0.2224
batch size: (891, 891)
Epoch 158, accuracy: 0.2247
Epoch 158, Train Loss: 1.1610, Val Loss: 1.0970
batch size: (899, 899)
Epoch 159, accuracy: 0.2228
batch size: (896, 896)
Epoch 160, accuracy: 0.2224
Epoch 160, Train Loss: 1.2099, Val Loss: 1.0961
batch size: (907, 907)
Epoch 161, accuracy: 0.2232
batch size: (921, 921)
Epoch 162, accuracy: 0.2248
Epoch 162, Train Loss: 1.1230, Val Loss: 1.0974
batch size: (906, 906)
Epoch 163, accuracy: 0.2276
batch size: (896, 896)
Epoch 164, accuracy: 0.2242
Epoch 164, Train Loss: 1.1323, Val Loss: 1.0980
batch size: (914, 914)
Epoch 165, accuracy: 0.2230
batch size: (878, 878)
Epoch 166, accuracy: 0.2263
Epoch 166, Train Loss: 1.0736, Val Loss: 1.0973
batch size: (892, 892)
Epoch 167, accuracy: 0.2258
batch size: (912, 912)
Epoch 168, accuracy: 0.2242
Epoch 168, Train Loss: 1.1017, Val Loss: 1.0981
batch size: (902, 902)
Epoch 169, accuracy: 0.2267
batch size: (897, 897)
Epoch 170, accuracy: 0.2261
Epoch 170, Train Loss: 1.0603, Val Loss: 1.0972
batch size: (900, 900)
Epoch 171, accuracy: 0.2232
batch size: (924, 924)
Epoch 172, accuracy: 0.2220
Epoch 172, Train Loss: 1.1320, Val Loss: 1.0967
batch size: (911, 911)
Epoch 173, accuracy: 0.2257
batch size: (887, 887)
Epoch 174, accuracy: 0.2244
Epoch 174, Train Loss: 1.1178, Val Loss: 1.0971
batch size: (914, 914)
Epoch 175, accuracy: 0.2277
batch size: (901, 901)
Epoch 176, accuracy: 0.2252
Epoch 176, Train Loss: 1.0953, Val Loss: 1.0978
batch size: (905, 905)
Epoch 177, accuracy: 0.2251
batch size: (904, 904)
Epoch 178, accuracy: 0.2231
Epoch 178, Train Loss: 1.1370, Val Loss: 1.0968
batch size: (897, 897)
Epoch 179, accuracy: 0.2238
batch size: (904, 904)
Epoch 180, accuracy: 0.2219
Epoch 180, Train Loss: 1.1436, Val Loss: 1.0960
batch size: (910, 910)
Epoch 181, accuracy: 0.2245
batch size: (894, 894)
Epoch 182, accuracy: 0.2245
Epoch 182, Train Loss: 1.2826, Val Loss: 1.0969
batch size: (918, 918)
Epoch 183, accuracy: 0.2219
batch size: (896, 896)
Epoch 184, accuracy: 0.2238
Epoch 184, Train Loss: 1.0940, Val Loss: 1.0967
batch size: (901, 901)
Epoch 185, accuracy: 0.2231
batch size: (903, 903)
Epoch 186, accuracy: 0.2269
Epoch 186, Train Loss: 1.1176, Val Loss: 1.0972
batch size: (911, 911)
Epoch 187, accuracy: 0.2214
batch size: (896, 896)
Epoch 188, accuracy: 0.2240
Epoch 188, Train Loss: 1.0635, Val Loss: 1.0974
batch size: (923, 923)
Epoch 189, accuracy: 0.2243
batch size: (898, 898)
Epoch 190, accuracy: 0.2261
Epoch 190, Train Loss: 1.1905, Val Loss: 1.0978
batch size: (902, 902)
Epoch 191, accuracy: 0.2256
batch size: (905, 905)
Epoch 192, accuracy: 0.2273
Epoch 192, Train Loss: 1.1431, Val Loss: 1.0977
batch size: (899, 899)
Epoch 193, accuracy: 0.2228
batch size: (890, 890)
Epoch 194, accuracy: 0.2215
Epoch 194, Train Loss: 1.1053, Val Loss: 1.0979
batch size: (895, 895)
Epoch 195, accuracy: 0.2243
batch size: (888, 888)
Epoch 196, accuracy: 0.2280
Epoch 196, Train Loss: 1.1826, Val Loss: 1.0982
batch size: (918, 918)
Epoch 197, accuracy: 0.2205
batch size: (920, 920)
Epoch 198, accuracy: 0.2232
Epoch 198, Train Loss: 1.2156, Val Loss: 1.0983
batch size: (898, 898)
Epoch 199, accuracy: 0.2220
Loaded best model with val_loss = 1.0898686647415161
test :accuracy 0.4280, f1_macro: 0.1998, f1_micro: 0.4280, auc: 0.5069
Training GraphSAGE with 32 layers...
可训练参数: 98057_GraphSAGE
不可训练参数: 0
batch size: (906, 906)
✅ Epoch 0: New best model saved with val_loss = 1.0986
Epoch 0, accuracy: 0.1677
Epoch 0, Train Loss: 1.1387, Val Loss: 1.0986
batch size: (902, 902)
Epoch 1, accuracy: 0.3538
batch size: (902, 902)
Epoch 2, accuracy: 0.3577
Epoch 2, Train Loss: 1.1142, Val Loss: 1.0986
batch size: (908, 908)
Epoch 3, accuracy: 0.3558
batch size: (903, 903)
Epoch 4, accuracy: 0.3527
Epoch 4, Train Loss: 1.1369, Val Loss: 1.0986
batch size: (884, 884)
Epoch 5, accuracy: 0.3545
batch size: (910, 910)
Epoch 6, accuracy: 0.3544
Epoch 6, Train Loss: 1.1291, Val Loss: 1.0986
batch size: (911, 911)
Epoch 7, accuracy: 0.3564
batch size: (891, 891)
Epoch 8, accuracy: 0.3565
Epoch 8, Train Loss: 1.1210, Val Loss: 1.0986
batch size: (911, 911)
Epoch 9, accuracy: 0.3581
batch size: (904, 904)
Epoch 10, accuracy: 0.3557
Epoch 10, Train Loss: 1.0901, Val Loss: 1.0986
batch size: (889, 889)
Epoch 11, accuracy: 0.3529
batch size: (903, 903)
Epoch 12, accuracy: 0.3577
Epoch 12, Train Loss: 1.1873, Val Loss: 1.0986
batch size: (901, 901)
Epoch 13, accuracy: 0.3536
batch size: (902, 902)
Epoch 14, accuracy: 0.3563
Epoch 14, Train Loss: 1.1715, Val Loss: 1.0986
batch size: (913, 913)
Epoch 15, accuracy: 0.3567
batch size: (898, 898)
Epoch 16, accuracy: 0.3528
Epoch 16, Train Loss: 1.1054, Val Loss: 1.0986
batch size: (893, 893)
Epoch 17, accuracy: 0.3566
batch size: (904, 904)
Epoch 18, accuracy: 0.3539
Epoch 18, Train Loss: 1.1941, Val Loss: 1.0986
batch size: (914, 914)
Epoch 19, accuracy: 0.3569
batch size: (918, 918)
Epoch 20, accuracy: 0.3576
Epoch 20, Train Loss: 1.1341, Val Loss: 1.0986
batch size: (890, 890)
Epoch 21, accuracy: 0.3536
batch size: (898, 898)
Epoch 22, accuracy: 0.3576
Epoch 22, Train Loss: 1.0892, Val Loss: 1.0986
batch size: (903, 903)
Epoch 23, accuracy: 0.3552
batch size: (898, 898)
Epoch 24, accuracy: 0.3554
Epoch 24, Train Loss: 1.1250, Val Loss: 1.0986
batch size: (906, 906)
Epoch 25, accuracy: 0.3527
batch size: (903, 903)
Epoch 26, accuracy: 0.3557
Epoch 26, Train Loss: 1.1179, Val Loss: 1.0986
batch size: (909, 909)
Epoch 27, accuracy: 0.3532
batch size: (919, 919)
Epoch 28, accuracy: 0.3567
Epoch 28, Train Loss: 1.1102, Val Loss: 1.0986
batch size: (877, 877)
Epoch 29, accuracy: 0.3540
batch size: (895, 895)
Epoch 30, accuracy: 0.3551
Epoch 30, Train Loss: 1.1107, Val Loss: 1.0986
batch size: (899, 899)
Epoch 31, accuracy: 0.3566
batch size: (889, 889)
Epoch 32, accuracy: 0.3553
Epoch 32, Train Loss: 1.1520, Val Loss: 1.0986
batch size: (903, 903)
Epoch 33, accuracy: 0.3552
batch size: (890, 890)
Epoch 34, accuracy: 0.3585
Epoch 34, Train Loss: 1.1074, Val Loss: 1.0986
batch size: (896, 896)
Epoch 35, accuracy: 0.3560
batch size: (903, 903)
Epoch 36, accuracy: 0.3530
Epoch 36, Train Loss: 1.1344, Val Loss: 1.0986
batch size: (902, 902)
Epoch 37, accuracy: 0.3540
batch size: (905, 905)
Epoch 38, accuracy: 0.3557
Epoch 38, Train Loss: 1.1606, Val Loss: 1.0986
batch size: (877, 877)
Epoch 39, accuracy: 0.3544
batch size: (906, 906)
Epoch 40, accuracy: 0.3538
Epoch 40, Train Loss: 1.1255, Val Loss: 1.0986
batch size: (913, 913)
Epoch 41, accuracy: 0.3496
batch size: (918, 918)
Epoch 42, accuracy: 0.3526
Epoch 42, Train Loss: 1.1693, Val Loss: 1.0986
batch size: (906, 906)
Epoch 43, accuracy: 0.3554
batch size: (899, 899)
Epoch 44, accuracy: 0.3554
Epoch 44, Train Loss: 1.1166, Val Loss: 1.0986
batch size: (913, 913)
Epoch 45, accuracy: 0.3546
batch size: (897, 897)
Epoch 46, accuracy: 0.3562
Epoch 46, Train Loss: 1.0963, Val Loss: 1.0986
batch size: (891, 891)
Epoch 47, accuracy: 0.3554
batch size: (900, 900)
Epoch 48, accuracy: 0.3559
Epoch 48, Train Loss: 1.1095, Val Loss: 1.0986
batch size: (890, 890)
Epoch 49, accuracy: 0.3516
batch size: (897, 897)
Epoch 50, accuracy: 0.3534
Epoch 50, Train Loss: 1.2093, Val Loss: 1.0986
batch size: (893, 893)
Epoch 51, accuracy: 0.3539
batch size: (912, 912)
Epoch 52, accuracy: 0.3593
Epoch 52, Train Loss: 1.1232, Val Loss: 1.0986
batch size: (880, 880)
Epoch 53, accuracy: 0.3526
batch size: (915, 915)
Epoch 54, accuracy: 0.3558
Epoch 54, Train Loss: 1.1012, Val Loss: 1.0986
batch size: (901, 901)
Epoch 55, accuracy: 0.3549
batch size: (892, 892)
Epoch 56, accuracy: 0.3551
Epoch 56, Train Loss: 1.1312, Val Loss: 1.0986
batch size: (906, 906)
Epoch 57, accuracy: 0.3532
batch size: (911, 911)
Epoch 58, accuracy: 0.3539
Epoch 58, Train Loss: 1.1075, Val Loss: 1.0986
batch size: (879, 879)
Epoch 59, accuracy: 0.3569
batch size: (899, 899)
Epoch 60, accuracy: 0.3555
Epoch 60, Train Loss: 1.1345, Val Loss: 1.0986
batch size: (913, 913)
Epoch 61, accuracy: 0.3552
batch size: (890, 890)
Epoch 62, accuracy: 0.3528
Epoch 62, Train Loss: 1.0936, Val Loss: 1.0986
batch size: (904, 904)
Epoch 63, accuracy: 0.3575
batch size: (876, 876)
Epoch 64, accuracy: 0.3561
Epoch 64, Train Loss: 1.0777, Val Loss: 1.0986
batch size: (903, 903)
Epoch 65, accuracy: 0.3541
batch size: (909, 909)
Epoch 66, accuracy: 0.3561
Epoch 66, Train Loss: 1.1504, Val Loss: 1.0986
batch size: (927, 927)
Epoch 67, accuracy: 0.3568
batch size: (881, 881)
Epoch 68, accuracy: 0.3530
Epoch 68, Train Loss: 1.1061, Val Loss: 1.0986
batch size: (907, 907)
Epoch 69, accuracy: 0.3574
batch size: (889, 889)
Epoch 70, accuracy: 0.3591
Epoch 70, Train Loss: 1.1282, Val Loss: 1.0986
batch size: (909, 909)
Epoch 71, accuracy: 0.3588
batch size: (898, 898)
Epoch 72, accuracy: 0.3568
Epoch 72, Train Loss: 1.1125, Val Loss: 1.0986
batch size: (892, 892)
Epoch 73, accuracy: 0.3517
batch size: (911, 911)
Epoch 74, accuracy: 0.3592
Epoch 74, Train Loss: 1.1246, Val Loss: 1.0986
batch size: (891, 891)
Epoch 75, accuracy: 0.3574
batch size: (915, 915)
Epoch 76, accuracy: 0.3550
Epoch 76, Train Loss: 1.1019, Val Loss: 1.0986
batch size: (906, 906)
Epoch 77, accuracy: 0.3555
batch size: (898, 898)
Epoch 78, accuracy: 0.3537
Epoch 78, Train Loss: 1.1041, Val Loss: 1.0986
batch size: (909, 909)
Epoch 79, accuracy: 0.3564
batch size: (876, 876)
Epoch 80, accuracy: 0.3554
Epoch 80, Train Loss: 1.1572, Val Loss: 1.0986
batch size: (898, 898)
Epoch 81, accuracy: 0.3545
batch size: (900, 900)
Epoch 82, accuracy: 0.3513
Epoch 82, Train Loss: 1.0898, Val Loss: 1.0986
batch size: (899, 899)
Epoch 83, accuracy: 0.3523
batch size: (897, 897)
Epoch 84, accuracy: 0.3512
Epoch 84, Train Loss: 1.1448, Val Loss: 1.0986
batch size: (899, 899)
Epoch 85, accuracy: 0.3550
batch size: (882, 882)
Epoch 86, accuracy: 0.3555
Epoch 86, Train Loss: 1.1231, Val Loss: 1.0986
batch size: (906, 906)
Epoch 87, accuracy: 0.3592
batch size: (908, 908)
Epoch 88, accuracy: 0.3584
Epoch 88, Train Loss: 1.1288, Val Loss: 1.0986
batch size: (915, 915)
Epoch 89, accuracy: 0.3558
batch size: (894, 894)
Epoch 90, accuracy: 0.3580
Epoch 90, Train Loss: 1.1564, Val Loss: 1.0986
batch size: (898, 898)
Epoch 91, accuracy: 0.3519
batch size: (873, 873)
Epoch 92, accuracy: 0.3550
Epoch 92, Train Loss: 1.1068, Val Loss: 1.0986
batch size: (893, 893)
Epoch 93, accuracy: 0.3571
batch size: (901, 901)
Epoch 94, accuracy: 0.3554
Epoch 94, Train Loss: 1.1095, Val Loss: 1.0986
batch size: (896, 896)
Epoch 95, accuracy: 0.3563
batch size: (898, 898)
Epoch 96, accuracy: 0.3538
Epoch 96, Train Loss: 1.1194, Val Loss: 1.0986
batch size: (901, 901)
Epoch 97, accuracy: 0.3560
batch size: (895, 895)
Epoch 98, accuracy: 0.3543
Epoch 98, Train Loss: 1.0819, Val Loss: 1.0986
batch size: (912, 912)
Epoch 99, accuracy: 0.3545
batch size: (897, 897)
Epoch 100, accuracy: 0.3579
Epoch 100, Train Loss: 1.1197, Val Loss: 1.0986
batch size: (904, 904)
Epoch 101, accuracy: 0.3535
batch size: (884, 884)
Epoch 102, accuracy: 0.3569
Epoch 102, Train Loss: 1.0809, Val Loss: 1.0986
batch size: (910, 910)
Epoch 103, accuracy: 0.3567
batch size: (886, 886)
Epoch 104, accuracy: 0.3539
Epoch 104, Train Loss: 1.0924, Val Loss: 1.0986
batch size: (888, 888)
Epoch 105, accuracy: 0.3554
batch size: (888, 888)
Epoch 106, accuracy: 0.3564
Epoch 106, Train Loss: 1.1476, Val Loss: 1.0986
batch size: (907, 907)
Epoch 107, accuracy: 0.3532
batch size: (901, 901)
Epoch 108, accuracy: 0.3570
Epoch 108, Train Loss: 1.1984, Val Loss: 1.0986
batch size: (905, 905)
Epoch 109, accuracy: 0.3544
batch size: (891, 891)
Epoch 110, accuracy: 0.3541
Epoch 110, Train Loss: 1.1428, Val Loss: 1.0986
batch size: (913, 913)
Epoch 111, accuracy: 0.3577
batch size: (884, 884)
Epoch 112, accuracy: 0.3564
Epoch 112, Train Loss: 1.1494, Val Loss: 1.0986
batch size: (891, 891)
Epoch 113, accuracy: 0.3529
batch size: (901, 901)
Epoch 114, accuracy: 0.3564
Epoch 114, Train Loss: 1.0921, Val Loss: 1.0986
batch size: (910, 910)
Epoch 115, accuracy: 0.3528
batch size: (895, 895)
Epoch 116, accuracy: 0.3546
Epoch 116, Train Loss: 1.1103, Val Loss: 1.0986
batch size: (903, 903)
Epoch 117, accuracy: 0.3518
batch size: (895, 895)
Epoch 118, accuracy: 0.3545
Epoch 118, Train Loss: 1.1799, Val Loss: 1.0986
batch size: (913, 913)
Epoch 119, accuracy: 0.3557
batch size: (892, 892)
Epoch 120, accuracy: 0.3528
Epoch 120, Train Loss: 1.1428, Val Loss: 1.0986
batch size: (892, 892)
Epoch 121, accuracy: 0.3556
batch size: (881, 881)
Epoch 122, accuracy: 0.3539
Epoch 122, Train Loss: 1.1549, Val Loss: 1.0986
batch size: (916, 916)
Epoch 123, accuracy: 0.3574
batch size: (888, 888)
Epoch 124, accuracy: 0.3547
Epoch 124, Train Loss: 1.1208, Val Loss: 1.0986
batch size: (904, 904)
Epoch 125, accuracy: 0.3577
batch size: (901, 901)
Epoch 126, accuracy: 0.3565
Epoch 126, Train Loss: 1.1062, Val Loss: 1.0986
batch size: (890, 890)
Epoch 127, accuracy: 0.3575
batch size: (890, 890)
Epoch 128, accuracy: 0.3565
Epoch 128, Train Loss: 1.0964, Val Loss: 1.0986
batch size: /root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
(905, 905)
Epoch 129, accuracy: 0.3556
batch size: (910, 910)
Epoch 130, accuracy: 0.3569
Epoch 130, Train Loss: 1.2051, Val Loss: 1.0986
batch size: (887, 887)
Epoch 131, accuracy: 0.3568
batch size: (906, 906)
Epoch 132, accuracy: 0.3552
Epoch 132, Train Loss: 1.1057, Val Loss: 1.0986
batch size: (896, 896)
Epoch 133, accuracy: 0.3568
batch size: (888, 888)
Epoch 134, accuracy: 0.3564
Epoch 134, Train Loss: 1.1086, Val Loss: 1.0986
batch size: (887, 887)
Epoch 135, accuracy: 0.3517
batch size: (883, 883)
Epoch 136, accuracy: 0.3555
Epoch 136, Train Loss: 1.1078, Val Loss: 1.0986
batch size: (894, 894)
Epoch 137, accuracy: 0.3554
batch size: (904, 904)
Epoch 138, accuracy: 0.3557
Epoch 138, Train Loss: 1.1327, Val Loss: 1.0986
batch size: (914, 914)
Epoch 139, accuracy: 0.3561
batch size: (875, 875)
Epoch 140, accuracy: 0.3578
Epoch 140, Train Loss: 1.1071, Val Loss: 1.0986
batch size: (907, 907)
Epoch 141, accuracy: 0.3559
batch size: (903, 903)
Epoch 142, accuracy: 0.3562
Epoch 142, Train Loss: 1.1406, Val Loss: 1.0986
batch size: (897, 897)
Epoch 143, accuracy: 0.3552
batch size: (890, 890)
Epoch 144, accuracy: 0.3557
Epoch 144, Train Loss: 1.0583, Val Loss: 1.0986
batch size: (903, 903)
Epoch 145, accuracy: 0.3576
batch size: (895, 895)
Epoch 146, accuracy: 0.3617
Epoch 146, Train Loss: 1.1168, Val Loss: 1.0986
batch size: (909, 909)
Epoch 147, accuracy: 0.3543
batch size: (898, 898)
Epoch 148, accuracy: 0.3519
Epoch 148, Train Loss: 1.1036, Val Loss: 1.0986
batch size: (907, 907)
Epoch 149, accuracy: 0.3535
batch size: (922, 922)
Epoch 150, accuracy: 0.3557
Epoch 150, Train Loss: 1.1321, Val Loss: 1.0986
batch size: (875, 875)
Epoch 151, accuracy: 0.3565
batch size: (913, 913)
Epoch 152, accuracy: 0.3543
Epoch 152, Train Loss: 1.1621, Val Loss: 1.0986
batch size: (907, 907)
Epoch 153, accuracy: 0.3576
batch size: (909, 909)
Epoch 154, accuracy: 0.3538
Epoch 154, Train Loss: 1.1043, Val Loss: 1.0986
batch size: (888, 888)
Epoch 155, accuracy: 0.3551
batch size: (915, 915)
Epoch 156, accuracy: 0.3577
Epoch 156, Train Loss: 1.2246, Val Loss: 1.0986
batch size: (900, 900)
Epoch 157, accuracy: 0.3540
batch size: (893, 893)
Epoch 158, accuracy: 0.3564
Epoch 158, Train Loss: 1.0880, Val Loss: 1.0986
batch size: (909, 909)
Epoch 159, accuracy: 0.3575
batch size: (877, 877)
Epoch 160, accuracy: 0.3555
Epoch 160, Train Loss: 1.1029, Val Loss: 1.0986
batch size: (906, 906)
Epoch 161, accuracy: 0.3528
batch size: (910, 910)
Epoch 162, accuracy: 0.3530
Epoch 162, Train Loss: 1.1006, Val Loss: 1.0986
batch size: (892, 892)
Epoch 163, accuracy: 0.3578
batch size: (897, 897)
Epoch 164, accuracy: 0.3606
Epoch 164, Train Loss: 1.1143, Val Loss: 1.0986
batch size: (909, 909)
Epoch 165, accuracy: 0.3569
batch size: (903, 903)
Epoch 166, accuracy: 0.3555
Epoch 166, Train Loss: 1.1088, Val Loss: 1.0986
batch size: (914, 914)
Epoch 167, accuracy: 0.3584
batch size: (918, 918)
Epoch 168, accuracy: 0.3547
Epoch 168, Train Loss: 1.0904, Val Loss: 1.0986
batch size: (897, 897)
Epoch 169, accuracy: 0.3571
batch size: (899, 899)
Epoch 170, accuracy: 0.3549
Epoch 170, Train Loss: 1.1481, Val Loss: 1.0986
batch size: (895, 895)
Epoch 171, accuracy: 0.3558
batch size: (911, 911)
Epoch 172, accuracy: 0.3570
Epoch 172, Train Loss: 1.1906, Val Loss: 1.0986
batch size: (893, 893)
Epoch 173, accuracy: 0.3585
batch size: (886, 886)
Epoch 174, accuracy: 0.3538
Epoch 174, Train Loss: 1.1455, Val Loss: 1.0986
batch size: (893, 893)
Epoch 175, accuracy: 0.3540
batch size: (914, 914)
Epoch 176, accuracy: 0.3513
Epoch 176, Train Loss: 1.1179, Val Loss: 1.0986
batch size: (892, 892)
Epoch 177, accuracy: 0.3557
batch size: (907, 907)
Epoch 178, accuracy: 0.3566
Epoch 178, Train Loss: 1.1137, Val Loss: 1.0986
batch size: (903, 903)
Epoch 179, accuracy: 0.3548
batch size: (900, 900)
Epoch 180, accuracy: 0.3524
Epoch 180, Train Loss: 1.1073, Val Loss: 1.0986
batch size: (910, 910)
Epoch 181, accuracy: 0.3560
batch size: (887, 887)
Epoch 182, accuracy: 0.3589
Epoch 182, Train Loss: 1.1120, Val Loss: 1.0986
batch size: (908, 908)
Epoch 183, accuracy: 0.3541
batch size: (914, 914)
Epoch 184, accuracy: 0.3553
Epoch 184, Train Loss: 1.1058, Val Loss: 1.0986
batch size: (880, 880)
Epoch 185, accuracy: 0.3543
batch size: (893, 893)
Epoch 186, accuracy: 0.3543
Epoch 186, Train Loss: 1.1177, Val Loss: 1.0986
batch size: (908, 908)
Epoch 187, accuracy: 0.3558
batch size: (884, 884)
Epoch 188, accuracy: 0.3561
Epoch 188, Train Loss: 1.0970, Val Loss: 1.0986
batch size: (892, 892)
Epoch 189, accuracy: 0.3583
batch size: (913, 913)
Epoch 190, accuracy: 0.3528
Epoch 190, Train Loss: 1.0984, Val Loss: 1.0986
batch size: (902, 902)
Epoch 191, accuracy: 0.3545
batch size: (887, 887)
Epoch 192, accuracy: 0.3568
Epoch 192, Train Loss: 1.1858, Val Loss: 1.0986
batch size: (903, 903)
Epoch 193, accuracy: 0.3552
batch size: (906, 906)
Epoch 194, accuracy: 0.3571
Epoch 194, Train Loss: 1.2743, Val Loss: 1.0986
batch size: (901, 901)
Epoch 195, accuracy: 0.3557
batch size: (891, 891)
Epoch 196, accuracy: 0.3568
Epoch 196, Train Loss: 1.0843, Val Loss: 1.0986
batch size: (894, 894)
Epoch 197, accuracy: 0.3574
batch size: (898, 898)
Epoch 198, accuracy: 0.3542
Epoch 198, Train Loss: 1.1169, Val Loss: 1.0986
batch size: (917, 917)
Epoch 199, accuracy: 0.3555
Loaded best model with val_loss = 1.0986120700836182
test :accuracy 0.1681, f1_macro: 0.0960, f1_micro: 0.1681, auc: 0.5000
Training GAT with 2 layers...
可训练参数: 196495_GAT
不可训练参数: 0
batch size: (896, 896)
✅ Epoch 0: New best model saved with val_loss = 1.0779
Epoch 0, accuracy: 0.6960
Epoch 0, Train Loss: 1.0956, Val Loss: 1.0779
batch size: (899, 899)
✅ Epoch 1: New best model saved with val_loss = 1.0565
Epoch 1, accuracy: 0.7216
batch size: (907, 907)
✅ Epoch 2: New best model saved with val_loss = 1.0315
Epoch 2, accuracy: 0.7248
Epoch 2, Train Loss: 1.0284, Val Loss: 1.0315
batch size: (904, 904)
✅ Epoch 3: New best model saved with val_loss = 1.0019
Epoch 3, accuracy: 0.7255
batch size: (891, 891)
✅ Epoch 4: New best model saved with val_loss = 0.9684
Epoch 4, accuracy: 0.7252
Epoch 4, Train Loss: 0.9353, Val Loss: 0.9684
batch size: (908, 908)
✅ Epoch 5: New best model saved with val_loss = 0.9305
Epoch 5, accuracy: 0.7285
batch size: (913, 913)
✅ Epoch 6: New best model saved with val_loss = 0.8935
Epoch 6, accuracy: 0.7299
Epoch 6, Train Loss: 0.8254, Val Loss: 0.8935
batch size: (898, 898)
✅ Epoch 7: New best model saved with val_loss = 0.8545
Epoch 7, accuracy: 0.7317
batch size: (895, 895)
✅ Epoch 8: New best model saved with val_loss = 0.8192
Epoch 8, accuracy: 0.7324
Epoch 8, Train Loss: 0.7039, Val Loss: 0.8192
batch size: (893, 893)
✅ Epoch 9: New best model saved with val_loss = 0.7837
Epoch 9, accuracy: 0.7351
batch size: (895, 895)
✅ Epoch 10: New best model saved with val_loss = 0.7476
Epoch 10, accuracy: 0.7339
Epoch 10, Train Loss: 0.5920, Val Loss: 0.7476
batch size: (897, 897)
✅ Epoch 11: New best model saved with val_loss = 0.7133
Epoch 11, accuracy: 0.7370
batch size: (900, 900)
✅ Epoch 12: New best model saved with val_loss = 0.6905
Epoch 12, accuracy: 0.7375
Epoch 12, Train Loss: 0.4777, Val Loss: 0.6905
batch size: (886, 886)
✅ Epoch 13: New best model saved with val_loss = 0.6582
Epoch 13, accuracy: 0.7413
batch size: (902, 902)
✅ Epoch 14: New best model saved with val_loss = 0.6388
Epoch 14, accuracy: 0.7442
Epoch 14, Train Loss: 0.3771, Val Loss: 0.6388
batch size: (891, 891)
✅ Epoch 15: New best model saved with val_loss = 0.6245
Epoch 15, accuracy: 0.7458
batch size: (883, 883)
✅ Epoch 16: New best model saved with val_loss = 0.6041
Epoch 16, accuracy: 0.7471
Epoch 16, Train Loss: 0.3042, Val Loss: 0.6041
batch size: (898, 898)
✅ Epoch 17: New best model saved with val_loss = 0.5876
Epoch 17, accuracy: 0.7489
batch size: (897, 897)
✅ Epoch 18: New best model saved with val_loss = 0.5756
Epoch 18, accuracy: 0.7504
Epoch 18, Train Loss: 0.2679, Val Loss: 0.5756
batch size: (890, 890)
✅ Epoch 19: New best model saved with val_loss = 0.5632
Epoch 19, accuracy: 0.7523
batch size: (892, 892)
✅ Epoch 20: New best model saved with val_loss = 0.5599
Epoch 20, accuracy: 0.7502
Epoch 20, Train Loss: 0.1918, Val Loss: 0.5599
batch size: (906, 906)
✅ Epoch 21: New best model saved with val_loss = 0.5520
Epoch 21, accuracy: 0.7492
batch size: (892, 892)
Epoch 22, accuracy: 0.7454
Epoch 22, Train Loss: 0.1557, Val Loss: 0.5565
batch size: (878, 878)
✅ Epoch 23: New best model saved with val_loss = 0.5494
Epoch 23, accuracy: 0.7423
batch size: (888, 888)
Epoch 24, accuracy: 0.7379
Epoch 24, Train Loss: 0.1226, Val Loss: 0.5553
batch size: (904, 904)
Epoch 25, accuracy: 0.7392
batch size: (903, 903)
Epoch 26, accuracy: 0.7356
Epoch 26, Train Loss: 0.1012, Val Loss: 0.5703
batch size: (906, 906)
Epoch 27, accuracy: 0.7357
batch size: (904, 904)
Epoch 28, accuracy: 0.7335
Epoch 28, Train Loss: 0.0874, Val Loss: 0.5576
batch size: (895, 895)
Epoch 29, accuracy: 0.7318
batch size: (891, 891)
Epoch 30, accuracy: 0.7319
Epoch 30, Train Loss: 0.0921, Val Loss: 0.5713
batch size: (899, 899)
Epoch 31, accuracy: 0.7343
batch size: (895, 895)
Epoch 32, accuracy: 0.7344
Epoch 32, Train Loss: 0.0892, Val Loss: 0.5669
batch size: (905, 905)
Epoch 33, accuracy: 0.7345
batch size: (896, 896)
✅ Epoch 34: New best model saved with val_loss = 0.5447
Epoch 34, accuracy: 0.7328
Epoch 34, Train Loss: 0.0798, Val Loss: 0.5447
batch size: (899, 899)
Epoch 35, accuracy: 0.7339
batch size: (895, 895)
Epoch 36, accuracy: 0.7329
Epoch 36, Train Loss: 0.0790, Val Loss: 0.5600
batch size: (880, 880)
Epoch 37, accuracy: 0.7349
batch size: (910, 910)
Epoch 38, accuracy: 0.7347
Epoch 38, Train Loss: 0.0702, Val Loss: 0.5523
batch size: (901, 901)
Epoch 39, accuracy: 0.7342
batch size: (899, 899)
Epoch 40, accuracy: 0.7347
Epoch 40, Train Loss: 0.0760, Val Loss: 0.5450
batch size: (894, 894)
Epoch 41, accuracy: 0.7321
batch size: (890, 890)
Epoch 42, accuracy: 0.7354
Epoch 42, Train Loss: 0.0737, Val Loss: 0.5704
batch size: (930, 930)
Epoch 43, accuracy: 0.7337
batch size: (907, 907)
Epoch 44, accuracy: 0.7295
Epoch 44, Train Loss: 0.0844, Val Loss: 0.5567
batch size: (907, 907)
Epoch 45, accuracy: 0.7327
batch size: (913, 913)
Epoch 46, accuracy: 0.7325
Epoch 46, Train Loss: 0.0712, Val Loss: 0.5649
batch size: (897, 897)
✅ Epoch 47: New best model saved with val_loss = 0.5436
Epoch 47, accuracy: 0.7331
batch size: (872, 872)
Epoch 48, accuracy: 0.7342
Epoch 48, Train Loss: 0.0755, Val Loss: 0.5567
batch size: (890, 890)
Epoch 49, accuracy: 0.7330
batch size: (888, 888)
Epoch 50, accuracy: 0.7329
Epoch 50, Train Loss: 0.0766, Val Loss: 0.5478
batch size: (912, 912)
Epoch 51, accuracy: 0.7336
batch size: (910, 910)
Epoch 52, accuracy: 0.7330
Epoch 52, Train Loss: 0.0648, Val Loss: 0.5590
batch size: (911, 911)
Epoch 53, accuracy: 0.7330
batch size: (904, 904)
Epoch 54, accuracy: 0.7321
Epoch 54, Train Loss: 0.0806, Val Loss: 0.5627
batch size: (914, 914)
Epoch 55, accuracy: 0.7332
batch size: (886, 886)
Epoch 56, accuracy: 0.7351
Epoch 56, Train Loss: 0.0703, Val Loss: 0.5480
batch size: (899, 899)
Epoch 57, accuracy: 0.7327
batch size: (891, 891)
Epoch 58, accuracy: 0.7308
Epoch 58, Train Loss: 0.0686, Val Loss: 0.5585
batch size: (906, 906)
Epoch 59, accuracy: 0.7359
batch size: (902, 902)
✅ Epoch 60: New best model saved with val_loss = 0.5424
Epoch 60, accuracy: 0.7332
Epoch 60, Train Loss: 0.0812, Val Loss: 0.5424
batch size: (911, 911)
Epoch 61, accuracy: 0.7317
batch size: (908, 908)
Epoch 62, accuracy: 0.7328
Epoch 62, Train Loss: 0.0634, Val Loss: 0.5635
batch size: (905, 905)
Epoch 63, accuracy: 0.7342
batch size: (927, 927)
Epoch 64, accuracy: 0.7319
Epoch 64, Train Loss: 0.0770, Val Loss: 0.5560
batch size: (887, 887)
Epoch 65, accuracy: 0.7328
batch size: (883, 883)
Epoch 66, accuracy: 0.7328
Epoch 66, Train Loss: 0.0698, Val Loss: 0.5645
batch size: (913, 913)
Epoch 67, accuracy: 0.7328
batch size: (906, 906)
Epoch 68, accuracy: 0.7309
Epoch 68, Train Loss: 0.0835, Val Loss: 0.5596
batch size: (882, 882)
Epoch 69, accuracy: 0.7336
batch size: (906, 906)
Epoch 70, accuracy: 0.7326
Epoch 70, Train Loss: 0.0704, Val Loss: 0.5568
batch size: (880, 880)
Epoch 71, accuracy: 0.7317
batch size: (906, 906)
Epoch 72, accuracy: 0.7333
Epoch 72, Train Loss: 0.0592, Val Loss: 0.5438
batch size: (908, 908)
Epoch 73, accuracy: 0.7316
batch size: (895, 895)
Epoch 74, accuracy: 0.7343
Epoch 74, Train Loss: 0.0672, Val Loss: 0.5577
batch size: (893, 893)
Epoch 75, accuracy: 0.7317
batch size: (898, 898)
Epoch 76, accuracy: 0.7325
Epoch 76, Train Loss: 0.0831, Val Loss: 0.5613
batch size: (887, 887)
Epoch 77, accuracy: 0.7336
batch size: (887, 887)
Epoch 78, accuracy: 0.7313
Epoch 78, Train Loss: 0.0708, Val Loss: 0.5572
batch size: (894, 894)
Epoch 79, accuracy: 0.7331
batch size: (897, 897)
Epoch 80, accuracy: 0.7330
Epoch 80, Train Loss: 0.0777, Val Loss: 0.5626
batch size: (884, 884)
Epoch 81, accuracy: 0.7341
batch size: (906, 906)
Epoch 82, accuracy: 0.7335
Epoch 82, Train Loss: 0.0649, Val Loss: 0.5552
batch size: (908, 908)
Epoch 83, accuracy: 0.7341
batch size: (892, 892)
Epoch 84, accuracy: 0.7361
Epoch 84, Train Loss: 0.0736, Val Loss: 0.5641
batch size: (917, 917)
Epoch 85, accuracy: 0.7355
batch size: (888, 888)
Epoch 86, accuracy: 0.7337
Epoch 86, Train Loss: 0.0834, Val Loss: 0.5513
batch size: (900, 900)
Epoch 87, accuracy: 0.7322
batch size: (904, 904)
Epoch 88, accuracy: 0.7334
Epoch 88, Train Loss: 0.0748, Val Loss: 0.5590
batch size: (885, 885)
Epoch 89, accuracy: 0.7312
batch size: (904, 904)
Epoch 90, accuracy: 0.7342
Epoch 90, Train Loss: 0.0786, Val Loss: 0.5548
batch size: (904, 904)
Epoch 91, accuracy: 0.7326
batch size: (885, 885)
Epoch 92, accuracy: 0.7330
Epoch 92, Train Loss: 0.0863, Val Loss: 0.5670
batch size: (884, 884)
Epoch 93, accuracy: 0.7328
batch size: (901, 901)
Epoch 94, accuracy: 0.7330
Epoch 94, Train Loss: 0.0707, Val Loss: 0.5616
batch size: (903, 903)
Epoch 95, accuracy: 0.7355
batch size: (884, 884)
Epoch 96, accuracy: 0.7315
Epoch 96, Train Loss: 0.0772, Val Loss: 0.5610
batch size: (899, 899)
Epoch 97, accuracy: 0.7328
batch size: (906, 906)
Epoch 98, accuracy: 0.7341
Epoch 98, Train Loss: 0.0823, Val Loss: 0.5536
batch size: (903, 903)
Epoch 99, accuracy: 0.7368
batch size: (917, 917)
Epoch 100, accuracy: 0.7326
Epoch 100, Train Loss: 0.0742, Val Loss: 0.5604
batch size: (904, 904)
Epoch 101, accuracy: 0.7330
batch size: (906, 906)
Epoch 102, accuracy: 0.7327
Epoch 102, Train Loss: 0.0707, Val Loss: 0.5654
batch size: (909, 909)
Epoch 103, accuracy: 0.7336
batch size: (898, 898)
Epoch 104, accuracy: 0.7316
Epoch 104, Train Loss: 0.0744, Val Loss: 0.5526
batch size: (905, 905)
Epoch 105, accuracy: 0.7339
batch size: (888, 888)
Epoch 106, accuracy: 0.7319
Epoch 106, Train Loss: 0.0743, Val Loss: 0.5534
batch size: (912, 912)
Epoch 107, accuracy: 0.7336
batch size: (900, 900)
Epoch 108, accuracy: 0.7322
Epoch 108, Train Loss: 0.0714, Val Loss: 0.5508
batch size: (904, 904)
Epoch 109, accuracy: 0.7319
batch size: (897, 897)
Epoch 110, accuracy: 0.7330
Epoch 110, Train Loss: 0.0849, Val Loss: 0.5547
batch size: (881, 881)
Epoch 111, accuracy: 0.7311
batch size: (898, 898)
Epoch 112, accuracy: 0.7343
Epoch 112, Train Loss: 0.0685, Val Loss: 0.5567
batch size: (921, 921)
Epoch 113, accuracy: 0.7336
batch size: (916, 916)
Epoch 114, accuracy: 0.7315
Epoch 114, Train Loss: 0.0838, Val Loss: 0.5544
batch size: (901, 901)
Epoch 115, accuracy: 0.7346
batch size: (902, 902)
Epoch 116, accuracy: 0.7340
Epoch 116, Train Loss: 0.0820, Val Loss: 0.5560
batch size: (909, 909)
Epoch 117, accuracy: 0.7355
batch size: (912, 912)
Epoch 118, accuracy: 0.7324
Epoch 118, Train Loss: 0.0794, Val Loss: 0.5611
batch size: (908, 908)
Epoch 119, accuracy: 0.7327
batch size: (896, 896)
Epoch 120, accuracy: 0.7322
Epoch 120, Train Loss: 0.0716, Val Loss: 0.5629
batch size: (913, 913)
Epoch 121, accuracy: 0.7331
batch size: (904, 904)
Epoch 122, accuracy: 0.7336
Epoch 122, Train Loss: 0.0834, Val Loss: 0.5538
batch size: (888, 888)
Epoch 123, accuracy: 0.7324
batch size: (899, 899)
Epoch 124, accuracy: 0.7334
Epoch 124, Train Loss: 0.0764, Val Loss: 0.5568
batch size: (898, 898)
Epoch 125, accuracy: 0.7313
batch size: (915, 915)
Epoch 126, accuracy: 0.7333
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 126, Train Loss: 0.0942, Val Loss: 0.5530
batch size: (913, 913)
Epoch 127, accuracy: 0.7323
batch size: (921, 921)
Epoch 128, accuracy: 0.7323
Epoch 128, Train Loss: 0.0857, Val Loss: 0.5577
batch size: (877, 877)
Epoch 129, accuracy: 0.7323
batch size: (903, 903)
Epoch 130, accuracy: 0.7335
Epoch 130, Train Loss: 0.0851, Val Loss: 0.5712
batch size: (914, 914)
Epoch 131, accuracy: 0.7336
batch size: (906, 906)
Epoch 132, accuracy: 0.7350
Epoch 132, Train Loss: 0.0699, Val Loss: 0.5432
batch size: (893, 893)
Epoch 133, accuracy: 0.7347
batch size: (878, 878)
Epoch 134, accuracy: 0.7339
Epoch 134, Train Loss: 0.0754, Val Loss: 0.5660
batch size: (878, 878)
Epoch 135, accuracy: 0.7315
batch size: (891, 891)
Epoch 136, accuracy: 0.7310
Epoch 136, Train Loss: 0.0751, Val Loss: 0.5648
batch size: (897, 897)
Epoch 137, accuracy: 0.7342
batch size: (893, 893)
Epoch 138, accuracy: 0.7342
Epoch 138, Train Loss: 0.0749, Val Loss: 0.5547
batch size: (912, 912)
Epoch 139, accuracy: 0.7319
batch size: (874, 874)
Epoch 140, accuracy: 0.7331
Epoch 140, Train Loss: 0.0720, Val Loss: 0.5721
batch size: (910, 910)
Epoch 141, accuracy: 0.7335
batch size: (921, 921)
Epoch 142, accuracy: 0.7337
Epoch 142, Train Loss: 0.0731, Val Loss: 0.5563
batch size: (912, 912)
Epoch 143, accuracy: 0.7307
batch size: (908, 908)
Epoch 144, accuracy: 0.7329
Epoch 144, Train Loss: 0.0772, Val Loss: 0.5611
batch size: (907, 907)
Epoch 145, accuracy: 0.7305
batch size: (881, 881)
Epoch 146, accuracy: 0.7321
Epoch 146, Train Loss: 0.0745, Val Loss: 0.5530
batch size: (898, 898)
Epoch 147, accuracy: 0.7329
batch size: (920, 920)
Epoch 148, accuracy: 0.7313
Epoch 148, Train Loss: 0.0802, Val Loss: 0.5573
batch size: (912, 912)
Epoch 149, accuracy: 0.7313
batch size: (906, 906)
Epoch 150, accuracy: 0.7336
Epoch 150, Train Loss: 0.0818, Val Loss: 0.5455
batch size: (888, 888)
Epoch 151, accuracy: 0.7332
batch size: (897, 897)
Epoch 152, accuracy: 0.7347
Epoch 152, Train Loss: 0.0766, Val Loss: 0.5520
batch size: (885, 885)
Epoch 153, accuracy: 0.7352
batch size: (900, 900)
Epoch 154, accuracy: 0.7319
Epoch 154, Train Loss: 0.0811, Val Loss: 0.5522
batch size: (901, 901)
Epoch 155, accuracy: 0.7341
batch size: (918, 918)
Epoch 156, accuracy: 0.7301
Epoch 156, Train Loss: 0.0787, Val Loss: 0.5648
batch size: (902, 902)
✅ Epoch 157: New best model saved with val_loss = 0.5404
Epoch 157, accuracy: 0.7325
batch size: (909, 909)
Epoch 158, accuracy: 0.7316
Epoch 158, Train Loss: 0.0805, Val Loss: 0.5609
batch size: (902, 902)
Epoch 159, accuracy: 0.7326
batch size: (899, 899)
Epoch 160, accuracy: 0.7320
Epoch 160, Train Loss: 0.0750, Val Loss: 0.5507
batch size: (910, 910)
Epoch 161, accuracy: 0.7338
batch size: (896, 896)
Epoch 162, accuracy: 0.7323
Epoch 162, Train Loss: 0.0695, Val Loss: 0.5687
batch size: (898, 898)
Epoch 163, accuracy: 0.7346
batch size: (892, 892)
Epoch 164, accuracy: 0.7317
Epoch 164, Train Loss: 0.0705, Val Loss: 0.5579
batch size: (919, 919)
Epoch 165, accuracy: 0.7326
batch size: (902, 902)
Epoch 166, accuracy: 0.7316
Epoch 166, Train Loss: 0.0681, Val Loss: 0.5630
batch size: (911, 911)
Epoch 167, accuracy: 0.7333
batch size: (874, 874)
Epoch 168, accuracy: 0.7329
Epoch 168, Train Loss: 0.0735, Val Loss: 0.5600
batch size: (912, 912)
Epoch 169, accuracy: 0.7334
batch size: (897, 897)
Epoch 170, accuracy: 0.7333
Epoch 170, Train Loss: 0.0744, Val Loss: 0.5661
batch size: (900, 900)
Epoch 171, accuracy: 0.7314
batch size: (884, 884)
Epoch 172, accuracy: 0.7303
Epoch 172, Train Loss: 0.0706, Val Loss: 0.5424
batch size: (906, 906)
Epoch 173, accuracy: 0.7313
batch size: (898, 898)
Epoch 174, accuracy: 0.7332
Epoch 174, Train Loss: 0.0700, Val Loss: 0.5486
batch size: (902, 902)
Epoch 175, accuracy: 0.7336
batch size: (892, 892)
Epoch 176, accuracy: 0.7324
Epoch 176, Train Loss: 0.0736, Val Loss: 0.5674
batch size: (898, 898)
Epoch 177, accuracy: 0.7323
batch size: (913, 913)
Epoch 178, accuracy: 0.7334
Epoch 178, Train Loss: 0.0730, Val Loss: 0.5610
batch size: (904, 904)
Epoch 179, accuracy: 0.7327
batch size: (898, 898)
Epoch 180, accuracy: 0.7325
Epoch 180, Train Loss: 0.0702, Val Loss: 0.5654
batch size: (893, 893)
Epoch 181, accuracy: 0.7353
batch size: (912, 912)
✅ Epoch 182: New best model saved with val_loss = 0.5396
Epoch 182, accuracy: 0.7313
Epoch 182, Train Loss: 0.0743, Val Loss: 0.5396
batch size: (901, 901)
Epoch 183, accuracy: 0.7342
batch size: (903, 903)
Epoch 184, accuracy: 0.7348
Epoch 184, Train Loss: 0.0812, Val Loss: 0.5492
batch size: (913, 913)
Epoch 185, accuracy: 0.7312
batch size: (894, 894)
Epoch 186, accuracy: 0.7355
Epoch 186, Train Loss: 0.0818, Val Loss: 0.5539
batch size: (898, 898)
Epoch 187, accuracy: 0.7321
batch size: (917, 917)
Epoch 188, accuracy: 0.7324
Epoch 188, Train Loss: 0.0887, Val Loss: 0.5466
batch size: (886, 886)
Epoch 189, accuracy: 0.7326
batch size: (902, 902)
Epoch 190, accuracy: 0.7333
Epoch 190, Train Loss: 0.0643, Val Loss: 0.5558
batch size: (906, 906)
Epoch 191, accuracy: 0.7321
batch size: (883, 883)
Epoch 192, accuracy: 0.7310
Epoch 192, Train Loss: 0.0775, Val Loss: 0.5595
batch size: (908, 908)
Epoch 193, accuracy: 0.7351
batch size: (888, 888)
Epoch 194, accuracy: 0.7338
Epoch 194, Train Loss: 0.0765, Val Loss: 0.5581
batch size: (889, 889)
Epoch 195, accuracy: 0.7335
batch size: (910, 910)
Epoch 196, accuracy: 0.7334
Epoch 196, Train Loss: 0.0789, Val Loss: 0.5514
batch size: (904, 904)
Epoch 197, accuracy: 0.7328
batch size: (895, 895)
Epoch 198, accuracy: 0.7323
Epoch 198, Train Loss: 0.0720, Val Loss: 0.5723
batch size: (922, 922)
Epoch 199, accuracy: 0.7302
Loaded best model with val_loss = 0.5396281480789185
test :accuracy 0.7328, f1_macro: 0.7291, f1_micro: 0.7328, auc: 0.8860
Training GAT with 8 layers...
可训练参数: 1090447_GAT
不可训练参数: 0
batch size: (897, 897)
✅ Epoch 0: New best model saved with val_loss = 1.1068
Epoch 0, accuracy: 0.1649
Epoch 0, Train Loss: 1.0991, Val Loss: 1.1068
batch size: (888, 888)
✅ Epoch 1: New best model saved with val_loss = 1.1065
Epoch 1, accuracy: 0.1651
batch size: (908, 908)
✅ Epoch 2: New best model saved with val_loss = 1.0951
Epoch 2, accuracy: 0.6109
Epoch 2, Train Loss: 1.1021, Val Loss: 1.0951
batch size: (912, 912)
Epoch 3, accuracy: 0.1671
batch size: (888, 888)
Epoch 4, accuracy: 0.1665
Epoch 4, Train Loss: 1.0790, Val Loss: 1.1507
batch size: (906, 906)
✅ Epoch 5: New best model saved with val_loss = 1.0572
Epoch 5, accuracy: 0.4291
batch size: (885, 885)
✅ Epoch 6: New best model saved with val_loss = 1.0343
Epoch 6, accuracy: 0.5826
Epoch 6, Train Loss: 1.0809, Val Loss: 1.0343
batch size: (915, 915)
Epoch 7, accuracy: 0.4086
batch size: (892, 892)
✅ Epoch 8: New best model saved with val_loss = 0.9682
Epoch 8, accuracy: 0.5047
Epoch 8, Train Loss: 0.9182, Val Loss: 0.9682
batch size: (888, 888)
✅ Epoch 9: New best model saved with val_loss = 0.9343
Epoch 9, accuracy: 0.4997
batch size: (900, 900)
Epoch 10, accuracy: 0.5053
Epoch 10, Train Loss: 0.7972, Val Loss: 0.9430
batch size: (915, 915)
✅ Epoch 11: New best model saved with val_loss = 0.8781
Epoch 11, accuracy: 0.6040
batch size: (915, 915)
Epoch 12, accuracy: 0.5157
Epoch 12, Train Loss: 0.6818, Val Loss: 0.8851
batch size: (903, 903)
✅ Epoch 13: New best model saved with val_loss = 0.8533
Epoch 13, accuracy: 0.5247
batch size: (894, 894)
Epoch 14, accuracy: 0.5385
Epoch 14, Train Loss: 0.5702, Val Loss: 1.3476
batch size: (894, 894)
Epoch 15, accuracy: 0.6439
batch size: (902, 902)
Epoch 16, accuracy: 0.5726
Epoch 16, Train Loss: 0.6011, Val Loss: 0.9189
batch size: (905, 905)
✅ Epoch 17: New best model saved with val_loss = 0.8078
Epoch 17, accuracy: 0.6797
batch size: (905, 905)
Epoch 18, accuracy: 0.6427
Epoch 18, Train Loss: 0.4431, Val Loss: 0.8816
batch size: (894, 894)
Epoch 19, accuracy: 0.6853
batch size: (911, 911)
✅ Epoch 20: New best model saved with val_loss = 0.7482
Epoch 20, accuracy: 0.7183
Epoch 20, Train Loss: 0.4374, Val Loss: 0.7482
batch size: (900, 900)
Epoch 21, accuracy: 0.6615
batch size: (884, 884)
Epoch 22, accuracy: 0.7131
Epoch 22, Train Loss: 0.2679, Val Loss: 0.8690
batch size: (909, 909)
Epoch 23, accuracy: 0.6891
batch size: (907, 907)
Epoch 24, accuracy: 0.6596
Epoch 24, Train Loss: 0.1375, Val Loss: 1.2418
batch size: (909, 909)
Epoch 25, accuracy: 0.6230
batch size: (905, 905)
Epoch 26, accuracy: 0.6536
Epoch 26, Train Loss: 0.0818, Val Loss: 1.3897
batch size: (901, 901)
Epoch 27, accuracy: 0.6593
batch size: (900, 900)
Epoch 28, accuracy: 0.6666
Epoch 28, Train Loss: 0.0663, Val Loss: 1.3536
batch size: (909, 909)
Epoch 29, accuracy: 0.6702
batch size: (910, 910)
Epoch 30, accuracy: 0.6802
Epoch 30, Train Loss: 0.0115, Val Loss: 1.3486
batch size: (908, 908)
Epoch 31, accuracy: 0.6825
batch size: (883, 883)
Epoch 32, accuracy: 0.6837
Epoch 32, Train Loss: 0.0361, Val Loss: 1.4644
batch size: (899, 899)
Epoch 33, accuracy: 0.6847
batch size: (897, 897)
Epoch 34, accuracy: 0.6841
Epoch 34, Train Loss: 0.0144, Val Loss: 1.2819
batch size: (874, 874)
Epoch 35, accuracy: 0.6856
batch size: (899, 899)
Epoch 36, accuracy: 0.6857
Epoch 36, Train Loss: 0.0441, Val Loss: 1.3360
batch size: (900, 900)
Epoch 37, accuracy: 0.6848
batch size: (899, 899)
Epoch 38, accuracy: 0.6860
Epoch 38, Train Loss: 0.1043, Val Loss: 1.4068
batch size: (911, 911)
Epoch 39, accuracy: 0.6854
batch size: (883, 883)
Epoch 40, accuracy: 0.6865
Epoch 40, Train Loss: 0.0423, Val Loss: 1.3831
batch size: (902, 902)
Epoch 41, accuracy: 0.6857
batch size: (893, 893)
Epoch 42, accuracy: 0.6855
Epoch 42, Train Loss: 0.0084, Val Loss: 1.2889
batch size: (907, 907)
Epoch 43, accuracy: 0.6866
batch size: (909, 909)
Epoch 44, accuracy: 0.6862
Epoch 44, Train Loss: 0.0122, Val Loss: 1.3685
batch size: (874, 874)
Epoch 45, accuracy: 0.6870
batch size: (886, 886)
Epoch 46, accuracy: 0.6877
Epoch 46, Train Loss: 0.0431, Val Loss: 1.3585
batch size: (886, 886)
Epoch 47, accuracy: 0.6854
batch size: (897, 897)
Epoch 48, accuracy: 0.6880
Epoch 48, Train Loss: 0.0106, Val Loss: 1.4021
batch size: (897, 897)
Epoch 49, accuracy: 0.6892
batch size: (898, 898)
Epoch 50, accuracy: 0.6848
Epoch 50, Train Loss: 0.0641, Val Loss: 1.2862
batch size: (904, 904)
Epoch 51, accuracy: 0.6858
batch size: (910, 910)
Epoch 52, accuracy: 0.6838
Epoch 52, Train Loss: 0.0204, Val Loss: 1.4000
batch size: (920, 920)
Epoch 53, accuracy: 0.6833
batch size: (912, 912)
Epoch 54, accuracy: 0.6863
Epoch 54, Train Loss: 0.0893, Val Loss: 1.2499
batch size: (895, 895)
Epoch 55, accuracy: 0.6869
batch size: (905, 905)
Epoch 56, accuracy: 0.6869
Epoch 56, Train Loss: 0.0258, Val Loss: 1.3913
batch size: (903, 903)
Epoch 57, accuracy: 0.6856
batch size: (900, 900)
Epoch 58, accuracy: 0.6870
Epoch 58, Train Loss: 0.1693, Val Loss: 1.4873
batch size: (895, 895)
Epoch 59, accuracy: 0.6857
batch size: (903, 903)
Epoch 60, accuracy: 0.6868
Epoch 60, Train Loss: 0.1332, Val Loss: 1.3982
batch size: (911, 911)
Epoch 61, accuracy: 0.6861
batch size: (901, 901)
Epoch 62, accuracy: 0.6884
Epoch 62, Train Loss: 0.0202, Val Loss: 1.3205
batch size: (920, 920)
Epoch 63, accuracy: 0.6857
batch size: (898, 898)
Epoch 64, accuracy: 0.6865
Epoch 64, Train Loss: 0.1571, Val Loss: 1.4018
batch size: (895, 895)
Epoch 65, accuracy: 0.6861
batch size: (906, 906)
Epoch 66, accuracy: 0.6834
Epoch 66, Train Loss: 0.0093, Val Loss: 1.3515
batch size: (896, 896)
Epoch 67, accuracy: 0.6865
batch size: (886, 886)
Epoch 68, accuracy: 0.6846
Epoch 68, Train Loss: 0.0073, Val Loss: 1.3364
batch size: (886, 886)
Epoch 69, accuracy: 0.6842
batch size: (890, 890)
Epoch 70, accuracy: 0.6861
Epoch 70, Train Loss: 0.0215, Val Loss: 1.3590
batch size: (918, 918)
Epoch 71, accuracy: 0.6863
batch size: (917, 917)
Epoch 72, accuracy: 0.6856
Epoch 72, Train Loss: 0.0181, Val Loss: 1.3481
batch size: (909, 909)
Epoch 73, accuracy: 0.6867
batch size: (902, 902)
Epoch 74, accuracy: 0.6858
Epoch 74, Train Loss: 0.1587, Val Loss: 1.3488
batch size: (904, 904)
Epoch 75, accuracy: 0.6864
batch size: (902, 902)
Epoch 76, accuracy: 0.6872
Epoch 76, Train Loss: 0.0204, Val Loss: 1.4061
batch size: (883, 883)
Epoch 77, accuracy: 0.6834
batch size: (905, 905)
Epoch 78, accuracy: 0.6852
Epoch 78, Train Loss: 0.1048, Val Loss: 1.3964
batch size: (890, 890)
Epoch 79, accuracy: 0.6854
batch size: (902, 902)
Epoch 80, accuracy: 0.6849
Epoch 80, Train Loss: 0.0225, Val Loss: 1.3758
batch size: (880, 880)
Epoch 81, accuracy: 0.6873
batch size: (889, 889)
Epoch 82, accuracy: 0.6844
Epoch 82, Train Loss: 0.1610, Val Loss: 1.3760
batch size: (910, 910)
Epoch 83, accuracy: 0.6865
batch size: (883, 883)
Epoch 84, accuracy: 0.6841
Epoch 84, Train Loss: 0.0065, Val Loss: 1.3237
batch size: (913, 913)
Epoch 85, accuracy: 0.6835
batch size: (910, 910)
Epoch 86, accuracy: 0.6880
Epoch 86, Train Loss: 0.0243, Val Loss: 1.3706
batch size: (904, 904)
Epoch 87, accuracy: 0.6841
batch size: (892, 892)
Epoch 88, accuracy: 0.6856
Epoch 88, Train Loss: 0.0195, Val Loss: 1.3408
batch size: (885, 885)
Epoch 89, accuracy: 0.6825
batch size: (889, 889)
Epoch 90, accuracy: 0.6904
Epoch 90, Train Loss: 0.0428, Val Loss: 1.3908
batch size: (906, 906)
Epoch 91, accuracy: 0.6860
batch size: (889, 889)
Epoch 92, accuracy: 0.6869
Epoch 92, Train Loss: 0.0095, Val Loss: 1.3403
batch size: (888, 888)
Epoch 93, accuracy: 0.6863
batch size: (900, 900)
Epoch 94, accuracy: 0.6849
Epoch 94, Train Loss: 0.0364, Val Loss: 1.3587
batch size: (880, 880)
Epoch 95, accuracy: 0.6865
batch size: (916, 916)
Epoch 96, accuracy: 0.6865
Epoch 96, Train Loss: 0.0440, Val Loss: 1.3373
batch size: (894, 894)
Epoch 97, accuracy: 0.6877
batch size: (898, 898)
Epoch 98, accuracy: 0.6877
Epoch 98, Train Loss: 0.1002, Val Loss: 1.3321
batch size: (895, 895)
Epoch 99, accuracy: 0.6856
batch size: (902, 902)
Epoch 100, accuracy: 0.6836
Epoch 100, Train Loss: 0.0257, Val Loss: 1.3632
batch size: (892, 892)
Epoch 101, accuracy: 0.6860
batch size: (906, 906)
Epoch 102, accuracy: 0.6837
Epoch 102, Train Loss: 0.0312, Val Loss: 1.3492
batch size: (905, 905)
Epoch 103, accuracy: 0.6859
batch size: (891, 891)
Epoch 104, accuracy: 0.6849
Epoch 104, Train Loss: 0.0584, Val Loss: 1.2764
batch size: (909, 909)
Epoch 105, accuracy: 0.6867
batch size: (897, 897)
Epoch 106, accuracy: 0.6868
Epoch 106, Train Loss: 0.0059, Val Loss: 1.4080
batch size: (909, 909)
Epoch 107, accuracy: 0.6870
batch size: (896, 896)
Epoch 108, accuracy: 0.6835
Epoch 108, Train Loss: 0.0307, Val Loss: 1.3486
batch size: (916, 916)
Epoch 109, accuracy: 0.6859
batch size: (887, 887)
Epoch 110, accuracy: 0.6893
Epoch 110, Train Loss: 0.0151, Val Loss: 1.3270
batch size: (905, 905)
Epoch 111, accuracy: 0.6856
batch size: (881, 881)
Epoch 112, accuracy: 0.6883
Epoch 112, Train Loss: 0.0123, Val Loss: 1.4306
batch size: (897, 897)
Epoch 113, accuracy: 0.6846
batch size: (895, 895)
Epoch 114, accuracy: 0.6840
Epoch 114, Train Loss: 0.0193, Val Loss: 1.4096
batch size: (891, 891)
Epoch 115, accuracy: 0.6840
batch size: (905, 905)
Epoch 116, accuracy: 0.6847
Epoch 116, Train Loss: 0.1052, Val Loss: 1.4238
batch size: (898, 898)
Epoch 117, accuracy: 0.6846
batch size: (910, 910)
Epoch 118, accuracy: 0.6883
Epoch 118, Train Loss: 0.0165, Val Loss: 1.3912
batch size: (910, 910)
Epoch 119, accuracy: 0.6847
batch size: (902, 902)
Epoch 120, accuracy: 0.6865
Epoch 120, Train Loss: 0.0266, Val Loss: 1.4411
batch size: (909, 909)
Epoch 121, accuracy: 0.6838
batch size: (904, 904)
Epoch 122, accuracy: 0.6871
Epoch 122, Train Loss: 0.1149, Val Loss: 1.3733
batch size: (902, 902)
Epoch 123, accuracy: 0.6869
batch size: (904, 904)
Epoch 124, accuracy: 0.6851
Epoch 124, Train Loss: 0.0142, Val Loss: 1.3705
batch size: (899, 899)
Epoch 125, accuracy: 0.6872
batch size: (901, 901)
Epoch 126, accuracy: 0.6907
Epoch 126, Train Loss: 0.0199, Val Loss: 1.3959
batch size: (891, 891)
Epoch 127, accuracy: 0.6863
batch size: (902, 902)
Epoch 128, accuracy: 0.6877
Epoch 128, Train Loss: 0.0180, Val Loss: 1.4665
batch size: (891, 891)
Epoch 129, accuracy: 0.6852
batch size: (890, 890)
Epoch 130, accuracy: 0.6828
Epoch 130, Train Loss: 0.0493, Val Loss: 1.3332
batch size: (895, 895)
Epoch 131, accuracy: 0.6842
batch size: (906, 906)
Epoch 132, accuracy: 0.6867
Epoch 132, Train Loss: 0.0191, Val Loss: 1.3701
batch size: (914, 914)
Epoch 133, accuracy: 0.6864
batch size: (893, 893)
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 134, accuracy: 0.6862
Epoch 134, Train Loss: 0.0219, Val Loss: 1.3298
batch size: (902, 902)
Epoch 135, accuracy: 0.6840
batch size: (885, 885)
Epoch 136, accuracy: 0.6851
Epoch 136, Train Loss: 0.1222, Val Loss: 1.3250
batch size: (891, 891)
Epoch 137, accuracy: 0.6869
batch size: (909, 909)
Epoch 138, accuracy: 0.6853
Epoch 138, Train Loss: 0.0815, Val Loss: 1.3080
batch size: (889, 889)
Epoch 139, accuracy: 0.6863
batch size: (908, 908)
Epoch 140, accuracy: 0.6859
Epoch 140, Train Loss: 0.0226, Val Loss: 1.3449
batch size: (893, 893)
Epoch 141, accuracy: 0.6825
batch size: (894, 894)
Epoch 142, accuracy: 0.6838
Epoch 142, Train Loss: 0.0072, Val Loss: 1.5053
batch size: (880, 880)
Epoch 143, accuracy: 0.6866
batch size: (917, 917)
Epoch 144, accuracy: 0.6838
Epoch 144, Train Loss: 0.0134, Val Loss: 1.3354
batch size: (876, 876)
Epoch 145, accuracy: 0.6861
batch size: (885, 885)
Epoch 146, accuracy: 0.6877
Epoch 146, Train Loss: 0.0465, Val Loss: 1.3581
batch size: (891, 891)
Epoch 147, accuracy: 0.6846
batch size: (905, 905)
Epoch 148, accuracy: 0.6856
Epoch 148, Train Loss: 0.0129, Val Loss: 1.4887
batch size: (890, 890)
Epoch 149, accuracy: 0.6868
batch size: (888, 888)
Epoch 150, accuracy: 0.6877
Epoch 150, Train Loss: 0.0528, Val Loss: 1.3344
batch size: (899, 899)
Epoch 151, accuracy: 0.6845
batch size: (891, 891)
Epoch 152, accuracy: 0.6821
Epoch 152, Train Loss: 0.0256, Val Loss: 1.3209
batch size: (894, 894)
Epoch 153, accuracy: 0.6878
batch size: (902, 902)
Epoch 154, accuracy: 0.6849
Epoch 154, Train Loss: 0.0050, Val Loss: 1.3841
batch size: (915, 915)
Epoch 155, accuracy: 0.6847
batch size: (910, 910)
Epoch 156, accuracy: 0.6857
Epoch 156, Train Loss: 0.0191, Val Loss: 1.4238
batch size: (900, 900)
Epoch 157, accuracy: 0.6849
batch size: (893, 893)
Epoch 158, accuracy: 0.6873
Epoch 158, Train Loss: 0.0106, Val Loss: 1.4279
batch size: (910, 910)
Epoch 159, accuracy: 0.6847
batch size: (906, 906)
Epoch 160, accuracy: 0.6832
Epoch 160, Train Loss: 0.0135, Val Loss: 1.3753
batch size: (926, 926)
Epoch 161, accuracy: 0.6845
batch size: (913, 913)
Epoch 162, accuracy: 0.6850
Epoch 162, Train Loss: 0.0290, Val Loss: 1.3553
batch size: (905, 905)
Epoch 163, accuracy: 0.6851
batch size: (898, 898)
Epoch 164, accuracy: 0.6891
Epoch 164, Train Loss: 0.0084, Val Loss: 1.4259
batch size: (900, 900)
Epoch 165, accuracy: 0.6859
batch size: (896, 896)
Epoch 166, accuracy: 0.6859
Epoch 166, Train Loss: 0.0160, Val Loss: 1.4536
batch size: (887, 887)
Epoch 167, accuracy: 0.6844
batch size: (886, 886)
Epoch 168, accuracy: 0.6866
Epoch 168, Train Loss: 0.1430, Val Loss: 1.3528
batch size: (893, 893)
Epoch 169, accuracy: 0.6848
batch size: (890, 890)
Epoch 170, accuracy: 0.6869
Epoch 170, Train Loss: 0.0800, Val Loss: 1.4703
batch size: (902, 902)
Epoch 171, accuracy: 0.6854
batch size: (891, 891)
Epoch 172, accuracy: 0.6876
Epoch 172, Train Loss: 0.0255, Val Loss: 1.3922
batch size: (907, 907)
Epoch 173, accuracy: 0.6864
batch size: (900, 900)
Epoch 174, accuracy: 0.6874
Epoch 174, Train Loss: 0.1180, Val Loss: 1.3836
batch size: (917, 917)
Epoch 175, accuracy: 0.6864
batch size: (901, 901)
Epoch 176, accuracy: 0.6849
Epoch 176, Train Loss: 0.0191, Val Loss: 1.3352
batch size: (915, 915)
Epoch 177, accuracy: 0.6849
batch size: (912, 912)
Epoch 178, accuracy: 0.6852
Epoch 178, Train Loss: 0.0163, Val Loss: 1.3656
batch size: (914, 914)
Epoch 179, accuracy: 0.6832
batch size: (901, 901)
Epoch 180, accuracy: 0.6889
Epoch 180, Train Loss: 0.0201, Val Loss: 1.4022
batch size: (902, 902)
Epoch 181, accuracy: 0.6857
batch size: (906, 906)
Epoch 182, accuracy: 0.6832
Epoch 182, Train Loss: 0.0048, Val Loss: 1.2877
batch size: (903, 903)
Epoch 183, accuracy: 0.6853
batch size: (889, 889)
Epoch 184, accuracy: 0.6849
Epoch 184, Train Loss: 0.0336, Val Loss: 1.3244
batch size: (903, 903)
Epoch 185, accuracy: 0.6866
batch size: (907, 907)
Epoch 186, accuracy: 0.6866
Epoch 186, Train Loss: 0.0308, Val Loss: 1.4112
batch size: (905, 905)
Epoch 187, accuracy: 0.6856
batch size: (897, 897)
Epoch 188, accuracy: 0.6860
Epoch 188, Train Loss: 0.0101, Val Loss: 1.3971
batch size: (919, 919)
Epoch 189, accuracy: 0.6853
batch size: (902, 902)
Epoch 190, accuracy: 0.6863
Epoch 190, Train Loss: 0.0086, Val Loss: 1.4331
batch size: (888, 888)
Epoch 191, accuracy: 0.6866
batch size: (881, 881)
Epoch 192, accuracy: 0.6862
Epoch 192, Train Loss: 0.0219, Val Loss: 1.3556
batch size: (907, 907)
Epoch 193, accuracy: 0.6856
batch size: (891, 891)
Epoch 194, accuracy: 0.6858
Epoch 194, Train Loss: 0.0298, Val Loss: 1.3604
batch size: (895, 895)
Epoch 195, accuracy: 0.6866
batch size: (885, 885)
Epoch 196, accuracy: 0.6847
Epoch 196, Train Loss: 0.0080, Val Loss: 1.3752
batch size: (907, 907)
Epoch 197, accuracy: 0.6854
batch size: (911, 911)
Epoch 198, accuracy: 0.6871
Epoch 198, Train Loss: 0.0568, Val Loss: 1.2685
batch size: (918, 918)
Epoch 199, accuracy: 0.6814
Loaded best model with val_loss = 0.7482166886329651
test :accuracy 0.7183, f1_macro: 0.7052, f1_micro: 0.7183, auc: 0.8541
Training GAT with 32 layers...
可训练参数: 4666255_GAT
不可训练参数: 0
batch size: (920, 920)
✅ Epoch 0: New best model saved with val_loss = 1.0970
Epoch 0, accuracy: 0.4311
Epoch 0, Train Loss: 1.0987, Val Loss: 1.0970
batch size: (901, 901)
Epoch 1, accuracy: 0.4018
batch size: (918, 918)
✅ Epoch 2: New best model saved with val_loss = 1.0897
Epoch 2, accuracy: 0.4309
Epoch 2, Train Loss: 1.1107, Val Loss: 1.0897
batch size: (908, 908)
Epoch 3, accuracy: 0.4289
batch size: (888, 888)
Epoch 4, accuracy: 0.1676
Epoch 4, Train Loss: 1.0907, Val Loss: 1.1013
batch size: (899, 899)
Epoch 5, accuracy: 0.1672
batch size: (899, 899)
✅ Epoch 6: New best model saved with val_loss = 1.0785
Epoch 6, accuracy: 0.4306
Epoch 6, Train Loss: 1.1046, Val Loss: 1.0785
batch size: (904, 904)
Epoch 7, accuracy: 0.1662
batch size: (879, 879)
Epoch 8, accuracy: 0.1641
Epoch 8, Train Loss: 1.0876, Val Loss: 1.1183
batch size: (889, 889)
Epoch 9, accuracy: 0.1667
batch size: (893, 893)
Epoch 10, accuracy: 0.4292
Epoch 10, Train Loss: 1.1139, Val Loss: 1.0907
batch size: (893, 893)
Epoch 11, accuracy: 0.4305
batch size: (893, 893)
Epoch 12, accuracy: 0.4282
Epoch 12, Train Loss: 1.1008, Val Loss: 1.0924
batch size: (892, 892)
Epoch 13, accuracy: 0.4326
batch size: (903, 903)
Epoch 14, accuracy: 0.4327
Epoch 14, Train Loss: 1.0854, Val Loss: 1.0941
batch size: (919, 919)
Epoch 15, accuracy: 0.4297
batch size: (904, 904)
Epoch 16, accuracy: 0.4277
Epoch 16, Train Loss: 1.0986, Val Loss: 1.0959
batch size: (901, 901)
Epoch 17, accuracy: 0.4282
batch size: (905, 905)
Epoch 18, accuracy: 0.4310
Epoch 18, Train Loss: 1.1022, Val Loss: 1.0978
batch size: (910, 910)
Epoch 19, accuracy: 0.4252
batch size: (898, 898)
Epoch 20, accuracy: 0.4271
Epoch 20, Train Loss: 1.1048, Val Loss: 1.0980
batch size: (902, 902)
Epoch 21, accuracy: 0.4266
batch size: (914, 914)
Epoch 22, accuracy: 0.4305
Epoch 22, Train Loss: 1.0919, Val Loss: 1.0981
batch size: (920, 920)
Epoch 23, accuracy: 0.4324
batch size: (904, 904)
Epoch 24, accuracy: 0.4296
Epoch 24, Train Loss: 1.0837, Val Loss: 1.0983
batch size: (903, 903)
Epoch 25, accuracy: 0.4307
batch size: (904, 904)
Epoch 26, accuracy: 0.4266
Epoch 26, Train Loss: 1.0999, Val Loss: 1.0983
batch size: (912, 912)
Epoch 27, accuracy: 0.4297
batch size: (912, 912)
Epoch 28, accuracy: 0.4281
Epoch 28, Train Loss: 1.0866, Val Loss: 1.0983
batch size: (889, 889)
Epoch 29, accuracy: 0.4276
batch size: (926, 926)
Epoch 30, accuracy: 0.4346
Epoch 30, Train Loss: 1.0981, Val Loss: 1.0983
batch size: (915, 915)
Epoch 31, accuracy: 0.4295
batch size: (926, 926)
Epoch 32, accuracy: 0.4299
Epoch 32, Train Loss: 1.1015, Val Loss: 1.0983
batch size: (883, 883)
Epoch 33, accuracy: 0.4299
batch size: (892, 892)
Epoch 34, accuracy: 0.4329
Epoch 34, Train Loss: 1.1100, Val Loss: 1.0983
batch size: (910, 910)
Epoch 35, accuracy: 0.4307
batch size: (884, 884)
Epoch 36, accuracy: 0.4297
Epoch 36, Train Loss: 1.1020, Val Loss: 1.0983
batch size: (897, 897)
Epoch 37, accuracy: 0.4310
batch size: (892, 892)
Epoch 38, accuracy: 0.4292
Epoch 38, Train Loss: 1.1007, Val Loss: 1.0983
batch size: (898, 898)
Epoch 39, accuracy: 0.4308
batch size: (913, 913)
Epoch 40, accuracy: 0.4300
Epoch 40, Train Loss: 1.0850, Val Loss: 1.0983
batch size: (909, 909)
Epoch 41, accuracy: 0.4255
batch size: (890, 890)
Epoch 42, accuracy: 0.4288
Epoch 42, Train Loss: 1.1009, Val Loss: 1.0983
batch size: (909, 909)
Epoch 43, accuracy: 0.4317
batch size: (900, 900)
Epoch 44, accuracy: 0.4283
Epoch 44, Train Loss: 1.0911, Val Loss: 1.0983
batch size: (906, 906)
Epoch 45, accuracy: 0.4293
batch size: (896, 896)
Epoch 46, accuracy: 0.4295
Epoch 46, Train Loss: 1.0924, Val Loss: 1.0983
batch size: (888, 888)
Epoch 47, accuracy: 0.4277
batch size: (905, 905)
Epoch 48, accuracy: 0.4268
Epoch 48, Train Loss: 1.1048, Val Loss: 1.0983
batch size: (890, 890)
Epoch 49, accuracy: 0.4297
batch size: (883, 883)
Epoch 50, accuracy: 0.4331
Epoch 50, Train Loss: 1.1012, Val Loss: 1.0983
batch size: (905, 905)
Epoch 51, accuracy: 0.4311
batch size: (899, 899)
Epoch 52, accuracy: 0.4309
Epoch 52, Train Loss: 1.0878, Val Loss: 1.0983
batch size: (882, 882)
Epoch 53, accuracy: 0.4297
batch size: (910, 910)
Epoch 54, accuracy: 0.4330
Epoch 54, Train Loss: 1.0963, Val Loss: 1.0983
batch size: (907, 907)
Epoch 55, accuracy: 0.4345
batch size: (913, 913)
Epoch 56, accuracy: 0.4286
Epoch 56, Train Loss: 1.0991, Val Loss: 1.0983
batch size: (895, 895)
Epoch 57, accuracy: 0.4308
batch size: (906, 906)
Epoch 58, accuracy: 0.4323
Epoch 58, Train Loss: 1.0942, Val Loss: 1.0983
batch size: (900, 900)
Epoch 59, accuracy: 0.4303
batch size: (887, 887)
Epoch 60, accuracy: 0.4291
Epoch 60, Train Loss: 1.0983, Val Loss: 1.0983
batch size: (898, 898)
Epoch 61, accuracy: 0.4297
batch size: (913, 913)
Epoch 62, accuracy: 0.4295
Epoch 62, Train Loss: 1.1036, Val Loss: 1.0983
batch size: (891, 891)
Epoch 63, accuracy: 0.4295
batch size: (893, 893)
Epoch 64, accuracy: 0.4294
Epoch 64, Train Loss: 1.0971, Val Loss: 1.0983
batch size: (908, 908)
Epoch 65, accuracy: 0.4277
batch size: (896, 896)
Epoch 66, accuracy: 0.4298
Epoch 66, Train Loss: 1.0887, Val Loss: 1.0983
batch size: (901, 901)
Epoch 67, accuracy: 0.4304
batch size: (895, 895)
Epoch 68, accuracy: 0.4326
Epoch 68, Train Loss: 1.0980, Val Loss: 1.0983
batch size: (905, 905)
Epoch 69, accuracy: 0.4320
batch size: (880, 880)
Epoch 70, accuracy: 0.4265
Epoch 70, Train Loss: 1.1047, Val Loss: 1.0983
batch size: (895, 895)
Epoch 71, accuracy: 0.4329
batch size: (918, 918)
Epoch 72, accuracy: 0.4307
Epoch 72, Train Loss: 1.1116, Val Loss: 1.0983
batch size: (886, 886)
Epoch 73, accuracy: 0.4298
batch size: (893, 893)
Epoch 74, accuracy: 0.4274
Epoch 74, Train Loss: 1.1007, Val Loss: 1.0983
batch size: (894, 894)
Epoch 75, accuracy: 0.4302
batch size: (896, 896)
Epoch 76, accuracy: 0.4274
Epoch 76, Train Loss: 1.0996, Val Loss: 1.0983
batch size: (895, 895)
Epoch 77, accuracy: 0.4289
batch size: (921, 921)
Epoch 78, accuracy: 0.4286
Epoch 78, Train Loss: 1.0932, Val Loss: 1.0983
batch size: (888, 888)
Epoch 79, accuracy: 0.4313
batch size: (901, 901)
Epoch 80, accuracy: 0.4265
Epoch 80, Train Loss: 1.0957, Val Loss: 1.0983
batch size: (877, 877)
Epoch 81, accuracy: 0.4295
batch size: (886, 886)
Epoch 82, accuracy: 0.4279
Epoch 82, Train Loss: 1.1058, Val Loss: 1.0983
batch size: (898, 898)
Epoch 83, accuracy: 0.4338
batch size: (905, 905)
Epoch 84, accuracy: 0.4292
Epoch 84, Train Loss: 1.1104, Val Loss: 1.0983
batch size: (911, 911)
Epoch 85, accuracy: 0.4336
batch size: (926, 926)
Epoch 86, accuracy: 0.4283
Epoch 86, Train Loss: 1.0920, Val Loss: 1.0983
batch size: (891, 891)
Epoch 87, accuracy: 0.4304
batch size: (901, 901)
Epoch 88, accuracy: 0.4323
Epoch 88, Train Loss: 1.1037, Val Loss: 1.0983
batch size: (883, 883)
Epoch 89, accuracy: 0.4326
batch size: (903, 903)
Epoch 90, accuracy: 0.4290
Epoch 90, Train Loss: 1.1020, Val Loss: 1.0983
batch size: (888, 888)
Epoch 91, accuracy: 0.4269
batch size: (901, 901)
Epoch 92, accuracy: 0.4320
Epoch 92, Train Loss: 1.1075, Val Loss: 1.0983
batch size: (905, 905)
Epoch 93, accuracy: 0.4287
batch size: (903, 903)
Epoch 94, accuracy: 0.4286
Epoch 94, Train Loss: 1.0858, Val Loss: 1.0983
batch size: (903, 903)
Epoch 95, accuracy: 0.4279
batch size: (889, 889)
Epoch 96, accuracy: 0.4296
Epoch 96, Train Loss: 1.0955, Val Loss: 1.0983
batch size: (911, 911)
Epoch 97, accuracy: 0.4314
batch size: (909, 909)
Epoch 98, accuracy: 0.4321
Epoch 98, Train Loss: 1.1082, Val Loss: 1.0983
batch size: (904, 904)
Epoch 99, accuracy: 0.4322
batch size: (900, 900)
Epoch 100, accuracy: 0.4312
Epoch 100, Train Loss: 1.0995, Val Loss: 1.0983
batch size: (922, 922)
Epoch 101, accuracy: 0.4318
batch size: (905, 905)
Epoch 102, accuracy: 0.4271
Epoch 102, Train Loss: 1.0969, Val Loss: 1.0983
batch size: (884, 884)
Epoch 103, accuracy: 0.4298
batch size: (898, 898)
Epoch 104, accuracy: 0.4244
Epoch 104, Train Loss: 1.0983, Val Loss: 1.0983
batch size: (886, 886)
Epoch 105, accuracy: 0.4246
batch size: (889, 889)
Epoch 106, accuracy: 0.4269
Epoch 106, Train Loss: 1.1006, Val Loss: 1.0983
batch size: (880, 880)
Epoch 107, accuracy: 0.4295
batch size: (895, 895)
Epoch 108, accuracy: 0.4302
Epoch 108, Train Loss: 1.1066, Val Loss: 1.0983
batch size: (892, 892)
Epoch 109, accuracy: 0.4283
batch size: (899, 899)
Epoch 110, accuracy: 0.4328
Epoch 110, Train Loss: 1.0975, Val Loss: 1.0983
batch size: (900, 900)
Epoch 111, accuracy: 0.4281
batch size: (890, 890)
Epoch 112, accuracy: 0.4287
Epoch 112, Train Loss: 1.1048, Val Loss: 1.0983
batch size: (906, 906)
Epoch 113, accuracy: 0.4334
batch size: (908, 908)
Epoch 114, accuracy: 0.4299
Epoch 114, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (907, 907)
Epoch 115, accuracy: 0.4323
batch size: (904, 904)
Epoch 116, accuracy: 0.4304
Epoch 116, Train Loss: 1.0957, Val Loss: 1.0983
batch size: (889, 889)
Epoch 117, accuracy: 0.4293
batch size: (893, 893)
Epoch 118, accuracy: 0.4303
Epoch 118, Train Loss: 1.1161, Val Loss: 1.0983
batch size: (889, 889)
Epoch 119, accuracy: 0.4279
batch size: (910, 910)
Epoch 120, accuracy: 0.4295
Epoch 120, Train Loss: 1.1055, Val Loss: 1.0983
batch size: (917, 917)
Epoch 121, accuracy: 0.4322
batch size: (896, 896)
Epoch 122, accuracy: 0.4273
Epoch 122, Train Loss: 1.0983, Val Loss: 1.0983
batch size: (905, 905)
Epoch 123, accuracy: 0.4314
batch size: (905, 905)
Epoch 124, accuracy: 0.4277
Epoch 124, Train Loss: 1.0972, Val Loss: 1.0983
batch size: (892, 892)
Epoch 125, accuracy: 0.4279
batch size: (884, 884)
Epoch 126, accuracy: 0.4325
Epoch 126, Train Loss: 1.1023, Val Loss: 1.0983
batch size: (907, 907)
Epoch 127, accuracy: 0.4274
batch size: (884, 884)
Epoch 128, accuracy: 0.4294
Epoch 128, Train Loss: 1.0900, Val Loss: 1.0983
batch size: (899, 899)
Epoch 129, accuracy: 0.4324
batch size: (899, 899)
Epoch 130, accuracy: 0.4251
Epoch 130, Train Loss: 1.1058, Val Loss: 1.0983
batch size: (910, 910)
Epoch 131, accuracy: 0.4296
batch size: (907, 907)
Epoch 132, accuracy: 0.4297
Epoch 132, Train Loss: 1.0903, Val Loss: 1.0983
batch size: (893, 893)
Epoch 133, accuracy: 0.4298
batch size: (888, 888)
Epoch 134, accuracy: 0.4319
Epoch 134, Train Loss: 1.0948, Val Loss: 1.0983
batch size: (882, 882)
Epoch 135, accuracy: 0.4318
batch size: (886, 886)
Epoch 136, accuracy: 0.4278
Epoch 136, Train Loss: 1.0937, Val Loss: 1.0983
batch size: (905, 905)
Epoch 137, accuracy: 0.4282
batch size: (900, 900)
Epoch 138, accuracy: 0.4326
Epoch 138, Train Loss: 1.1030, Val Loss: 1.0983
batch size: (891, 891)
Epoch 139, accuracy: 0.4306
batch size: (898, 898)
Epoch 140, accuracy: 0.4297
Epoch 140, Train Loss: 1.1061, Val Loss: 1.0983
batch size: (909, 909)
Epoch 141, accuracy: 0.4315
batch size: (904, 904)
Epoch 142, accuracy: 0.4300
Epoch 142, Train Loss: 1.0924, Val Loss: 1.0983
batch size: (903, 903)
Epoch 143, accuracy: 0.4326
batch size: (898, 898)
Epoch 144, accuracy: 0.4281
Epoch 144, Train Loss: 1.0940, Val Loss: 1.0983
batch size: (894, 894)
Epoch 145, accuracy: 0.4247
batch size: (912, 912)
Epoch 146, accuracy: 0.4307
Epoch 146, Train Loss: 1.1049, Val Loss: 1.0983
batch size: (900, 900)
Epoch 147, accuracy: 0.4296
batch size: (911, 911)
Epoch 148, accuracy: 0.4285
Epoch 148, Train Loss: 1.0979, Val Loss: 1.0983
batch size: (910, 910)
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 149, accuracy: 0.4324
batch size: (903, 903)
Epoch 150, accuracy: 0.4326
Epoch 150, Train Loss: 1.1092, Val Loss: 1.0983
batch size: (916, 916)
Epoch 151, accuracy: 0.4293
batch size: (885, 885)
Epoch 152, accuracy: 0.4292
Epoch 152, Train Loss: 1.1171, Val Loss: 1.0983
batch size: (898, 898)
Epoch 153, accuracy: 0.4304
batch size: (896, 896)
Epoch 154, accuracy: 0.4300
Epoch 154, Train Loss: 1.0975, Val Loss: 1.0983
batch size: (913, 913)
Epoch 155, accuracy: 0.4294
batch size: (906, 906)
Epoch 156, accuracy: 0.4319
Epoch 156, Train Loss: 1.1030, Val Loss: 1.0983
batch size: (873, 873)
Epoch 157, accuracy: 0.4269
batch size: (904, 904)
Epoch 158, accuracy: 0.4286
Epoch 158, Train Loss: 1.0969, Val Loss: 1.0983
batch size: (880, 880)
Epoch 159, accuracy: 0.4313
batch size: (899, 899)
Epoch 160, accuracy: 0.4313
Epoch 160, Train Loss: 1.1042, Val Loss: 1.0983
batch size: (906, 906)
Epoch 161, accuracy: 0.4325
batch size: (900, 900)
Epoch 162, accuracy: 0.4316
Epoch 162, Train Loss: 1.1013, Val Loss: 1.0983
batch size: (920, 920)
Epoch 163, accuracy: 0.4306
batch size: (908, 908)
Epoch 164, accuracy: 0.4329
Epoch 164, Train Loss: 1.0914, Val Loss: 1.0983
batch size: (883, 883)
Epoch 165, accuracy: 0.4308
batch size: (913, 913)
Epoch 166, accuracy: 0.4334
Epoch 166, Train Loss: 1.1063, Val Loss: 1.0983
batch size: (876, 876)
Epoch 167, accuracy: 0.4312
batch size: (888, 888)
Epoch 168, accuracy: 0.4281
Epoch 168, Train Loss: 1.0903, Val Loss: 1.0983
batch size: (895, 895)
Epoch 169, accuracy: 0.4315
batch size: (910, 910)
Epoch 170, accuracy: 0.4319
Epoch 170, Train Loss: 1.0947, Val Loss: 1.0983
batch size: (923, 923)
Epoch 171, accuracy: 0.4270
batch size: (913, 913)
Epoch 172, accuracy: 0.4289
Epoch 172, Train Loss: 1.1074, Val Loss: 1.0983
batch size: (885, 885)
Epoch 173, accuracy: 0.4333
batch size: (902, 902)
Epoch 174, accuracy: 0.4302
Epoch 174, Train Loss: 1.0893, Val Loss: 1.0983
batch size: (895, 895)
Epoch 175, accuracy: 0.4268
batch size: (894, 894)
Epoch 176, accuracy: 0.4320
Epoch 176, Train Loss: 1.0906, Val Loss: 1.0983
batch size: (898, 898)
Epoch 177, accuracy: 0.4349
batch size: (910, 910)
Epoch 178, accuracy: 0.4307
Epoch 178, Train Loss: 1.1075, Val Loss: 1.0983
batch size: (890, 890)
Epoch 179, accuracy: 0.4323
batch size: (910, 910)
Epoch 180, accuracy: 0.4338
Epoch 180, Train Loss: 1.0910, Val Loss: 1.0983
batch size: (902, 902)
Epoch 181, accuracy: 0.4312
batch size: (906, 906)
Epoch 182, accuracy: 0.4314
Epoch 182, Train Loss: 1.1003, Val Loss: 1.0983
batch size: (882, 882)
Epoch 183, accuracy: 0.4293
batch size: (899, 899)
Epoch 184, accuracy: 0.4309
Epoch 184, Train Loss: 1.1049, Val Loss: 1.0983
batch size: (899, 899)
Epoch 185, accuracy: 0.4322
batch size: (905, 905)
Epoch 186, accuracy: 0.4294
Epoch 186, Train Loss: 1.0956, Val Loss: 1.0983
batch size: (898, 898)
Epoch 187, accuracy: 0.4284
batch size: (888, 888)
Epoch 188, accuracy: 0.4341
Epoch 188, Train Loss: 1.1030, Val Loss: 1.0983
batch size: (892, 892)
Epoch 189, accuracy: 0.4318
batch size: (897, 897)
Epoch 190, accuracy: 0.4290
Epoch 190, Train Loss: 1.0922, Val Loss: 1.0983
batch size: (883, 883)
Epoch 191, accuracy: 0.4318
batch size: (887, 887)
Epoch 192, accuracy: 0.4294
Epoch 192, Train Loss: 1.0988, Val Loss: 1.0983
batch size: (904, 904)
Epoch 193, accuracy: 0.4313
batch size: (888, 888)
Epoch 194, accuracy: 0.4286
Epoch 194, Train Loss: 1.0941, Val Loss: 1.0983
batch size: (911, 911)
Epoch 195, accuracy: 0.4304
batch size: (893, 893)
Epoch 196, accuracy: 0.4309
Epoch 196, Train Loss: 1.0991, Val Loss: 1.0983
batch size: (881, 881)
Epoch 197, accuracy: 0.4297
batch size: (910, 910)
Epoch 198, accuracy: 0.4318
Epoch 198, Train Loss: 1.1028, Val Loss: 1.0983
batch size: (902, 902)
Epoch 199, accuracy: 0.4312
Loaded best model with val_loss = 1.078541874885559
test :accuracy 0.4271, f1_macro: 0.1995, f1_micro: 0.4271, auc: 0.4957
Training JKNet with 2 layers...
可训练参数: 391942_JKNet
不可训练参数: 0
batch size: (902, 902)
✅ Epoch 0: New best model saved with val_loss = 1.0940
Epoch 0, accuracy: 0.4427
Epoch 0, Train Loss: 1.2877, Val Loss: 1.0940
batch size: (913, 913)
✅ Epoch 1: New best model saved with val_loss = 1.0899
Epoch 1, accuracy: 0.4485
batch size: (897, 897)
✅ Epoch 2: New best model saved with val_loss = 1.0795
Epoch 2, accuracy: 0.4430
Epoch 2, Train Loss: 0.1210, Val Loss: 1.0795
batch size: (906, 906)
✅ Epoch 3: New best model saved with val_loss = 1.0677
Epoch 3, accuracy: 0.4369
batch size: (882, 882)
✅ Epoch 4: New best model saved with val_loss = 1.0543
Epoch 4, accuracy: 0.4414
Epoch 4, Train Loss: 0.0098, Val Loss: 1.0543
batch size: (887, 887)
✅ Epoch 5: New best model saved with val_loss = 1.0402
Epoch 5, accuracy: 0.4411
batch size: (899, 899)
✅ Epoch 6: New best model saved with val_loss = 1.0270
Epoch 6, accuracy: 0.4386
Epoch 6, Train Loss: 0.0031, Val Loss: 1.0270
batch size: (889, 889)
✅ Epoch 7: New best model saved with val_loss = 1.0152
Epoch 7, accuracy: 0.4402
batch size: (918, 918)
✅ Epoch 8: New best model saved with val_loss = 1.0065
Epoch 8, accuracy: 0.4412
Epoch 8, Train Loss: 0.0006, Val Loss: 1.0065
batch size: (895, 895)
✅ Epoch 9: New best model saved with val_loss = 0.9983
Epoch 9, accuracy: 0.4573
batch size: (905, 905)
✅ Epoch 10: New best model saved with val_loss = 0.9927
Epoch 10, accuracy: 0.4594
Epoch 10, Train Loss: 0.0005, Val Loss: 0.9927
batch size: (895, 895)
✅ Epoch 11: New best model saved with val_loss = 0.9878
Epoch 11, accuracy: 0.4663
batch size: (908, 908)
✅ Epoch 12: New best model saved with val_loss = 0.9846
Epoch 12, accuracy: 0.4673
Epoch 12, Train Loss: 0.0004, Val Loss: 0.9846
batch size: (906, 906)
✅ Epoch 13: New best model saved with val_loss = 0.9825
Epoch 13, accuracy: 0.4650
batch size: (896, 896)
✅ Epoch 14: New best model saved with val_loss = 0.9803
Epoch 14, accuracy: 0.4665
Epoch 14, Train Loss: 0.0002, Val Loss: 0.9803
batch size: (899, 899)
✅ Epoch 15: New best model saved with val_loss = 0.9798
Epoch 15, accuracy: 0.4596
batch size: (913, 913)
Epoch 16, accuracy: 0.4676
Epoch 16, Train Loss: 0.0004, Val Loss: 0.9800
batch size: (890, 890)
Epoch 17, accuracy: 0.4557
batch size: (911, 911)
Epoch 18, accuracy: 0.4609
Epoch 18, Train Loss: 0.0004, Val Loss: 0.9835
batch size: (891, 891)
Epoch 19, accuracy: 0.4583
batch size: (887, 887)
Epoch 20, accuracy: 0.4569
Epoch 20, Train Loss: 0.0001, Val Loss: 0.9851
batch size: (888, 888)
Epoch 21, accuracy: 0.4577
batch size: (909, 909)
Epoch 22, accuracy: 0.4585
Epoch 22, Train Loss: 0.0001, Val Loss: 0.9870
batch size: (873, 873)
Epoch 23, accuracy: 0.4562
batch size: (895, 895)
Epoch 24, accuracy: 0.4574
Epoch 24, Train Loss: 0.0031, Val Loss: 0.9871
batch size: (914, 914)
Epoch 25, accuracy: 0.4658
batch size: (903, 903)
Epoch 26, accuracy: 0.4610
Epoch 26, Train Loss: 0.0000, Val Loss: 0.9882
batch size: (910, 910)
Epoch 27, accuracy: 0.4702
batch size: (888, 888)
Epoch 28, accuracy: 0.4694
Epoch 28, Train Loss: 0.0001, Val Loss: 0.9885
batch size: (904, 904)
Epoch 29, accuracy: 0.4644
batch size: (910, 910)
Epoch 30, accuracy: 0.4736
Epoch 30, Train Loss: 0.0000, Val Loss: 0.9894
batch size: (880, 880)
Epoch 31, accuracy: 0.4702
batch size: (892, 892)
Epoch 32, accuracy: 0.4708
Epoch 32, Train Loss: 0.0002, Val Loss: 0.9885
batch size: (886, 886)
Epoch 33, accuracy: 0.4707
batch size: (889, 889)
Epoch 34, accuracy: 0.4668
Epoch 34, Train Loss: 0.0013, Val Loss: 0.9878
batch size: (892, 892)
Epoch 35, accuracy: 0.4662
batch size: (865, 865)
Epoch 36, accuracy: 0.4620
Epoch 36, Train Loss: 0.0000, Val Loss: 0.9879
batch size: (902, 902)
Epoch 37, accuracy: 0.4633
batch size: (901, 901)
Epoch 38, accuracy: 0.4663
Epoch 38, Train Loss: 0.0001, Val Loss: 0.9887
batch size: (893, 893)
Epoch 39, accuracy: 0.4750
batch size: (901, 901)
Epoch 40, accuracy: 0.4775
Epoch 40, Train Loss: 0.0000, Val Loss: 0.9887
batch size: (900, 900)
Epoch 41, accuracy: 0.4793
batch size: (909, 909)
Epoch 42, accuracy: 0.4681
Epoch 42, Train Loss: 0.0000, Val Loss: 0.9899
batch size: (912, 912)
Epoch 43, accuracy: 0.4735
batch size: (917, 917)
Epoch 44, accuracy: 0.4799
Epoch 44, Train Loss: 0.0000, Val Loss: 0.9897
batch size: (909, 909)
Epoch 45, accuracy: 0.4729
batch size: (909, 909)
Epoch 46, accuracy: 0.4984
Epoch 46, Train Loss: 0.0002, Val Loss: 0.9895
batch size: (907, 907)
Epoch 47, accuracy: 0.4985
batch size: (893, 893)
Epoch 48, accuracy: 0.5046
Epoch 48, Train Loss: 0.0000, Val Loss: 0.9891
batch size: (895, 895)
Epoch 49, accuracy: 0.4995
batch size: (932, 932)
Epoch 50, accuracy: 0.4835
Epoch 50, Train Loss: 0.0001, Val Loss: 0.9890
batch size: (902, 902)
Epoch 51, accuracy: 0.4797
batch size: (880, 880)
Epoch 52, accuracy: 0.4798
Epoch 52, Train Loss: 0.0001, Val Loss: 0.9886
batch size: (876, 876)
Epoch 53, accuracy: 0.4748
batch size: (885, 885)
Epoch 54, accuracy: 0.4807
Epoch 54, Train Loss: 0.0000, Val Loss: 0.9880
batch size: (902, 902)
Epoch 55, accuracy: 0.4866
batch size: (910, 910)
Epoch 56, accuracy: 0.4899
Epoch 56, Train Loss: 0.0001, Val Loss: 0.9891
batch size: (895, 895)
Epoch 57, accuracy: 0.4937
batch size: (911, 911)
Epoch 58, accuracy: 0.5006
Epoch 58, Train Loss: 0.0000, Val Loss: 0.9896
batch size: (887, 887)
Epoch 59, accuracy: 0.4934
batch size: (895, 895)
Epoch 60, accuracy: 0.4937
Epoch 60, Train Loss: 0.0003, Val Loss: 0.9891
batch size: (889, 889)
Epoch 61, accuracy: 0.4851
batch size: (882, 882)
Epoch 62, accuracy: 0.4593
Epoch 62, Train Loss: 0.0001, Val Loss: 0.9890
batch size: (896, 896)
Epoch 63, accuracy: 0.4713
batch size: (896, 896)
Epoch 64, accuracy: 0.4701
Epoch 64, Train Loss: 0.0000, Val Loss: 0.9879
batch size: (911, 911)
Epoch 65, accuracy: 0.4736
batch size: (898, 898)
Epoch 66, accuracy: 0.4703
Epoch 66, Train Loss: 0.0001, Val Loss: 0.9892
batch size: (889, 889)
Epoch 67, accuracy: 0.4755
batch size: (885, 885)
Epoch 68, accuracy: 0.4664
Epoch 68, Train Loss: 0.0000, Val Loss: 0.9887
batch size: (888, 888)
Epoch 69, accuracy: 0.4739
batch size: (907, 907)
Epoch 70, accuracy: 0.4853
Epoch 70, Train Loss: 0.0001, Val Loss: 0.9876
batch size: (897, 897)
Epoch 71, accuracy: 0.4919
batch size: (899, 899)
Epoch 72, accuracy: 0.4932
Epoch 72, Train Loss: 0.0005, Val Loss: 0.9878
batch size: (892, 892)
Epoch 73, accuracy: 0.4850
batch size: (889, 889)
Epoch 74, accuracy: 0.4875
Epoch 74, Train Loss: 0.0042, Val Loss: 0.9890
batch size: (902, 902)
Epoch 75, accuracy: 0.4912
batch size: (888, 888)
Epoch 76, accuracy: 0.4922
Epoch 76, Train Loss: 0.0001, Val Loss: 0.9883
batch size: (911, 911)
Epoch 77, accuracy: 0.4828
batch size: (888, 888)
Epoch 78, accuracy: 0.4771
Epoch 78, Train Loss: 0.0002, Val Loss: 0.9876
batch size: (898, 898)
Epoch 79, accuracy: 0.4778
batch size: (894, 894)
Epoch 80, accuracy: 0.4826
Epoch 80, Train Loss: 0.0000, Val Loss: 0.9900
batch size: (893, 893)
Epoch 81, accuracy: 0.4876
batch size: (889, 889)
Epoch 82, accuracy: 0.4781
Epoch 82, Train Loss: 0.0001, Val Loss: 0.9899
batch size: (921, 921)
Epoch 83, accuracy: 0.4632
batch size: (899, 899)
Epoch 84, accuracy: 0.4643
Epoch 84, Train Loss: 0.0000, Val Loss: 0.9920
batch size: (906, 906)
Epoch 85, accuracy: 0.4806
batch size: (888, 888)
Epoch 86, accuracy: 0.4798
Epoch 86, Train Loss: 0.0001, Val Loss: 0.9899
batch size: (918, 918)
Epoch 87, accuracy: 0.4842
batch size: (890, 890)
Epoch 88, accuracy: 0.4700
Epoch 88, Train Loss: 0.0001, Val Loss: 0.9898
batch size: (893, 893)
Epoch 89, accuracy: 0.4661
batch size: (897, 897)
Epoch 90, accuracy: 0.4669
Epoch 90, Train Loss: 0.0003, Val Loss: 0.9907
batch size: (886, 886)
Epoch 91, accuracy: 0.4633
batch size: (906, 906)
Epoch 92, accuracy: 0.4744
Epoch 92, Train Loss: 0.0001, Val Loss: 0.9906
batch size: (903, 903)
Epoch 93, accuracy: 0.4692
batch size: (895, 895)
Epoch 94, accuracy: 0.4818
Epoch 94, Train Loss: 0.0000, Val Loss: 0.9920
batch size: (890, 890)
Epoch 95, accuracy: 0.4802
batch size: (903, 903)
Epoch 96, accuracy: 0.4844
Epoch 96, Train Loss: 0.0002, Val Loss: 0.9903
batch size: (924, 924)
Epoch 97, accuracy: 0.4833
batch size: (885, 885)
Epoch 98, accuracy: 0.4781
Epoch 98, Train Loss: 0.0002, Val Loss: 0.9906
batch size: (890, 890)
Epoch 99, accuracy: 0.4719
batch size: (900, 900)
Epoch 100, accuracy: 0.4747
Epoch 100, Train Loss: 0.0000, Val Loss: 0.9914
batch size: (910, 910)
Epoch 101, accuracy: 0.4762
batch size: (894, 894)
Epoch 102, accuracy: 0.4803
Epoch 102, Train Loss: 0.0001, Val Loss: 0.9910
batch size: (903, 903)
Epoch 103, accuracy: 0.4788
batch size: (869, 869)
Epoch 104, accuracy: 0.4843
Epoch 104, Train Loss: 0.0001, Val Loss: 0.9887
batch size: (899, 899)
Epoch 105, accuracy: 0.4905
batch size: (912, 912)
Epoch 106, accuracy: 0.4893
Epoch 106, Train Loss: 0.0000, Val Loss: 0.9887
batch size: (898, 898)
Epoch 107, accuracy: 0.4884
batch size: (903, 903)
Epoch 108, accuracy: 0.4949
Epoch 108, Train Loss: 0.0001, Val Loss: 0.9880
batch size: (924, 924)
Epoch 109, accuracy: 0.4884
batch size: (911, 911)
Epoch 110, accuracy: 0.4865
Epoch 110, Train Loss: 0.0002, Val Loss: 0.9903
batch size: (899, 899)
Epoch 111, accuracy: 0.4974
batch size: (910, 910)
Epoch 112, accuracy: 0.4831
Epoch 112, Train Loss: 0.0000, Val Loss: 0.9891
batch size: (901, 901)
Epoch 113, accuracy: 0.4774
batch size: (892, 892)
Epoch 114, accuracy: 0.4840
Epoch 114, Train Loss: 0.0000, Val Loss: 0.9895
batch size: (914, 914)
Epoch 115, accuracy: 0.4769
batch size: (900, 900)
Epoch 116, accuracy: 0.4829
Epoch 116, Train Loss: 0.0001, Val Loss: 0.9904
batch size: (900, 900)
Epoch 117, accuracy: 0.4718
batch size: (906, 906)
Epoch 118, accuracy: 0.4704
Epoch 118, Train Loss: 0.0002, Val Loss: 0.9908
batch size: (897, 897)
Epoch 119, accuracy: 0.4632
batch size: (906, 906)
Epoch 120, accuracy: 0.4667
Epoch 120, Train Loss: 0.0002, Val Loss: 0.9896
batch size: (877, 877)
Epoch 121, accuracy: 0.4671
batch size: (894, 894)
Epoch 122, accuracy: 0.4628
Epoch 122, Train Loss: 0.0001, Val Loss: 0.9892
batch size: (898, 898)
Epoch 123, accuracy: 0.4711
batch size: (901, 901)
Epoch 124, accuracy: 0.4735
Epoch 124, Train Loss: 0.0000, Val Loss: 0.9889
batch size: (903, 903)
Epoch 125, accuracy: 0.4765
batch size: (877, 877)
Epoch 126, accuracy: 0.4762
Epoch 126, Train Loss: 0.0000, Val Loss: 0.9878
batch size: (892, 892)
Epoch 127, accuracy: 0.4827
batch size: (904, 904)
Epoch 128, accuracy: 0.4819
Epoch 128, Train Loss: 0.0001, Val Loss: 0.9873
batch size: (882, 882)
Epoch 129, accuracy: 0.4749
batch size: (889, 889)
Epoch 130, accuracy: 0.4721
Epoch 130, Train Loss: 0.0000, Val Loss: 0.9873
batch size: (898, 898)
Epoch 131, accuracy: 0.4810
batch size: (917, 917)
Epoch 132, accuracy: 0.4794
Epoch 132, Train Loss: 0.0000, Val Loss: 0.9879
batch size: (907, 907)
Epoch 133, accuracy: 0.4976
batch size: (900, 900)
Epoch 134, accuracy: 0.4939
Epoch 134, Train Loss: 0.0001, Val Loss: 0.9887
batch size: (907, 907)
Epoch 135, accuracy: 0.4901
batch size: (909, 909)
Epoch 136, accuracy: 0.4895
Epoch 136, Train Loss: 0.0001, Val Loss: 0.9878
batch size: (904, 904)
Epoch 137, accuracy: 0.4931
batch size: (894, 894)
Epoch 138, accuracy: 0.4777
Epoch 138, Train Loss: 0.0002, Val Loss: 0.9890
batch size: (918, 918)
Epoch 139, accuracy: 0.4959
batch size: (888, 888)
Epoch 140, accuracy: 0.4806
Epoch 140, Train Loss: 0.0000, Val Loss: 0.9891
batch size: (907, 907)
Epoch 141, accuracy: 0.4745
batch size: (916, 916)
Epoch 142, accuracy: 0.4810
Epoch 142, Train Loss: 0.0000, Val Loss: 0.9889
batch size: (879, 879)
Epoch 143, accuracy: 0.4758
batch size: (915, 915)
Epoch 144, accuracy: 0.4797
Epoch 144, Train Loss: 0.0001, Val Loss: 0.9900
batch size: (912, 912)
Epoch 145, accuracy: 0.4761
batch size: (913, 913)
Epoch 146, accuracy: 0.4714
Epoch 146, Train Loss: 0.0007, Val Loss: 0.9902
batch size: (883, 883)
Epoch 147, accuracy: 0.4840
batch size: (893, 893)
Epoch 148, accuracy: 0.4755
Epoch 148, Train Loss: 0.0000, Val Loss: 0.9886
batch size: (890, 890)
Epoch 149, accuracy: 0.4787
batch size: (896, 896)
Epoch 150, accuracy: 0.4733
Epoch 150, Train Loss: 0.0001, Val Loss: 0.9873
batch size: (900, 900)
Epoch 151, accuracy: 0.4673
batch size: (900, 900)
Epoch 152, accuracy: 0.4715
Epoch 152, Train Loss: 0.0001, Val Loss: 0.9872
batch size: (884, 884)
Epoch 153, accuracy: 0.4705
batch size: (910, 910)
Epoch 154, accuracy: 0.4761
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 154, Train Loss: 0.0001, Val Loss: 0.9891
batch size: (908, 908)
Epoch 155, accuracy: 0.4814
batch size: (879, 879)
Epoch 156, accuracy: 0.4766
Epoch 156, Train Loss: 0.0000, Val Loss: 0.9894
batch size: (899, 899)
Epoch 157, accuracy: 0.4815
batch size: (881, 881)
Epoch 158, accuracy: 0.4779
Epoch 158, Train Loss: 0.0000, Val Loss: 0.9885
batch size: (898, 898)
Epoch 159, accuracy: 0.4855
batch size: (899, 899)
Epoch 160, accuracy: 0.5062
Epoch 160, Train Loss: 0.0000, Val Loss: 0.9884
batch size: (902, 902)
Epoch 161, accuracy: 0.4981
batch size: (912, 912)
Epoch 162, accuracy: 0.5019
Epoch 162, Train Loss: 0.0000, Val Loss: 0.9889
batch size: (913, 913)
Epoch 163, accuracy: 0.5171
batch size: (914, 914)
Epoch 164, accuracy: 0.5030
Epoch 164, Train Loss: 0.0000, Val Loss: 0.9891
batch size: (889, 889)
Epoch 165, accuracy: 0.4921
batch size: (885, 885)
Epoch 166, accuracy: 0.4907
Epoch 166, Train Loss: 0.0001, Val Loss: 0.9895
batch size: (892, 892)
Epoch 167, accuracy: 0.4853
batch size: (888, 888)
Epoch 168, accuracy: 0.4848
Epoch 168, Train Loss: 0.0000, Val Loss: 0.9908
batch size: (877, 877)
Epoch 169, accuracy: 0.4837
batch size: (893, 893)
Epoch 170, accuracy: 0.4894
Epoch 170, Train Loss: 0.0000, Val Loss: 0.9903
batch size: (899, 899)
Epoch 171, accuracy: 0.4903
batch size: (878, 878)
Epoch 172, accuracy: 0.4801
Epoch 172, Train Loss: 0.0000, Val Loss: 0.9902
batch size: (893, 893)
Epoch 173, accuracy: 0.4746
batch size: (897, 897)
Epoch 174, accuracy: 0.4844
Epoch 174, Train Loss: 0.0000, Val Loss: 0.9898
batch size: (886, 886)
Epoch 175, accuracy: 0.4738
batch size: (911, 911)
Epoch 176, accuracy: 0.4667
Epoch 176, Train Loss: 0.0000, Val Loss: 0.9901
batch size: (906, 906)
Epoch 177, accuracy: 0.4661
batch size: (885, 885)
Epoch 178, accuracy: 0.4693
Epoch 178, Train Loss: 0.0001, Val Loss: 0.9909
batch size: (896, 896)
Epoch 179, accuracy: 0.4716
batch size: (899, 899)
Epoch 180, accuracy: 0.4764
Epoch 180, Train Loss: 0.0001, Val Loss: 0.9921
batch size: (889, 889)
Epoch 181, accuracy: 0.4739
batch size: (891, 891)
Epoch 182, accuracy: 0.4696
Epoch 182, Train Loss: 0.0000, Val Loss: 0.9913
batch size: (889, 889)
Epoch 183, accuracy: 0.4859
batch size: (905, 905)
Epoch 184, accuracy: 0.4848
Epoch 184, Train Loss: 0.0000, Val Loss: 0.9924
batch size: (908, 908)
Epoch 185, accuracy: 0.4801
batch size: (897, 897)
Epoch 186, accuracy: 0.4869
Epoch 186, Train Loss: 0.0000, Val Loss: 0.9917
batch size: (899, 899)
Epoch 187, accuracy: 0.4985
batch size: (884, 884)
Epoch 188, accuracy: 0.4989
Epoch 188, Train Loss: 0.0001, Val Loss: 0.9919
batch size: (904, 904)
Epoch 189, accuracy: 0.4950
batch size: (906, 906)
Epoch 190, accuracy: 0.4792
Epoch 190, Train Loss: 0.0019, Val Loss: 0.9918
batch size: (915, 915)
Epoch 191, accuracy: 0.4887
batch size: (896, 896)
Epoch 192, accuracy: 0.4759
Epoch 192, Train Loss: 0.0000, Val Loss: 0.9911
batch size: (901, 901)
Epoch 193, accuracy: 0.4728
batch size: (886, 886)
Epoch 194, accuracy: 0.4730
Epoch 194, Train Loss: 0.0001, Val Loss: 0.9904
batch size: (902, 902)
Epoch 195, accuracy: 0.4832
batch size: (891, 891)
Epoch 196, accuracy: 0.4759
Epoch 196, Train Loss: 0.0000, Val Loss: 0.9912
batch size: (907, 907)
Epoch 197, accuracy: 0.4698
batch size: (900, 900)
Epoch 198, accuracy: 0.4813
Epoch 198, Train Loss: 0.0000, Val Loss: 0.9918
batch size: (884, 884)
Epoch 199, accuracy: 0.4828
Loaded best model with val_loss = 0.9798100590705872
test :accuracy 0.4588, f1_macro: 0.2820, f1_micro: 0.4588, auc: 0.8332
Training JKNet with 8 layers...
可训练参数: 1184518_JKNet
不可训练参数: 0
batch size: (887, 887)
✅ Epoch 0: New best model saved with val_loss = 1.1215
Epoch 0, accuracy: 0.4289
Epoch 0, Train Loss: 1.2272, Val Loss: 1.1215
batch size: (888, 888)
Epoch 1, accuracy: 0.1676
batch size: (904, 904)
Epoch 2, accuracy: 0.3987
Epoch 2, Train Loss: 2.6427, Val Loss: 1.5513
batch size: (905, 905)
Epoch 3, accuracy: 0.4223
batch size: (888, 888)
Epoch 4, accuracy: 0.4114
Epoch 4, Train Loss: 0.6416, Val Loss: 1.1756
batch size: (902, 902)
Epoch 5, accuracy: 0.4301
batch size: (910, 910)
Epoch 6, accuracy: 0.3756
Epoch 6, Train Loss: 0.5529, Val Loss: 1.8346
batch size: (913, 913)
Epoch 7, accuracy: 0.3769
batch size: (908, 908)
Epoch 8, accuracy: 0.3713
Epoch 8, Train Loss: 0.3481, Val Loss: 1.4420
batch size: (912, 912)
Epoch 9, accuracy: 0.3722
batch size: (882, 882)
Epoch 10, accuracy: 0.3747
Epoch 10, Train Loss: 0.2473, Val Loss: 1.3772
batch size: (904, 904)
Epoch 11, accuracy: 0.3699
batch size: (892, 892)
Epoch 12, accuracy: 0.3738
Epoch 12, Train Loss: 0.1993, Val Loss: 1.4301
batch size: (896, 896)
Epoch 13, accuracy: 0.3723
batch size: (895, 895)
Epoch 14, accuracy: 0.3747
Epoch 14, Train Loss: 0.2005, Val Loss: 1.3972
batch size: (899, 899)
Epoch 15, accuracy: 0.3730
batch size: (898, 898)
Epoch 16, accuracy: 0.3771
Epoch 16, Train Loss: 0.1544, Val Loss: 1.3062
batch size: (892, 892)
Epoch 17, accuracy: 0.3717
batch size: (908, 908)
Epoch 18, accuracy: 0.3732
Epoch 18, Train Loss: 0.1359, Val Loss: 1.3101
batch size: (924, 924)
Epoch 19, accuracy: 0.3734
batch size: (885, 885)
Epoch 20, accuracy: 0.3756
Epoch 20, Train Loss: 0.1132, Val Loss: 1.2841
batch size: (916, 916)
Epoch 21, accuracy: 0.3745
batch size: (897, 897)
Epoch 22, accuracy: 0.3727
Epoch 22, Train Loss: 0.1384, Val Loss: 1.2885
batch size: (898, 898)
Epoch 23, accuracy: 0.3742
batch size: (900, 900)
Epoch 24, accuracy: 0.3722
Epoch 24, Train Loss: 0.1610, Val Loss: 1.2970
batch size: (898, 898)
Epoch 25, accuracy: 0.3749
batch size: (888, 888)
Epoch 26, accuracy: 0.3733
Epoch 26, Train Loss: 0.1317, Val Loss: 1.3296
batch size: (903, 903)
Epoch 27, accuracy: 0.3743
batch size: (888, 888)
Epoch 28, accuracy: 0.3749
Epoch 28, Train Loss: 0.1714, Val Loss: 1.3635
batch size: (908, 908)
Epoch 29, accuracy: 0.3714
batch size: (899, 899)
Epoch 30, accuracy: 0.3740
Epoch 30, Train Loss: 0.1446, Val Loss: 1.3237
batch size: (904, 904)
Epoch 31, accuracy: 0.3680
batch size: (905, 905)
Epoch 32, accuracy: 0.3728
Epoch 32, Train Loss: 0.1310, Val Loss: 1.3164
batch size: (919, 919)
Epoch 33, accuracy: 0.3735
batch size: (900, 900)
Epoch 34, accuracy: 0.3726
Epoch 34, Train Loss: 0.1052, Val Loss: 1.3309
batch size: (884, 884)
Epoch 35, accuracy: 0.3758
batch size: (908, 908)
Epoch 36, accuracy: 0.3743
Epoch 36, Train Loss: 0.2259, Val Loss: 1.3186
batch size: (894, 894)
Epoch 37, accuracy: 0.3726
batch size: (890, 890)
Epoch 38, accuracy: 0.3705
Epoch 38, Train Loss: 0.1310, Val Loss: 1.3402
batch size: (903, 903)
Epoch 39, accuracy: 0.3718
batch size: (902, 902)
Epoch 40, accuracy: 0.3727
Epoch 40, Train Loss: 0.1119, Val Loss: 1.3487
batch size: (910, 910)
Epoch 41, accuracy: 0.3729
batch size: (901, 901)
Epoch 42, accuracy: 0.3769
Epoch 42, Train Loss: 0.1343, Val Loss: 1.2962
batch size: (902, 902)
Epoch 43, accuracy: 0.3775
batch size: (892, 892)
Epoch 44, accuracy: 0.3736
Epoch 44, Train Loss: 0.1381, Val Loss: 1.3119
batch size: (905, 905)
Epoch 45, accuracy: 0.3727
batch size: (893, 893)
Epoch 46, accuracy: 0.3760
Epoch 46, Train Loss: 0.1298, Val Loss: 1.3376
batch size: (902, 902)
Epoch 47, accuracy: 0.3733
batch size: (881, 881)
Epoch 48, accuracy: 0.3725
Epoch 48, Train Loss: 0.2248, Val Loss: 1.3559
batch size: (896, 896)
Epoch 49, accuracy: 0.3743
batch size: (915, 915)
Epoch 50, accuracy: 0.3734
Epoch 50, Train Loss: 0.1052, Val Loss: 1.3727
batch size: (903, 903)
Epoch 51, accuracy: 0.3752
batch size: (885, 885)
Epoch 52, accuracy: 0.3716
Epoch 52, Train Loss: 0.1074, Val Loss: 1.3285
batch size: (898, 898)
Epoch 53, accuracy: 0.3720
batch size: (888, 888)
Epoch 54, accuracy: 0.3737
Epoch 54, Train Loss: 0.1604, Val Loss: 1.3459
batch size: (912, 912)
Epoch 55, accuracy: 0.3751
batch size: (887, 887)
Epoch 56, accuracy: 0.3744
Epoch 56, Train Loss: 0.1864, Val Loss: 1.2960
batch size: (890, 890)
Epoch 57, accuracy: 0.3717
batch size: (921, 921)
Epoch 58, accuracy: 0.3738
Epoch 58, Train Loss: 0.1147, Val Loss: 1.2979
batch size: (899, 899)
Epoch 59, accuracy: 0.3754
batch size: (909, 909)
Epoch 60, accuracy: 0.3721
Epoch 60, Train Loss: 0.0895, Val Loss: 1.3266
batch size: (899, 899)
Epoch 61, accuracy: 0.3691
batch size: (901, 901)
Epoch 62, accuracy: 0.3747
Epoch 62, Train Loss: 0.1237, Val Loss: 1.3629
batch size: (909, 909)
Epoch 63, accuracy: 0.3728
batch size: (914, 914)
Epoch 64, accuracy: 0.3735
Epoch 64, Train Loss: 0.1404, Val Loss: 1.3370
batch size: (905, 905)
Epoch 65, accuracy: 0.3748
batch size: (888, 888)
Epoch 66, accuracy: 0.3749
Epoch 66, Train Loss: 0.2074, Val Loss: 1.3420
batch size: (908, 908)
Epoch 67, accuracy: 0.3731
batch size: (899, 899)
Epoch 68, accuracy: 0.3743
Epoch 68, Train Loss: 0.1512, Val Loss: 1.3575
batch size: (912, 912)
Epoch 69, accuracy: 0.3733
batch size: (899, 899)
Epoch 70, accuracy: 0.3724
Epoch 70, Train Loss: 0.1299, Val Loss: 1.3602
batch size: (888, 888)
Epoch 71, accuracy: 0.3742
batch size: (901, 901)
Epoch 72, accuracy: 0.3706
Epoch 72, Train Loss: 0.1343, Val Loss: 1.3419
batch size: (913, 913)
Epoch 73, accuracy: 0.3766
batch size: (909, 909)
Epoch 74, accuracy: 0.3708
Epoch 74, Train Loss: 0.1027, Val Loss: 1.3155
batch size: (900, 900)
Epoch 75, accuracy: 0.3733
batch size: (897, 897)
Epoch 76, accuracy: 0.3735
Epoch 76, Train Loss: 0.1983, Val Loss: 1.3535
batch size: (907, 907)
Epoch 77, accuracy: 0.3756
batch size: (896, 896)
Epoch 78, accuracy: 0.3709
Epoch 78, Train Loss: 0.2296, Val Loss: 1.3206
batch size: (907, 907)
Epoch 79, accuracy: 0.3761
batch size: (898, 898)
Epoch 80, accuracy: 0.3733
Epoch 80, Train Loss: 0.1993, Val Loss: 1.3352
batch size: (891, 891)
Epoch 81, accuracy: 0.3728
batch size: (904, 904)
Epoch 82, accuracy: 0.3727
Epoch 82, Train Loss: 0.1470, Val Loss: 1.3325
batch size: (907, 907)
Epoch 83, accuracy: 0.3756
batch size: (864, 864)
Epoch 84, accuracy: 0.3735
Epoch 84, Train Loss: 0.1927, Val Loss: 1.3068
batch size: (887, 887)
Epoch 85, accuracy: 0.3730
batch size: (879, 879)
Epoch 86, accuracy: 0.3723
Epoch 86, Train Loss: 0.1140, Val Loss: 1.3258
batch size: (904, 904)
Epoch 87, accuracy: 0.3743
batch size: (894, 894)
Epoch 88, accuracy: 0.3728
Epoch 88, Train Loss: 0.0936, Val Loss: 1.3517
batch size: (892, 892)
Epoch 89, accuracy: 0.3733
batch size: (898, 898)
Epoch 90, accuracy: 0.3743
Epoch 90, Train Loss: 0.1163, Val Loss: 1.3278
batch size: (903, 903)
Epoch 91, accuracy: 0.3740
batch size: (889, 889)
Epoch 92, accuracy: 0.3706
Epoch 92, Train Loss: 0.1513, Val Loss: 1.3472
batch size: (906, 906)
Epoch 93, accuracy: 0.3704
batch size: (878, 878)
Epoch 94, accuracy: 0.3748
Epoch 94, Train Loss: 0.1645, Val Loss: 1.3746
batch size: (893, 893)
Epoch 95, accuracy: 0.3723
batch size: (898, 898)
Epoch 96, accuracy: 0.3725
Epoch 96, Train Loss: 0.2213, Val Loss: 1.3348
batch size: (909, 909)
Epoch 97, accuracy: 0.3728
batch size: (919, 919)
Epoch 98, accuracy: 0.3716
Epoch 98, Train Loss: 0.1282, Val Loss: 1.3412
batch size: (904, 904)
Epoch 99, accuracy: 0.3734
batch size: (896, 896)
Epoch 100, accuracy: 0.3735
Epoch 100, Train Loss: 0.1066, Val Loss: 1.3135
batch size: (897, 897)
Epoch 101, accuracy: 0.3743
batch size: (894, 894)
Epoch 102, accuracy: 0.3718
Epoch 102, Train Loss: 0.1532, Val Loss: 1.3150
batch size: (909, 909)
Epoch 103, accuracy: 0.3716
batch size: (899, 899)
Epoch 104, accuracy: 0.3706
Epoch 104, Train Loss: 0.1115, Val Loss: 1.2946
batch size: (904, 904)
Epoch 105, accuracy: 0.3724
batch size: (897, 897)
Epoch 106, accuracy: 0.3753
Epoch 106, Train Loss: 0.1970, Val Loss: 1.3558
batch size: (886, 886)
Epoch 107, accuracy: 0.3732
batch size: (897, 897)
Epoch 108, accuracy: 0.3736
Epoch 108, Train Loss: 0.1223, Val Loss: 1.2948
batch size: (888, 888)
Epoch 109, accuracy: 0.3756
batch size: (908, 908)
Epoch 110, accuracy: 0.3678
Epoch 110, Train Loss: 0.0956, Val Loss: 1.3234
batch size: (889, 889)
Epoch 111, accuracy: 0.3682
batch size: (901, 901)
Epoch 112, accuracy: 0.3763
Epoch 112, Train Loss: 0.1397, Val Loss: 1.3582
batch size: (900, 900)
Epoch 113, accuracy: 0.3709
batch size: (906, 906)
Epoch 114, accuracy: 0.3722
Epoch 114, Train Loss: 0.1060, Val Loss: 1.3391
batch size: (900, 900)
Epoch 115, accuracy: 0.3739
batch size: (904, 904)
Epoch 116, accuracy: 0.3766
Epoch 116, Train Loss: 0.1164, Val Loss: 1.3190
batch size: (895, 895)
Epoch 117, accuracy: 0.3753
batch size: (910, 910)
Epoch 118, accuracy: 0.3726
Epoch 118, Train Loss: 0.1352, Val Loss: 1.2819
batch size: (908, 908)
Epoch 119, accuracy: 0.3733
batch size: (929, 929)
Epoch 120, accuracy: 0.3715
Epoch 120, Train Loss: 0.1589, Val Loss: 1.3059
batch size: (922, 922)
Epoch 121, accuracy: 0.3696
batch size: (903, 903)
Epoch 122, accuracy: 0.3722
Epoch 122, Train Loss: 0.1805, Val Loss: 1.3514
batch size: (890, 890)
Epoch 123, accuracy: 0.3712
batch size: (881, 881)
Epoch 124, accuracy: 0.3746
Epoch 124, Train Loss: 0.2284, Val Loss: 1.3142
batch size: (893, 893)
Epoch 125, accuracy: 0.3726
batch size: (923, 923)
Epoch 126, accuracy: 0.3700
Epoch 126, Train Loss: 0.1760, Val Loss: 1.3457
batch size: (886, 886)
Epoch 127, accuracy: 0.3738
batch size: (901, 901)
Epoch 128, accuracy: 0.3714
Epoch 128, Train Loss: 0.1199, Val Loss: 1.3666
batch size: (895, 895)
Epoch 129, accuracy: 0.3735
batch size: (896, 896)
Epoch 130, accuracy: 0.3763
Epoch 130, Train Loss: 0.1384, Val Loss: 1.3223
batch size: (905, 905)
Epoch 131, accuracy: 0.3746
batch size: (903, 903)
Epoch 132, accuracy: 0.3747
Epoch 132, Train Loss: 0.1320, Val Loss: 1.3660
batch size: (901, 901)
Epoch 133, accuracy: 0.3701
batch size: (925, 925)
Epoch 134, accuracy: 0.3754
Epoch 134, Train Loss: 0.1515, Val Loss: 1.3049
batch size: (908, 908)
Epoch 135, accuracy: 0.3725
batch size: (903, 903)
Epoch 136, accuracy: 0.3707
Epoch 136, Train Loss: 0.0987, Val Loss: 1.3372
batch size: (884, 884)
Epoch 137, accuracy: 0.3755
batch size: (902, 902)
Epoch 138, accuracy: 0.3763
Epoch 138, Train Loss: 0.1096, Val Loss: 1.3460
batch size: (917, 917)
Epoch 139, accuracy: 0.3718
batch size: (876, 876)
Epoch 140, accuracy: 0.3701
Epoch 140, Train Loss: 0.2139, Val Loss: 1.3707
batch size: (899, 899)
Epoch 141, accuracy: 0.3717
batch size: (897, 897)
Epoch 142, accuracy: 0.3766
Epoch 142, Train Loss: 0.0915, Val Loss: 1.3628
batch size: (875, 875)
Epoch 143, accuracy: 0.3737
batch size: (909, 909)
Epoch 144, accuracy: 0.3748
Epoch 144, Train Loss: 0.1153, Val Loss: 1.3617
batch size: (875, 875)
Epoch 145, accuracy: 0.3745
batch size: (878, 878)
Epoch 146, accuracy: 0.3720
Epoch 146, Train Loss: 0.1376, Val Loss: 1.3580
batch size: (902, 902)
Epoch 147, accuracy: 0.3720
batch size: (905, 905)
Epoch 148, accuracy: 0.3770
Epoch 148, Train Loss: 0.1017, Val Loss: 1.3227
batch size: (901, 901)
Epoch 149, accuracy: 0.3732
batch size: (899, 899)
Epoch 150, accuracy: 0.3740
Epoch 150, Train Loss: 0.1421, Val Loss: 1.3222
batch size: (890, 890)
Epoch 151, accuracy: 0.3713
batch size: (908, 908)
Epoch 152, accuracy: 0.3749
Epoch 152, Train Loss: 0.1600, Val Loss: 1.3303
batch size: (915, 915)
Epoch 153, accuracy: 0.3689
batch size: (909, 909)
Epoch 154, accuracy: 0.3726
Epoch 154, Train Loss: 0.0965, Val Loss: 1.2844
batch size: (900, 900)
Epoch 155, accuracy: 0.3763
batch size: (890, 890)
Epoch 156, accuracy: 0.3736
Epoch 156, Train Loss: 0.1006, Val Loss: 1.3447
batch size: (908, 908)
Epoch 157, accuracy: 0.3754
batch size: (895, 895)
Epoch 158, accuracy: 0.3701
Epoch 158, Train Loss: 0.1881, Val Loss: 1.3767
batch size: (902, 902)
Epoch 159, accuracy: 0.3720
batch size: (901, 901)
Epoch 160, accuracy: 0.3732
Epoch 160, Train Loss: 0.1280, Val Loss: 1.3497
batch size: (902, 902)
Epoch 161, accuracy: 0.3726
batch size: (900, 900)
Epoch 162, accuracy: 0.3739
Epoch 162, Train Loss: 0.1974, Val Loss: 1.3306
batch size: (905, 905)
Epoch 163, accuracy: 0.3714
batch size: (898, 898)
Epoch 164, accuracy: 0.3745
Epoch 164, Train Loss: 0.0841, Val Loss: 1.3160
batch size: (907, 907)
Epoch 165, accuracy: 0.3741
batch size: (908, 908)
Epoch 166, accuracy: 0.3760
Epoch 166, Train Loss: 0.1323, Val Loss: 1.3350
batch size: (892, 892)
Epoch 167, accuracy: 0.3754
batch size: (904, 904)
Epoch 168, accuracy: 0.3750
Epoch 168, Train Loss: 0.2278, Val Loss: 1.3457
batch size: (892, 892)
Epoch 169, accuracy: 0.3741
batch size: (909, 909)
Epoch 170, accuracy: 0.3750
Epoch 170, Train Loss: 0.1183, Val Loss: 1.2901
batch size: (897, 897)
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 171, accuracy: 0.3755
batch size: (909, 909)
Epoch 172, accuracy: 0.3720
Epoch 172, Train Loss: 0.1113, Val Loss: 1.3057
batch size: (891, 891)
Epoch 173, accuracy: 0.3755
batch size: (925, 925)
Epoch 174, accuracy: 0.3734
Epoch 174, Train Loss: 0.1481, Val Loss: 1.3289
batch size: (893, 893)
Epoch 175, accuracy: 0.3742
batch size: (878, 878)
Epoch 176, accuracy: 0.3729
Epoch 176, Train Loss: 0.0927, Val Loss: 1.3049
batch size: (919, 919)
Epoch 177, accuracy: 0.3741
batch size: (891, 891)
Epoch 178, accuracy: 0.3736
Epoch 178, Train Loss: 0.1307, Val Loss: 1.3486
batch size: (917, 917)
Epoch 179, accuracy: 0.3727
batch size: (907, 907)
Epoch 180, accuracy: 0.3727
Epoch 180, Train Loss: 0.1057, Val Loss: 1.3103
batch size: (901, 901)
Epoch 181, accuracy: 0.3707
batch size: (879, 879)
Epoch 182, accuracy: 0.3752
Epoch 182, Train Loss: 0.1358, Val Loss: 1.3692
batch size: (903, 903)
Epoch 183, accuracy: 0.3693
batch size: (895, 895)
Epoch 184, accuracy: 0.3730
Epoch 184, Train Loss: 0.0849, Val Loss: 1.3692
batch size: (897, 897)
Epoch 185, accuracy: 0.3760
batch size: (900, 900)
Epoch 186, accuracy: 0.3729
Epoch 186, Train Loss: 0.1376, Val Loss: 1.2972
batch size: (910, 910)
Epoch 187, accuracy: 0.3702
batch size: (906, 906)
Epoch 188, accuracy: 0.3766
Epoch 188, Train Loss: 0.2202, Val Loss: 1.3506
batch size: (901, 901)
Epoch 189, accuracy: 0.3745
batch size: (908, 908)
Epoch 190, accuracy: 0.3733
Epoch 190, Train Loss: 0.1534, Val Loss: 1.3334
batch size: (897, 897)
Epoch 191, accuracy: 0.3705
batch size: (916, 916)
Epoch 192, accuracy: 0.3738
Epoch 192, Train Loss: 0.2099, Val Loss: 1.2989
batch size: (883, 883)
Epoch 193, accuracy: 0.3696
batch size: (906, 906)
Epoch 194, accuracy: 0.3740
Epoch 194, Train Loss: 0.0952, Val Loss: 1.2921
batch size: (899, 899)
Epoch 195, accuracy: 0.3734
batch size: (919, 919)
Epoch 196, accuracy: 0.3754
Epoch 196, Train Loss: 0.1112, Val Loss: 1.2986
batch size: (891, 891)
Epoch 197, accuracy: 0.3752
batch size: (906, 906)
Epoch 198, accuracy: 0.3735
Epoch 198, Train Loss: 0.1612, Val Loss: 1.3572
batch size: (907, 907)
Epoch 199, accuracy: 0.3716
Loaded best model with val_loss = 1.1215482950210571
test :accuracy 0.4292, f1_macro: 0.2002, f1_micro: 0.4292, auc: 0.6067
Training JKNet with 32 layers...
可训练参数: 4354822_JKNet
不可训练参数: 0
batch size: (895, 895)
✅ Epoch 0: New best model saved with val_loss = 1.3612
Epoch 0, accuracy: 0.3706
Epoch 0, Train Loss: 1.1817, Val Loss: 1.3612
batch size: (909, 909)
✅ Epoch 1: New best model saved with val_loss = 1.1162
Epoch 1, accuracy: 0.4001
batch size: (896, 896)
Epoch 2, accuracy: 0.4322
Epoch 2, Train Loss: 2.3605, Val Loss: 1.2379
batch size: (917, 917)
Epoch 3, accuracy: 0.4275
batch size: (893, 893)
Epoch 4, accuracy: 0.3984
Epoch 4, Train Loss: 1.2392, Val Loss: 1.1231
batch size: (884, 884)
✅ Epoch 5: New best model saved with val_loss = 1.0726
Epoch 5, accuracy: 0.4024
batch size: (905, 905)
✅ Epoch 6: New best model saved with val_loss = 1.0588
Epoch 6, accuracy: 0.4278
Epoch 6, Train Loss: 1.0326, Val Loss: 1.0588
batch size: (884, 884)
✅ Epoch 7: New best model saved with val_loss = 1.0578
Epoch 7, accuracy: 0.4278
batch size: (893, 893)
Epoch 8, accuracy: 0.4321
Epoch 8, Train Loss: 0.0881, Val Loss: 1.0585
batch size: (918, 918)
✅ Epoch 9: New best model saved with val_loss = 1.0546
Epoch 9, accuracy: 0.4300
batch size: (897, 897)
Epoch 10, accuracy: 0.4310
Epoch 10, Train Loss: 0.0614, Val Loss: 1.0569
batch size: (904, 904)
✅ Epoch 11: New best model saved with val_loss = 1.0472
Epoch 11, accuracy: 0.4305
batch size: (888, 888)
✅ Epoch 12: New best model saved with val_loss = 1.0433
Epoch 12, accuracy: 0.4234
Epoch 12, Train Loss: 0.1421, Val Loss: 1.0433
batch size: (904, 904)
Epoch 13, accuracy: 0.4350
batch size: (922, 922)
Epoch 14, accuracy: 0.4316
Epoch 14, Train Loss: 1.2163, Val Loss: 1.0505
batch size: (900, 900)
Epoch 15, accuracy: 0.4315
batch size: (899, 899)
Epoch 16, accuracy: 0.4329
Epoch 16, Train Loss: 0.0095, Val Loss: 1.0522
batch size: (897, 897)
Epoch 17, accuracy: 0.4278
batch size: (904, 904)
Epoch 18, accuracy: 0.4277
Epoch 18, Train Loss: 0.0067, Val Loss: 1.0594
batch size: (901, 901)
Epoch 19, accuracy: 0.4297
batch size: (876, 876)
Epoch 20, accuracy: 0.4302
Epoch 20, Train Loss: 0.0056, Val Loss: 1.0636
batch size: (882, 882)
Epoch 21, accuracy: 0.4293
batch size: (907, 907)
Epoch 22, accuracy: 0.4293
Epoch 22, Train Loss: 0.0051, Val Loss: 1.0631
batch size: (897, 897)
Epoch 23, accuracy: 0.4330
batch size: (908, 908)
Epoch 24, accuracy: 0.4295
Epoch 24, Train Loss: 0.0081, Val Loss: 1.0639
batch size: (904, 904)
Epoch 25, accuracy: 0.4291
batch size: (919, 919)
Epoch 26, accuracy: 0.4287
Epoch 26, Train Loss: 0.1205, Val Loss: 1.0679
batch size: (884, 884)
Epoch 27, accuracy: 0.4300
batch size: (888, 888)
Epoch 28, accuracy: 0.4288
Epoch 28, Train Loss: 0.0519, Val Loss: 1.0634
batch size: (910, 910)
Epoch 29, accuracy: 0.4304
batch size: (903, 903)
Epoch 30, accuracy: 0.4295
Epoch 30, Train Loss: 0.0070, Val Loss: 1.0605
batch size: (897, 897)
Epoch 31, accuracy: 0.4345
batch size: (870, 870)
Epoch 32, accuracy: 0.4266
Epoch 32, Train Loss: 0.6683, Val Loss: 1.0947
batch size: (900, 900)
Epoch 33, accuracy: 0.4286
batch size: (908, 908)
Epoch 34, accuracy: 0.4294
Epoch 34, Train Loss: 0.0061, Val Loss: 1.0616
batch size: (892, 892)
Epoch 35, accuracy: 0.4294
batch size: (907, 907)
Epoch 36, accuracy: 0.4287
Epoch 36, Train Loss: 0.0066, Val Loss: 1.0646
batch size: (901, 901)
Epoch 37, accuracy: 0.4293
batch size: (894, 894)
Epoch 38, accuracy: 0.4298
Epoch 38, Train Loss: 0.0041, Val Loss: 1.0628
batch size: (889, 889)
Epoch 39, accuracy: 0.4287
batch size: (882, 882)
Epoch 40, accuracy: 0.4249
Epoch 40, Train Loss: 0.0059, Val Loss: 1.0625
batch size: (902, 902)
Epoch 41, accuracy: 0.4301
batch size: (896, 896)
Epoch 42, accuracy: 0.4252
Epoch 42, Train Loss: 0.0041, Val Loss: 1.0637
batch size: (889, 889)
Epoch 43, accuracy: 0.4272
batch size: (905, 905)
Epoch 44, accuracy: 0.4304
Epoch 44, Train Loss: 0.0068, Val Loss: 1.0630
batch size: (892, 892)
Epoch 45, accuracy: 0.4293
batch size: (885, 885)
Epoch 46, accuracy: 0.4290
Epoch 46, Train Loss: 0.0086, Val Loss: 1.0636
batch size: (873, 873)
Epoch 47, accuracy: 0.4310
batch size: (901, 901)
Epoch 48, accuracy: 0.4310
Epoch 48, Train Loss: 0.0025, Val Loss: 1.0640
batch size: (906, 906)
Epoch 49, accuracy: 0.4266
batch size: (890, 890)
Epoch 50, accuracy: 0.4290
Epoch 50, Train Loss: 0.0061, Val Loss: 1.0616
batch size: (894, 894)
Epoch 51, accuracy: 0.4288
batch size: (896, 896)
Epoch 52, accuracy: 0.4348
Epoch 52, Train Loss: 0.0044, Val Loss: 1.0617
batch size: (908, 908)
Epoch 53, accuracy: 0.4283
batch size: (876, 876)
Epoch 54, accuracy: 0.4285
Epoch 54, Train Loss: 0.0037, Val Loss: 1.0640
batch size: (893, 893)
Epoch 55, accuracy: 0.4267
batch size: (905, 905)
Epoch 56, accuracy: 0.4331
Epoch 56, Train Loss: 1.3412, Val Loss: 1.1004
batch size: (900, 900)
Epoch 57, accuracy: 0.4339
batch size: (904, 904)
Epoch 58, accuracy: 0.4288
Epoch 58, Train Loss: 0.0044, Val Loss: 1.0617
batch size: (892, 892)
Epoch 59, accuracy: 0.4271
batch size: (900, 900)
Epoch 60, accuracy: 0.4275
Epoch 60, Train Loss: 0.0283, Val Loss: 1.0643
batch size: (922, 922)
Epoch 61, accuracy: 0.4319
batch size: (906, 906)
Epoch 62, accuracy: 0.4310
Epoch 62, Train Loss: 0.0032, Val Loss: 1.0646
batch size: (909, 909)
Epoch 63, accuracy: 0.4314
batch size: (914, 914)
Epoch 64, accuracy: 0.4304
Epoch 64, Train Loss: 0.0042, Val Loss: 1.0624
batch size: (891, 891)
Epoch 65, accuracy: 0.4320
batch size: (904, 904)
Epoch 66, accuracy: 0.4323
Epoch 66, Train Loss: 0.0029, Val Loss: 1.0618
batch size: (918, 918)
Epoch 67, accuracy: 0.4302
batch size: (908, 908)
Epoch 68, accuracy: 0.4320
Epoch 68, Train Loss: 0.0056, Val Loss: 1.0638
batch size: (889, 889)
Epoch 69, accuracy: 0.4314
batch size: (874, 874)
Epoch 70, accuracy: 0.4276
Epoch 70, Train Loss: 0.0040, Val Loss: 1.0606
batch size: (906, 906)
Epoch 71, accuracy: 0.4302
batch size: (890, 890)
Epoch 72, accuracy: 0.4296
Epoch 72, Train Loss: 0.0712, Val Loss: 1.0637
batch size: (908, 908)
Epoch 73, accuracy: 0.4280
batch size: (907, 907)
Epoch 74, accuracy: 0.4326
Epoch 74, Train Loss: 0.0037, Val Loss: 1.0643
batch size: (920, 920)
Epoch 75, accuracy: 0.4289
batch size: (890, 890)
Epoch 76, accuracy: 0.4296
Epoch 76, Train Loss: 0.0030, Val Loss: 1.0651
batch size: (899, 899)
Epoch 77, accuracy: 0.4300
batch size: (894, 894)
Epoch 78, accuracy: 0.4311
Epoch 78, Train Loss: 0.0034, Val Loss: 1.0647
batch size: (905, 905)
Epoch 79, accuracy: 0.4306
batch size: (907, 907)
Epoch 80, accuracy: 0.4314
Epoch 80, Train Loss: 0.0022, Val Loss: 1.0652
batch size: (894, 894)
Epoch 81, accuracy: 0.4326
batch size: (913, 913)
Epoch 82, accuracy: 0.4309
Epoch 82, Train Loss: 0.0041, Val Loss: 1.0635
batch size: (902, 902)
Epoch 83, accuracy: 0.4315
batch size: (906, 906)
Epoch 84, accuracy: 0.4271
Epoch 84, Train Loss: 0.0084, Val Loss: 1.0601
batch size: (897, 897)
Epoch 85, accuracy: 0.4327
batch size: (889, 889)
Epoch 86, accuracy: 0.4300
Epoch 86, Train Loss: 0.0026, Val Loss: 1.0649
batch size: (906, 906)
Epoch 87, accuracy: 0.4316
batch size: (887, 887)
Epoch 88, accuracy: 0.4291
Epoch 88, Train Loss: 0.0072, Val Loss: 1.0636
batch size: (916, 916)
Epoch 89, accuracy: 0.4313
batch size: (883, 883)
Epoch 90, accuracy: 0.4305
Epoch 90, Train Loss: 0.0050, Val Loss: 1.0654
batch size: (889, 889)
Epoch 91, accuracy: 0.4335
batch size: (905, 905)
Epoch 92, accuracy: 0.4334
Epoch 92, Train Loss: 0.0056, Val Loss: 1.0651
batch size: (908, 908)
Epoch 93, accuracy: 0.4308
batch size: (912, 912)
Epoch 94, accuracy: 0.4324
Epoch 94, Train Loss: 0.0047, Val Loss: 1.0646
batch size: (905, 905)
Epoch 95, accuracy: 0.4261
batch size: (886, 886)
Epoch 96, accuracy: 0.4284
Epoch 96, Train Loss: 0.0073, Val Loss: 1.0616
batch size: (902, 902)
Epoch 97, accuracy: 0.4327
batch size: (904, 904)
Epoch 98, accuracy: 0.4320
Epoch 98, Train Loss: 0.0051, Val Loss: 1.0607
batch size: (915, 915)
Epoch 99, accuracy: 0.4293
batch size: (898, 898)
Epoch 100, accuracy: 0.4301
Epoch 100, Train Loss: 0.0054, Val Loss: 1.0650
batch size: (906, 906)
Epoch 101, accuracy: 0.4288
batch size: (890, 890)
Epoch 102, accuracy: 0.4264
Epoch 102, Train Loss: 0.3084, Val Loss: 1.0698
batch size: (896, 896)
Epoch 103, accuracy: 0.4297
batch size: (871, 871)
Epoch 104, accuracy: 0.4316
Epoch 104, Train Loss: 0.0048, Val Loss: 1.0639
batch size: (913, 913)
Epoch 105, accuracy: 0.4265
batch size: (887, 887)
Epoch 106, accuracy: 0.4303
Epoch 106, Train Loss: 0.0096, Val Loss: 1.0646
batch size: (902, 902)
Epoch 107, accuracy: 0.4296
batch size: (901, 901)
Epoch 108, accuracy: 0.4300
Epoch 108, Train Loss: 0.0052, Val Loss: 1.0635
batch size: (882, 882)
Epoch 109, accuracy: 0.4242
batch size: (872, 872)
Epoch 110, accuracy: 0.4307
Epoch 110, Train Loss: 0.0077, Val Loss: 1.0625
batch size: (887, 887)
Epoch 111, accuracy: 0.4316
batch size: (914, 914)
Epoch 112, accuracy: 0.4259
Epoch 112, Train Loss: 0.0028, Val Loss: 1.0606
batch size: (900, 900)
Epoch 113, accuracy: 0.4299
batch size: (918, 918)
Epoch 114, accuracy: 0.4286
Epoch 114, Train Loss: 0.0051, Val Loss: 1.0626
batch size: (917, 917)
Epoch 115, accuracy: 0.4305
batch size: (902, 902)
Epoch 116, accuracy: 0.4260
Epoch 116, Train Loss: 0.0034, Val Loss: 1.0640
batch size: (882, 882)
Epoch 117, accuracy: 0.4323
batch size: (889, 889)
Epoch 118, accuracy: 0.4318
Epoch 118, Train Loss: 0.0148, Val Loss: 1.0643
batch size: (900, 900)
Epoch 119, accuracy: 0.4230
batch size: (889, 889)
Epoch 120, accuracy: 0.4324
Epoch 120, Train Loss: 0.1498, Val Loss: 1.0813
batch size: (901, 901)
Epoch 121, accuracy: 0.4314
batch size: (896, 896)
Epoch 122, accuracy: 0.4274
Epoch 122, Train Loss: 0.0076, Val Loss: 1.0610
batch size: (920, 920)
Epoch 123, accuracy: 0.4306
batch size: (887, 887)
Epoch 124, accuracy: 0.4333
Epoch 124, Train Loss: 0.0024, Val Loss: 1.0649
batch size: (901, 901)
Epoch 125, accuracy: 0.4268
batch size: (894, 894)
Epoch 126, accuracy: 0.4301
Epoch 126, Train Loss: 0.0040, Val Loss: 1.0636
batch size: (892, 892)
Epoch 127, accuracy: 0.4316
batch size: (892, 892)
Epoch 128, accuracy: 0.4278
Epoch 128, Train Loss: 0.0281, Val Loss: 1.0655
batch size: (914, 914)
Epoch 129, accuracy: 0.4319
batch size: (893, 893)
Epoch 130, accuracy: 0.4290
Epoch 130, Train Loss: 0.0041, Val Loss: 1.0636
batch size: (902, 902)
Epoch 131, accuracy: 0.4341
batch size: (893, 893)
Epoch 132, accuracy: 0.4280
Epoch 132, Train Loss: 0.0032, Val Loss: 1.0643
batch size: (922, 922)
Epoch 133, accuracy: 0.4290
batch size: (911, 911)
Epoch 134, accuracy: 0.4312
Epoch 134, Train Loss: 0.0027, Val Loss: 1.0625
batch size: (899, 899)
Epoch 135, accuracy: 0.4276
batch size: (906, 906)
Epoch 136, accuracy: 0.4301
Epoch 136, Train Loss: 0.0148, Val Loss: 1.0609
batch size: (897, 897)
Epoch 137, accuracy: 0.4332
batch size: (880, 880)
Epoch 138, accuracy: 0.4304
Epoch 138, Train Loss: 0.0089, Val Loss: 1.0656
batch size: (924, 924)
Epoch 139, accuracy: 0.4291
batch size: (896, 896)
Epoch 140, accuracy: 0.4299
Epoch 140, Train Loss: 0.0040, Val Loss: 1.0641
batch size: (899, 899)
Epoch 141, accuracy: 0.4311
batch size: (879, 879)
Epoch 142, accuracy: 0.4269
Epoch 142, Train Loss: 0.0201, Val Loss: 1.0633
batch size: (894, 894)
Epoch 143, accuracy: 0.4268
batch size: (893, 893)
Epoch 144, accuracy: 0.4291
Epoch 144, Train Loss: 0.0040, Val Loss: 1.0635
batch size: (906, 906)
Epoch 145, accuracy: 0.4308
batch size: (926, 926)
Epoch 146, accuracy: 0.4290
Epoch 146, Train Loss: 0.0139, Val Loss: 1.0627
batch size: (900, 900)
Epoch 147, accuracy: 0.4280
batch size: (911, 911)
Epoch 148, accuracy: 0.4274
Epoch 148, Train Loss: 0.0061, Val Loss: 1.0624
batch size: (886, 886)
Epoch 149, accuracy: 0.4322
batch size: (910, 910)
Epoch 150, accuracy: 0.4299
Epoch 150, Train Loss: 0.0024, Val Loss: 1.0614
batch size: (889, 889)
Epoch 151, accuracy: 0.4339
batch size: (903, 903)
Epoch 152, accuracy: 0.4270
Epoch 152, Train Loss: 0.0070, Val Loss: 1.0646
batch size: (893, 893)
Epoch 153, accuracy: 0.4305
batch size: (888, 888)
Epoch 154, accuracy: 0.4354
Epoch 154, Train Loss: 0.4288, Val Loss: 1.0833
batch size: (931, 931)
Epoch 155, accuracy: 0.4300
batch size: (917, 917)
Epoch 156, accuracy: 0.4306
Epoch 156, Train Loss: 0.0047, Val Loss: 1.0603
batch size: (902, 902)
Epoch 157, accuracy: 0.4282
batch size: (889, 889)
Epoch 158, accuracy: 0.4311
Epoch 158, Train Loss: 0.0051, Val Loss: 1.0624
batch size: (902, 902)
Epoch 159, accuracy: 0.4269
batch size: (917, 917)
Epoch 160, accuracy: 0.4285
Epoch 160, Train Loss: 0.0079, Val Loss: 1.0636
batch size: (894, 894)
Epoch 161, accuracy: 0.4304
batch size: (891, 891)
Epoch 162, accuracy: 0.4304
Epoch 162, Train Loss: 0.0047, Val Loss: 1.0604
batch size: (894, 894)
Epoch 163, accuracy: 0.4303
batch size: (899, 899)
Epoch 164, accuracy: 0.4266
Epoch 164, Train Loss: 0.0081, Val Loss: 1.0628
batch size: (898, 898)
Epoch 165, accuracy: 0.4325
batch size: (889, 889)
Epoch 166, accuracy: 0.4316
Epoch 166, Train Loss: 0.0053, Val Loss: 1.0621
batch size: (884, 884)
Epoch 167, accuracy: 0.4309
batch size: (900, 900)
Epoch 168, accuracy: 0.4300
Epoch 168, Train Loss: 0.0044, Val Loss: 1.0621
batch size: (890, 890)
Epoch 169, accuracy: 0.4283
batch size: (908, 908)
Epoch 170, accuracy: 0.4302
Epoch 170, Train Loss: 0.3507, Val Loss: 1.0729
batch size: (916, 916)
Epoch 171, accuracy: 0.4288
batch size: (869, 869)
Epoch 172, accuracy: 0.4305
Epoch 172, Train Loss: 0.0434, Val Loss: 1.1002
batch size: (912, 912)
Epoch 173, accuracy: 0.4281
batch size: (905, 905)
Epoch 174, accuracy: 0.4327
Epoch 174, Train Loss: 0.0027, Val Loss: 1.0652
batch size: (892, 892)
Epoch 175, accuracy: 0.4266
batch size: (903, 903)
Epoch 176, accuracy: 0.4328
Epoch 176, Train Loss: 0.0037, Val Loss: 1.0621
batch size: (904, 904)
Epoch 177, accuracy: 0.4296
batch size: (902, 902)
Epoch 178, accuracy: 0.4331
Epoch 178, Train Loss: 0.0160, Val Loss: 1.0628
batch size: (891, 891)
Epoch 179, accuracy: 0.4293
batch size: (912, 912)
Epoch 180, accuracy: 0.4300
Epoch 180, Train Loss: 0.0049, Val Loss: 1.0609
batch size: (901, 901)
Epoch 181, accuracy: 0.4305
batch size: (896, 896)
Epoch 182, accuracy: 0.4265
Epoch 182, Train Loss: 0.1262, Val Loss: 1.0765
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
batch size: (888, 888)
Epoch 183, accuracy: 0.4290
batch size: (901, 901)
Epoch 184, accuracy: 0.4321
Epoch 184, Train Loss: 0.0027, Val Loss: 1.0607
batch size: (908, 908)
Epoch 185, accuracy: 0.4292
batch size: (896, 896)
Epoch 186, accuracy: 0.4302
Epoch 186, Train Loss: 0.0032, Val Loss: 1.0630
batch size: (893, 893)
Epoch 187, accuracy: 0.4274
batch size: (908, 908)
Epoch 188, accuracy: 0.4296
Epoch 188, Train Loss: 0.0074, Val Loss: 1.0606
batch size: (886, 886)
Epoch 189, accuracy: 0.4274
batch size: (903, 903)
Epoch 190, accuracy: 0.4277
Epoch 190, Train Loss: 0.0131, Val Loss: 1.0623
batch size: (879, 879)
Epoch 191, accuracy: 0.4325
batch size: (892, 892)
Epoch 192, accuracy: 0.4319
Epoch 192, Train Loss: 0.0113, Val Loss: 1.0616
batch size: (902, 902)
Epoch 193, accuracy: 0.4276
batch size: (905, 905)
Epoch 194, accuracy: 0.4341
Epoch 194, Train Loss: 0.0044, Val Loss: 1.0630
batch size: (889, 889)
Epoch 195, accuracy: 0.4334
batch size: (895, 895)
Epoch 196, accuracy: 0.4303
Epoch 196, Train Loss: 0.0037, Val Loss: 1.0642
batch size: (902, 902)
Epoch 197, accuracy: 0.4297
batch size: (926, 926)
Epoch 198, accuracy: 0.4277
Epoch 198, Train Loss: 0.0117, Val Loss: 1.0636
batch size: (891, 891)
Epoch 199, accuracy: 0.4288
Loaded best model with val_loss = 1.0433319807052612
test :accuracy 0.4305, f1_macro: 0.2006, f1_micro: 0.4305, auc: 0.6950
Training resGCN with 2 layers...
可训练参数: 132626_resGCN
不可训练参数: 0
batch size: (906, 906)
✅ Epoch 0: New best model saved with val_loss = 1.0884
Epoch 0, accuracy: 0.4292
Epoch 0, Train Loss: 1.1020, Val Loss: 1.0884
batch size: (916, 916)
✅ Epoch 1: New best model saved with val_loss = 1.0782
Epoch 1, accuracy: 0.4455
batch size: (881, 881)
✅ Epoch 2: New best model saved with val_loss = 1.0650
Epoch 2, accuracy: 0.4503
Epoch 2, Train Loss: 1.0589, Val Loss: 1.0650
batch size: (891, 891)
✅ Epoch 3: New best model saved with val_loss = 1.0444
Epoch 3, accuracy: 0.4644
batch size: (906, 906)
✅ Epoch 4: New best model saved with val_loss = 1.0251
Epoch 4, accuracy: 0.4859
Epoch 4, Train Loss: 1.0163, Val Loss: 1.0251
batch size: (893, 893)
✅ Epoch 5: New best model saved with val_loss = 1.0103
Epoch 5, accuracy: 0.5040
batch size: (906, 906)
✅ Epoch 6: New best model saved with val_loss = 0.9934
Epoch 6, accuracy: 0.5285
Epoch 6, Train Loss: 0.9731, Val Loss: 0.9934
batch size: (901, 901)
✅ Epoch 7: New best model saved with val_loss = 0.9827
Epoch 7, accuracy: 0.5274
batch size: (894, 894)
✅ Epoch 8: New best model saved with val_loss = 0.9679
Epoch 8, accuracy: 0.5230
Epoch 8, Train Loss: 0.9244, Val Loss: 0.9679
batch size: (891, 891)
✅ Epoch 9: New best model saved with val_loss = 0.9495
Epoch 9, accuracy: 0.5301
batch size: (898, 898)
✅ Epoch 10: New best model saved with val_loss = 0.9334
Epoch 10, accuracy: 0.5355
Epoch 10, Train Loss: 0.7932, Val Loss: 0.9334
batch size: (912, 912)
✅ Epoch 11: New best model saved with val_loss = 0.9175
Epoch 11, accuracy: 0.5456
batch size: (897, 897)
✅ Epoch 12: New best model saved with val_loss = 0.8994
Epoch 12, accuracy: 0.5549
Epoch 12, Train Loss: 0.7372, Val Loss: 0.8994
batch size: (924, 924)
✅ Epoch 13: New best model saved with val_loss = 0.8853
Epoch 13, accuracy: 0.5663
batch size: (897, 897)
✅ Epoch 14: New best model saved with val_loss = 0.8716
Epoch 14, accuracy: 0.5793
Epoch 14, Train Loss: 0.6461, Val Loss: 0.8716
batch size: (891, 891)
✅ Epoch 15: New best model saved with val_loss = 0.8619
Epoch 15, accuracy: 0.5934
batch size: (909, 909)
✅ Epoch 16: New best model saved with val_loss = 0.8431
Epoch 16, accuracy: 0.6036
Epoch 16, Train Loss: 0.6437, Val Loss: 0.8431
batch size: (896, 896)
✅ Epoch 17: New best model saved with val_loss = 0.8377
Epoch 17, accuracy: 0.6239
batch size: (888, 888)
✅ Epoch 18: New best model saved with val_loss = 0.8315
Epoch 18, accuracy: 0.6296
Epoch 18, Train Loss: 0.5944, Val Loss: 0.8315
batch size: (899, 899)
✅ Epoch 19: New best model saved with val_loss = 0.8235
Epoch 19, accuracy: 0.6331
batch size: (890, 890)
✅ Epoch 20: New best model saved with val_loss = 0.8164
Epoch 20, accuracy: 0.6329
Epoch 20, Train Loss: 0.5293, Val Loss: 0.8164
batch size: (892, 892)
✅ Epoch 21: New best model saved with val_loss = 0.8113
Epoch 21, accuracy: 0.6340
batch size: (900, 900)
✅ Epoch 22: New best model saved with val_loss = 0.8104
Epoch 22, accuracy: 0.6330
Epoch 22, Train Loss: 0.5837, Val Loss: 0.8104
batch size: (897, 897)
Epoch 23, accuracy: 0.6326
batch size: (897, 897)
Epoch 24, accuracy: 0.6343
Epoch 24, Train Loss: 0.4766, Val Loss: 0.8118
batch size: (914, 914)
Epoch 25, accuracy: 0.6315
batch size: (904, 904)
Epoch 26, accuracy: 0.6210
Epoch 26, Train Loss: 0.5477, Val Loss: 0.8265
batch size: (891, 891)
Epoch 27, accuracy: 0.6246
batch size: (902, 902)
Epoch 28, accuracy: 0.6329
Epoch 28, Train Loss: 0.4922, Val Loss: 0.8466
batch size: (910, 910)
Epoch 29, accuracy: 0.6294
batch size: (898, 898)
Epoch 30, accuracy: 0.6333
Epoch 30, Train Loss: 0.4682, Val Loss: 0.8328
batch size: (893, 893)
Epoch 31, accuracy: 0.6284
batch size: (908, 908)
Epoch 32, accuracy: 0.6320
Epoch 32, Train Loss: 0.4804, Val Loss: 0.8190
batch size: (894, 894)
Epoch 33, accuracy: 0.6319
batch size: (893, 893)
Epoch 34, accuracy: 0.6303
Epoch 34, Train Loss: 0.4795, Val Loss: 0.8177
batch size: (904, 904)
Epoch 35, accuracy: 0.6304
batch size: (898, 898)
Epoch 36, accuracy: 0.6272
Epoch 36, Train Loss: 0.4785, Val Loss: 0.8235
batch size: (886, 886)
Epoch 37, accuracy: 0.6296
batch size: (885, 885)
Epoch 38, accuracy: 0.6308
Epoch 38, Train Loss: 0.4681, Val Loss: 0.8123
batch size: (903, 903)
Epoch 39, accuracy: 0.6317
batch size: (903, 903)
Epoch 40, accuracy: 0.6330
Epoch 40, Train Loss: 0.4887, Val Loss: 0.8168
batch size: (891, 891)
Epoch 41, accuracy: 0.6309
batch size: (916, 916)
Epoch 42, accuracy: 0.6298
Epoch 42, Train Loss: 0.4375, Val Loss: 0.8216
batch size: (911, 911)
Epoch 43, accuracy: 0.6310
batch size: (900, 900)
Epoch 44, accuracy: 0.6296
Epoch 44, Train Loss: 0.4631, Val Loss: 0.8158
batch size: (887, 887)
Epoch 45, accuracy: 0.6300
batch size: (885, 885)
Epoch 46, accuracy: 0.6272
Epoch 46, Train Loss: 0.4892, Val Loss: 0.8185
batch size: (891, 891)
Epoch 47, accuracy: 0.6309
batch size: (926, 926)
Epoch 48, accuracy: 0.6306
Epoch 48, Train Loss: 0.4714, Val Loss: 0.8196
batch size: (887, 887)
Epoch 49, accuracy: 0.6323
batch size: (905, 905)
Epoch 50, accuracy: 0.6310
Epoch 50, Train Loss: 0.4204, Val Loss: 0.8164
batch size: (901, 901)
Epoch 51, accuracy: 0.6296
batch size: (911, 911)
Epoch 52, accuracy: 0.6296
Epoch 52, Train Loss: 0.4126, Val Loss: 0.8335
batch size: (887, 887)
Epoch 53, accuracy: 0.6288
batch size: (913, 913)
Epoch 54, accuracy: 0.6308
Epoch 54, Train Loss: 0.4647, Val Loss: 0.8278
batch size: (904, 904)
Epoch 55, accuracy: 0.6274
batch size: (914, 914)
Epoch 56, accuracy: 0.6291
Epoch 56, Train Loss: 0.5234, Val Loss: 0.8241
batch size: (900, 900)
Epoch 57, accuracy: 0.6332
batch size: (918, 918)
Epoch 58, accuracy: 0.6312
Epoch 58, Train Loss: 0.4908, Val Loss: 0.8299
batch size: (885, 885)
Epoch 59, accuracy: 0.6308
batch size: (902, 902)
Epoch 60, accuracy: 0.6297
Epoch 60, Train Loss: 0.4419, Val Loss: 0.8234
batch size: (890, 890)
Epoch 61, accuracy: 0.6326
batch size: (904, 904)
Epoch 62, accuracy: 0.6300
Epoch 62, Train Loss: 0.4378, Val Loss: 0.8266
batch size: (883, 883)
Epoch 63, accuracy: 0.6315
batch size: (907, 907)
Epoch 64, accuracy: 0.6305
Epoch 64, Train Loss: 0.4951, Val Loss: 0.8217
batch size: (892, 892)
Epoch 65, accuracy: 0.6302
batch size: (915, 915)
Epoch 66, accuracy: 0.6297
Epoch 66, Train Loss: 0.4394, Val Loss: 0.8204
batch size: (903, 903)
Epoch 67, accuracy: 0.6317
batch size: (887, 887)
Epoch 68, accuracy: 0.6299
Epoch 68, Train Loss: 0.4149, Val Loss: 0.8151
batch size: (905, 905)
Epoch 69, accuracy: 0.6311
batch size: (883, 883)
Epoch 70, accuracy: 0.6328
Epoch 70, Train Loss: 0.4542, Val Loss: 0.8224
batch size: (888, 888)
Epoch 71, accuracy: 0.6309
batch size: (899, 899)
Epoch 72, accuracy: 0.6301
Epoch 72, Train Loss: 0.4727, Val Loss: 0.8280
batch size: (910, 910)
Epoch 73, accuracy: 0.6300
batch size: (898, 898)
Epoch 74, accuracy: 0.6303
Epoch 74, Train Loss: 0.4405, Val Loss: 0.8205
batch size: (895, 895)
Epoch 75, accuracy: 0.6334
batch size: (884, 884)
Epoch 76, accuracy: 0.6281
Epoch 76, Train Loss: 0.4480, Val Loss: 0.8188
batch size: (897, 897)
Epoch 77, accuracy: 0.6290
batch size: (906, 906)
Epoch 78, accuracy: 0.6320
Epoch 78, Train Loss: 0.4433, Val Loss: 0.8302
batch size: (901, 901)
Epoch 79, accuracy: 0.6290
batch size: (907, 907)
Epoch 80, accuracy: 0.6307
Epoch 80, Train Loss: 0.4959, Val Loss: 0.8307
batch size: (912, 912)
Epoch 81, accuracy: 0.6309
batch size: (888, 888)
Epoch 82, accuracy: 0.6302
Epoch 82, Train Loss: 0.4315, Val Loss: 0.8287
batch size: (907, 907)
Epoch 83, accuracy: 0.6320
batch size: (904, 904)
Epoch 84, accuracy: 0.6318
Epoch 84, Train Loss: 0.3965, Val Loss: 0.8220
batch size: (887, 887)
Epoch 85, accuracy: 0.6308
batch size: (894, 894)
Epoch 86, accuracy: 0.6327
Epoch 86, Train Loss: 0.5139, Val Loss: 0.8167
batch size: (908, 908)
Epoch 87, accuracy: 0.6301
batch size: (904, 904)
Epoch 88, accuracy: 0.6303
Epoch 88, Train Loss: 0.5052, Val Loss: 0.8183
batch size: (908, 908)
Epoch 89, accuracy: 0.6316
batch size: (903, 903)
Epoch 90, accuracy: 0.6308
Epoch 90, Train Loss: 0.4452, Val Loss: 0.8126
batch size: (884, 884)
Epoch 91, accuracy: 0.6293
batch size: (914, 914)
Epoch 92, accuracy: 0.6335
Epoch 92, Train Loss: 0.4789, Val Loss: 0.8408
batch size: (890, 890)
Epoch 93, accuracy: 0.6294
batch size: (893, 893)
Epoch 94, accuracy: 0.6337
Epoch 94, Train Loss: 0.4966, Val Loss: 0.8138
batch size: (894, 894)
Epoch 95, accuracy: 0.6306
batch size: (888, 888)
Epoch 96, accuracy: 0.6319
Epoch 96, Train Loss: 0.4962, Val Loss: 0.8231
batch size: (894, 894)
Epoch 97, accuracy: 0.6309
batch size: (904, 904)
Epoch 98, accuracy: 0.6331
Epoch 98, Train Loss: 0.4324, Val Loss: 0.8286
batch size: (914, 914)
Epoch 99, accuracy: 0.6315
batch size: (912, 912)
Epoch 100, accuracy: 0.6310
Epoch 100, Train Loss: 0.4928, Val Loss: 0.8203
batch size: (879, 879)
Epoch 101, accuracy: 0.6302
batch size: (897, 897)
Epoch 102, accuracy: 0.6319
Epoch 102, Train Loss: 0.4541, Val Loss: 0.8263
batch size: (898, 898)
Epoch 103, accuracy: 0.6340
batch size: (914, 914)
Epoch 104, accuracy: 0.6311
Epoch 104, Train Loss: 0.4762, Val Loss: 0.8191
batch size: (890, 890)
Epoch 105, accuracy: 0.6308
batch size: (902, 902)
Epoch 106, accuracy: 0.6313
Epoch 106, Train Loss: 0.4583, Val Loss: 0.8148
batch size: (905, 905)
Epoch 107, accuracy: 0.6303
batch size: (899, 899)
✅ Epoch 108: New best model saved with val_loss = 0.8099
Epoch 108, accuracy: 0.6309
Epoch 108, Train Loss: 0.4194, Val Loss: 0.8099
batch size: (905, 905)
Epoch 109, accuracy: 0.6299
batch size: (906, 906)
Epoch 110, accuracy: 0.6296
Epoch 110, Train Loss: 0.4409, Val Loss: 0.8182
batch size: (917, 917)
Epoch 111, accuracy: 0.6305
batch size: (904, 904)
Epoch 112, accuracy: 0.6297
Epoch 112, Train Loss: 0.5084, Val Loss: 0.8222
batch size: (907, 907)
Epoch 113, accuracy: 0.6304
batch size: (904, 904)
Epoch 114, accuracy: 0.6320
Epoch 114, Train Loss: 0.4849, Val Loss: 0.8131
batch size: (895, 895)
Epoch 115, accuracy: 0.6327
batch size: (904, 904)
Epoch 116, accuracy: 0.6303
Epoch 116, Train Loss: 0.5032, Val Loss: 0.8244
batch size: (896, 896)
Epoch 117, accuracy: 0.6293
batch size: (911, 911)
Epoch 118, accuracy: 0.6315
Epoch 118, Train Loss: 0.4784, Val Loss: 0.8183
batch size: (900, 900)
Epoch 119, accuracy: 0.6307
batch size: (903, 903)
Epoch 120, accuracy: 0.6321
Epoch 120, Train Loss: 0.4482, Val Loss: 0.8214
batch size: (923, 923)
Epoch 121, accuracy: 0.6311
batch size: (884, 884)
Epoch 122, accuracy: 0.6302
Epoch 122, Train Loss: 0.4202, Val Loss: 0.8165
batch size: (899, 899)
Epoch 123, accuracy: 0.6324
batch size: (915, 915)
Epoch 124, accuracy: 0.6282
Epoch 124, Train Loss: 0.5021, Val Loss: 0.8266
batch size: (905, 905)
Epoch 125, accuracy: 0.6311
batch size: (922, 922)
Epoch 126, accuracy: 0.6318
Epoch 126, Train Loss: 0.4818, Val Loss: 0.8334
batch size: (903, 903)
Epoch 127, accuracy: 0.6324
batch size: (908, 908)
Epoch 128, accuracy: 0.6324
Epoch 128, Train Loss: 0.4328, Val Loss: 0.8275
batch size: (883, 883)
Epoch 129, accuracy: 0.6284
batch size: (921, 921)
Epoch 130, accuracy: 0.6316
Epoch 130, Train Loss: 0.5044, Val Loss: 0.8235
batch size: (901, 901)
Epoch 131, accuracy: 0.6336
batch size: (879, 879)
Epoch 132, accuracy: 0.6297
Epoch 132, Train Loss: 0.4494, Val Loss: 0.8238
batch size: (921, 921)
Epoch 133, accuracy: 0.6313
batch size: (896, 896)
Epoch 134, accuracy: 0.6299
Epoch 134, Train Loss: 0.4290, Val Loss: 0.8261
batch size: (884, 884)
Epoch 135, accuracy: 0.6287
batch size: (909, 909)
Epoch 136, accuracy: 0.6332
Epoch 136, Train Loss: 0.4686, Val Loss: 0.8133
batch size: (899, 899)
Epoch 137, accuracy: 0.6313
batch size: (927, 927)
Epoch 138, accuracy: 0.6294
Epoch 138, Train Loss: 0.4075, Val Loss: 0.8233
batch size: (891, 891)
Epoch 139, accuracy: 0.6328
batch size: (902, 902)
Epoch 140, accuracy: 0.6313
Epoch 140, Train Loss: 0.5650, Val Loss: 0.8203
batch size: (903, 903)
Epoch 141, accuracy: 0.6284
batch size: (897, 897)
Epoch 142, accuracy: 0.6312
Epoch 142, Train Loss: 0.4428, Val Loss: 0.8291
batch size: (900, 900)
Epoch 143, accuracy: 0.6335
batch size: (886, 886)
Epoch 144, accuracy: 0.6290
Epoch 144, Train Loss: 0.4567, Val Loss: 0.8106
batch size: (920, 920)
Epoch 145, accuracy: 0.6313
batch size: (900, 900)
Epoch 146, accuracy: 0.6326
Epoch 146, Train Loss: 0.4389, Val Loss: 0.8192
batch size: (893, 893)
Epoch 147, accuracy: 0.6314
batch size: (907, 907)
Epoch 148, accuracy: 0.6307
Epoch 148, Train Loss: 0.5608, Val Loss: 0.8218
batch size: (883, 883)
Epoch 149, accuracy: 0.6295
batch size: (903, 903)
Epoch 150, accuracy: 0.6324
Epoch 150, Train Loss: 0.4530, Val Loss: 0.8272
batch size: (880, 880)
Epoch 151, accuracy: 0.6316
batch size: (913, 913)
Epoch 152, accuracy: 0.6315
Epoch 152, Train Loss: 0.4037, Val Loss: 0.8203
batch size: (885, 885)
Epoch 153, accuracy: 0.6284
batch size: (887, 887)
Epoch 154, accuracy: 0.6300
Epoch 154, Train Loss: 0.4522, Val Loss: 0.8211
batch size: (885, 885)
Epoch 155, accuracy: 0.6334
batch size: (901, 901)
Epoch 156, accuracy: 0.6297
Epoch 156, Train Loss: 0.4693, Val Loss: 0.8330
batch size: (899, 899)
Epoch 157, accuracy: 0.6302
batch size: (901, 901)
Epoch 158, accuracy: 0.6304
Epoch 158, Train Loss: 0.4974, Val Loss: 0.8260
batch size: (905, 905)
Epoch 159, accuracy: 0.6296
batch size: (886, 886)
Epoch 160, accuracy: 0.6318
Epoch 160, Train Loss: 0.4456, Val Loss: 0.8400
batch size: (918, 918)
Epoch 161, accuracy: 0.6282
batch size: (892, 892)
Epoch 162, accuracy: 0.6293
Epoch 162, Train Loss: 0.4787, Val Loss: 0.8318
batch size: (892, 892)
Epoch 163, accuracy: 0.6306
batch size: (898, 898)
Epoch 164, accuracy: 0.6300
Epoch 164, Train Loss: 0.4639, Val Loss: 0.8271
batch size: (899, 899)
Epoch 165, accuracy: 0.6296
batch size: (894, 894)
Epoch 166, accuracy: 0.6308
Epoch 166, Train Loss: 0.4090, Val Loss: 0.8148
batch size: (899, 899)
Epoch 167, accuracy: 0.6309
batch size: (911, 911)
Epoch 168, accuracy: 0.6310
Epoch 168, Train Loss: 0.4524, Val Loss: 0.8277
batch size: (924, 924)
Epoch 169, accuracy: 0.6354
batch size: (888, 888)
Epoch 170, accuracy: 0.6310
Epoch 170, Train Loss: 0.4170, Val Loss: 0.8351
batch size: (887, 887)
Epoch 171, accuracy: 0.6295
batch size: (900, 900)
Epoch 172, accuracy: 0.6290
Epoch 172, Train Loss: 0.4256, Val Loss: 0.8227
batch size: (900, 900)
Epoch 173, accuracy: 0.6284
batch size: (894, 894)
Epoch 174, accuracy: 0.6297
Epoch 174, Train Loss: 0.4701, Val Loss: 0.8357
batch size: (896, 896)
Epoch 175, accuracy: 0.6328
batch size: (908, 908)
Epoch 176, accuracy: 0.6312
Epoch 176, Train Loss: 0.4392, Val Loss: 0.8246
batch size: (897, 897)
Epoch 177, accuracy: 0.6314
batch size: (902, 902)
Epoch 178, accuracy: 0.6310
Epoch 178, Train Loss: 0.4188, Val Loss: 0.8185
batch size: (902, 902)
✅ Epoch 179: New best model saved with val_loss = 0.8093
Epoch 179, accuracy: 0.6307
batch size: (898, 898)
Epoch 180, accuracy: 0.6326
Epoch 180, Train Loss: 0.4897, Val Loss: 0.8156
batch size: (900, 900)
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 181, accuracy: 0.6313
batch size: (920, 920)
Epoch 182, accuracy: 0.6298
Epoch 182, Train Loss: 0.4950, Val Loss: 0.8183
batch size: (903, 903)
Epoch 183, accuracy: 0.6292
batch size: (904, 904)
Epoch 184, accuracy: 0.6304
Epoch 184, Train Loss: 0.4254, Val Loss: 0.8287
batch size: (894, 894)
Epoch 185, accuracy: 0.6262
batch size: (896, 896)
Epoch 186, accuracy: 0.6303
Epoch 186, Train Loss: 0.4724, Val Loss: 0.8093
batch size: (910, 910)
Epoch 187, accuracy: 0.6297
batch size: (895, 895)
Epoch 188, accuracy: 0.6295
Epoch 188, Train Loss: 0.5196, Val Loss: 0.8133
batch size: (891, 891)
Epoch 189, accuracy: 0.6327
batch size: (911, 911)
Epoch 190, accuracy: 0.6308
Epoch 190, Train Loss: 0.5032, Val Loss: 0.8360
batch size: (904, 904)
Epoch 191, accuracy: 0.6292
batch size: (896, 896)
Epoch 192, accuracy: 0.6286
Epoch 192, Train Loss: 0.5892, Val Loss: 0.8211
batch size: (904, 904)
Epoch 193, accuracy: 0.6326
batch size: (888, 888)
Epoch 194, accuracy: 0.6316
Epoch 194, Train Loss: 0.4788, Val Loss: 0.8350
batch size: (921, 921)
Epoch 195, accuracy: 0.6305
batch size: (895, 895)
Epoch 196, accuracy: 0.6323
Epoch 196, Train Loss: 0.4935, Val Loss: 0.8172
batch size: (891, 891)
Epoch 197, accuracy: 0.6289
batch size: (925, 925)
Epoch 198, accuracy: 0.6304
Epoch 198, Train Loss: 0.4159, Val Loss: 0.8310
batch size: (910, 910)
Epoch 199, accuracy: 0.6287
Loaded best model with val_loss = 0.8092743754386902
test :accuracy 0.6321, f1_macro: 0.6144, f1_micro: 0.6321, auc: 0.8484
Training resGCN with 8 layers...
可训练参数: 1850386_resGCN
不可训练参数: 0
batch size: (914, 914)
✅ Epoch 0: New best model saved with val_loss = 1.0992
Epoch 0, accuracy: 0.1650
Epoch 0, Train Loss: 1.1432, Val Loss: 1.0992
batch size: (911, 911)
Epoch 1, accuracy: 0.4301
batch size: (921, 921)
Epoch 2, accuracy: 0.4316
Epoch 2, Train Loss: 1.0986, Val Loss: 1.0995
batch size: (898, 898)
Epoch 3, accuracy: 0.4301
batch size: (884, 884)
✅ Epoch 4: New best model saved with val_loss = 1.0992
Epoch 4, accuracy: 0.4302
Epoch 4, Train Loss: 1.0987, Val Loss: 1.0992
batch size: (881, 881)
✅ Epoch 5: New best model saved with val_loss = 1.0989
Epoch 5, accuracy: 0.4321
batch size: (906, 906)
✅ Epoch 6: New best model saved with val_loss = 1.0986
Epoch 6, accuracy: 0.4293
Epoch 6, Train Loss: 1.0987, Val Loss: 1.0986
batch size: (900, 900)
✅ Epoch 7: New best model saved with val_loss = 1.0984
Epoch 7, accuracy: 0.4280
batch size: (895, 895)
✅ Epoch 8: New best model saved with val_loss = 1.0981
Epoch 8, accuracy: 0.4321
Epoch 8, Train Loss: 1.0987, Val Loss: 1.0981
batch size: (900, 900)
✅ Epoch 9: New best model saved with val_loss = 1.0979
Epoch 9, accuracy: 0.4324
batch size: (914, 914)
✅ Epoch 10: New best model saved with val_loss = 1.0978
Epoch 10, accuracy: 0.4312
Epoch 10, Train Loss: 1.0987, Val Loss: 1.0978
batch size: (914, 914)
✅ Epoch 11: New best model saved with val_loss = 1.0977
Epoch 11, accuracy: 0.4320
batch size: (913, 913)
✅ Epoch 12: New best model saved with val_loss = 1.0977
Epoch 12, accuracy: 0.4274
Epoch 12, Train Loss: 1.0987, Val Loss: 1.0977
batch size: (907, 907)
Epoch 13, accuracy: 0.4344
batch size: (900, 900)
Epoch 14, accuracy: 0.4301
Epoch 14, Train Loss: 1.0987, Val Loss: 1.0979
batch size: (883, 883)
Epoch 15, accuracy: 0.4296
batch size: (898, 898)
Epoch 16, accuracy: 0.4323
Epoch 16, Train Loss: 1.0986, Val Loss: 1.0982
batch size: (915, 915)
Epoch 17, accuracy: 0.4310
batch size: (900, 900)
Epoch 18, accuracy: 0.4259
Epoch 18, Train Loss: 1.0986, Val Loss: 1.0982
batch size: (905, 905)
Epoch 19, accuracy: 0.4313
batch size: (904, 904)
Epoch 20, accuracy: 0.4321
Epoch 20, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (885, 885)
Epoch 21, accuracy: 0.4318
batch size: (898, 898)
Epoch 22, accuracy: 0.4287
Epoch 22, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (905, 905)
Epoch 23, accuracy: 0.4284
batch size: (898, 898)
Epoch 24, accuracy: 0.4302
Epoch 24, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (902, 902)
Epoch 25, accuracy: 0.4272
batch size: (914, 914)
Epoch 26, accuracy: 0.4305
Epoch 26, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (908, 908)
Epoch 27, accuracy: 0.4324
batch size: (902, 902)
Epoch 28, accuracy: 0.4293
Epoch 28, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (889, 889)
Epoch 29, accuracy: 0.4311
batch size: (898, 898)
Epoch 30, accuracy: 0.4317
Epoch 30, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (901, 901)
Epoch 31, accuracy: 0.4287
batch size: (902, 902)
Epoch 32, accuracy: 0.4289
Epoch 32, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (879, 879)
Epoch 33, accuracy: 0.4312
batch size: (890, 890)
Epoch 34, accuracy: 0.4314
Epoch 34, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (911, 911)
Epoch 35, accuracy: 0.4318
batch size: (921, 921)
Epoch 36, accuracy: 0.4301
Epoch 36, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (877, 877)
Epoch 37, accuracy: 0.4274
batch size: (906, 906)
Epoch 38, accuracy: 0.4296
Epoch 38, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (909, 909)
Epoch 39, accuracy: 0.4307
batch size: (914, 914)
Epoch 40, accuracy: 0.4277
Epoch 40, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (921, 921)
Epoch 41, accuracy: 0.4272
batch size: (901, 901)
Epoch 42, accuracy: 0.4289
Epoch 42, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (891, 891)
Epoch 43, accuracy: 0.4300
batch size: (888, 888)
Epoch 44, accuracy: 0.4290
Epoch 44, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (900, 900)
Epoch 45, accuracy: 0.4314
batch size: (892, 892)
Epoch 46, accuracy: 0.4375
Epoch 46, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (910, 910)
Epoch 47, accuracy: 0.4308
batch size: (887, 887)
Epoch 48, accuracy: 0.4311
Epoch 48, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (886, 886)
Epoch 49, accuracy: 0.4346
batch size: (915, 915)
Epoch 50, accuracy: 0.4303
Epoch 50, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (897, 897)
Epoch 51, accuracy: 0.4322
batch size: (892, 892)
Epoch 52, accuracy: 0.4269
Epoch 52, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (899, 899)
Epoch 53, accuracy: 0.4300
batch size: (914, 914)
Epoch 54, accuracy: 0.4296
Epoch 54, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (899, 899)
Epoch 55, accuracy: 0.4288
batch size: (919, 919)
Epoch 56, accuracy: 0.4329
Epoch 56, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (892, 892)
Epoch 57, accuracy: 0.4317
batch size: (906, 906)
Epoch 58, accuracy: 0.4274
Epoch 58, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (910, 910)
Epoch 59, accuracy: 0.4285
batch size: (892, 892)
Epoch 60, accuracy: 0.4303
Epoch 60, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (895, 895)
Epoch 61, accuracy: 0.4307
batch size: (902, 902)
Epoch 62, accuracy: 0.4303
Epoch 62, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (919, 919)
Epoch 63, accuracy: 0.4299
batch size: (887, 887)
Epoch 64, accuracy: 0.4297
Epoch 64, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (895, 895)
Epoch 65, accuracy: 0.4294
batch size: (892, 892)
Epoch 66, accuracy: 0.4267
Epoch 66, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (907, 907)
Epoch 67, accuracy: 0.4271
batch size: (898, 898)
Epoch 68, accuracy: 0.4304
Epoch 68, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (899, 899)
Epoch 69, accuracy: 0.4281
batch size: (897, 897)
Epoch 70, accuracy: 0.4329
Epoch 70, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (907, 907)
Epoch 71, accuracy: 0.4293
batch size: (899, 899)
Epoch 72, accuracy: 0.4292
Epoch 72, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (889, 889)
Epoch 73, accuracy: 0.4329
batch size: (919, 919)
Epoch 74, accuracy: 0.4319
Epoch 74, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (893, 893)
Epoch 75, accuracy: 0.4324
batch size: (901, 901)
Epoch 76, accuracy: 0.4298
Epoch 76, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (916, 916)
Epoch 77, accuracy: 0.4292
batch size: (899, 899)
Epoch 78, accuracy: 0.4300
Epoch 78, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (907, 907)
Epoch 79, accuracy: 0.4263
batch size: (906, 906)
Epoch 80, accuracy: 0.4309
Epoch 80, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (901, 901)
Epoch 81, accuracy: 0.4300
batch size: (914, 914)
Epoch 82, accuracy: 0.4322
Epoch 82, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (901, 901)
Epoch 83, accuracy: 0.4299
batch size: (901, 901)
Epoch 84, accuracy: 0.4297
Epoch 84, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (896, 896)
Epoch 85, accuracy: 0.4304
batch size: (895, 895)
Epoch 86, accuracy: 0.4287
Epoch 86, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (903, 903)
Epoch 87, accuracy: 0.4314
batch size: (887, 887)
Epoch 88, accuracy: 0.4306
Epoch 88, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (908, 908)
Epoch 89, accuracy: 0.4316
batch size: (903, 903)
Epoch 90, accuracy: 0.4289
Epoch 90, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (915, 915)
Epoch 91, accuracy: 0.4299
batch size: (896, 896)
Epoch 92, accuracy: 0.4311
Epoch 92, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (920, 920)
Epoch 93, accuracy: 0.4304
batch size: (900, 900)
Epoch 94, accuracy: 0.4298
Epoch 94, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (900, 900)
Epoch 95, accuracy: 0.4285
batch size: (901, 901)
Epoch 96, accuracy: 0.4304
Epoch 96, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (901, 901)
Epoch 97, accuracy: 0.4312
batch size: (904, 904)
Epoch 98, accuracy: 0.4298
Epoch 98, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (873, 873)
Epoch 99, accuracy: 0.4321
batch size: (886, 886)
Epoch 100, accuracy: 0.4326
Epoch 100, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (912, 912)
Epoch 101, accuracy: 0.4239
batch size: (897, 897)
Epoch 102, accuracy: 0.4302
Epoch 102, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (914, 914)
Epoch 103, accuracy: 0.4316
batch size: (901, 901)
Epoch 104, accuracy: 0.4315
Epoch 104, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (898, 898)
Epoch 105, accuracy: 0.4281
batch size: (892, 892)
Epoch 106, accuracy: 0.4333
Epoch 106, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (900, 900)
Epoch 107, accuracy: 0.4328
batch size: (900, 900)
Epoch 108, accuracy: 0.4298
Epoch 108, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (900, 900)
Epoch 109, accuracy: 0.4273
batch size: (920, 920)
Epoch 110, accuracy: 0.4347
Epoch 110, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (901, 901)
Epoch 111, accuracy: 0.4270
batch size: (905, 905)
Epoch 112, accuracy: 0.4298
Epoch 112, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (903, 903)
Epoch 113, accuracy: 0.4298
batch size: (890, 890)
Epoch 114, accuracy: 0.4284
Epoch 114, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (895, 895)
Epoch 115, accuracy: 0.4296
batch size: (906, 906)
Epoch 116, accuracy: 0.4287
Epoch 116, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (894, 894)
Epoch 117, accuracy: 0.4316
batch size: (906, 906)
Epoch 118, accuracy: 0.4306
Epoch 118, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (896, 896)
Epoch 119, accuracy: 0.4298
batch size: (898, 898)
Epoch 120, accuracy: 0.4346
Epoch 120, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (894, 894)
Epoch 121, accuracy: 0.4300
batch size: (903, 903)
Epoch 122, accuracy: 0.4249
Epoch 122, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (879, 879)
Epoch 123, accuracy: 0.4280
batch size: (919, 919)
Epoch 124, accuracy: 0.4289
Epoch 124, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (901, 901)
Epoch 125, accuracy: 0.4316
batch size: (896, 896)
Epoch 126, accuracy: 0.4336
Epoch 126, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (908, 908)
Epoch 127, accuracy: 0.4289
batch size: (926, 926)
Epoch 128, accuracy: 0.4295
Epoch 128, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (892, 892)
Epoch 129, accuracy: 0.4326
batch size: (890, 890)
Epoch 130, accuracy: 0.4337
Epoch 130, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (908, 908)
Epoch 131, accuracy: 0.4290
batch size: (897, 897)
Epoch 132, accuracy: 0.4285
Epoch 132, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (889, 889)
Epoch 133, accuracy: 0.4304
batch size: (884, 884)
Epoch 134, accuracy: 0.4308
Epoch 134, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (911, 911)
Epoch 135, accuracy: 0.4305
batch size: (896, 896)
Epoch 136, accuracy: 0.4303
Epoch 136, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (908, 908)
Epoch 137, accuracy: 0.4294
batch size: (894, 894)
Epoch 138, accuracy: 0.4304
Epoch 138, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (890, 890)
Epoch 139, accuracy: 0.4338
batch size: (907, 907)
Epoch 140, accuracy: 0.4322
Epoch 140, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (913, 913)
Epoch 141, accuracy: 0.4297
batch size: (905, 905)
Epoch 142, accuracy: 0.4265
Epoch 142, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (909, 909)
Epoch 143, accuracy: 0.4331
batch size: (899, 899)
Epoch 144, accuracy: 0.4325
Epoch 144, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (884, 884)
Epoch 145, accuracy: 0.4277
batch size: (896, 896)
Epoch 146, accuracy: 0.4307
Epoch 146, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (908, 908)
Epoch 147, accuracy: 0.4270
batch size: (909, 909)
Epoch 148, accuracy: 0.4341
Epoch 148, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (902, 902)
Epoch 149, accuracy: 0.4294
batch size: (914, 914)
Epoch 150, accuracy: 0.4270
Epoch 150, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (909, 909)
Epoch 151, accuracy: 0.4326
batch size: (904, 904)
Epoch 152, accuracy: 0.4290
Epoch 152, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (881, 881)
Epoch 153, accuracy: 0.4309
batch size: (897, 897)
Epoch 154, accuracy: 0.4299
Epoch 154, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (919, 919)
Epoch 155, accuracy: 0.4284
batch size: (898, 898)
Epoch 156, accuracy: 0.4308
Epoch 156, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (909, 909)
Epoch 157, accuracy: 0.4323
batch size: (922, 922)
Epoch 158, accuracy: 0.4265
Epoch 158, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (899, 899)
Epoch 159, accuracy: 0.4311
batch size: (899, 899)
Epoch 160, accuracy: 0.4294
Epoch 160, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (889, 889)
Epoch 161, accuracy: 0.4325
batch size: (909, 909)
Epoch 162, accuracy: 0.4303
Epoch 162, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (909, 909)
Epoch 163, accuracy: 0.4333
batch size: (906, 906)
Epoch 164, accuracy: 0.4342
Epoch 164, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (912, 912)
Epoch 165, accuracy: 0.4310
batch size: (911, 911)
Epoch 166, accuracy: 0.4282
Epoch 166, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (899, 899)
Epoch 167, accuracy: 0.4346
batch size: (918, 918)
Epoch 168, accuracy: 0.4291
Epoch 168, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (884, 884)
Epoch 169, accuracy: 0.4312
batch size: (901, 901)
Epoch 170, accuracy: 0.4314
Epoch 170, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (889, 889)
Epoch 171, accuracy: 0.4259
batch size: (903, 903)
Epoch 172, accuracy: 0.4329
Epoch 172, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (888, 888)
Epoch 173, accuracy: 0.4355
batch size: (881, 881)
Epoch 174, accuracy: 0.4264
Epoch 174, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (877, 877)
Epoch 175, accuracy: 0.4329
batch size: (898, 898)
Epoch 176, accuracy: 0.4319
Epoch 176, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (898, 898)
Epoch 177, accuracy: 0.4285
batch size: (913, 913)
Epoch 178, accuracy: 0.4286
Epoch 178, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (890, 890)
Epoch 179, accuracy: 0.4324
batch size: (890, 890)
Epoch 180, accuracy: 0.4322
Epoch 180, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (898, 898)
Epoch 181, accuracy: 0.4310
batch size: (893, 893)
Epoch 182, accuracy: 0.4317
Epoch 182, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (918, 918)
Epoch 183, accuracy: 0.4301
batch size: (896, 896)
Epoch 184, accuracy: 0.4303
Epoch 184, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (884, 884)
Epoch 185, accuracy: 0.4310
batch size: (903, 903)
Epoch 186, accuracy: 0.4252
Epoch 186, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (902, 902)
Epoch 187, accuracy: 0.4288
batch size: (886, 886)
Epoch 188, accuracy: 0.4299
Epoch 188, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (908, 908)
Epoch 189, accuracy: 0.4292
batch size: (898, 898)
Epoch 190, accuracy: 0.4291
Epoch 190, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (914, 914)
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 191, accuracy: 0.4315
batch size: (905, 905)
Epoch 192, accuracy: 0.4317
Epoch 192, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (890, 890)
Epoch 193, accuracy: 0.4296
batch size: (891, 891)
Epoch 194, accuracy: 0.4284
Epoch 194, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (895, 895)
Epoch 195, accuracy: 0.4285
batch size: (892, 892)
Epoch 196, accuracy: 0.4303
Epoch 196, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (905, 905)
Epoch 197, accuracy: 0.4260
batch size: (902, 902)
Epoch 198, accuracy: 0.4251
Epoch 198, Train Loss: 1.0986, Val Loss: 1.0983
batch size: (907, 907)
Epoch 199, accuracy: 0.4331
Loaded best model with val_loss = 1.0977392196655273
test :accuracy 0.4279, f1_macro: 0.1998, f1_micro: 0.4279, auc: 0.5000
Training resGCN with 32 layers...
可训练参数: 573202_resGCN
不可训练参数: 0
batch size: (889, 889)
✅ Epoch 0: New best model saved with val_loss = 1.0995
Epoch 0, accuracy: 0.1692
Epoch 0, Train Loss: 151813.3438, Val Loss: 1.0995
batch size: (897, 897)
Epoch 1, accuracy: 0.1662
batch size: (884, 884)
Epoch 2, accuracy: 0.1697
Epoch 2, Train Loss: 1.0986, Val Loss: 1.1006
batch size: (907, 907)
Epoch 3, accuracy: 0.1659
batch size: (902, 902)
Epoch 4, accuracy: 0.1697
Epoch 4, Train Loss: 1.0987, Val Loss: 1.1013
batch size: (940, 940)
Epoch 5, accuracy: 0.1667
batch size: (893, 893)
Epoch 6, accuracy: 0.1661
Epoch 6, Train Loss: 1.0987, Val Loss: 1.1018
batch size: (886, 886)
Epoch 7, accuracy: 0.1651
batch size: (889, 889)
Epoch 8, accuracy: 0.1673
Epoch 8, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (903, 903)
Epoch 9, accuracy: 0.1700
batch size: (901, 901)
Epoch 10, accuracy: 0.1640
Epoch 10, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (899, 899)
Epoch 11, accuracy: 0.1682
batch size: (894, 894)
Epoch 12, accuracy: 0.1686
Epoch 12, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (913, 913)
Epoch 13, accuracy: 0.1697
batch size: (875, 875)
Epoch 14, accuracy: 0.1699
Epoch 14, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (904, 904)
Epoch 15, accuracy: 0.1682
batch size: (884, 884)
Epoch 16, accuracy: 0.1649
Epoch 16, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (909, 909)
Epoch 17, accuracy: 0.1670
batch size: (886, 886)
Epoch 18, accuracy: 0.1654
Epoch 18, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (903, 903)
Epoch 19, accuracy: 0.1680
batch size: (904, 904)
Epoch 20, accuracy: 0.1650
Epoch 20, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (891, 891)
Epoch 21, accuracy: 0.1680
batch size: (917, 917)
Epoch 22, accuracy: 0.1685
Epoch 22, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (886, 886)
Epoch 23, accuracy: 0.1688
batch size: (887, 887)
Epoch 24, accuracy: 0.1671
Epoch 24, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (886, 886)
Epoch 25, accuracy: 0.1665
batch size: (896, 896)
Epoch 26, accuracy: 0.1692
Epoch 26, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (911, 911)
Epoch 27, accuracy: 0.1700
batch size: (890, 890)
Epoch 28, accuracy: 0.1678
Epoch 28, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (893, 893)
Epoch 29, accuracy: 0.1684
batch size: (889, 889)
Epoch 30, accuracy: 0.1662
Epoch 30, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (907, 907)
Epoch 31, accuracy: 0.1702
batch size: (893, 893)
Epoch 32, accuracy: 0.1647
Epoch 32, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (903, 903)
Epoch 33, accuracy: 0.1690
batch size: (906, 906)
Epoch 34, accuracy: 0.1672
Epoch 34, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (915, 915)
Epoch 35, accuracy: 0.1674
batch size: (911, 911)
Epoch 36, accuracy: 0.1683
Epoch 36, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (895, 895)
Epoch 37, accuracy: 0.1689
batch size: (907, 907)
Epoch 38, accuracy: 0.1687
Epoch 38, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (897, 897)
Epoch 39, accuracy: 0.1646
batch size: (910, 910)
Epoch 40, accuracy: 0.1655
Epoch 40, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (908, 908)
Epoch 41, accuracy: 0.1628
batch size: (885, 885)
Epoch 42, accuracy: 0.1669
Epoch 42, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (897, 897)
Epoch 43, accuracy: 0.1650
batch size: (892, 892)
Epoch 44, accuracy: 0.1679
Epoch 44, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (908, 908)
Epoch 45, accuracy: 0.1667
batch size: (890, 890)
Epoch 46, accuracy: 0.1686
Epoch 46, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (916, 916)
Epoch 47, accuracy: 0.1663
batch size: (886, 886)
Epoch 48, accuracy: 0.1665
Epoch 48, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (887, 887)
Epoch 49, accuracy: 0.1661
batch size: (883, 883)
Epoch 50, accuracy: 0.1674
Epoch 50, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (915, 915)
Epoch 51, accuracy: 0.1688
batch size: (894, 894)
Epoch 52, accuracy: 0.1674
Epoch 52, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (882, 882)
Epoch 53, accuracy: 0.1686
batch size: (893, 893)
Epoch 54, accuracy: 0.1671
Epoch 54, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (898, 898)
Epoch 55, accuracy: 0.1646
batch size: (914, 914)
Epoch 56, accuracy: 0.1708
Epoch 56, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (900, 900)
Epoch 57, accuracy: 0.1650
batch size: (906, 906)
Epoch 58, accuracy: 0.1655
Epoch 58, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (906, 906)
Epoch 59, accuracy: 0.1678
batch size: (903, 903)
Epoch 60, accuracy: 0.1670
Epoch 60, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (899, 899)
Epoch 61, accuracy: 0.1637
batch size: (885, 885)
Epoch 62, accuracy: 0.1660
Epoch 62, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (895, 895)
Epoch 63, accuracy: 0.1644
batch size: (877, 877)
Epoch 64, accuracy: 0.1686
Epoch 64, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (905, 905)
Epoch 65, accuracy: 0.1654
batch size: (904, 904)
Epoch 66, accuracy: 0.1666
Epoch 66, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (926, 926)
Epoch 67, accuracy: 0.1670
batch size: (896, 896)
Epoch 68, accuracy: 0.1678
Epoch 68, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (888, 888)
Epoch 69, accuracy: 0.1662
batch size: (907, 907)
Epoch 70, accuracy: 0.1691
Epoch 70, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (888, 888)
Epoch 71, accuracy: 0.1690
batch size: (888, 888)
Epoch 72, accuracy: 0.1683
Epoch 72, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (926, 926)
Epoch 73, accuracy: 0.1677
batch size: (898, 898)
Epoch 74, accuracy: 0.1665
Epoch 74, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (903, 903)
Epoch 75, accuracy: 0.1683
batch size: (892, 892)
Epoch 76, accuracy: 0.1672
Epoch 76, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (889, 889)
Epoch 77, accuracy: 0.1662
batch size: (901, 901)
Epoch 78, accuracy: 0.1696
Epoch 78, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (909, 909)
Epoch 79, accuracy: 0.1681
batch size: (891, 891)
Epoch 80, accuracy: 0.1699
Epoch 80, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (920, 920)
Epoch 81, accuracy: 0.1693
batch size: (891, 891)
Epoch 82, accuracy: 0.1660
Epoch 82, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (904, 904)
Epoch 83, accuracy: 0.1659
batch size: (894, 894)
Epoch 84, accuracy: 0.1656
Epoch 84, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (911, 911)
Epoch 85, accuracy: 0.1691
batch size: (889, 889)
Epoch 86, accuracy: 0.1676
Epoch 86, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (895, 895)
Epoch 87, accuracy: 0.1638
batch size: (930, 930)
Epoch 88, accuracy: 0.1677
Epoch 88, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (882, 882)
Epoch 89, accuracy: 0.1645
batch size: (923, 923)
Epoch 90, accuracy: 0.1674
Epoch 90, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (905, 905)
Epoch 91, accuracy: 0.1689
batch size: (893, 893)
Epoch 92, accuracy: 0.1698
Epoch 92, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (897, 897)
Epoch 93, accuracy: 0.1666
batch size: (889, 889)
Epoch 94, accuracy: 0.1669
Epoch 94, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (902, 902)
Epoch 95, accuracy: 0.1669
batch size: (925, 925)
Epoch 96, accuracy: 0.1672
Epoch 96, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (896, 896)
Epoch 97, accuracy: 0.1677
batch size: (904, 904)
Epoch 98, accuracy: 0.1666
Epoch 98, Train Loss: 1.0988, Val Loss: 1.1019
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
batch size: (908, 908)
Epoch 99, accuracy: 0.1685
batch size: (903, 903)
Epoch 100, accuracy: 0.1675
Epoch 100, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (903, 903)
Epoch 101, accuracy: 0.1655
batch size: (897, 897)
Epoch 102, accuracy: 0.1671
Epoch 102, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (905, 905)
Epoch 103, accuracy: 0.1695
batch size: (881, 881)
Epoch 104, accuracy: 0.1667
Epoch 104, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (885, 885)
Epoch 105, accuracy: 0.1701
batch size: (885, 885)
Epoch 106, accuracy: 0.1691
Epoch 106, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (902, 902)
Epoch 107, accuracy: 0.1689
batch size: (919, 919)
Epoch 108, accuracy: 0.1649
Epoch 108, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (900, 900)
Epoch 109, accuracy: 0.1644
batch size: (914, 914)
Epoch 110, accuracy: 0.1659
Epoch 110, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (893, 893)
Epoch 111, accuracy: 0.1681
batch size: (914, 914)
Epoch 112, accuracy: 0.1662
Epoch 112, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (908, 908)
Epoch 113, accuracy: 0.1649
batch size: (902, 902)
Epoch 114, accuracy: 0.1695
Epoch 114, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (878, 878)
Epoch 115, accuracy: 0.1668
batch size: (883, 883)
Epoch 116, accuracy: 0.1687
Epoch 116, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (902, 902)
Epoch 117, accuracy: 0.1658
batch size: (915, 915)
Epoch 118, accuracy: 0.1649
Epoch 118, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (899, 899)
Epoch 119, accuracy: 0.1684
batch size: (920, 920)
Epoch 120, accuracy: 0.1693
Epoch 120, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (913, 913)
Epoch 121, accuracy: 0.1666
batch size: (904, 904)
Epoch 122, accuracy: 0.1687
Epoch 122, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (895, 895)
Epoch 123, accuracy: 0.1668
batch size: (900, 900)
Epoch 124, accuracy: 0.1671
Epoch 124, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (900, 900)
Epoch 125, accuracy: 0.1687
batch size: (891, 891)
Epoch 126, accuracy: 0.1676
Epoch 126, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (909, 909)
Epoch 127, accuracy: 0.1693
batch size: (889, 889)
Epoch 128, accuracy: 0.1637
Epoch 128, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (905, 905)
Epoch 129, accuracy: 0.1691
batch size: (893, 893)
Epoch 130, accuracy: 0.1685
Epoch 130, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (885, 885)
Epoch 131, accuracy: 0.1654
batch size: (893, 893)
Epoch 132, accuracy: 0.1682
Epoch 132, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (914, 914)
Epoch 133, accuracy: 0.1666
batch size: (909, 909)
Epoch 134, accuracy: 0.1685
Epoch 134, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (896, 896)
Epoch 135, accuracy: 0.1658
batch size: (875, 875)
Epoch 136, accuracy: 0.1669
Epoch 136, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (888, 888)
Epoch 137, accuracy: 0.1704
batch size: (898, 898)
Epoch 138, accuracy: 0.1717
Epoch 138, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (908, 908)
Epoch 139, accuracy: 0.1674
batch size: (924, 924)
Epoch 140, accuracy: 0.1669
Epoch 140, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (897, 897)
Epoch 141, accuracy: 0.1662
batch size: (894, 894)
Epoch 142, accuracy: 0.1653
Epoch 142, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (904, 904)
Epoch 143, accuracy: 0.1673
batch size: (894, 894)
Epoch 144, accuracy: 0.1679
Epoch 144, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (918, 918)
Epoch 145, accuracy: 0.1642
batch size: (897, 897)
Epoch 146, accuracy: 0.1672
Epoch 146, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (898, 898)
Epoch 147, accuracy: 0.1670
batch size: (905, 905)
Epoch 148, accuracy: 0.1679
Epoch 148, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (894, 894)
Epoch 149, accuracy: 0.1688
batch size: (889, 889)
Epoch 150, accuracy: 0.1661
Epoch 150, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (904, 904)
Epoch 151, accuracy: 0.1670
batch size: (897, 897)
Epoch 152, accuracy: 0.1669
Epoch 152, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (899, 899)
Epoch 153, accuracy: 0.1677
batch size: (913, 913)
Epoch 154, accuracy: 0.1663
Epoch 154, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (903, 903)
Epoch 155, accuracy: 0.1693
batch size: (894, 894)
Epoch 156, accuracy: 0.1685
Epoch 156, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (911, 911)
Epoch 157, accuracy: 0.1693
batch size: (892, 892)
Epoch 158, accuracy: 0.1680
Epoch 158, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (912, 912)
Epoch 159, accuracy: 0.1690
batch size: (912, 912)
Epoch 160, accuracy: 0.1661
Epoch 160, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (907, 907)
Epoch 161, accuracy: 0.1687
batch size: (903, 903)
Epoch 162, accuracy: 0.1679
Epoch 162, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (895, 895)
Epoch 163, accuracy: 0.1692
batch size: (879, 879)
Epoch 164, accuracy: 0.1679
Epoch 164, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (901, 901)
Epoch 165, accuracy: 0.1665
batch size: (893, 893)
Epoch 166, accuracy: 0.1712
Epoch 166, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (895, 895)
Epoch 167, accuracy: 0.1663
batch size: (924, 924)
Epoch 168, accuracy: 0.1676
Epoch 168, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (902, 902)
Epoch 169, accuracy: 0.1668
batch size: (913, 913)
Epoch 170, accuracy: 0.1671
Epoch 170, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (893, 893)
Epoch 171, accuracy: 0.1688
batch size: (897, 897)
Epoch 172, accuracy: 0.1673
Epoch 172, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (905, 905)
Epoch 173, accuracy: 0.1639
batch size: (895, 895)
Epoch 174, accuracy: 0.1687
Epoch 174, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (897, 897)
Epoch 175, accuracy: 0.1675
batch size: (901, 901)
Epoch 176, accuracy: 0.1685
Epoch 176, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (905, 905)
Epoch 177, accuracy: 0.1697
batch size: (908, 908)
Epoch 178, accuracy: 0.1663
Epoch 178, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (924, 924)
Epoch 179, accuracy: 0.1682
batch size: (912, 912)
Epoch 180, accuracy: 0.1700
Epoch 180, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (905, 905)
Epoch 181, accuracy: 0.1664
batch size: (907, 907)
Epoch 182, accuracy: 0.1657
Epoch 182, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (891, 891)
Epoch 183, accuracy: 0.1640
batch size: (903, 903)
Epoch 184, accuracy: 0.1652
Epoch 184, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (895, 895)
Epoch 185, accuracy: 0.1656
batch size: (903, 903)
Epoch 186, accuracy: 0.1647
Epoch 186, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (890, 890)
Epoch 187, accuracy: 0.1652
batch size: (892, 892)
Epoch 188, accuracy: 0.1657
Epoch 188, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (881, 881)
Epoch 189, accuracy: 0.1661
batch size: (891, 891)
Epoch 190, accuracy: 0.1663
Epoch 190, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (920, 920)
Epoch 191, accuracy: 0.1644
batch size: (896, 896)
Epoch 192, accuracy: 0.1699
Epoch 192, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (891, 891)
Epoch 193, accuracy: 0.1677
batch size: (890, 890)
Epoch 194, accuracy: 0.1704
Epoch 194, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (898, 898)
Epoch 195, accuracy: 0.1664
batch size: (902, 902)
Epoch 196, accuracy: 0.1684
Epoch 196, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (903, 903)
Epoch 197, accuracy: 0.1674
batch size: (895, 895)
Epoch 198, accuracy: 0.1665
Epoch 198, Train Loss: 1.0988, Val Loss: 1.1019
batch size: (909, 909)
Epoch 199, accuracy: 0.1687
Loaded best model with val_loss = 1.0994502305984497
test :accuracy 0.1702, f1_macro: 0.0969, f1_micro: 0.1702, auc: 0.5000
Training GINConv with 2 layers...
可训练参数: 265483_GINConv
不可训练参数: 0
batch size: (899, 899)
✅ Epoch 0: New best model saved with val_loss = 1.1512
Epoch 0, accuracy: 0.2190
Epoch 0, Train Loss: 1.5627, Val Loss: 1.1512
batch size: (875, 875)
✅ Epoch 1: New best model saved with val_loss = 1.1105
Epoch 1, accuracy: 0.1685
batch size: (906, 906)
✅ Epoch 2: New best model saved with val_loss = 1.0626
Epoch 2, accuracy: 0.2336
Epoch 2, Train Loss: 2.4367, Val Loss: 1.0626
batch size: (910, 910)
✅ Epoch 3: New best model saved with val_loss = 1.0562
Epoch 3, accuracy: 0.5603
batch size: (920, 920)
✅ Epoch 4: New best model saved with val_loss = 1.0548
Epoch 4, accuracy: 0.4829
Epoch 4, Train Loss: 0.4366, Val Loss: 1.0548
batch size: (907, 907)
✅ Epoch 5: New best model saved with val_loss = 1.0435
Epoch 5, accuracy: 0.4629
batch size: (905, 905)
✅ Epoch 6: New best model saved with val_loss = 1.0230
Epoch 6, accuracy: 0.4659
Epoch 6, Train Loss: 0.3383, Val Loss: 1.0230
batch size: (897, 897)
✅ Epoch 7: New best model saved with val_loss = 0.9945
Epoch 7, accuracy: 0.4720
batch size: (898, 898)
✅ Epoch 8: New best model saved with val_loss = 0.9606
Epoch 8, accuracy: 0.4789
Epoch 8, Train Loss: 0.2276, Val Loss: 0.9606
batch size: (897, 897)
✅ Epoch 9: New best model saved with val_loss = 0.9239
Epoch 9, accuracy: 0.4908
batch size: (898, 898)
✅ Epoch 10: New best model saved with val_loss = 0.8832
Epoch 10, accuracy: 0.5162
Epoch 10, Train Loss: 0.1092, Val Loss: 0.8832
batch size: (893, 893)
✅ Epoch 11: New best model saved with val_loss = 0.8593
Epoch 11, accuracy: 0.5411
batch size: (912, 912)
✅ Epoch 12: New best model saved with val_loss = 0.8388
Epoch 12, accuracy: 0.5555
Epoch 12, Train Loss: 0.0782, Val Loss: 0.8388
batch size: (906, 906)
✅ Epoch 13: New best model saved with val_loss = 0.8282
Epoch 13, accuracy: 0.5751
batch size: (881, 881)
✅ Epoch 14: New best model saved with val_loss = 0.8218
Epoch 14, accuracy: 0.5795
Epoch 14, Train Loss: 0.0796, Val Loss: 0.8218
batch size: (918, 918)
✅ Epoch 15: New best model saved with val_loss = 0.7552
Epoch 15, accuracy: 0.6009
batch size: (885, 885)
✅ Epoch 16: New best model saved with val_loss = 0.7145
Epoch 16, accuracy: 0.6172
Epoch 16, Train Loss: 0.0170, Val Loss: 0.7145
batch size: (898, 898)
✅ Epoch 17: New best model saved with val_loss = 0.6869
Epoch 17, accuracy: 0.6334
batch size: (905, 905)
Epoch 18, accuracy: 0.6484
Epoch 18, Train Loss: 0.0042, Val Loss: 0.6870
batch size: (888, 888)
Epoch 19, accuracy: 0.6556
batch size: (898, 898)
Epoch 20, accuracy: 0.6637
Epoch 20, Train Loss: 0.0030, Val Loss: 0.7169
batch size: (930, 930)
Epoch 21, accuracy: 0.6809
batch size: (915, 915)
Epoch 22, accuracy: 0.6851
Epoch 22, Train Loss: 0.0017, Val Loss: 0.7584
batch size: (899, 899)
Epoch 23, accuracy: 0.6942
batch size: (901, 901)
Epoch 24, accuracy: 0.6926
Epoch 24, Train Loss: 0.0796, Val Loss: 0.8009
batch size: (914, 914)
Epoch 25, accuracy: 0.6940
batch size: (908, 908)
Epoch 26, accuracy: 0.6923
Epoch 26, Train Loss: 0.0488, Val Loss: 0.8350
batch size: (872, 872)
Epoch 27, accuracy: 0.6917
batch size: (914, 914)
Epoch 28, accuracy: 0.6867
Epoch 28, Train Loss: 0.0030, Val Loss: 0.8559
batch size: (885, 885)
Epoch 29, accuracy: 0.6843
batch size: (882, 882)
Epoch 30, accuracy: 0.6874
Epoch 30, Train Loss: 0.0012, Val Loss: 0.8842
batch size: (898, 898)
Epoch 31, accuracy: 0.6851
batch size: (907, 907)
Epoch 32, accuracy: 0.6869
Epoch 32, Train Loss: 0.0054, Val Loss: 0.9270
batch size: (898, 898)
Epoch 33, accuracy: 0.6855
batch size: (879, 879)
Epoch 34, accuracy: 0.6857
Epoch 34, Train Loss: 0.0012, Val Loss: 0.9830
batch size: (890, 890)
Epoch 35, accuracy: 0.6880
batch size: (916, 916)
Epoch 36, accuracy: 0.6836
Epoch 36, Train Loss: 0.0005, Val Loss: 1.0139
batch size: (933, 933)
Epoch 37, accuracy: 0.6899
batch size: (897, 897)
Epoch 38, accuracy: 0.6885
Epoch 38, Train Loss: 0.0007, Val Loss: 1.0322
batch size: (906, 906)
Epoch 39, accuracy: 0.6919
batch size: (908, 908)
Epoch 40, accuracy: 0.6934
Epoch 40, Train Loss: 0.0007, Val Loss: 1.1359
batch size: (883, 883)
Epoch 41, accuracy: 0.6939
batch size: (892, 892)
Epoch 42, accuracy: 0.6945
Epoch 42, Train Loss: 0.0013, Val Loss: 1.2592
batch size: (896, 896)
Epoch 43, accuracy: 0.6929
batch size: (929, 929)
Epoch 44, accuracy: 0.6995
Epoch 44, Train Loss: 0.0007, Val Loss: 1.2688
batch size: (894, 894)
Epoch 45, accuracy: 0.7017
batch size: (915, 915)
Epoch 46, accuracy: 0.7046
Epoch 46, Train Loss: 0.0447, Val Loss: 1.3164
batch size: (909, 909)
Epoch 47, accuracy: 0.7026
batch size: (909, 909)
Epoch 48, accuracy: 0.7030
Epoch 48, Train Loss: 0.0019, Val Loss: 1.4178
batch size: (908, 908)
Epoch 49, accuracy: 0.7064
batch size: (917, 917)
Epoch 50, accuracy: 0.7036
Epoch 50, Train Loss: 0.0007, Val Loss: 1.5031
batch size: (903, 903)
Epoch 51, accuracy: 0.7095
batch size: (894, 894)
Epoch 52, accuracy: 0.7101
Epoch 52, Train Loss: 0.0006, Val Loss: 1.4017
batch size: (891, 891)
Epoch 53, accuracy: 0.7097
batch size: (889, 889)
Epoch 54, accuracy: 0.7106
Epoch 54, Train Loss: 0.0024, Val Loss: 1.5772
batch size: (890, 890)
Epoch 55, accuracy: 0.7132
batch size: (901, 901)
Epoch 56, accuracy: 0.7124
Epoch 56, Train Loss: 0.0011, Val Loss: 1.6221
batch size: (911, 911)
Epoch 57, accuracy: 0.7147
batch size: (921, 921)
Epoch 58, accuracy: 0.7133
Epoch 58, Train Loss: 0.0011, Val Loss: 1.6721
batch size: (903, 903)
Epoch 59, accuracy: 0.7149
batch size: (914, 914)
Epoch 60, accuracy: 0.7182
Epoch 60, Train Loss: 0.0030, Val Loss: 1.6849
batch size: (910, 910)
Epoch 61, accuracy: 0.7192
batch size: (887, 887)
Epoch 62, accuracy: 0.7186
Epoch 62, Train Loss: 0.0016, Val Loss: 1.7742
batch size: (883, 883)
Epoch 63, accuracy: 0.7148
batch size: (911, 911)
Epoch 64, accuracy: 0.7183
Epoch 64, Train Loss: 0.0010, Val Loss: 1.6744
batch size: (919, 919)
Epoch 65, accuracy: 0.7182
batch size: (899, 899)
Epoch 66, accuracy: 0.7200
Epoch 66, Train Loss: 0.0012, Val Loss: 1.7905
batch size: (886, 886)
Epoch 67, accuracy: 0.7168
batch size: (908, 908)
Epoch 68, accuracy: 0.7182
Epoch 68, Train Loss: 0.0034, Val Loss: 1.8168
batch size: (902, 902)
Epoch 69, accuracy: 0.7183
batch size: (908, 908)
Epoch 70, accuracy: 0.7198
Epoch 70, Train Loss: 0.0011, Val Loss: 1.8099
batch size: (909, 909)
Epoch 71, accuracy: 0.7193
batch size: (878, 878)
Epoch 72, accuracy: 0.7191
Epoch 72, Train Loss: 0.0013, Val Loss: 1.8638
batch size: (891, 891)
Epoch 73, accuracy: 0.7189
batch size: (906, 906)
Epoch 74, accuracy: 0.7176
Epoch 74, Train Loss: 0.0023, Val Loss: 1.9021
batch size: (897, 897)
Epoch 75, accuracy: 0.7192
batch size: (894, 894)
Epoch 76, accuracy: 0.7228
Epoch 76, Train Loss: 0.0011, Val Loss: 1.9700
batch size: (910, 910)
Epoch 77, accuracy: 0.7209
batch size: (890, 890)
Epoch 78, accuracy: 0.7201
Epoch 78, Train Loss: 0.0007, Val Loss: 1.8560
batch size: (895, 895)
Epoch 79, accuracy: 0.7220
batch size: (895, 895)
Epoch 80, accuracy: 0.7199
Epoch 80, Train Loss: 0.0011, Val Loss: 1.8771
batch size: (902, 902)
Epoch 81, accuracy: 0.7173
batch size: (900, 900)
Epoch 82, accuracy: 0.7171
Epoch 82, Train Loss: 0.0011, Val Loss: 1.9570
batch size: (910, 910)
Epoch 83, accuracy: 0.7203
batch size: (894, 894)
Epoch 84, accuracy: 0.7183
Epoch 84, Train Loss: 0.0010, Val Loss: 1.7010
batch size: (922, 922)
Epoch 85, accuracy: 0.7221
batch size: (905, 905)
Epoch 86, accuracy: 0.7195
Epoch 86, Train Loss: 0.0018, Val Loss: 1.9963
batch size: (896, 896)
Epoch 87, accuracy: 0.7193
batch size: (886, 886)
Epoch 88, accuracy: 0.7192
Epoch 88, Train Loss: 0.0007, Val Loss: 1.8504
batch size: (884, 884)
Epoch 89, accuracy: 0.7218
batch size: (910, 910)
Epoch 90, accuracy: 0.7197
Epoch 90, Train Loss: 0.0010, Val Loss: 1.9513
batch size: (903, 903)
Epoch 91, accuracy: 0.7197
batch size: (908, 908)
Epoch 92, accuracy: 0.7192
Epoch 92, Train Loss: 0.0009, Val Loss: 1.8047
batch size: (917, 917)
Epoch 93, accuracy: 0.7194
batch size: (921, 921)
Epoch 94, accuracy: 0.7180
Epoch 94, Train Loss: 0.0121, Val Loss: 1.9032
batch size: (902, 902)
Epoch 95, accuracy: 0.7212
batch size: (900, 900)
Epoch 96, accuracy: 0.7190
Epoch 96, Train Loss: 0.0006, Val Loss: 1.9094
batch size: (907, 907)
Epoch 97, accuracy: 0.7177
batch size: (890, 890)
Epoch 98, accuracy: 0.7199
Epoch 98, Train Loss: 0.0016, Val Loss: 1.9027
batch size: (906, 906)
Epoch 99, accuracy: 0.7219
batch size: (884, 884)
Epoch 100, accuracy: 0.7179
Epoch 100, Train Loss: 0.0015, Val Loss: 1.9444
batch size: (905, 905)
Epoch 101, accuracy: 0.7189
batch size: (881, 881)
Epoch 102, accuracy: 0.7192
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 102, Train Loss: 0.0007, Val Loss: 1.8280
batch size: (892, 892)
Epoch 103, accuracy: 0.7212
batch size: (901, 901)
Epoch 104, accuracy: 0.7189
Epoch 104, Train Loss: 0.0004, Val Loss: 1.8525
batch size: (900, 900)
Epoch 105, accuracy: 0.7211
batch size: (910, 910)
Epoch 106, accuracy: 0.7196
Epoch 106, Train Loss: 0.0008, Val Loss: 1.8586
batch size: (897, 897)
Epoch 107, accuracy: 0.7186
batch size: (885, 885)
Epoch 108, accuracy: 0.7216
Epoch 108, Train Loss: 0.0043, Val Loss: 1.8712
batch size: (912, 912)
Epoch 109, accuracy: 0.7164
batch size: (903, 903)
Epoch 110, accuracy: 0.7195
Epoch 110, Train Loss: 0.0039, Val Loss: 1.9794
batch size: (888, 888)
Epoch 111, accuracy: 0.7230
batch size: (902, 902)
Epoch 112, accuracy: 0.7201
Epoch 112, Train Loss: 0.0012, Val Loss: 1.9213
batch size: (906, 906)
Epoch 113, accuracy: 0.7179
batch size: (884, 884)
Epoch 114, accuracy: 0.7190
Epoch 114, Train Loss: 0.0012, Val Loss: 1.9475
batch size: (915, 915)
Epoch 115, accuracy: 0.7187
batch size: (914, 914)
Epoch 116, accuracy: 0.7216
Epoch 116, Train Loss: 0.0005, Val Loss: 1.8976
batch size: (887, 887)
Epoch 117, accuracy: 0.7197
batch size: (908, 908)
Epoch 118, accuracy: 0.7215
Epoch 118, Train Loss: 0.0971, Val Loss: 2.0215
batch size: (892, 892)
Epoch 119, accuracy: 0.7204
batch size: (907, 907)
Epoch 120, accuracy: 0.7231
Epoch 120, Train Loss: 0.0008, Val Loss: 1.8815
batch size: (890, 890)
Epoch 121, accuracy: 0.7222
batch size: (900, 900)
Epoch 122, accuracy: 0.7222
Epoch 122, Train Loss: 0.0010, Val Loss: 2.0389
batch size: (920, 920)
Epoch 123, accuracy: 0.7215
batch size: (902, 902)
Epoch 124, accuracy: 0.7234
Epoch 124, Train Loss: 0.0014, Val Loss: 2.0320
batch size: (905, 905)
Epoch 125, accuracy: 0.7229
batch size: (902, 902)
Epoch 126, accuracy: 0.7219
Epoch 126, Train Loss: 0.0007, Val Loss: 1.8635
batch size: (902, 902)
Epoch 127, accuracy: 0.7220
batch size: (902, 902)
Epoch 128, accuracy: 0.7218
Epoch 128, Train Loss: 0.0013, Val Loss: 1.8462
batch size: (913, 913)
Epoch 129, accuracy: 0.7189
batch size: (901, 901)
Epoch 130, accuracy: 0.7226
Epoch 130, Train Loss: 0.0012, Val Loss: 1.9233
batch size: (899, 899)
Epoch 131, accuracy: 0.7224
batch size: (891, 891)
Epoch 132, accuracy: 0.7216
Epoch 132, Train Loss: 0.0011, Val Loss: 1.9369
batch size: (895, 895)
Epoch 133, accuracy: 0.7195
batch size: (899, 899)
Epoch 134, accuracy: 0.7225
Epoch 134, Train Loss: 0.0012, Val Loss: 1.9734
batch size: (908, 908)
Epoch 135, accuracy: 0.7200
batch size: (895, 895)
Epoch 136, accuracy: 0.7231
Epoch 136, Train Loss: 0.0006, Val Loss: 1.9283
batch size: (899, 899)
Epoch 137, accuracy: 0.7206
batch size: (890, 890)
Epoch 138, accuracy: 0.7209
Epoch 138, Train Loss: 0.0125, Val Loss: 1.9265
batch size: (911, 911)
Epoch 139, accuracy: 0.7209
batch size: (904, 904)
Epoch 140, accuracy: 0.7230
Epoch 140, Train Loss: 0.0005, Val Loss: 1.9482
batch size: (884, 884)
Epoch 141, accuracy: 0.7218
batch size: (917, 917)
Epoch 142, accuracy: 0.7209
Epoch 142, Train Loss: 0.0008, Val Loss: 1.8902
batch size: (910, 910)
Epoch 143, accuracy: 0.7212
batch size: (893, 893)
Epoch 144, accuracy: 0.7176
Epoch 144, Train Loss: 0.0009, Val Loss: 1.9547
batch size: (887, 887)
Epoch 145, accuracy: 0.7197
batch size: (894, 894)
Epoch 146, accuracy: 0.7206
Epoch 146, Train Loss: 0.0053, Val Loss: 1.8947
batch size: (903, 903)
Epoch 147, accuracy: 0.7219
batch size: (895, 895)
Epoch 148, accuracy: 0.7190
Epoch 148, Train Loss: 0.0015, Val Loss: 1.7938
batch size: (902, 902)
Epoch 149, accuracy: 0.7196
batch size: (909, 909)
Epoch 150, accuracy: 0.7189
Epoch 150, Train Loss: 0.0030, Val Loss: 1.8951
batch size: (910, 910)
Epoch 151, accuracy: 0.7195
batch size: (878, 878)
Epoch 152, accuracy: 0.7215
Epoch 152, Train Loss: 0.0013, Val Loss: 1.9057
batch size: (895, 895)
Epoch 153, accuracy: 0.7216
batch size: (912, 912)
Epoch 154, accuracy: 0.7186
Epoch 154, Train Loss: 0.0020, Val Loss: 1.8905
batch size: (904, 904)
Epoch 155, accuracy: 0.7212
batch size: (905, 905)
Epoch 156, accuracy: 0.7189
Epoch 156, Train Loss: 0.0006, Val Loss: 1.9163
batch size: (920, 920)
Epoch 157, accuracy: 0.7224
batch size: (897, 897)
Epoch 158, accuracy: 0.7198
Epoch 158, Train Loss: 0.0010, Val Loss: 1.8567
batch size: (895, 895)
Epoch 159, accuracy: 0.7194
batch size: (895, 895)
Epoch 160, accuracy: 0.7201
Epoch 160, Train Loss: 0.0010, Val Loss: 1.8574
batch size: (902, 902)
Epoch 161, accuracy: 0.7198
batch size: (900, 900)
Epoch 162, accuracy: 0.7198
Epoch 162, Train Loss: 0.0007, Val Loss: 1.9343
batch size: (901, 901)
Epoch 163, accuracy: 0.7169
batch size: (912, 912)
Epoch 164, accuracy: 0.7175
Epoch 164, Train Loss: 0.0009, Val Loss: 1.9796
batch size: (899, 899)
Epoch 165, accuracy: 0.7201
batch size: (901, 901)
Epoch 166, accuracy: 0.7193
Epoch 166, Train Loss: 0.0013, Val Loss: 1.9292
batch size: (888, 888)
Epoch 167, accuracy: 0.7193
batch size: (895, 895)
Epoch 168, accuracy: 0.7188
Epoch 168, Train Loss: 0.0021, Val Loss: 1.8961
batch size: (910, 910)
Epoch 169, accuracy: 0.7240
batch size: (885, 885)
Epoch 170, accuracy: 0.7226
Epoch 170, Train Loss: 0.0007, Val Loss: 1.9460
batch size: (890, 890)
Epoch 171, accuracy: 0.7234
batch size: (890, 890)
Epoch 172, accuracy: 0.7220
Epoch 172, Train Loss: 0.0019, Val Loss: 1.9006
batch size: (895, 895)
Epoch 173, accuracy: 0.7209
batch size: (915, 915)
Epoch 174, accuracy: 0.7220
Epoch 174, Train Loss: 0.0021, Val Loss: 1.9526
batch size: (911, 911)
Epoch 175, accuracy: 0.7215
batch size: (895, 895)
Epoch 176, accuracy: 0.7216
Epoch 176, Train Loss: 0.0007, Val Loss: 2.0343
batch size: (882, 882)
Epoch 177, accuracy: 0.7233
batch size: (889, 889)
Epoch 178, accuracy: 0.7232
Epoch 178, Train Loss: 0.0032, Val Loss: 1.8478
batch size: (894, 894)
Epoch 179, accuracy: 0.7195
batch size: (901, 901)
Epoch 180, accuracy: 0.7212
Epoch 180, Train Loss: 0.0012, Val Loss: 1.9670
batch size: (911, 911)
Epoch 181, accuracy: 0.7232
batch size: (908, 908)
Epoch 182, accuracy: 0.7232
Epoch 182, Train Loss: 0.0003, Val Loss: 1.9099
batch size: (902, 902)
Epoch 183, accuracy: 0.7213
batch size: (883, 883)
Epoch 184, accuracy: 0.7203
Epoch 184, Train Loss: 0.0013, Val Loss: 1.8461
batch size: (890, 890)
Epoch 185, accuracy: 0.7198
batch size: (894, 894)
Epoch 186, accuracy: 0.7223
Epoch 186, Train Loss: 0.0003, Val Loss: 1.8497
batch size: (869, 869)
Epoch 187, accuracy: 0.7214
batch size: (892, 892)
Epoch 188, accuracy: 0.7229
Epoch 188, Train Loss: 0.0015, Val Loss: 1.9443
batch size: (901, 901)
Epoch 189, accuracy: 0.7192
batch size: (891, 891)
Epoch 190, accuracy: 0.7175
Epoch 190, Train Loss: 0.0012, Val Loss: 1.9493
batch size: (895, 895)
Epoch 191, accuracy: 0.7172
batch size: (914, 914)
Epoch 192, accuracy: 0.7209
Epoch 192, Train Loss: 0.0010, Val Loss: 1.8781
batch size: (909, 909)
Epoch 193, accuracy: 0.7213
batch size: (897, 897)
Epoch 194, accuracy: 0.7225
Epoch 194, Train Loss: 0.0170, Val Loss: 1.9293
batch size: (918, 918)
Epoch 195, accuracy: 0.7215
batch size: (897, 897)
Epoch 196, accuracy: 0.7201
Epoch 196, Train Loss: 0.0008, Val Loss: 1.8173
batch size: (894, 894)
Epoch 197, accuracy: 0.7203
batch size: (891, 891)
Epoch 198, accuracy: 0.7226
Epoch 198, Train Loss: 0.0013, Val Loss: 1.9462
batch size: (911, 911)
Epoch 199, accuracy: 0.7214
Loaded best model with val_loss = 0.6869270205497742
test :accuracy 0.6347, f1_macro: 0.6033, f1_micro: 0.6347, auc: 0.8725
Training GINConv with 8 layers...
可训练参数: 1059595_GINConv
不可训练参数: 0
batch size: (877, 877)
✅ Epoch 0: New best model saved with val_loss = 5.4177
Epoch 0, accuracy: 0.2230
Epoch 0, Train Loss: 2.1687, Val Loss: 5.4177
batch size: (897, 897)
Epoch 1, accuracy: 0.1675
batch size: (910, 910)
Epoch 2, accuracy: 0.1692
Epoch 2, Train Loss: 4.2162, Val Loss: 7.6285
batch size: (899, 899)
✅ Epoch 3: New best model saved with val_loss = 2.3184
Epoch 3, accuracy: 0.1678
batch size: (895, 895)
✅ Epoch 4: New best model saved with val_loss = 1.3925
Epoch 4, accuracy: 0.1648
Epoch 4, Train Loss: 1.0953, Val Loss: 1.3925
batch size: (890, 890)
✅ Epoch 5: New best model saved with val_loss = 1.2899
Epoch 5, accuracy: 0.3564
batch size: (896, 896)
✅ Epoch 6: New best model saved with val_loss = 1.1931
Epoch 6, accuracy: 0.3566
Epoch 6, Train Loss: 1.0513, Val Loss: 1.1931
batch size: (920, 920)
✅ Epoch 7: New best model saved with val_loss = 1.1635
Epoch 7, accuracy: 0.3523
batch size: (898, 898)
Epoch 8, accuracy: 0.3531
Epoch 8, Train Loss: 1.0326, Val Loss: 1.1679
batch size: (896, 896)
Epoch 9, accuracy: 0.3511
batch size: (894, 894)
Epoch 10, accuracy: 0.3578
Epoch 10, Train Loss: 0.9910, Val Loss: 1.2043
batch size: (891, 891)
Epoch 11, accuracy: 0.3561
batch size: (896, 896)
Epoch 12, accuracy: 0.3536
Epoch 12, Train Loss: 0.9542, Val Loss: 1.2727
batch size: (891, 891)
Epoch 13, accuracy: 0.3554
batch size: (896, 896)
Epoch 14, accuracy: 0.3564
Epoch 14, Train Loss: 0.9664, Val Loss: 1.2703
batch size: (892, 892)
Epoch 15, accuracy: 0.3529
batch size: (892, 892)
Epoch 16, accuracy: 0.3565
Epoch 16, Train Loss: 0.9490, Val Loss: 1.2278
batch size: (902, 902)
Epoch 17, accuracy: 0.3568
batch size: (900, 900)
Epoch 18, accuracy: 0.3585
Epoch 18, Train Loss: 0.9570, Val Loss: 1.2383
batch size: (887, 887)
Epoch 19, accuracy: 0.3557
batch size: (892, 892)
Epoch 20, accuracy: 0.3574
Epoch 20, Train Loss: 0.9137, Val Loss: 1.2405
batch size: (895, 895)
Epoch 21, accuracy: 0.3598
batch size: (888, 888)
Epoch 22, accuracy: 0.3559
Epoch 22, Train Loss: 0.9642, Val Loss: 1.2472
batch size: (918, 918)
Epoch 23, accuracy: 0.3571
batch size: (905, 905)
Epoch 24, accuracy: 0.3574
Epoch 24, Train Loss: 0.9723, Val Loss: 1.2500
batch size: (905, 905)
Epoch 25, accuracy: 0.3565
batch size: (905, 905)
Epoch 26, accuracy: 0.3551
Epoch 26, Train Loss: 0.9155, Val Loss: 1.2669
batch size: (894, 894)
Epoch 27, accuracy: 0.3571
batch size: (881, 881)
Epoch 28, accuracy: 0.3578
Epoch 28, Train Loss: 0.9134, Val Loss: 1.2818
batch size: (910, 910)
Epoch 29, accuracy: 0.3568
batch size: (889, 889)
Epoch 30, accuracy: 0.3577
Epoch 30, Train Loss: 0.9416, Val Loss: 1.2483
batch size: (890, 890)
Epoch 31, accuracy: 0.3573
batch size: (908, 908)
Epoch 32, accuracy: 0.3595
Epoch 32, Train Loss: 0.9452, Val Loss: 1.2308
batch size: (890, 890)
Epoch 33, accuracy: 0.3575
batch size: (914, 914)
Epoch 34, accuracy: 0.3561
Epoch 34, Train Loss: 0.9351, Val Loss: 1.2486
batch size: (909, 909)
Epoch 35, accuracy: 0.3541
batch size: (894, 894)
Epoch 36, accuracy: 0.3554
Epoch 36, Train Loss: 0.9061, Val Loss: 1.2691
batch size: (905, 905)
Epoch 37, accuracy: 0.3573
batch size: (903, 903)
Epoch 38, accuracy: 0.3558
Epoch 38, Train Loss: 0.9385, Val Loss: 1.2615
batch size: (912, 912)
Epoch 39, accuracy: 0.3579
batch size: (904, 904)
Epoch 40, accuracy: 0.3553
Epoch 40, Train Loss: 0.9588, Val Loss: 1.2460
batch size: (895, 895)
Epoch 41, accuracy: 0.3555
batch size: (917, 917)
Epoch 42, accuracy: 0.3592
Epoch 42, Train Loss: 0.9198, Val Loss: 1.2788
batch size: (881, 881)
Epoch 43, accuracy: 0.3568
batch size: (908, 908)
Epoch 44, accuracy: 0.3533
Epoch 44, Train Loss: 0.9622, Val Loss: 1.2747
batch size: (895, 895)
Epoch 45, accuracy: 0.3585
batch size: (908, 908)
Epoch 46, accuracy: 0.3571
Epoch 46, Train Loss: 0.9502, Val Loss: 1.2342
batch size: (902, 902)
Epoch 47, accuracy: 0.3593
batch size: (882, 882)
Epoch 48, accuracy: 0.3541
Epoch 48, Train Loss: 0.9222, Val Loss: 1.2698
batch size: (916, 916)
Epoch 49, accuracy: 0.3597
batch size: (893, 893)
Epoch 50, accuracy: 0.3568
Epoch 50, Train Loss: 0.9479, Val Loss: 1.2775
batch size: (906, 906)
Epoch 51, accuracy: 0.3542
batch size: (889, 889)
Epoch 52, accuracy: 0.3547
Epoch 52, Train Loss: 0.9079, Val Loss: 1.2390
batch size: (919, 919)
Epoch 53, accuracy: 0.3595
batch size: (896, 896)
Epoch 54, accuracy: 0.3562
Epoch 54, Train Loss: 0.9517, Val Loss: 1.2470
batch size: (900, 900)
Epoch 55, accuracy: 0.3581
batch size: (886, 886)
Epoch 56, accuracy: 0.3563
Epoch 56, Train Loss: 0.9219, Val Loss: 1.2767
batch size: (881, 881)
Epoch 57, accuracy: 0.3569
batch size: (895, 895)
Epoch 58, accuracy: 0.3531
Epoch 58, Train Loss: 0.9357, Val Loss: 1.2637
batch size: (895, 895)
Epoch 59, accuracy: 0.3550
batch size: (891, 891)
Epoch 60, accuracy: 0.3552
Epoch 60, Train Loss: 0.9501, Val Loss: 1.2487
batch size: (905, 905)
Epoch 61, accuracy: 0.3552
batch size: (890, 890)
Epoch 62, accuracy: 0.3579
Epoch 62, Train Loss: 0.9501, Val Loss: 1.2249
batch size: (879, 879)
Epoch 63, accuracy: 0.3570
batch size: (930, 930)
Epoch 64, accuracy: 0.3565
Epoch 64, Train Loss: 0.9815, Val Loss: 1.2349
batch size: (904, 904)
Epoch 65, accuracy: 0.3559
batch size: (904, 904)
Epoch 66, accuracy: 0.3556
Epoch 66, Train Loss: 0.9373, Val Loss: 1.2636
batch size: (910, 910)
Epoch 67, accuracy: 0.3565
batch size: (907, 907)
Epoch 68, accuracy: 0.3568
Epoch 68, Train Loss: 0.9565, Val Loss: 1.2756
batch size: (910, 910)
Epoch 69, accuracy: 0.3578
batch size: (903, 903)
Epoch 70, accuracy: 0.3575
Epoch 70, Train Loss: 0.9354, Val Loss: 1.2805
batch size: (902, 902)
Epoch 71, accuracy: 0.3526
batch size: (896, 896)
Epoch 72, accuracy: 0.3565
Epoch 72, Train Loss: 0.9461, Val Loss: 1.2439
batch size: (909, 909)
Epoch 73, accuracy: 0.3590
batch size: (916, 916)
Epoch 74, accuracy: 0.3587
Epoch 74, Train Loss: 0.9362, Val Loss: 1.2564
batch size: (895, 895)
Epoch 75, accuracy: 0.3573
batch size: (895, 895)
Epoch 76, accuracy: 0.3604
Epoch 76, Train Loss: 0.9521, Val Loss: 1.2403
batch size: (899, 899)
Epoch 77, accuracy: 0.3528
batch size: (898, 898)
Epoch 78, accuracy: 0.3572
Epoch 78, Train Loss: 0.9476, Val Loss: 1.2459
batch size: (903, 903)
Epoch 79, accuracy: 0.3564
batch size: (875, 875)
Epoch 80, accuracy: 0.3592
Epoch 80, Train Loss: 0.9009, Val Loss: 1.2591
batch size: (905, 905)
Epoch 81, accuracy: 0.3570
batch size: (923, 923)
Epoch 82, accuracy: 0.3549
Epoch 82, Train Loss: 0.9412, Val Loss: 1.2557
batch size: (891, 891)
Epoch 83, accuracy: 0.3562
batch size: (885, 885)
Epoch 84, accuracy: 0.3550
Epoch 84, Train Loss: 0.9409, Val Loss: 1.2446
batch size: (899, 899)
Epoch 85, accuracy: 0.3564
batch size: (879, 879)
Epoch 86, accuracy: 0.3554
Epoch 86, Train Loss: 0.9235, Val Loss: 1.2700
batch size: (879, 879)
Epoch 87, accuracy: 0.3547
batch size: (915, 915)
Epoch 88, accuracy: 0.3588
Epoch 88, Train Loss: 0.9284, Val Loss: 1.2498
batch size: (909, 909)
Epoch 89, accuracy: 0.3574
batch size: (903, 903)
Epoch 90, accuracy: 0.3550
Epoch 90, Train Loss: 0.9258, Val Loss: 1.2386
batch size: (884, 884)
Epoch 91, accuracy: 0.3546
batch size: (898, 898)
Epoch 92, accuracy: 0.3596
Epoch 92, Train Loss: 0.9685, Val Loss: 1.2469
batch size: (904, 904)
Epoch 93, accuracy: 0.3574
batch size: (905, 905)
Epoch 94, accuracy: 0.3539
Epoch 94, Train Loss: 0.9266, Val Loss: 1.2621
batch size: (871, 871)
Epoch 95, accuracy: 0.3543
batch size: (894, 894)
Epoch 96, accuracy: 0.3548
Epoch 96, Train Loss: 0.9330, Val Loss: 1.2562
batch size: (887, 887)
Epoch 97, accuracy: 0.3551
batch size: (910, 910)
Epoch 98, accuracy: 0.3577
Epoch 98, Train Loss: 0.9694, Val Loss: 1.2375
batch size: (872, 872)
Epoch 99, accuracy: 0.3577
batch size: (900, 900)
Epoch 100, accuracy: 0.3538
Epoch 100, Train Loss: 0.9307, Val Loss: 1.2434
batch size: (897, 897)
Epoch 101, accuracy: 0.3552
batch size: (912, 912)
Epoch 102, accuracy: 0.3512
Epoch 102, Train Loss: 0.9397, Val Loss: 1.2638
batch size: (887, 887)
Epoch 103, accuracy: 0.3540
batch size: (901, 901)
Epoch 104, accuracy: 0.3599
Epoch 104, Train Loss: 0.9565, Val Loss: 1.2395
batch size: (900, 900)
Epoch 105, accuracy: 0.3550
batch size: (904, 904)
Epoch 106, accuracy: 0.3544
Epoch 106, Train Loss: 0.9450, Val Loss: 1.2302
batch size: (908, 908)
Epoch 107, accuracy: 0.3568
batch size: (911, 911)
Epoch 108, accuracy: 0.3575
Epoch 108, Train Loss: 0.9456, Val Loss: 1.2445
batch size: (890, 890)
Epoch 109, accuracy: 0.3554
batch size: (896, 896)
Epoch 110, accuracy: 0.3607
Epoch 110, Train Loss: 0.9479, Val Loss: 1.2353
batch size: (897, 897)
Epoch 111, accuracy: 0.3548
batch size: (901, 901)
Epoch 112, accuracy: 0.3568
Epoch 112, Train Loss: 0.9517, Val Loss: 1.2543
batch size: (882, 882)
Epoch 113, accuracy: 0.3578
batch size: (914, 914)
Epoch 114, accuracy: 0.3587
Epoch 114, Train Loss: 0.9506, Val Loss: 1.2353
batch size: (903, 903)
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 115, accuracy: 0.3580
batch size: (898, 898)
Epoch 116, accuracy: 0.3547
Epoch 116, Train Loss: 0.9396, Val Loss: 1.2406
batch size: (898, 898)
Epoch 117, accuracy: 0.3529
batch size: (910, 910)
Epoch 118, accuracy: 0.3529
Epoch 118, Train Loss: 0.9348, Val Loss: 1.2526
batch size: (884, 884)
Epoch 119, accuracy: 0.3554
batch size: (897, 897)
Epoch 120, accuracy: 0.3536
Epoch 120, Train Loss: 0.9321, Val Loss: 1.2398
batch size: (873, 873)
Epoch 121, accuracy: 0.3560
batch size: (896, 896)
Epoch 122, accuracy: 0.3568
Epoch 122, Train Loss: 0.9384, Val Loss: 1.2587
batch size: (898, 898)
Epoch 123, accuracy: 0.3613
batch size: (895, 895)
Epoch 124, accuracy: 0.3541
Epoch 124, Train Loss: 0.9519, Val Loss: 1.2637
batch size: (880, 880)
Epoch 125, accuracy: 0.3599
batch size: (908, 908)
Epoch 126, accuracy: 0.3580
Epoch 126, Train Loss: 0.9572, Val Loss: 1.2419
batch size: (916, 916)
Epoch 127, accuracy: 0.3569
batch size: (909, 909)
Epoch 128, accuracy: 0.3598
Epoch 128, Train Loss: 0.9673, Val Loss: 1.2741
batch size: (892, 892)
Epoch 129, accuracy: 0.3584
batch size: (911, 911)
Epoch 130, accuracy: 0.3557
Epoch 130, Train Loss: 0.9679, Val Loss: 1.2531
batch size: (902, 902)
Epoch 131, accuracy: 0.3579
batch size: (892, 892)
Epoch 132, accuracy: 0.3549
Epoch 132, Train Loss: 0.9127, Val Loss: 1.2568
batch size: (926, 926)
Epoch 133, accuracy: 0.3573
batch size: (887, 887)
Epoch 134, accuracy: 0.3568
Epoch 134, Train Loss: 0.9686, Val Loss: 1.2404
batch size: (898, 898)
Epoch 135, accuracy: 0.3550
batch size: (886, 886)
Epoch 136, accuracy: 0.3588
Epoch 136, Train Loss: 0.9553, Val Loss: 1.2531
batch size: (893, 893)
Epoch 137, accuracy: 0.3540
batch size: (913, 913)
Epoch 138, accuracy: 0.3594
Epoch 138, Train Loss: 0.9489, Val Loss: 1.2335
batch size: (899, 899)
Epoch 139, accuracy: 0.3534
batch size: (895, 895)
Epoch 140, accuracy: 0.3575
Epoch 140, Train Loss: 0.9690, Val Loss: 1.2477
batch size: (897, 897)
Epoch 141, accuracy: 0.3548
batch size: (893, 893)
Epoch 142, accuracy: 0.3543
Epoch 142, Train Loss: 0.9161, Val Loss: 1.2498
batch size: (892, 892)
Epoch 143, accuracy: 0.3586
batch size: (896, 896)
Epoch 144, accuracy: 0.3561
Epoch 144, Train Loss: 0.9716, Val Loss: 1.2347
batch size: (892, 892)
Epoch 145, accuracy: 0.3552
batch size: (890, 890)
Epoch 146, accuracy: 0.3575
Epoch 146, Train Loss: 0.9422, Val Loss: 1.2451
batch size: (899, 899)
Epoch 147, accuracy: 0.3571
batch size: (887, 887)
Epoch 148, accuracy: 0.3573
Epoch 148, Train Loss: 0.9581, Val Loss: 1.2660
batch size: (923, 923)
Epoch 149, accuracy: 0.3494
batch size: (919, 919)
Epoch 150, accuracy: 0.3572
Epoch 150, Train Loss: 0.9480, Val Loss: 1.2309
batch size: (902, 902)
Epoch 151, accuracy: 0.3573
batch size: (896, 896)
Epoch 152, accuracy: 0.3556
Epoch 152, Train Loss: 0.9695, Val Loss: 1.2316
batch size: (905, 905)
Epoch 153, accuracy: 0.3555
batch size: (913, 913)
Epoch 154, accuracy: 0.3550
Epoch 154, Train Loss: 0.9534, Val Loss: 1.2522
batch size: (900, 900)
Epoch 155, accuracy: 0.3551
batch size: (886, 886)
Epoch 156, accuracy: 0.3593
Epoch 156, Train Loss: 0.9598, Val Loss: 1.2333
batch size: (898, 898)
Epoch 157, accuracy: 0.3598
batch size: (878, 878)
Epoch 158, accuracy: 0.3603
Epoch 158, Train Loss: 0.9492, Val Loss: 1.2493
batch size: (903, 903)
Epoch 159, accuracy: 0.3606
batch size: (910, 910)
Epoch 160, accuracy: 0.3560
Epoch 160, Train Loss: 0.9369, Val Loss: 1.2646
batch size: (904, 904)
Epoch 161, accuracy: 0.3562
batch size: (899, 899)
Epoch 162, accuracy: 0.3557
Epoch 162, Train Loss: 0.9964, Val Loss: 1.2462
batch size: (894, 894)
Epoch 163, accuracy: 0.3578
batch size: (901, 901)
Epoch 164, accuracy: 0.3567
Epoch 164, Train Loss: 0.9230, Val Loss: 1.2552
batch size: (889, 889)
Epoch 165, accuracy: 0.3539
batch size: (897, 897)
Epoch 166, accuracy: 0.3578
Epoch 166, Train Loss: 0.9403, Val Loss: 1.2616
batch size: (917, 917)
Epoch 167, accuracy: 0.3539
batch size: (899, 899)
Epoch 168, accuracy: 0.3560
Epoch 168, Train Loss: 0.9529, Val Loss: 1.2450
batch size: (917, 917)
Epoch 169, accuracy: 0.3558
batch size: (908, 908)
Epoch 170, accuracy: 0.3581
Epoch 170, Train Loss: 0.9539, Val Loss: 1.2374
batch size: (867, 867)
Epoch 171, accuracy: 0.3542
batch size: (905, 905)
Epoch 172, accuracy: 0.3609
Epoch 172, Train Loss: 0.9368, Val Loss: 1.2627
batch size: (899, 899)
Epoch 173, accuracy: 0.3574
batch size: (918, 918)
Epoch 174, accuracy: 0.3559
Epoch 174, Train Loss: 0.9315, Val Loss: 1.2298
batch size: (918, 918)
Epoch 175, accuracy: 0.3547
batch size: (892, 892)
Epoch 176, accuracy: 0.3555
Epoch 176, Train Loss: 0.9115, Val Loss: 1.2665
batch size: (896, 896)
Epoch 177, accuracy: 0.3539
batch size: (907, 907)
Epoch 178, accuracy: 0.3567
Epoch 178, Train Loss: 0.9676, Val Loss: 1.2765
batch size: (905, 905)
Epoch 179, accuracy: 0.3571
batch size: (892, 892)
Epoch 180, accuracy: 0.3548
Epoch 180, Train Loss: 0.9602, Val Loss: 1.2464
batch size: (881, 881)
Epoch 181, accuracy: 0.3564
batch size: (906, 906)
Epoch 182, accuracy: 0.3563
Epoch 182, Train Loss: 0.9366, Val Loss: 1.2444
batch size: (888, 888)
Epoch 183, accuracy: 0.3590
batch size: (893, 893)
Epoch 184, accuracy: 0.3594
Epoch 184, Train Loss: 0.9582, Val Loss: 1.2390
batch size: (892, 892)
Epoch 185, accuracy: 0.3570
batch size: (893, 893)
Epoch 186, accuracy: 0.3556
Epoch 186, Train Loss: 0.9379, Val Loss: 1.2732
batch size: (913, 913)
Epoch 187, accuracy: 0.3547
batch size: (907, 907)
Epoch 188, accuracy: 0.3532
Epoch 188, Train Loss: 0.9358, Val Loss: 1.2810
batch size: (899, 899)
Epoch 189, accuracy: 0.3567
batch size: (912, 912)
Epoch 190, accuracy: 0.3569
Epoch 190, Train Loss: 0.9405, Val Loss: 1.2642
batch size: (889, 889)
Epoch 191, accuracy: 0.3529
batch size: (905, 905)
Epoch 192, accuracy: 0.3558
Epoch 192, Train Loss: 0.9506, Val Loss: 1.2651
batch size: (877, 877)
Epoch 193, accuracy: 0.3548
batch size: (886, 886)
Epoch 194, accuracy: 0.3541
Epoch 194, Train Loss: 0.9519, Val Loss: 1.2470
batch size: (908, 908)
Epoch 195, accuracy: 0.3584
batch size: (897, 897)
Epoch 196, accuracy: 0.3583
Epoch 196, Train Loss: 0.9769, Val Loss: 1.2413
batch size: (926, 926)
Epoch 197, accuracy: 0.3551
batch size: (906, 906)
Epoch 198, accuracy: 0.3572
Epoch 198, Train Loss: 0.9540, Val Loss: 1.2681
batch size: (897, 897)
Epoch 199, accuracy: 0.3570
Loaded best model with val_loss = 1.1635254621505737
test :accuracy 0.3589, f1_macro: 0.2357, f1_micro: 0.3589, auc: 0.5170
Training GINConv with 32 layers...
可训练参数: 4236043_GINConv
不可训练参数: 0
batch size: (894, 894)
✅ Epoch 0: New best model saved with val_loss = 44294980.0000
Epoch 0, accuracy: 0.4058
Epoch 0, Train Loss: 1.3657, Val Loss: 44294980.0000
batch size: (893, 893)
✅ Epoch 1: New best model saved with val_loss = 30150.2988
Epoch 1, accuracy: 0.3589
batch size: (911, 911)
✅ Epoch 2: New best model saved with val_loss = 89.8882
Epoch 2, accuracy: 0.3576
Epoch 2, Train Loss: 1.1137, Val Loss: 89.8882
batch size: (895, 895)
✅ Epoch 3: New best model saved with val_loss = 5.0078
Epoch 3, accuracy: 0.3588
batch size: (909, 909)
✅ Epoch 4: New best model saved with val_loss = 3.4958
Epoch 4, accuracy: 0.4069
Epoch 4, Train Loss: 1.4305, Val Loss: 3.4958
batch size: (914, 914)
✅ Epoch 5: New best model saved with val_loss = 1.2822
Epoch 5, accuracy: 0.4055
batch size: (913, 913)
✅ Epoch 6: New best model saved with val_loss = 1.2501
Epoch 6, accuracy: 0.4021
Epoch 6, Train Loss: 1.7783, Val Loss: 1.2501
batch size: (879, 879)
✅ Epoch 7: New best model saved with val_loss = 1.1578
Epoch 7, accuracy: 0.3543
batch size: (912, 912)
Epoch 8, accuracy: 0.3561
Epoch 8, Train Loss: 1.2586, Val Loss: 1.1712
batch size: (891, 891)
✅ Epoch 9: New best model saved with val_loss = 1.1350
Epoch 9, accuracy: 0.4017
batch size: (891, 891)
✅ Epoch 10: New best model saved with val_loss = 1.1293
Epoch 10, accuracy: 0.4051
Epoch 10, Train Loss: 1.2084, Val Loss: 1.1293
batch size: (901, 901)
✅ Epoch 11: New best model saved with val_loss = 1.1262
Epoch 11, accuracy: 0.4049
batch size: (887, 887)
Epoch 12, accuracy: 0.4028
Epoch 12, Train Loss: 1.3956, Val Loss: 1.1849
batch size: (888, 888)
Epoch 13, accuracy: 0.4037
batch size: (902, 902)
Epoch 14, accuracy: 0.3551
Epoch 14, Train Loss: 1.2388, Val Loss: 1.1620
batch size: (895, 895)
✅ Epoch 15: New best model saved with val_loss = 1.1206
Epoch 15, accuracy: 0.3572
batch size: (878, 878)
✅ Epoch 16: New best model saved with val_loss = 1.1149
Epoch 16, accuracy: 0.3759
Epoch 16, Train Loss: 1.1019, Val Loss: 1.1149
batch size: (913, 913)
Epoch 17, accuracy: 0.3754
batch size: (907, 907)
Epoch 18, accuracy: 0.3772
Epoch 18, Train Loss: 1.0695, Val Loss: 1.1232
batch size: (903, 903)
Epoch 19, accuracy: 0.3769
batch size: (906, 906)
✅ Epoch 20: New best model saved with val_loss = 1.1132
Epoch 20, accuracy: 0.3718
Epoch 20, Train Loss: 1.1317, Val Loss: 1.1132
batch size: (884, 884)
✅ Epoch 21: New best model saved with val_loss = 1.1125
Epoch 21, accuracy: 0.4205
batch size: (903, 903)
Epoch 22, accuracy: 0.4185
Epoch 22, Train Loss: 1.0851, Val Loss: 1.1169
batch size: (889, 889)
Epoch 23, accuracy: 0.4212
batch size: (878, 878)
✅ Epoch 24: New best model saved with val_loss = 1.1088
Epoch 24, accuracy: 0.3718
Epoch 24, Train Loss: 1.0688, Val Loss: 1.1088
batch size: (891, 891)
Epoch 25, accuracy: 0.3742
batch size: (890, 890)
Epoch 26, accuracy: 0.3723
Epoch 26, Train Loss: 1.1048, Val Loss: 1.1237
batch size: (899, 899)
Epoch 27, accuracy: 0.3730
batch size: (889, 889)
Epoch 28, accuracy: 0.3774
Epoch 28, Train Loss: 1.0781, Val Loss: 1.1280
batch size: (887, 887)
Epoch 29, accuracy: 0.3734
batch size: (885, 885)
Epoch 30, accuracy: 0.4183
Epoch 30, Train Loss: 1.0701, Val Loss: 1.1113
batch size: (891, 891)
✅ Epoch 31: New best model saved with val_loss = 1.1047
Epoch 31, accuracy: 0.4209
batch size: (910, 910)
Epoch 32, accuracy: 0.4199
Epoch 32, Train Loss: 1.0639, Val Loss: 1.1070
batch size: (897, 897)
✅ Epoch 33: New best model saved with val_loss = 1.1027
Epoch 33, accuracy: 0.4193
batch size: (898, 898)
Epoch 34, accuracy: 0.4228
Epoch 34, Train Loss: 1.0646, Val Loss: 1.1077
batch size: (900, 900)
Epoch 35, accuracy: 0.4197
batch size: (898, 898)
Epoch 36, accuracy: 0.4216
Epoch 36, Train Loss: 1.0593, Val Loss: 1.1096
batch size: (887, 887)
Epoch 37, accuracy: 0.4203
batch size: (882, 882)
Epoch 38, accuracy: 0.4194
Epoch 38, Train Loss: 1.0712, Val Loss: 1.1037
batch size: (916, 916)
Epoch 39, accuracy: 0.4223
batch size: (906, 906)
Epoch 40, accuracy: 0.4203
Epoch 40, Train Loss: 1.2667, Val Loss: 1.1195
batch size: (879, 879)
Epoch 41, accuracy: 0.4179
batch size: (895, 895)
Epoch 42, accuracy: 0.4172
Epoch 42, Train Loss: 1.0698, Val Loss: 1.1050
batch size: (875, 875)
Epoch 43, accuracy: 0.4186
batch size: (893, 893)
Epoch 44, accuracy: 0.4173
Epoch 44, Train Loss: 1.0761, Val Loss: 1.1213
batch size: (921, 921)
Epoch 45, accuracy: 0.4209
batch size: (862, 862)
Epoch 46, accuracy: 0.4175
Epoch 46, Train Loss: 1.0958, Val Loss: 1.1089
batch size: (906, 906)
Epoch 47, accuracy: 0.4171
batch size: (891, 891)
Epoch 48, accuracy: 0.4220
Epoch 48, Train Loss: 1.0646, Val Loss: 1.1070
batch size: (908, 908)
Epoch 49, accuracy: 0.4209
batch size: (891, 891)
Epoch 50, accuracy: 0.4203
Epoch 50, Train Loss: 1.1991, Val Loss: 1.1145
batch size: (908, 908)
Epoch 51, accuracy: 0.4247
batch size: (904, 904)
Epoch 52, accuracy: 0.4203
Epoch 52, Train Loss: 1.0622, Val Loss: 1.1066
batch size: (899, 899)
Epoch 53, accuracy: 0.4201
batch size: (904, 904)
✅ Epoch 54: New best model saved with val_loss = 1.1019
Epoch 54, accuracy: 0.4197
Epoch 54, Train Loss: 1.0660, Val Loss: 1.1019
batch size: (901, 901)
Epoch 55, accuracy: 0.4216
batch size: (901, 901)
Epoch 56, accuracy: 0.4179
Epoch 56, Train Loss: 1.1334, Val Loss: 1.1158
batch size: (887, 887)
Epoch 57, accuracy: 0.4227
batch size: (874, 874)
Epoch 58, accuracy: 0.4217
Epoch 58, Train Loss: 1.2262, Val Loss: 1.1176
batch size: (899, 899)
Epoch 59, accuracy: 0.4187
batch size: (891, 891)
Epoch 60, accuracy: 0.4200
Epoch 60, Train Loss: 1.0706, Val Loss: 1.1037
batch size: (914, 914)
Epoch 61, accuracy: 0.4226
batch size: (887, 887)
Epoch 62, accuracy: 0.4168
Epoch 62, Train Loss: 1.0645, Val Loss: 1.1067
batch size: (906, 906)
Epoch 63, accuracy: 0.4193
batch size: (917, 917)
Epoch 64, accuracy: 0.4182
Epoch 64, Train Loss: 1.0623, Val Loss: 1.1078
batch size: (895, 895)
Epoch 65, accuracy: 0.4239
batch size: (908, 908)
Epoch 66, accuracy: 0.4179
Epoch 66, Train Loss: 1.2465, Val Loss: 1.1155
batch size: (909, 909)
Epoch 67, accuracy: 0.4197
batch size: (901, 901)
Epoch 68, accuracy: 0.4187
Epoch 68, Train Loss: 1.2392, Val Loss: 1.1133
batch size: (886, 886)
Epoch 69, accuracy: 0.4204
batch size: (892, 892)
Epoch 70, accuracy: 0.4215
Epoch 70, Train Loss: 1.2532, Val Loss: 1.1199
batch size: (902, 902)
Epoch 71, accuracy: 0.4214
batch size: (902, 902)
Epoch 72, accuracy: 0.4197
Epoch 72, Train Loss: 1.1117, Val Loss: 1.1133
batch size: (902, 902)
Epoch 73, accuracy: 0.4207
batch size: (882, 882)
Epoch 74, accuracy: 0.4185
Epoch 74, Train Loss: 1.0632, Val Loss: 1.1097
batch size: (900, 900)
Epoch 75, accuracy: 0.4198
batch size: (900, 900)
Epoch 76, accuracy: 0.4202
Epoch 76, Train Loss: 1.0738, Val Loss: 1.1056
batch size: (904, 904)
Epoch 77, accuracy: 0.4210
batch size: (926, 926)
Epoch 78, accuracy: 0.4185
Epoch 78, Train Loss: 1.2587, Val Loss: 1.1153
batch size: (899, 899)
Epoch 79, accuracy: 0.4187
batch size: (898, 898)
Epoch 80, accuracy: 0.4203
Epoch 80, Train Loss: 1.0709, Val Loss: 1.1083
batch size: (900, 900)
Epoch 81, accuracy: 0.4189
batch size: (903, 903)
Epoch 82, accuracy: 0.4204
Epoch 82, Train Loss: 1.2746, Val Loss: 1.1148
batch size: (893, 893)
Epoch 83, accuracy: 0.4200
batch size: (892, 892)
Epoch 84, accuracy: 0.4189
Epoch 84, Train Loss: 1.0630, Val Loss: 1.1073
batch size: (906, 906)
Epoch 85, accuracy: 0.4188
batch size: (897, 897)
Epoch 86, accuracy: 0.4210
Epoch 86, Train Loss: 1.0674, Val Loss: 1.1053
batch size: (908, 908)
Epoch 87, accuracy: 0.4195
batch size: (911, 911)
Epoch 88, accuracy: 0.4231
Epoch 88, Train Loss: 1.1188, Val Loss: 1.1233
batch size: (900, 900)
Epoch 89, accuracy: 0.4203
batch size: (893, 893)
Epoch 90, accuracy: 0.4192
Epoch 90, Train Loss: 1.1029, Val Loss: 1.1370
batch size: (901, 901)
Epoch 91, accuracy: 0.4197
batch size: (888, 888)
Epoch 92, accuracy: 0.4185
Epoch 92, Train Loss: 1.1485, Val Loss: 1.1170
batch size: (909, 909)
Epoch 93, accuracy: 0.4196
batch size: (878, 878)
Epoch 94, accuracy: 0.4184
Epoch 94, Train Loss: 1.1847, Val Loss: 1.1178
batch size: (900, 900)
Epoch 95, accuracy: 0.4208
batch size: (892, 892)
Epoch 96, accuracy: 0.4211
Epoch 96, Train Loss: 1.0696, Val Loss: 1.1105
batch size: (900, 900)
Epoch 97, accuracy: 0.4217
batch size: (887, 887)
Epoch 98, accuracy: 0.4219
Epoch 98, Train Loss: 1.1815, Val Loss: 1.1169
batch size: (882, 882)
Epoch 99, accuracy: 0.4213
batch size: (892, 892)
Epoch 100, accuracy: 0.4188
Epoch 100, Train Loss: 1.0718, Val Loss: 1.1071
batch size: (896, 896)
Epoch 101, accuracy: 0.4230
batch size: (911, 911)
Epoch 102, accuracy: 0.4223
Epoch 102, Train Loss: 1.3055, Val Loss: 1.1150
batch size: (892, 892)
Epoch 103, accuracy: 0.4195
batch size: (893, 893)
Epoch 104, accuracy: 0.4182
Epoch 104, Train Loss: 1.1037, Val Loss: 1.1137
batch size: (897, 897)
Epoch 105, accuracy: 0.4216
batch size: (890, 890)
Epoch 106, accuracy: 0.4212
Epoch 106, Train Loss: 1.2481, Val Loss: 1.1132
batch size: (914, 914)
Epoch 107, accuracy: 0.4184
batch size: (913, 913)
Epoch 108, accuracy: 0.4190
Epoch 108, Train Loss: 1.0713, Val Loss: 1.1109
batch size: (898, 898)
Epoch 109, accuracy: 0.4207
batch size: (899, 899)
Epoch 110, accuracy: 0.4191
Epoch 110, Train Loss: 1.2553, Val Loss: 1.1219
batch size: (913, 913)
Epoch 111, accuracy: 0.4178
batch size: (913, 913)
Epoch 112, accuracy: 0.4176
Epoch 112, Train Loss: 1.0722, Val Loss: 1.1089
batch size: (874, 874)
Epoch 113, accuracy: 0.4200
batch size: (904, 904)
Epoch 114, accuracy: 0.4207
Epoch 114, Train Loss: 1.0711, Val Loss: 1.1042
batch size: (896, 896)
Epoch 115, accuracy: 0.4224
batch size: (908, 908)
Epoch 116, accuracy: 0.4163
Epoch 116, Train Loss: 1.0789, Val Loss: 1.1118
batch size: (904, 904)
Epoch 117, accuracy: 0.4218
batch size: (901, 901)
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 118, accuracy: 0.4179
Epoch 118, Train Loss: 1.1726, Val Loss: 1.1166
batch size: (903, 903)
Epoch 119, accuracy: 0.4209
batch size: (888, 888)
Epoch 120, accuracy: 0.4210
Epoch 120, Train Loss: 1.2274, Val Loss: 1.1223
batch size: (898, 898)
Epoch 121, accuracy: 0.4181
batch size: (898, 898)
Epoch 122, accuracy: 0.4201
Epoch 122, Train Loss: 1.1394, Val Loss: 1.1164
batch size: (896, 896)
Epoch 123, accuracy: 0.4170
batch size: (894, 894)
Epoch 124, accuracy: 0.4191
Epoch 124, Train Loss: 1.0698, Val Loss: 1.1031
batch size: (908, 908)
Epoch 125, accuracy: 0.4192
batch size: (898, 898)
Epoch 126, accuracy: 0.4174
Epoch 126, Train Loss: 1.0653, Val Loss: 1.1100
batch size: (909, 909)
Epoch 127, accuracy: 0.4199
batch size: (897, 897)
Epoch 128, accuracy: 0.4236
Epoch 128, Train Loss: 1.0638, Val Loss: 1.1123
batch size: (894, 894)
Epoch 129, accuracy: 0.4172
batch size: (888, 888)
Epoch 130, accuracy: 0.4185
Epoch 130, Train Loss: 1.0662, Val Loss: 1.1024
batch size: (902, 902)
Epoch 131, accuracy: 0.4185
batch size: (902, 902)
Epoch 132, accuracy: 0.4189
Epoch 132, Train Loss: 1.0797, Val Loss: 1.1157
batch size: (898, 898)
Epoch 133, accuracy: 0.4186
batch size: (894, 894)
Epoch 134, accuracy: 0.4176
Epoch 134, Train Loss: 1.0700, Val Loss: 1.1075
batch size: (907, 907)
Epoch 135, accuracy: 0.4199
batch size: (892, 892)
Epoch 136, accuracy: 0.4223
Epoch 136, Train Loss: 1.1772, Val Loss: 1.1163
batch size: (908, 908)
Epoch 137, accuracy: 0.4197
batch size: (915, 915)
Epoch 138, accuracy: 0.4203
Epoch 138, Train Loss: 1.2650, Val Loss: 1.1200
batch size: (901, 901)
Epoch 139, accuracy: 0.4210
batch size: (887, 887)
Epoch 140, accuracy: 0.4174
Epoch 140, Train Loss: 1.0873, Val Loss: 1.1152
batch size: (888, 888)
Epoch 141, accuracy: 0.4166
batch size: (886, 886)
✅ Epoch 142: New best model saved with val_loss = 1.1007
Epoch 142, accuracy: 0.4175
Epoch 142, Train Loss: 1.0597, Val Loss: 1.1007
batch size: (909, 909)
Epoch 143, accuracy: 0.4200
batch size: (909, 909)
Epoch 144, accuracy: 0.4188
Epoch 144, Train Loss: 1.0630, Val Loss: 1.1062
batch size: (896, 896)
Epoch 145, accuracy: 0.4211
batch size: (909, 909)
Epoch 146, accuracy: 0.4206
Epoch 146, Train Loss: 1.1595, Val Loss: 1.1123
batch size: (897, 897)
Epoch 147, accuracy: 0.4190
batch size: (894, 894)
Epoch 148, accuracy: 0.4212
Epoch 148, Train Loss: 1.1598, Val Loss: 1.1165
batch size: (904, 904)
Epoch 149, accuracy: 0.4171
batch size: (900, 900)
Epoch 150, accuracy: 0.4224
Epoch 150, Train Loss: 1.2178, Val Loss: 1.1233
batch size: (904, 904)
Epoch 151, accuracy: 0.4192
batch size: (889, 889)
Epoch 152, accuracy: 0.4208
Epoch 152, Train Loss: 1.0595, Val Loss: 1.1042
batch size: (893, 893)
Epoch 153, accuracy: 0.4170
batch size: (901, 901)
Epoch 154, accuracy: 0.4183
Epoch 154, Train Loss: 1.1244, Val Loss: 1.1188
batch size: (905, 905)
Epoch 155, accuracy: 0.4219
batch size: (891, 891)
Epoch 156, accuracy: 0.4234
Epoch 156, Train Loss: 1.0703, Val Loss: 1.1049
batch size: (912, 912)
Epoch 157, accuracy: 0.4216
batch size: (904, 904)
Epoch 158, accuracy: 0.4191
Epoch 158, Train Loss: 1.1122, Val Loss: 1.1129
batch size: (882, 882)
Epoch 159, accuracy: 0.4187
batch size: (905, 905)
Epoch 160, accuracy: 0.4216
Epoch 160, Train Loss: 1.1404, Val Loss: 1.1103
batch size: (861, 861)
Epoch 161, accuracy: 0.4191
batch size: (909, 909)
Epoch 162, accuracy: 0.4185
Epoch 162, Train Loss: 1.1718, Val Loss: 1.1219
batch size: (905, 905)
Epoch 163, accuracy: 0.4163
batch size: (916, 916)
Epoch 164, accuracy: 0.4197
Epoch 164, Train Loss: 1.0701, Val Loss: 1.1069
batch size: (918, 918)
Epoch 165, accuracy: 0.4214
batch size: (894, 894)
Epoch 166, accuracy: 0.4201
Epoch 166, Train Loss: 1.1329, Val Loss: 1.1189
batch size: (902, 902)
Epoch 167, accuracy: 0.4215
batch size: (892, 892)
Epoch 168, accuracy: 0.4202
Epoch 168, Train Loss: 1.0672, Val Loss: 1.1098
batch size: (924, 924)
Epoch 169, accuracy: 0.4209
batch size: (889, 889)
Epoch 170, accuracy: 0.4210
Epoch 170, Train Loss: 1.0599, Val Loss: 1.1075
batch size: (908, 908)
Epoch 171, accuracy: 0.4221
batch size: (889, 889)
Epoch 172, accuracy: 0.4172
Epoch 172, Train Loss: 1.1422, Val Loss: 1.1174
batch size: (903, 903)
Epoch 173, accuracy: 0.4199
batch size: (909, 909)
Epoch 174, accuracy: 0.4193
Epoch 174, Train Loss: 1.0693, Val Loss: 1.1069
batch size: (889, 889)
Epoch 175, accuracy: 0.4193
batch size: (890, 890)
Epoch 176, accuracy: 0.4219
Epoch 176, Train Loss: 1.0624, Val Loss: 1.1115
batch size: (916, 916)
Epoch 177, accuracy: 0.4165
batch size: (877, 877)
Epoch 178, accuracy: 0.4219
Epoch 178, Train Loss: 1.0731, Val Loss: 1.1151
batch size: (906, 906)
Epoch 179, accuracy: 0.4222
batch size: (904, 904)
Epoch 180, accuracy: 0.4180
Epoch 180, Train Loss: 1.0630, Val Loss: 1.1061
batch size: (889, 889)
Epoch 181, accuracy: 0.4159
batch size: (897, 897)
Epoch 182, accuracy: 0.4242
Epoch 182, Train Loss: 1.1141, Val Loss: 1.1238
batch size: (917, 917)
Epoch 183, accuracy: 0.4150
batch size: (900, 900)
Epoch 184, accuracy: 0.4191
Epoch 184, Train Loss: 1.0710, Val Loss: 1.1074
batch size: (907, 907)
Epoch 185, accuracy: 0.4171
batch size: (908, 908)
Epoch 186, accuracy: 0.4209
Epoch 186, Train Loss: 1.0679, Val Loss: 1.1127
batch size: (894, 894)
Epoch 187, accuracy: 0.4201
batch size: (888, 888)
Epoch 188, accuracy: 0.4187
Epoch 188, Train Loss: 1.0711, Val Loss: 1.1081
batch size: (898, 898)
Epoch 189, accuracy: 0.4200
batch size: (904, 904)
Epoch 190, accuracy: 0.4212
Epoch 190, Train Loss: 1.0703, Val Loss: 1.1028
batch size: (904, 904)
Epoch 191, accuracy: 0.4215
batch size: (900, 900)
Epoch 192, accuracy: 0.4209
Epoch 192, Train Loss: 1.2612, Val Loss: 1.1320
batch size: (915, 915)
Epoch 193, accuracy: 0.4192
batch size: (887, 887)
Epoch 194, accuracy: 0.4208
Epoch 194, Train Loss: 1.0752, Val Loss: 1.1074
batch size: (890, 890)
Epoch 195, accuracy: 0.4197
batch size: (906, 906)
Epoch 196, accuracy: 0.4225
Epoch 196, Train Loss: 1.0606, Val Loss: 1.1036
batch size: (888, 888)
Epoch 197, accuracy: 0.4199
batch size: (895, 895)
Epoch 198, accuracy: 0.4210
Epoch 198, Train Loss: 1.1406, Val Loss: 1.1171
batch size: (893, 893)
Epoch 199, accuracy: 0.4199
Loaded best model with val_loss = 1.1006817817687988
test :accuracy 0.4184, f1_macro: 0.2740, f1_micro: 0.4184, auc: 0.5007
Training mlp with 2 layers...
可训练参数: 83465_mlp
不可训练参数: 0
batch size: (913, 913)
✅ Epoch 0: New best model saved with val_loss = 1.0942
Epoch 0, accuracy: 0.4292
Epoch 0, Train Loss: 1.1424, Val Loss: 1.0942
batch size: (904, 904)
Epoch 1, accuracy: 0.4347
batch size: (898, 898)
✅ Epoch 2: New best model saved with val_loss = 1.0936
Epoch 2, accuracy: 0.5236
Epoch 2, Train Loss: 0.2150, Val Loss: 1.0936
batch size: (904, 904)
✅ Epoch 3: New best model saved with val_loss = 1.0909
Epoch 3, accuracy: 0.5031
batch size: (902, 902)
✅ Epoch 4: New best model saved with val_loss = 1.0877
Epoch 4, accuracy: 0.4518
Epoch 4, Train Loss: 0.0244, Val Loss: 1.0877
batch size: (906, 906)
✅ Epoch 5: New best model saved with val_loss = 1.0844
Epoch 5, accuracy: 0.4243
batch size: (895, 895)
✅ Epoch 6: New best model saved with val_loss = 1.0810
Epoch 6, accuracy: 0.4140
Epoch 6, Train Loss: 0.0035, Val Loss: 1.0810
batch size: (908, 908)
✅ Epoch 7: New best model saved with val_loss = 1.0776
Epoch 7, accuracy: 0.4129
batch size: (880, 880)
✅ Epoch 8: New best model saved with val_loss = 1.0741
Epoch 8, accuracy: 0.4092
Epoch 8, Train Loss: 0.0010, Val Loss: 1.0741
batch size: (893, 893)
✅ Epoch 9: New best model saved with val_loss = 1.0707
Epoch 9, accuracy: 0.4107
batch size: (910, 910)
✅ Epoch 10: New best model saved with val_loss = 1.0674
Epoch 10, accuracy: 0.4123
Epoch 10, Train Loss: 0.0004, Val Loss: 1.0674
batch size: (907, 907)
✅ Epoch 11: New best model saved with val_loss = 1.0641
Epoch 11, accuracy: 0.4101
batch size: (904, 904)
✅ Epoch 12: New best model saved with val_loss = 1.0605
Epoch 12, accuracy: 0.4160
Epoch 12, Train Loss: 0.0002, Val Loss: 1.0605
batch size: (891, 891)
✅ Epoch 13: New best model saved with val_loss = 1.0566
Epoch 13, accuracy: 0.4203
batch size: (911, 911)
✅ Epoch 14: New best model saved with val_loss = 1.0523
Epoch 14, accuracy: 0.4310
Epoch 14, Train Loss: 0.0001, Val Loss: 1.0523
batch size: (903, 903)
✅ Epoch 15: New best model saved with val_loss = 1.0474
Epoch 15, accuracy: 0.4432
batch size: (904, 904)
✅ Epoch 16: New best model saved with val_loss = 1.0421
Epoch 16, accuracy: 0.4556
Epoch 16, Train Loss: 0.0001, Val Loss: 1.0421
batch size: (907, 907)
✅ Epoch 17: New best model saved with val_loss = 1.0364
Epoch 17, accuracy: 0.4720
batch size: (900, 900)
✅ Epoch 18: New best model saved with val_loss = 1.0302
Epoch 18, accuracy: 0.4878
Epoch 18, Train Loss: 0.0000, Val Loss: 1.0302
batch size: (897, 897)
✅ Epoch 19: New best model saved with val_loss = 1.0236
Epoch 19, accuracy: 0.5015
batch size: (921, 921)
✅ Epoch 20: New best model saved with val_loss = 1.0167
Epoch 20, accuracy: 0.5170
Epoch 20, Train Loss: 0.0000, Val Loss: 1.0167
batch size: (891, 891)
✅ Epoch 21: New best model saved with val_loss = 1.0091
Epoch 21, accuracy: 0.5343
batch size: (912, 912)
✅ Epoch 22: New best model saved with val_loss = 1.0012
Epoch 22, accuracy: 0.5497
Epoch 22, Train Loss: 0.0000, Val Loss: 1.0012
batch size: (904, 904)
✅ Epoch 23: New best model saved with val_loss = 0.9929
Epoch 23, accuracy: 0.5676
batch size: (918, 918)
✅ Epoch 24: New best model saved with val_loss = 0.9844
Epoch 24, accuracy: 0.5780
Epoch 24, Train Loss: 0.0000, Val Loss: 0.9844
batch size: (912, 912)
✅ Epoch 25: New best model saved with val_loss = 0.9755
Epoch 25, accuracy: 0.5892
batch size: (918, 918)
✅ Epoch 26: New best model saved with val_loss = 0.9667
Epoch 26, accuracy: 0.5969
Epoch 26, Train Loss: 0.0000, Val Loss: 0.9667
batch size: (912, 912)
✅ Epoch 27: New best model saved with val_loss = 0.9576
Epoch 27, accuracy: 0.6101
batch size: (874, 874)
✅ Epoch 28: New best model saved with val_loss = 0.9486
Epoch 28, accuracy: 0.6131
Epoch 28, Train Loss: 0.0000, Val Loss: 0.9486
batch size: (905, 905)
✅ Epoch 29: New best model saved with val_loss = 0.9394
Epoch 29, accuracy: 0.6209
batch size: (885, 885)
✅ Epoch 30: New best model saved with val_loss = 0.9305
Epoch 30, accuracy: 0.6255
Epoch 30, Train Loss: 0.0000, Val Loss: 0.9305
batch size: (914, 914)
✅ Epoch 31: New best model saved with val_loss = 0.9218
Epoch 31, accuracy: 0.6334
batch size: (889, 889)
✅ Epoch 32: New best model saved with val_loss = 0.9133
Epoch 32, accuracy: 0.6355
Epoch 32, Train Loss: 0.0000, Val Loss: 0.9133
batch size: (908, 908)
✅ Epoch 33: New best model saved with val_loss = 0.9055
Epoch 33, accuracy: 0.6413
batch size: (912, 912)
✅ Epoch 34: New best model saved with val_loss = 0.8976
Epoch 34, accuracy: 0.6409
Epoch 34, Train Loss: 0.0000, Val Loss: 0.8976
batch size: (900, 900)
✅ Epoch 35: New best model saved with val_loss = 0.8896
Epoch 35, accuracy: 0.6426
batch size: (894, 894)
✅ Epoch 36: New best model saved with val_loss = 0.8822
Epoch 36, accuracy: 0.6432
Epoch 36, Train Loss: 0.0000, Val Loss: 0.8822
batch size: (897, 897)
✅ Epoch 37: New best model saved with val_loss = 0.8751
Epoch 37, accuracy: 0.6440
batch size: (901, 901)
✅ Epoch 38: New best model saved with val_loss = 0.8684
Epoch 38, accuracy: 0.6474
Epoch 38, Train Loss: 0.0000, Val Loss: 0.8684
batch size: (889, 889)
✅ Epoch 39: New best model saved with val_loss = 0.8618
Epoch 39, accuracy: 0.6511
batch size: (897, 897)
✅ Epoch 40: New best model saved with val_loss = 0.8556
Epoch 40, accuracy: 0.6506
Epoch 40, Train Loss: 0.0000, Val Loss: 0.8556
batch size: (922, 922)
✅ Epoch 41: New best model saved with val_loss = 0.8492
Epoch 41, accuracy: 0.6517
batch size: (908, 908)
✅ Epoch 42: New best model saved with val_loss = 0.8433
Epoch 42, accuracy: 0.6510
Epoch 42, Train Loss: 0.0000, Val Loss: 0.8433
batch size: (897, 897)
✅ Epoch 43: New best model saved with val_loss = 0.8373
Epoch 43, accuracy: 0.6542
batch size: (884, 884)
✅ Epoch 44: New best model saved with val_loss = 0.8317
Epoch 44, accuracy: 0.6526
Epoch 44, Train Loss: 0.0000, Val Loss: 0.8317
batch size: (910, 910)
✅ Epoch 45: New best model saved with val_loss = 0.8269
Epoch 45, accuracy: 0.6551
batch size: (908, 908)
✅ Epoch 46: New best model saved with val_loss = 0.8218
Epoch 46, accuracy: 0.6567
Epoch 46, Train Loss: 0.0000, Val Loss: 0.8218
batch size: (904, 904)
✅ Epoch 47: New best model saved with val_loss = 0.8172
Epoch 47, accuracy: 0.6581
batch size: (906, 906)
✅ Epoch 48: New best model saved with val_loss = 0.8131
Epoch 48, accuracy: 0.6571
Epoch 48, Train Loss: 0.0000, Val Loss: 0.8131
batch size: (899, 899)
✅ Epoch 49: New best model saved with val_loss = 0.8097
Epoch 49, accuracy: 0.6582
batch size: (884, 884)
✅ Epoch 50: New best model saved with val_loss = 0.8066
Epoch 50, accuracy: 0.6602
Epoch 50, Train Loss: 0.0000, Val Loss: 0.8066
batch size: (879, 879)
✅ Epoch 51: New best model saved with val_loss = 0.8039
Epoch 51, accuracy: 0.6590
batch size: (875, 875)
✅ Epoch 52: New best model saved with val_loss = 0.8016
Epoch 52, accuracy: 0.6583
Epoch 52, Train Loss: 0.0000, Val Loss: 0.8016
batch size: (889, 889)
✅ Epoch 53: New best model saved with val_loss = 0.8001
Epoch 53, accuracy: 0.6598
batch size: (914, 914)
✅ Epoch 54: New best model saved with val_loss = 0.7977
Epoch 54, accuracy: 0.6601
Epoch 54, Train Loss: 0.0000, Val Loss: 0.7977
batch size: (903, 903)
✅ Epoch 55: New best model saved with val_loss = 0.7965
Epoch 55, accuracy: 0.6623
batch size: (870, 870)
Epoch 56, accuracy: 0.6641
Epoch 56, Train Loss: 0.0001, Val Loss: 0.7966
batch size: (897, 897)
Epoch 57, accuracy: 0.6637
batch size: (880, 880)
Epoch 58, accuracy: 0.6638
Epoch 58, Train Loss: 0.0001, Val Loss: 0.7974
batch size: (916, 916)
Epoch 59, accuracy: 0.6660
batch size: (890, 890)
Epoch 60, accuracy: 0.6647
Epoch 60, Train Loss: 0.0001, Val Loss: 0.8007
batch size: (897, 897)
Epoch 61, accuracy: 0.6635
batch size: (908, 908)
Epoch 62, accuracy: 0.6648
Epoch 62, Train Loss: 0.0001, Val Loss: 0.8066
batch size: (905, 905)
Epoch 63, accuracy: 0.6667
batch size: (886, 886)
Epoch 64, accuracy: 0.6650
Epoch 64, Train Loss: 0.0001, Val Loss: 0.8159
batch size: (924, 924)
Epoch 65, accuracy: 0.6632
batch size: (905, 905)
Epoch 66, accuracy: 0.6650
Epoch 66, Train Loss: 0.0001, Val Loss: 0.8275
batch size: (894, 894)
Epoch 67, accuracy: 0.6695
batch size: (916, 916)
Epoch 68, accuracy: 0.6656
Epoch 68, Train Loss: 0.0001, Val Loss: 0.8397
batch size: (903, 903)
Epoch 69, accuracy: 0.6663
batch size: (893, 893)
Epoch 70, accuracy: 0.6680
Epoch 70, Train Loss: 0.0001, Val Loss: 0.8543
batch size: (898, 898)
Epoch 71, accuracy: 0.6661
batch size: (900, 900)
Epoch 72, accuracy: 0.6643
Epoch 72, Train Loss: 0.0001, Val Loss: 0.8719
batch size: (902, 902)
Epoch 73, accuracy: 0.6656
batch size: (903, 903)
Epoch 74, accuracy: 0.6667
Epoch 74, Train Loss: 0.0001, Val Loss: 0.8894
batch size: (895, 895)
Epoch 75, accuracy: 0.6689
batch size: (895, 895)
Epoch 76, accuracy: 0.6686
Epoch 76, Train Loss: 0.0001, Val Loss: 0.9072
batch size: (920, 920)
Epoch 77, accuracy: 0.6669
batch size: (902, 902)
Epoch 78, accuracy: 0.6672
Epoch 78, Train Loss: 0.0001, Val Loss: 0.9244
batch size: (914, 914)
Epoch 79, accuracy: 0.6684
batch size: (907, 907)
Epoch 80, accuracy: 0.6667
Epoch 80, Train Loss: 0.0001, Val Loss: 0.9415
batch size: (896, 896)
Epoch 81, accuracy: 0.6658
batch size: (901, 901)
Epoch 82, accuracy: 0.6682
Epoch 82, Train Loss: 0.0001, Val Loss: 0.9574
batch size: (900, 900)
Epoch 83, accuracy: 0.6677
batch size: (916, 916)
Epoch 84, accuracy: 0.6685
Epoch 84, Train Loss: 0.0001, Val Loss: 0.9735
batch size: (911, 911)
Epoch 85, accuracy: 0.6696
batch size: (907, 907)
Epoch 86, accuracy: 0.6695
Epoch 86, Train Loss: 0.0001, Val Loss: 0.9883
batch size: (901, 901)
Epoch 87, accuracy: 0.6717
batch size: (903, 903)
Epoch 88, accuracy: 0.6689
Epoch 88, Train Loss: 0.0001, Val Loss: 1.0006
batch size: (900, 900)
Epoch 89, accuracy: 0.6705
batch size: (894, 894)
Epoch 90, accuracy: 0.6685
Epoch 90, Train Loss: 0.0001, Val Loss: 1.0126
batch size: (883, 883)
Epoch 91, accuracy: 0.6660
batch size: (919, 919)
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 92, accuracy: 0.6681
Epoch 92, Train Loss: 0.0001, Val Loss: 1.0226
batch size: (898, 898)
Epoch 93, accuracy: 0.6683
batch size: (908, 908)
Epoch 94, accuracy: 0.6678
Epoch 94, Train Loss: 0.0001, Val Loss: 1.0312
batch size: (911, 911)
Epoch 95, accuracy: 0.6679
batch size: (903, 903)
Epoch 96, accuracy: 0.6686
Epoch 96, Train Loss: 0.0001, Val Loss: 1.0395
batch size: (874, 874)
Epoch 97, accuracy: 0.6679
batch size: (879, 879)
Epoch 98, accuracy: 0.6670
Epoch 98, Train Loss: 0.0001, Val Loss: 1.0459
batch size: (888, 888)
Epoch 99, accuracy: 0.6668
batch size: (907, 907)
Epoch 100, accuracy: 0.6665
Epoch 100, Train Loss: 0.0001, Val Loss: 1.0509
batch size: (928, 928)
Epoch 101, accuracy: 0.6676
batch size: (894, 894)
Epoch 102, accuracy: 0.6690
Epoch 102, Train Loss: 0.0001, Val Loss: 1.0551
batch size: (879, 879)
Epoch 103, accuracy: 0.6683
batch size: (886, 886)
Epoch 104, accuracy: 0.6676
Epoch 104, Train Loss: 0.0001, Val Loss: 1.0588
batch size: (890, 890)
Epoch 105, accuracy: 0.6671
batch size: (899, 899)
Epoch 106, accuracy: 0.6706
Epoch 106, Train Loss: 0.0001, Val Loss: 1.0617
batch size: (897, 897)
Epoch 107, accuracy: 0.6690
batch size: (891, 891)
Epoch 108, accuracy: 0.6676
Epoch 108, Train Loss: 0.0001, Val Loss: 1.0631
batch size: (908, 908)
Epoch 109, accuracy: 0.6687
batch size: (908, 908)
Epoch 110, accuracy: 0.6678
Epoch 110, Train Loss: 0.0001, Val Loss: 1.0647
batch size: (910, 910)
Epoch 111, accuracy: 0.6701
batch size: (916, 916)
Epoch 112, accuracy: 0.6680
Epoch 112, Train Loss: 0.0001, Val Loss: 1.0674
batch size: (909, 909)
Epoch 113, accuracy: 0.6680
batch size: (885, 885)
Epoch 114, accuracy: 0.6661
Epoch 114, Train Loss: 0.0001, Val Loss: 1.0685
batch size: (904, 904)
Epoch 115, accuracy: 0.6667
batch size: (889, 889)
Epoch 116, accuracy: 0.6681
Epoch 116, Train Loss: 0.0001, Val Loss: 1.0689
batch size: (900, 900)
Epoch 117, accuracy: 0.6685
batch size: (903, 903)
Epoch 118, accuracy: 0.6673
Epoch 118, Train Loss: 0.0001, Val Loss: 1.0693
batch size: (901, 901)
Epoch 119, accuracy: 0.6686
batch size: (871, 871)
Epoch 120, accuracy: 0.6664
Epoch 120, Train Loss: 0.0001, Val Loss: 1.0703
batch size: (896, 896)
Epoch 121, accuracy: 0.6694
batch size: (900, 900)
Epoch 122, accuracy: 0.6681
Epoch 122, Train Loss: 0.0001, Val Loss: 1.0700
batch size: (898, 898)
Epoch 123, accuracy: 0.6682
batch size: (894, 894)
Epoch 124, accuracy: 0.6706
Epoch 124, Train Loss: 0.0001, Val Loss: 1.0707
batch size: (908, 908)
Epoch 125, accuracy: 0.6694
batch size: (893, 893)
Epoch 126, accuracy: 0.6669
Epoch 126, Train Loss: 0.0001, Val Loss: 1.0711
batch size: (895, 895)
Epoch 127, accuracy: 0.6678
batch size: (899, 899)
Epoch 128, accuracy: 0.6682
Epoch 128, Train Loss: 0.0001, Val Loss: 1.0724
batch size: (893, 893)
Epoch 129, accuracy: 0.6682
batch size: (901, 901)
Epoch 130, accuracy: 0.6677
Epoch 130, Train Loss: 0.0001, Val Loss: 1.0744
batch size: (904, 904)
Epoch 131, accuracy: 0.6689
batch size: (887, 887)
Epoch 132, accuracy: 0.6666
Epoch 132, Train Loss: 0.0001, Val Loss: 1.0749
batch size: (900, 900)
Epoch 133, accuracy: 0.6700
batch size: (895, 895)
Epoch 134, accuracy: 0.6693
Epoch 134, Train Loss: 0.0001, Val Loss: 1.0749
batch size: (883, 883)
Epoch 135, accuracy: 0.6668
batch size: (911, 911)
Epoch 136, accuracy: 0.6676
Epoch 136, Train Loss: 0.0001, Val Loss: 1.0754
batch size: (898, 898)
Epoch 137, accuracy: 0.6696
batch size: (877, 877)
Epoch 138, accuracy: 0.6665
Epoch 138, Train Loss: 0.0001, Val Loss: 1.0758
batch size: (905, 905)
Epoch 139, accuracy: 0.6675
batch size: (901, 901)
Epoch 140, accuracy: 0.6676
Epoch 140, Train Loss: 0.0001, Val Loss: 1.0752
batch size: (907, 907)
Epoch 141, accuracy: 0.6657
batch size: (896, 896)
Epoch 142, accuracy: 0.6670
Epoch 142, Train Loss: 0.0001, Val Loss: 1.0745
batch size: (907, 907)
Epoch 143, accuracy: 0.6678
batch size: (888, 888)
Epoch 144, accuracy: 0.6660
Epoch 144, Train Loss: 0.0001, Val Loss: 1.0749
batch size: (893, 893)
Epoch 145, accuracy: 0.6673
batch size: (907, 907)
Epoch 146, accuracy: 0.6647
Epoch 146, Train Loss: 0.0001, Val Loss: 1.0759
batch size: (894, 894)
Epoch 147, accuracy: 0.6673
batch size: (888, 888)
Epoch 148, accuracy: 0.6694
Epoch 148, Train Loss: 0.0001, Val Loss: 1.0762
batch size: (903, 903)
Epoch 149, accuracy: 0.6683
batch size: (904, 904)
Epoch 150, accuracy: 0.6692
Epoch 150, Train Loss: 0.0001, Val Loss: 1.0780
batch size: (900, 900)
Epoch 151, accuracy: 0.6687
batch size: (886, 886)
Epoch 152, accuracy: 0.6671
Epoch 152, Train Loss: 0.0001, Val Loss: 1.0770
batch size: (891, 891)
Epoch 153, accuracy: 0.6679
batch size: (903, 903)
Epoch 154, accuracy: 0.6673
Epoch 154, Train Loss: 0.0001, Val Loss: 1.0764
batch size: (901, 901)
Epoch 155, accuracy: 0.6664
batch size: (896, 896)
Epoch 156, accuracy: 0.6687
Epoch 156, Train Loss: 0.0001, Val Loss: 1.0772
batch size: (904, 904)
Epoch 157, accuracy: 0.6712
batch size: (906, 906)
Epoch 158, accuracy: 0.6671
Epoch 158, Train Loss: 0.0001, Val Loss: 1.0783
batch size: (885, 885)
Epoch 159, accuracy: 0.6701
batch size: (916, 916)
Epoch 160, accuracy: 0.6684
Epoch 160, Train Loss: 0.0001, Val Loss: 1.0778
batch size: (895, 895)
Epoch 161, accuracy: 0.6672
batch size: (906, 906)
Epoch 162, accuracy: 0.6690
Epoch 162, Train Loss: 0.0001, Val Loss: 1.0776
batch size: (889, 889)
Epoch 163, accuracy: 0.6694
batch size: (902, 902)
Epoch 164, accuracy: 0.6669
Epoch 164, Train Loss: 0.0001, Val Loss: 1.0761
batch size: (908, 908)
Epoch 165, accuracy: 0.6666
batch size: (908, 908)
Epoch 166, accuracy: 0.6666
Epoch 166, Train Loss: 0.0001, Val Loss: 1.0762
batch size: (892, 892)
Epoch 167, accuracy: 0.6681
batch size: (896, 896)
Epoch 168, accuracy: 0.6660
Epoch 168, Train Loss: 0.0001, Val Loss: 1.0763
batch size: (898, 898)
Epoch 169, accuracy: 0.6693
batch size: (893, 893)
Epoch 170, accuracy: 0.6696
Epoch 170, Train Loss: 0.0001, Val Loss: 1.0750
batch size: (920, 920)
Epoch 171, accuracy: 0.6692
batch size: (894, 894)
Epoch 172, accuracy: 0.6694
Epoch 172, Train Loss: 0.0001, Val Loss: 1.0750
batch size: (891, 891)
Epoch 173, accuracy: 0.6670
batch size: (899, 899)
Epoch 174, accuracy: 0.6681
Epoch 174, Train Loss: 0.0001, Val Loss: 1.0759
batch size: (897, 897)
Epoch 175, accuracy: 0.6675
batch size: (905, 905)
Epoch 176, accuracy: 0.6694
Epoch 176, Train Loss: 0.0001, Val Loss: 1.0753
batch size: (906, 906)
Epoch 177, accuracy: 0.6681
batch size: (882, 882)
Epoch 178, accuracy: 0.6661
Epoch 178, Train Loss: 0.0001, Val Loss: 1.0746
batch size: (885, 885)
Epoch 179, accuracy: 0.6664
batch size: (883, 883)
Epoch 180, accuracy: 0.6681
Epoch 180, Train Loss: 0.0001, Val Loss: 1.0758
batch size: (910, 910)
Epoch 181, accuracy: 0.6680
batch size: (889, 889)
Epoch 182, accuracy: 0.6686
Epoch 182, Train Loss: 0.0001, Val Loss: 1.0769
batch size: (901, 901)
Epoch 183, accuracy: 0.6697
batch size: (909, 909)
Epoch 184, accuracy: 0.6693
Epoch 184, Train Loss: 0.0001, Val Loss: 1.0763
batch size: (911, 911)
Epoch 185, accuracy: 0.6700
batch size: (883, 883)
Epoch 186, accuracy: 0.6674
Epoch 186, Train Loss: 0.0001, Val Loss: 1.0753
batch size: (911, 911)
Epoch 187, accuracy: 0.6693
batch size: (907, 907)
Epoch 188, accuracy: 0.6676
Epoch 188, Train Loss: 0.0001, Val Loss: 1.0755
batch size: (895, 895)
Epoch 189, accuracy: 0.6683
batch size: (889, 889)
Epoch 190, accuracy: 0.6699
Epoch 190, Train Loss: 0.0001, Val Loss: 1.0742
batch size: (916, 916)
Epoch 191, accuracy: 0.6665
batch size: (886, 886)
Epoch 192, accuracy: 0.6670
Epoch 192, Train Loss: 0.0001, Val Loss: 1.0749
batch size: (914, 914)
Epoch 193, accuracy: 0.6699
batch size: (910, 910)
Epoch 194, accuracy: 0.6685
Epoch 194, Train Loss: 0.0001, Val Loss: 1.0758
batch size: (901, 901)
Epoch 195, accuracy: 0.6674
batch size: (916, 916)
Epoch 196, accuracy: 0.6689
Epoch 196, Train Loss: 0.0001, Val Loss: 1.0756
batch size: (887, 887)
Epoch 197, accuracy: 0.6674
batch size: (904, 904)
Epoch 198, accuracy: 0.6664
Epoch 198, Train Loss: 0.0001, Val Loss: 1.0754
batch size: (904, 904)
Epoch 199, accuracy: 0.6684
Loaded best model with val_loss = 0.7965183854103088
test :accuracy 0.6614, f1_macro: 0.6570, f1_micro: 0.6614, auc: 0.8324
Training mlp with 8 layers...
可训练参数: 186377_mlp
不可训练参数: 0
batch size: (915, 915)
✅ Epoch 0: New best model saved with val_loss = 1.0912
Epoch 0, accuracy: 0.4288
Epoch 0, Train Loss: 1.1099, Val Loss: 1.0912
batch size: (885, 885)
Epoch 1, accuracy: 0.4306
batch size: (892, 892)
Epoch 2, accuracy: 0.4276
Epoch 2, Train Loss: 0.3855, Val Loss: 1.0953
batch size: (898, 898)
Epoch 3, accuracy: 0.4304
batch size: (893, 893)
Epoch 4, accuracy: 0.4285
Epoch 4, Train Loss: 0.0265, Val Loss: 1.1074
batch size: (917, 917)
Epoch 5, accuracy: 0.1695
batch size: (893, 893)
Epoch 6, accuracy: 0.1681
Epoch 6, Train Loss: 0.0041, Val Loss: 1.1306
batch size: (910, 910)
Epoch 7, accuracy: 0.1662
batch size: (887, 887)
Epoch 8, accuracy: 0.1666
Epoch 8, Train Loss: 0.0019, Val Loss: 1.1449
batch size: (896, 896)
Epoch 9, accuracy: 0.1667
batch size: (888, 888)
Epoch 10, accuracy: 0.1662
Epoch 10, Train Loss: 0.0015, Val Loss: 1.1623
batch size: (893, 893)
Epoch 11, accuracy: 0.1647
batch size: (887, 887)
Epoch 12, accuracy: 0.1689
Epoch 12, Train Loss: 0.0013, Val Loss: 1.1836
batch size: (903, 903)
Epoch 13, accuracy: 0.1669
batch size: (914, 914)
Epoch 14, accuracy: 0.1671
Epoch 14, Train Loss: 0.0013, Val Loss: 1.2102
batch size: (904, 904)
Epoch 15, accuracy: 0.1678
batch size: (911, 911)
Epoch 16, accuracy: 0.1697
Epoch 16, Train Loss: 0.0012, Val Loss: 1.2383
batch size: (911, 911)
Epoch 17, accuracy: 0.1686
batch size: (913, 913)
Epoch 18, accuracy: 0.1667
Epoch 18, Train Loss: 0.0011, Val Loss: 1.2668
batch size: (891, 891)
Epoch 19, accuracy: 0.1674
batch size: (909, 909)
Epoch 20, accuracy: 0.1656
Epoch 20, Train Loss: 0.0012, Val Loss: 1.2993
batch size: (906, 906)
Epoch 21, accuracy: 0.1698
batch size: (888, 888)
Epoch 22, accuracy: 0.1665
Epoch 22, Train Loss: 0.0012, Val Loss: 1.3369
batch size: (874, 874)
Epoch 23, accuracy: 0.1691
batch size: (913, 913)
Epoch 24, accuracy: 0.1615
Epoch 24, Train Loss: 0.0011, Val Loss: 1.3785
batch size: (894, 894)
Epoch 25, accuracy: 0.1677
batch size: (904, 904)
Epoch 26, accuracy: 0.1663
Epoch 26, Train Loss: 0.0012, Val Loss: 1.4187
batch size: (893, 893)
Epoch 27, accuracy: 0.1674
batch size: (899, 899)
Epoch 28, accuracy: 0.1689
Epoch 28, Train Loss: 0.0011, Val Loss: 1.4566
batch size: (905, 905)
Epoch 29, accuracy: 0.1647
batch size: (914, 914)
Epoch 30, accuracy: 0.1700
Epoch 30, Train Loss: 0.0012, Val Loss: 1.4935
batch size: (892, 892)
Epoch 31, accuracy: 0.1677
batch size: (894, 894)
Epoch 32, accuracy: 0.1695
Epoch 32, Train Loss: 0.0012, Val Loss: 1.5242
batch size: (890, 890)
Epoch 33, accuracy: 0.1683
batch size: (887, 887)
Epoch 34, accuracy: 0.1684
Epoch 34, Train Loss: 0.0012, Val Loss: 1.5526
batch size: (882, 882)
Epoch 35, accuracy: 0.1663
batch size: (907, 907)
Epoch 36, accuracy: 0.1646
Epoch 36, Train Loss: 0.0012, Val Loss: 1.5738
batch size: (894, 894)
Epoch 37, accuracy: 0.1668
batch size: (879, 879)
Epoch 38, accuracy: 0.1692
Epoch 38, Train Loss: 0.0012, Val Loss: 1.5923
batch size: (900, 900)
Epoch 39, accuracy: 0.1687
batch size: (905, 905)
Epoch 40, accuracy: 0.1707
Epoch 40, Train Loss: 0.0012, Val Loss: 1.6071
batch size: (899, 899)
Epoch 41, accuracy: 0.1694
batch size: (907, 907)
Epoch 42, accuracy: 0.1663
Epoch 42, Train Loss: 0.0012, Val Loss: 1.6145
batch size: (887, 887)
Epoch 43, accuracy: 0.1675
batch size: (893, 893)
Epoch 44, accuracy: 0.1683
Epoch 44, Train Loss: 0.0012, Val Loss: 1.6155
batch size: (902, 902)
Epoch 45, accuracy: 0.1673
batch size: (890, 890)
Epoch 46, accuracy: 0.1714
Epoch 46, Train Loss: 0.0012, Val Loss: 1.6138
batch size: (918, 918)
Epoch 47, accuracy: 0.1640
batch size: (898, 898)
Epoch 48, accuracy: 0.1664
Epoch 48, Train Loss: 0.0011, Val Loss: 1.6131
batch size: (914, 914)
Epoch 49, accuracy: 0.1661
batch size: (880, 880)
Epoch 50, accuracy: 0.1692
Epoch 50, Train Loss: 0.0011, Val Loss: 1.6037
batch size: (909, 909)
Epoch 51, accuracy: 0.1674
batch size: (902, 902)
Epoch 52, accuracy: 0.1684
Epoch 52, Train Loss: 0.0012, Val Loss: 1.5915
batch size: (895, 895)
Epoch 53, accuracy: 0.1719
batch size: (914, 914)
Epoch 54, accuracy: 0.1707
Epoch 54, Train Loss: 0.0012, Val Loss: 1.5758
batch size: (894, 894)
Epoch 55, accuracy: 0.1709
batch size: (898, 898)
Epoch 56, accuracy: 0.1748
Epoch 56, Train Loss: 0.0012, Val Loss: 1.5565
batch size: (906, 906)
Epoch 57, accuracy: 0.1782
batch size: (905, 905)
Epoch 58, accuracy: 0.1831
Epoch 58, Train Loss: 0.0011, Val Loss: 1.5333
batch size: (897, 897)
Epoch 59, accuracy: 0.1909
batch size: (896, 896)
Epoch 60, accuracy: 0.1923
Epoch 60, Train Loss: 0.0011, Val Loss: 1.5117
batch size: (874, 874)
Epoch 61, accuracy: 0.2023
batch size: (914, 914)
Epoch 62, accuracy: 0.2105
Epoch 62, Train Loss: 0.0011, Val Loss: 1.4867
batch size: (911, 911)
Epoch 63, accuracy: 0.2213
batch size: (904, 904)
Epoch 64, accuracy: 0.2298
Epoch 64, Train Loss: 0.0012, Val Loss: 1.4586
batch size: (902, 902)
Epoch 65, accuracy: 0.2364
batch size: (905, 905)
Epoch 66, accuracy: 0.2511
Epoch 66, Train Loss: 0.0011, Val Loss: 1.4330
batch size: (873, 873)
Epoch 67, accuracy: 0.2622
batch size: (903, 903)
Epoch 68, accuracy: 0.2762
Epoch 68, Train Loss: 0.0011, Val Loss: 1.4075
batch size: (891, 891)
Epoch 69, accuracy: 0.2842
batch size: (898, 898)
Epoch 70, accuracy: 0.3002
Epoch 70, Train Loss: 0.0011, Val Loss: 1.3850
batch size: (892, 892)
Epoch 71, accuracy: 0.3124
batch size: (897, 897)
Epoch 72, accuracy: 0.3241
Epoch 72, Train Loss: 0.0012, Val Loss: 1.3658
batch size: (894, 894)
Epoch 73, accuracy: 0.3366
batch size: (904, 904)
Epoch 74, accuracy: 0.3473
Epoch 74, Train Loss: 0.0012, Val Loss: 1.3492
batch size: (892, 892)
Epoch 75, accuracy: 0.3578
batch size: (908, 908)
Epoch 76, accuracy: 0.3671
Epoch 76, Train Loss: 0.0011, Val Loss: 1.3347
batch size: (890, 890)
Epoch 77, accuracy: 0.3776
batch size: (872, 872)
Epoch 78, accuracy: 0.3881
Epoch 78, Train Loss: 0.0012, Val Loss: 1.3251
batch size: (902, 902)
Epoch 79, accuracy: 0.3984
batch size: (902, 902)
Epoch 80, accuracy: 0.4054
Epoch 80, Train Loss: 0.0011, Val Loss: 1.3171
batch size: (919, 919)
Epoch 81, accuracy: 0.4155
batch size: (906, 906)
Epoch 82, accuracy: 0.4211
Epoch 82, Train Loss: 0.0011, Val Loss: 1.3142
batch size: (890, 890)
Epoch 83, accuracy: 0.4295
batch size: (915, 915)
Epoch 84, accuracy: 0.4372
Epoch 84, Train Loss: 0.0012, Val Loss: 1.3149
batch size: (896, 896)
Epoch 85, accuracy: 0.4452
batch size: (909, 909)
Epoch 86, accuracy: 0.4488
Epoch 86, Train Loss: 0.0012, Val Loss: 1.3159
batch size: (900, 900)
Epoch 87, accuracy: 0.4530
batch size: (900, 900)
Epoch 88, accuracy: 0.4618
Epoch 88, Train Loss: 0.0011, Val Loss: 1.3160
batch size: (887, 887)
Epoch 89, accuracy: 0.4714
batch size: (915, 915)
Epoch 90, accuracy: 0.4754
Epoch 90, Train Loss: 0.0011, Val Loss: 1.3156
batch size: (919, 919)
Epoch 91, accuracy: 0.4767
batch size: (898, 898)
Epoch 92, accuracy: 0.4789
Epoch 92, Train Loss: 0.0012, Val Loss: 1.3165
batch size: (897, 897)
Epoch 93, accuracy: 0.4854
batch size: (907, 907)
Epoch 94, accuracy: 0.4865
Epoch 94, Train Loss: 0.0012, Val Loss: 1.3191
batch size: (904, 904)
Epoch 95, accuracy: 0.4908
batch size: (902, 902)
Epoch 96, accuracy: 0.4928
Epoch 96, Train Loss: 0.0012, Val Loss: 1.3221
batch size: (889, 889)
Epoch 97, accuracy: 0.4959
batch size: (887, 887)
Epoch 98, accuracy: 0.5001
Epoch 98, Train Loss: 0.0011, Val Loss: 1.3255
batch size: (900, 900)
Epoch 99, accuracy: 0.5024
batch size: (895, 895)
Epoch 100, accuracy: 0.5073
Epoch 100, Train Loss: 0.0011, Val Loss: 1.3269
batch size: (908, 908)
Epoch 101, accuracy: 0.5096
batch size: (906, 906)
Epoch 102, accuracy: 0.5139
Epoch 102, Train Loss: 0.0012, Val Loss: 1.3291
batch size: (917, 917)
Epoch 103, accuracy: 0.5147
batch size: (888, 888)
Epoch 104, accuracy: 0.5184
Epoch 104, Train Loss: 0.0012, Val Loss: 1.3288
batch size: (918, 918)
Epoch 105, accuracy: 0.5220
batch size: (892, 892)
Epoch 106, accuracy: 0.5210
Epoch 106, Train Loss: 0.0012, Val Loss: 1.3311
batch size: (912, 912)
Epoch 107, accuracy: 0.5237
batch size: (886, 886)
Epoch 108, accuracy: 0.5230
Epoch 108, Train Loss: 0.0012, Val Loss: 1.3348
batch size: /root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
(904, 904)
Epoch 109, accuracy: 0.5246
batch size: (909, 909)
Epoch 110, accuracy: 0.5255
Epoch 110, Train Loss: 0.0012, Val Loss: 1.3371
batch size: (900, 900)
Epoch 111, accuracy: 0.5286
batch size: (889, 889)
Epoch 112, accuracy: 0.5283
Epoch 112, Train Loss: 0.0012, Val Loss: 1.3393
batch size: (903, 903)
Epoch 113, accuracy: 0.5263
batch size: (894, 894)
Epoch 114, accuracy: 0.5257
Epoch 114, Train Loss: 0.0012, Val Loss: 1.3424
batch size: (909, 909)
Epoch 115, accuracy: 0.5301
batch size: (932, 932)
Epoch 116, accuracy: 0.5276
Epoch 116, Train Loss: 0.0011, Val Loss: 1.3430
batch size: (913, 913)
Epoch 117, accuracy: 0.5272
batch size: (907, 907)
Epoch 118, accuracy: 0.5300
Epoch 118, Train Loss: 0.0012, Val Loss: 1.3456
batch size: (903, 903)
Epoch 119, accuracy: 0.5313
batch size: (905, 905)
Epoch 120, accuracy: 0.5293
Epoch 120, Train Loss: 0.0011, Val Loss: 1.3474
batch size: (887, 887)
Epoch 121, accuracy: 0.5328
batch size: (912, 912)
Epoch 122, accuracy: 0.5330
Epoch 122, Train Loss: 0.0011, Val Loss: 1.3482
batch size: (908, 908)
Epoch 123, accuracy: 0.5326
batch size: (913, 913)
Epoch 124, accuracy: 0.5313
Epoch 124, Train Loss: 0.0012, Val Loss: 1.3489
batch size: (891, 891)
Epoch 125, accuracy: 0.5342
batch size: (895, 895)
Epoch 126, accuracy: 0.5326
Epoch 126, Train Loss: 0.0011, Val Loss: 1.3478
batch size: (879, 879)
Epoch 127, accuracy: 0.5335
batch size: (909, 909)
Epoch 128, accuracy: 0.5330
Epoch 128, Train Loss: 0.0012, Val Loss: 1.3479
batch size: (907, 907)
Epoch 129, accuracy: 0.5335
batch size: (898, 898)
Epoch 130, accuracy: 0.5345
Epoch 130, Train Loss: 0.0011, Val Loss: 1.3461
batch size: (907, 907)
Epoch 131, accuracy: 0.5361
batch size: (898, 898)
Epoch 132, accuracy: 0.5339
Epoch 132, Train Loss: 0.0012, Val Loss: 1.3488
batch size: (889, 889)
Epoch 133, accuracy: 0.5323
batch size: (881, 881)
Epoch 134, accuracy: 0.5328
Epoch 134, Train Loss: 0.0012, Val Loss: 1.3484
batch size: (905, 905)
Epoch 135, accuracy: 0.5353
batch size: (906, 906)
Epoch 136, accuracy: 0.5354
Epoch 136, Train Loss: 0.0011, Val Loss: 1.3495
batch size: (894, 894)
Epoch 137, accuracy: 0.5352
batch size: (898, 898)
Epoch 138, accuracy: 0.5367
Epoch 138, Train Loss: 0.0011, Val Loss: 1.3503
batch size: (902, 902)
Epoch 139, accuracy: 0.5351
batch size: (892, 892)
Epoch 140, accuracy: 0.5324
Epoch 140, Train Loss: 0.0012, Val Loss: 1.3507
batch size: (899, 899)
Epoch 141, accuracy: 0.5359
batch size: (901, 901)
Epoch 142, accuracy: 0.5350
Epoch 142, Train Loss: 0.0012, Val Loss: 1.3513
batch size: (895, 895)
Epoch 143, accuracy: 0.5353
batch size: (903, 903)
Epoch 144, accuracy: 0.5361
Epoch 144, Train Loss: 0.0011, Val Loss: 1.3504
batch size: (895, 895)
Epoch 145, accuracy: 0.5344
batch size: (902, 902)
Epoch 146, accuracy: 0.5344
Epoch 146, Train Loss: 0.0011, Val Loss: 1.3504
batch size: (899, 899)
Epoch 147, accuracy: 0.5366
batch size: (901, 901)
Epoch 148, accuracy: 0.5362
Epoch 148, Train Loss: 0.0012, Val Loss: 1.3480
batch size: (886, 886)
Epoch 149, accuracy: 0.5345
batch size: (920, 920)
Epoch 150, accuracy: 0.5372
Epoch 150, Train Loss: 0.0012, Val Loss: 1.3475
batch size: (904, 904)
Epoch 151, accuracy: 0.5387
batch size: (918, 918)
Epoch 152, accuracy: 0.5345
Epoch 152, Train Loss: 0.0012, Val Loss: 1.3481
batch size: (888, 888)
Epoch 153, accuracy: 0.5370
batch size: (888, 888)
Epoch 154, accuracy: 0.5369
Epoch 154, Train Loss: 0.0012, Val Loss: 1.3483
batch size: (911, 911)
Epoch 155, accuracy: 0.5366
batch size: (899, 899)
Epoch 156, accuracy: 0.5356
Epoch 156, Train Loss: 0.0012, Val Loss: 1.3489
batch size: (902, 902)
Epoch 157, accuracy: 0.5352
batch size: (880, 880)
Epoch 158, accuracy: 0.5352
Epoch 158, Train Loss: 0.0012, Val Loss: 1.3479
batch size: (883, 883)
Epoch 159, accuracy: 0.5346
batch size: (904, 904)
Epoch 160, accuracy: 0.5358
Epoch 160, Train Loss: 0.0011, Val Loss: 1.3486
batch size: (893, 893)
Epoch 161, accuracy: 0.5360
batch size: (891, 891)
Epoch 162, accuracy: 0.5362
Epoch 162, Train Loss: 0.0012, Val Loss: 1.3463
batch size: (889, 889)
Epoch 163, accuracy: 0.5349
batch size: (897, 897)
Epoch 164, accuracy: 0.5363
Epoch 164, Train Loss: 0.0012, Val Loss: 1.3441
batch size: (885, 885)
Epoch 165, accuracy: 0.5355
batch size: (896, 896)
Epoch 166, accuracy: 0.5359
Epoch 166, Train Loss: 0.0011, Val Loss: 1.3479
batch size: (922, 922)
Epoch 167, accuracy: 0.5343
batch size: (898, 898)
Epoch 168, accuracy: 0.5334
Epoch 168, Train Loss: 0.0011, Val Loss: 1.3507
batch size: (900, 900)
Epoch 169, accuracy: 0.5326
batch size: (900, 900)
Epoch 170, accuracy: 0.5343
Epoch 170, Train Loss: 0.0011, Val Loss: 1.3492
batch size: (923, 923)
Epoch 171, accuracy: 0.5349
batch size: (890, 890)
Epoch 172, accuracy: 0.5352
Epoch 172, Train Loss: 0.0011, Val Loss: 1.3503
batch size: (893, 893)
Epoch 173, accuracy: 0.5340
batch size: (891, 891)
Epoch 174, accuracy: 0.5350
Epoch 174, Train Loss: 0.0012, Val Loss: 1.3477
batch size: (906, 906)
Epoch 175, accuracy: 0.5357
batch size: (905, 905)
Epoch 176, accuracy: 0.5342
Epoch 176, Train Loss: 0.0011, Val Loss: 1.3469
batch size: (902, 902)
Epoch 177, accuracy: 0.5331
batch size: (904, 904)
Epoch 178, accuracy: 0.5350
Epoch 178, Train Loss: 0.0012, Val Loss: 1.3471
batch size: (897, 897)
Epoch 179, accuracy: 0.5338
batch size: (895, 895)
Epoch 180, accuracy: 0.5324
Epoch 180, Train Loss: 0.0012, Val Loss: 1.3450
batch size: (880, 880)
Epoch 181, accuracy: 0.5321
batch size: (905, 905)
Epoch 182, accuracy: 0.5314
Epoch 182, Train Loss: 0.0011, Val Loss: 1.3484
batch size: (906, 906)
Epoch 183, accuracy: 0.5322
batch size: (894, 894)
Epoch 184, accuracy: 0.5343
Epoch 184, Train Loss: 0.0012, Val Loss: 1.3494
batch size: (904, 904)
Epoch 185, accuracy: 0.5340
batch size: (907, 907)
Epoch 186, accuracy: 0.5347
Epoch 186, Train Loss: 0.0010, Val Loss: 1.3507
batch size: (914, 914)
Epoch 187, accuracy: 0.5347
batch size: (885, 885)
Epoch 188, accuracy: 0.5344
Epoch 188, Train Loss: 0.0012, Val Loss: 1.3508
batch size: (869, 869)
Epoch 189, accuracy: 0.5334
batch size: (906, 906)
Epoch 190, accuracy: 0.5333
Epoch 190, Train Loss: 0.0012, Val Loss: 1.3510
batch size: (900, 900)
Epoch 191, accuracy: 0.5341
batch size: (897, 897)
Epoch 192, accuracy: 0.5351
Epoch 192, Train Loss: 0.0011, Val Loss: 1.3528
batch size: (905, 905)
Epoch 193, accuracy: 0.5350
batch size: (900, 900)
Epoch 194, accuracy: 0.5337
Epoch 194, Train Loss: 0.0011, Val Loss: 1.3499
batch size: (904, 904)
Epoch 195, accuracy: 0.5355
batch size: (906, 906)
Epoch 196, accuracy: 0.5357
Epoch 196, Train Loss: 0.0011, Val Loss: 1.3486
batch size: (893, 893)
Epoch 197, accuracy: 0.5358
batch size: (888, 888)
Epoch 198, accuracy: 0.5339
Epoch 198, Train Loss: 0.0011, Val Loss: 1.3474
batch size: (921, 921)
Epoch 199, accuracy: 0.5345
Loaded best model with val_loss = 1.0911787748336792
test :accuracy 0.4308, f1_macro: 0.2007, f1_micro: 0.4308, auc: 0.5759
Training mlp with 32 layers...
可训练参数: 598025_mlp
不可训练参数: 0
batch size: (899, 899)
✅ Epoch 0: New best model saved with val_loss = 1.0952
Epoch 0, accuracy: 0.4035
Epoch 0, Train Loss: 1.1235, Val Loss: 1.0952
batch size: (894, 894)
✅ Epoch 1: New best model saved with val_loss = 1.0908
Epoch 1, accuracy: 0.4034
batch size: (905, 905)
✅ Epoch 2: New best model saved with val_loss = 1.0885
Epoch 2, accuracy: 0.4003
Epoch 2, Train Loss: 1.3694, Val Loss: 1.0885
batch size: (909, 909)
✅ Epoch 3: New best model saved with val_loss = 1.0860
Epoch 3, accuracy: 0.4042
batch size: (900, 900)
✅ Epoch 4: New best model saved with val_loss = 1.0815
Epoch 4, accuracy: 0.4056
Epoch 4, Train Loss: 1.1430, Val Loss: 1.0815
batch size: (913, 913)
✅ Epoch 5: New best model saved with val_loss = 1.0777
Epoch 5, accuracy: 0.4039
batch size: (905, 905)
✅ Epoch 6: New best model saved with val_loss = 1.0748
Epoch 6, accuracy: 0.4321
Epoch 6, Train Loss: 0.9226, Val Loss: 1.0748
batch size: (903, 903)
✅ Epoch 7: New best model saved with val_loss = 1.0725
Epoch 7, accuracy: 0.4296
batch size: (894, 894)
✅ Epoch 8: New best model saved with val_loss = 1.0709
Epoch 8, accuracy: 0.4300
Epoch 8, Train Loss: 0.9529, Val Loss: 1.0709
batch size: (901, 901)
✅ Epoch 9: New best model saved with val_loss = 1.0700
Epoch 9, accuracy: 0.4294
batch size: (908, 908)
✅ Epoch 10: New best model saved with val_loss = 1.0693
Epoch 10, accuracy: 0.4321
Epoch 10, Train Loss: 0.8339, Val Loss: 1.0693
batch size: (908, 908)
✅ Epoch 11: New best model saved with val_loss = 1.0690
Epoch 11, accuracy: 0.4277
batch size: (914, 914)
✅ Epoch 12: New best model saved with val_loss = 1.0690
Epoch 12, accuracy: 0.4303
Epoch 12, Train Loss: 0.7805, Val Loss: 1.0690
batch size: (885, 885)
Epoch 13, accuracy: 0.4312
batch size: (894, 894)
Epoch 14, accuracy: 0.4314
Epoch 14, Train Loss: 0.7327, Val Loss: 1.0701
batch size: (885, 885)
Epoch 15, accuracy: 0.4253
batch size: (928, 928)
Epoch 16, accuracy: 0.4350
Epoch 16, Train Loss: 0.6894, Val Loss: 1.0714
batch size: (906, 906)
Epoch 17, accuracy: 0.4301
batch size: (912, 912)
Epoch 18, accuracy: 0.4259
Epoch 18, Train Loss: 0.6564, Val Loss: 1.0719
batch size: (885, 885)
Epoch 19, accuracy: 0.4340
batch size: (905, 905)
Epoch 20, accuracy: 0.4241
Epoch 20, Train Loss: 0.6508, Val Loss: 1.0727
batch size: (917, 917)
Epoch 21, accuracy: 0.4316
batch size: (902, 902)
Epoch 22, accuracy: 0.4310
Epoch 22, Train Loss: 0.6385, Val Loss: 1.0734
batch size: (899, 899)
Epoch 23, accuracy: 0.4281
batch size: (902, 902)
Epoch 24, accuracy: 0.4305
Epoch 24, Train Loss: 0.6267, Val Loss: 1.0742
batch size: (902, 902)
Epoch 25, accuracy: 0.4298
batch size: (893, 893)
Epoch 26, accuracy: 0.4327
Epoch 26, Train Loss: 0.6230, Val Loss: 1.0749
batch size: (894, 894)
Epoch 27, accuracy: 0.4301
batch size: (903, 903)
Epoch 28, accuracy: 0.4301
Epoch 28, Train Loss: 0.6199, Val Loss: 1.0755
batch size: (888, 888)
Epoch 29, accuracy: 0.4281
batch size: (893, 893)
Epoch 30, accuracy: 0.4325
Epoch 30, Train Loss: 0.6201, Val Loss: 1.0760
batch size: (884, 884)
Epoch 31, accuracy: 0.4331
batch size: (893, 893)
Epoch 32, accuracy: 0.4284
Epoch 32, Train Loss: 0.6198, Val Loss: 1.0765
batch size: (902, 902)
Epoch 33, accuracy: 0.4312
batch size: (892, 892)
Epoch 34, accuracy: 0.4300
Epoch 34, Train Loss: 0.6192, Val Loss: 1.0768
batch size: (891, 891)
Epoch 35, accuracy: 0.4256
batch size: (895, 895)
Epoch 36, accuracy: 0.4311
Epoch 36, Train Loss: 0.6187, Val Loss: 1.0772
batch size: (900, 900)
Epoch 37, accuracy: 0.4339
batch size: (888, 888)
Epoch 38, accuracy: 0.4292
Epoch 38, Train Loss: 0.6202, Val Loss: 1.0774
batch size: (899, 899)
Epoch 39, accuracy: 0.4309
batch size: (887, 887)
Epoch 40, accuracy: 0.4313
Epoch 40, Train Loss: 0.6264, Val Loss: 1.0776
batch size: (898, 898)
Epoch 41, accuracy: 0.4318
batch size: (904, 904)
Epoch 42, accuracy: 0.4265
Epoch 42, Train Loss: 0.6177, Val Loss: 1.0778
batch size: (905, 905)
Epoch 43, accuracy: 0.4321
batch size: (900, 900)
Epoch 44, accuracy: 0.4311
Epoch 44, Train Loss: 0.6279, Val Loss: 1.0779
batch size: (883, 883)
Epoch 45, accuracy: 0.4324
batch size: (885, 885)
Epoch 46, accuracy: 0.4333
Epoch 46, Train Loss: 0.6227, Val Loss: 1.0780
batch size: (894, 894)
Epoch 47, accuracy: 0.4309
batch size: (906, 906)
Epoch 48, accuracy: 0.4269
Epoch 48, Train Loss: 0.6203, Val Loss: 1.0781
batch size: (903, 903)
Epoch 49, accuracy: 0.4290
batch size: (913, 913)
Epoch 50, accuracy: 0.4301
Epoch 50, Train Loss: 0.6195, Val Loss: 1.0782
batch size: (906, 906)
Epoch 51, accuracy: 0.4292
batch size: (898, 898)
Epoch 52, accuracy: 0.4298
Epoch 52, Train Loss: 0.6204, Val Loss: 1.0782
batch size: (913, 913)
Epoch 53, accuracy: 0.4299
batch size: (898, 898)
Epoch 54, accuracy: 0.4276
Epoch 54, Train Loss: 0.6187, Val Loss: 1.0783
batch size: (897, 897)
Epoch 55, accuracy: 0.4316
batch size: (902, 902)
Epoch 56, accuracy: 0.4290
Epoch 56, Train Loss: 0.6223, Val Loss: 1.0784
batch size: (893, 893)
Epoch 57, accuracy: 0.4333
batch size: (896, 896)
Epoch 58, accuracy: 0.4302
Epoch 58, Train Loss: 0.6198, Val Loss: 1.0785
batch size: (912, 912)
Epoch 59, accuracy: 0.4312
batch size: (913, 913)
Epoch 60, accuracy: 0.4318
Epoch 60, Train Loss: 0.6184, Val Loss: 1.0785
batch size: (888, 888)
Epoch 61, accuracy: 0.4319
batch size: (888, 888)
Epoch 62, accuracy: 0.4315
Epoch 62, Train Loss: 0.6175, Val Loss: 1.0785
batch size: (886, 886)
Epoch 63, accuracy: 0.4286
batch size: (900, 900)
Epoch 64, accuracy: 0.4300
Epoch 64, Train Loss: 0.6192, Val Loss: 1.0786
batch size: (887, 887)
Epoch 65, accuracy: 0.4314
batch size: (917, 917)
Epoch 66, accuracy: 0.4310
Epoch 66, Train Loss: 0.6185, Val Loss: 1.0785
batch size: (909, 909)
Epoch 67, accuracy: 0.4269
batch size: (904, 904)
Epoch 68, accuracy: 0.4350
Epoch 68, Train Loss: 0.6213, Val Loss: 1.0786
batch size: (925, 925)
Epoch 69, accuracy: 0.4331
batch size: (891, 891)
Epoch 70, accuracy: 0.4306
Epoch 70, Train Loss: 0.6212, Val Loss: 1.0786
batch size: (912, 912)
Epoch 71, accuracy: 0.4286
batch size: (889, 889)
Epoch 72, accuracy: 0.4286
Epoch 72, Train Loss: 0.6194, Val Loss: 1.0792
batch size: (905, 905)
Epoch 73, accuracy: 0.4297
batch size: (899, 899)
Epoch 74, accuracy: 0.4294
Epoch 74, Train Loss: 0.6202, Val Loss: 1.0795
batch size: (916, 916)
Epoch 75, accuracy: 0.4293
batch size: (890, 890)
Epoch 76, accuracy: 0.4320
Epoch 76, Train Loss: 0.6243, Val Loss: 1.0789
batch size: (913, 913)
Epoch 77, accuracy: 0.4296
batch size: (889, 889)
Epoch 78, accuracy: 0.4297
Epoch 78, Train Loss: 0.6216, Val Loss: 1.0786
batch size: (893, 893)
Epoch 79, accuracy: 0.4326
batch size: (916, 916)
Epoch 80, accuracy: 0.4266
Epoch 80, Train Loss: 0.6220, Val Loss: 1.0785
batch size: (916, 916)
Epoch 81, accuracy: 0.4290
batch size: (876, 876)
Epoch 82, accuracy: 0.4259
Epoch 82, Train Loss: 0.6214, Val Loss: 1.0799
batch size: (903, 903)
Epoch 83, accuracy: 0.4328
batch size: (889, 889)
Epoch 84, accuracy: 0.4325
Epoch 84, Train Loss: 0.6205, Val Loss: 1.0775
batch size: (894, 894)
Epoch 85, accuracy: 0.4277
batch size: (895, 895)
Epoch 86, accuracy: 0.4334
Epoch 86, Train Loss: 0.6201, Val Loss: 1.0793
batch size: (915, 915)
Epoch 87, accuracy: 0.4318
batch size: (904, 904)
Epoch 88, accuracy: 0.4320
Epoch 88, Train Loss: 0.6197, Val Loss: 1.0867
batch size: (894, 894)
Epoch 89, accuracy: 0.4333
batch size: (901, 901)
Epoch 90, accuracy: 0.4326
Epoch 90, Train Loss: 0.6253, Val Loss: 1.0917
batch size: (902, 902)
Epoch 91, accuracy: 0.4347
batch size: (898, 898)
Epoch 92, accuracy: 0.4334
Epoch 92, Train Loss: 0.6184, Val Loss: 1.0932
batch size: (901, 901)
Epoch 93, accuracy: 0.4377
batch size: (896, 896)
Epoch 94, accuracy: 0.4352
Epoch 94, Train Loss: 0.6191, Val Loss: 1.0928
batch size: (905, 905)
Epoch 95, accuracy: 0.4314
batch size: (886, 886)
Epoch 96, accuracy: 0.4372
Epoch 96, Train Loss: 0.6197, Val Loss: 1.0902
batch size: (887, 887)
Epoch 97, accuracy: 0.4388
batch size: (889, 889)
Epoch 98, accuracy: 0.4368
Epoch 98, Train Loss: 0.6185, Val Loss: 1.0893
batch size: (906, 906)
Epoch 99, accuracy: 0.4353
batch size: (903, 903)
Epoch 100, accuracy: 0.4363
Epoch 100, Train Loss: 0.6189, Val Loss: 1.0929
batch size: (904, 904)
Epoch 101, accuracy: 0.4360
batch size: (894, 894)
Epoch 102, accuracy: 0.4340
Epoch 102, Train Loss: 0.6217, Val Loss: 1.0956
batch size: (904, 904)
Epoch 103, accuracy: 0.4343
batch size: (892, 892)
Epoch 104, accuracy: 0.4362
Epoch 104, Train Loss: 0.6195, Val Loss: 1.0967
batch size: (898, 898)
Epoch 105, accuracy: 0.4347
batch size: (885, 885)
Epoch 106, accuracy: 0.4371
Epoch 106, Train Loss: 0.6199, Val Loss: 1.0977
batch size: (911, 911)
Epoch 107, accuracy: 0.4369
batch size: (894, 894)
Epoch 108, accuracy: 0.4336
Epoch 108, Train Loss: 0.6223, Val Loss: 1.0975
batch size: (908, 908)
Epoch 109, accuracy: 0.4400
batch size: (906, 906)
Epoch 110, accuracy: 0.4331
Epoch 110, Train Loss: 0.6195, Val Loss: 1.0967
batch size: (898, 898)
Epoch 111, accuracy: 0.4373
batch size: (904, 904)
Epoch 112, accuracy: 0.4372
Epoch 112, Train Loss: 0.6248, Val Loss: 1.0963
batch size: (904, 904)
Epoch 113, accuracy: 0.4365
batch size: (888, 888)
Epoch 114, accuracy: 0.4375
Epoch 114, Train Loss: 0.6197, Val Loss: 1.0952
batch size: (893, 893)
Epoch 115, accuracy: 0.4386
batch size: (902, 902)
Epoch 116, accuracy: 0.4302
Epoch 116, Train Loss: 0.6184, Val Loss: 1.0956
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
batch size: (895, 895)
Epoch 117, accuracy: 0.4359
batch size: (919, 919)
Epoch 118, accuracy: 0.4348
Epoch 118, Train Loss: 0.6198, Val Loss: 1.0957
batch size: (902, 902)
Epoch 119, accuracy: 0.4371
batch size: (885, 885)
Epoch 120, accuracy: 0.4364
Epoch 120, Train Loss: 0.6191, Val Loss: 1.0965
batch size: (896, 896)
Epoch 121, accuracy: 0.4314
batch size: (899, 899)
Epoch 122, accuracy: 0.4374
Epoch 122, Train Loss: 0.6224, Val Loss: 1.0973
batch size: (913, 913)
Epoch 123, accuracy: 0.4323
batch size: (891, 891)
Epoch 124, accuracy: 0.4363
Epoch 124, Train Loss: 0.6193, Val Loss: 1.1005
batch size: (915, 915)
Epoch 125, accuracy: 0.4333
batch size: (898, 898)
Epoch 126, accuracy: 0.4366
Epoch 126, Train Loss: 0.6171, Val Loss: 1.0999
batch size: (893, 893)
Epoch 127, accuracy: 0.4354
batch size: (912, 912)
Epoch 128, accuracy: 0.4328
Epoch 128, Train Loss: 0.6196, Val Loss: 1.0983
batch size: (901, 901)
Epoch 129, accuracy: 0.4367
batch size: (886, 886)
Epoch 130, accuracy: 0.4359
Epoch 130, Train Loss: 0.6226, Val Loss: 1.0970
batch size: (910, 910)
Epoch 131, accuracy: 0.4354
batch size: (889, 889)
Epoch 132, accuracy: 0.4369
Epoch 132, Train Loss: 0.6200, Val Loss: 1.0987
batch size: (906, 906)
Epoch 133, accuracy: 0.4354
batch size: (892, 892)
Epoch 134, accuracy: 0.4392
Epoch 134, Train Loss: 0.6213, Val Loss: 1.0998
batch size: (893, 893)
Epoch 135, accuracy: 0.4360
batch size: (921, 921)
Epoch 136, accuracy: 0.4289
Epoch 136, Train Loss: 0.6217, Val Loss: 1.1008
batch size: (887, 887)
Epoch 137, accuracy: 0.4355
batch size: (906, 906)
Epoch 138, accuracy: 0.4324
Epoch 138, Train Loss: 0.6186, Val Loss: 1.1005
batch size: (915, 915)
Epoch 139, accuracy: 0.4413
batch size: (898, 898)
Epoch 140, accuracy: 0.4378
Epoch 140, Train Loss: 0.6237, Val Loss: 1.1003
batch size: (898, 898)
Epoch 141, accuracy: 0.4334
batch size: (909, 909)
Epoch 142, accuracy: 0.4347
Epoch 142, Train Loss: 0.6203, Val Loss: 1.1018
batch size: (894, 894)
Epoch 143, accuracy: 0.4379
batch size: (888, 888)
Epoch 144, accuracy: 0.4370
Epoch 144, Train Loss: 0.6188, Val Loss: 1.1019
batch size: (912, 912)
Epoch 145, accuracy: 0.4399
batch size: (907, 907)
Epoch 146, accuracy: 0.4382
Epoch 146, Train Loss: 0.6206, Val Loss: 1.1011
batch size: (901, 901)
Epoch 147, accuracy: 0.4353
batch size: (881, 881)
Epoch 148, accuracy: 0.4335
Epoch 148, Train Loss: 0.6191, Val Loss: 1.0987
batch size: (915, 915)
Epoch 149, accuracy: 0.4366
batch size: (896, 896)
Epoch 150, accuracy: 0.4336
Epoch 150, Train Loss: 0.6184, Val Loss: 1.0982
batch size: (901, 901)
Epoch 151, accuracy: 0.4370
batch size: (873, 873)
Epoch 152, accuracy: 0.4339
Epoch 152, Train Loss: 0.6225, Val Loss: 1.0994
batch size: (897, 897)
Epoch 153, accuracy: 0.4338
batch size: (908, 908)
Epoch 154, accuracy: 0.4355
Epoch 154, Train Loss: 0.6186, Val Loss: 1.1015
batch size: (904, 904)
Epoch 155, accuracy: 0.4347
batch size: (918, 918)
Epoch 156, accuracy: 0.4354
Epoch 156, Train Loss: 0.6209, Val Loss: 1.1017
batch size: (915, 915)
Epoch 157, accuracy: 0.4359
batch size: (911, 911)
Epoch 158, accuracy: 0.4336
Epoch 158, Train Loss: 0.6185, Val Loss: 1.1022
batch size: (891, 891)
Epoch 159, accuracy: 0.4361
batch size: (906, 906)
Epoch 160, accuracy: 0.4361
Epoch 160, Train Loss: 0.6225, Val Loss: 1.1005
batch size: (893, 893)
Epoch 161, accuracy: 0.4368
batch size: (888, 888)
Epoch 162, accuracy: 0.4342
Epoch 162, Train Loss: 0.6206, Val Loss: 1.0997
batch size: (903, 903)
Epoch 163, accuracy: 0.4366
batch size: (910, 910)
Epoch 164, accuracy: 0.4364
Epoch 164, Train Loss: 0.6187, Val Loss: 1.0999
batch size: (897, 897)
Epoch 165, accuracy: 0.4328
batch size: (898, 898)
Epoch 166, accuracy: 0.4338
Epoch 166, Train Loss: 0.6173, Val Loss: 1.1017
batch size: (905, 905)
Epoch 167, accuracy: 0.4344
batch size: (902, 902)
Epoch 168, accuracy: 0.4322
Epoch 168, Train Loss: 0.6242, Val Loss: 1.1021
batch size: (900, 900)
Epoch 169, accuracy: 0.4376
batch size: (887, 887)
Epoch 170, accuracy: 0.4311
Epoch 170, Train Loss: 0.6191, Val Loss: 1.1029
batch size: (887, 887)
Epoch 171, accuracy: 0.4352
batch size: (906, 906)
Epoch 172, accuracy: 0.4373
Epoch 172, Train Loss: 0.6190, Val Loss: 1.1018
batch size: (901, 901)
Epoch 173, accuracy: 0.4336
batch size: (892, 892)
Epoch 174, accuracy: 0.4354
Epoch 174, Train Loss: 0.6209, Val Loss: 1.1005
batch size: (917, 917)
Epoch 175, accuracy: 0.4360
batch size: (907, 907)
Epoch 176, accuracy: 0.4366
Epoch 176, Train Loss: 0.6193, Val Loss: 1.1022
batch size: (910, 910)
Epoch 177, accuracy: 0.4366
batch size: (905, 905)
Epoch 178, accuracy: 0.4387
Epoch 178, Train Loss: 0.6195, Val Loss: 1.1024
batch size: (896, 896)
Epoch 179, accuracy: 0.4380
batch size: (904, 904)
Epoch 180, accuracy: 0.4364
Epoch 180, Train Loss: 0.6183, Val Loss: 1.1039
batch size: (898, 898)
Epoch 181, accuracy: 0.4366
batch size: (895, 895)
Epoch 182, accuracy: 0.4356
Epoch 182, Train Loss: 0.6207, Val Loss: 1.1017
batch size: (903, 903)
Epoch 183, accuracy: 0.4364
batch size: (894, 894)
Epoch 184, accuracy: 0.4333
Epoch 184, Train Loss: 0.6205, Val Loss: 1.1032
batch size: (877, 877)
Epoch 185, accuracy: 0.4338
batch size: (905, 905)
Epoch 186, accuracy: 0.4382
Epoch 186, Train Loss: 0.6211, Val Loss: 1.1050
batch size: (892, 892)
Epoch 187, accuracy: 0.4356
batch size: (892, 892)
Epoch 188, accuracy: 0.4345
Epoch 188, Train Loss: 0.6176, Val Loss: 1.1039
batch size: (882, 882)
Epoch 189, accuracy: 0.4356
batch size: (898, 898)
Epoch 190, accuracy: 0.4367
Epoch 190, Train Loss: 0.6236, Val Loss: 1.1037
batch size: (902, 902)
Epoch 191, accuracy: 0.4367
batch size: (904, 904)
Epoch 192, accuracy: 0.4348
Epoch 192, Train Loss: 0.6201, Val Loss: 1.1047
batch size: (891, 891)
Epoch 193, accuracy: 0.4369
batch size: (889, 889)
Epoch 194, accuracy: 0.4337
Epoch 194, Train Loss: 0.6214, Val Loss: 1.1052
batch size: (899, 899)
Epoch 195, accuracy: 0.4342
batch size: (903, 903)
Epoch 196, accuracy: 0.4351
Epoch 196, Train Loss: 0.6209, Val Loss: 1.1067
batch size: (899, 899)
Epoch 197, accuracy: 0.4347
batch size: (879, 879)
Epoch 198, accuracy: 0.4380
Epoch 198, Train Loss: 0.6180, Val Loss: 1.1076
batch size: (895, 895)
Epoch 199, accuracy: 0.4347
Loaded best model with val_loss = 1.0689886808395386
test :accuracy 0.4275, f1_macro: 0.1996, f1_micro: 0.4275, auc: 0.4805
Final Results: {'GCN_2_Pubmed': np.float64(0.7084593488703078), 'GCN_8_Pubmed': np.float64(0.42897434749518737), 'GCN_32_Pubmed': np.float64(0.16710792012564504), 'GraphSAGE_2_Pubmed': np.float64(0.6919313228360466), 'GraphSAGE_8_Pubmed': np.float64(0.4279917959693241), 'GraphSAGE_32_Pubmed': np.float64(0.16813764821253455), 'GAT_2_Pubmed': np.float64(0.7327815155807366), 'GAT_8_Pubmed': np.float64(0.7183042082738944), 'GAT_32_Pubmed': np.float64(0.42712758821161845), 'JKNet_2_Pubmed': np.float64(0.4588334667022317), 'JKNet_8_Pubmed': np.float64(0.4292450731794119), 'JKNet_32_Pubmed': np.float64(0.4304984299677148), 'resGCN_2_Pubmed': np.float64(0.6320528743789922), 'resGCN_8_Pubmed': np.float64(0.42790656442533576), 'resGCN_32_Pubmed': np.float64(0.17016703237651695), 'GINConv_2_Pubmed': np.float64(0.6347438752783965), 'GINConv_8_Pubmed': np.float64(0.35888143176733783), 'GINConv_32_Pubmed': np.float64(0.41837597330367077), 'mlp_2_Pubmed': np.float64(0.6613520976218046), 'mlp_8_Pubmed': np.float64(0.4308446623579229), 'mlp_32_Pubmed': np.float64(0.4274625267665953)} ['133385_GCN_0', '532745_GCN_0', '2130185_GCN_0', '262153_GraphSAGE_0', '45833_GraphSAGE_0', '98057_GraphSAGE_0', '196495_GAT_0', '1090447_GAT_0', '4666255_GAT_0', '391942_JKNet_0', '1184518_JKNet_0', '4354822_JKNet_0', '132626_resGCN_0', '1850386_resGCN_0', '573202_resGCN_0', '265483_GINConv_0', '1059595_GINConv_0', '4236043_GINConv_0', '83465_mlp_0', '186377_mlp_0', '598025_mlp_0']
GCN_2_Pubmed: Accuracy = 0.7190 ± 0.0091
GCN_8_Pubmed: Accuracy = 0.3429 ± 0.1522
GCN_32_Pubmed: Accuracy = 0.2524 ± 0.1396
GraphSAGE_2_Pubmed: Accuracy = 0.7102 ± 0.0194
GraphSAGE_8_Pubmed: Accuracy = 0.3330 ± 0.1440
GraphSAGE_32_Pubmed: Accuracy = 0.2521 ± 0.1284
GAT_2_Pubmed: Accuracy = 0.7333 ± 0.0132
GAT_8_Pubmed: Accuracy = 0.6471 ± 0.1017
GAT_32_Pubmed: Accuracy = 0.4193 ± 0.0160
JKNet_2_Pubmed: Accuracy = 0.3797 ± 0.0948
JKNet_8_Pubmed: Accuracy = 0.4484 ± 0.0309
JKNet_32_Pubmed: Accuracy = 0.4160 ± 0.0135
resGCN_2_Pubmed: Accuracy = 0.6240 ± 0.0808
resGCN_8_Pubmed: Accuracy = 0.3413 ± 0.1513
resGCN_32_Pubmed: Accuracy = 0.1861 ± 0.0317
GINConv_2_Pubmed: Accuracy = 0.6671 ± 0.0448
GINConv_8_Pubmed: Accuracy = 0.3966 ± 0.0348
GINConv_32_Pubmed: Accuracy = 0.4073 ± 0.0096
mlp_2_Pubmed: Accuracy = 0.6749 ± 0.0121
mlp_8_Pubmed: Accuracy = 0.4571 ± 0.0801
mlp_32_Pubmed: Accuracy = 0.4197 ± 0.0155
