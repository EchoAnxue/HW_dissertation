nohup: ignoring input
/root/miniconda3/lib/python3.12/site-packages/torch_geometric/graphgym/config.py:19: UserWarning: Could not define global config object. Please install 'yacs' via 'pip install yacs' in order to use GraphGym
  warnings.warn("Could not define global config object. Please install "
/root/miniconda3/lib/python3.12/site-packages/torch_geometric/graphgym/imports.py:14: UserWarning: Please install 'pytorch_lightning' via  'pip install pytorch_lightning' in order to use GraphGym
  warnings.warn("Please install 'pytorch_lightning' via  "
/root/miniconda3/lib/python3.12/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling
  warnings.warn(f"Using '{self.__class__.__name__}' without a "
========== Running baseline 1/3 ==========
Training mamba3 with 2 layers...
可训练参数: 1485758_mamba3
不可训练参数: 0
✅ Epoch 0: New best model saved with val_loss = 1.5552
Epoch 0, accuracy: 0.4640
Epoch 0, Train Loss: 1.9192, Val Loss: 1.5552
✅ Epoch 1: New best model saved with val_loss = 1.2905
Epoch 1, accuracy: 0.5530
Epoch 2, accuracy: 0.6120
Epoch 2, Train Loss: 0.9973, Val Loss: 1.3033
✅ Epoch 3: New best model saved with val_loss = 1.2710
Epoch 3, accuracy: 0.6160
✅ Epoch 4: New best model saved with val_loss = 1.2226
Epoch 4, accuracy: 0.5890
Epoch 4, Train Loss: 0.6716, Val Loss: 1.2226
Epoch 5, accuracy: 0.5910
Epoch 6, accuracy: 0.5930
Epoch 6, Train Loss: 0.4743, Val Loss: 1.2410
Epoch 7, accuracy: 0.6100
Epoch 8, accuracy: 0.6280
Epoch 8, Train Loss: 0.3199, Val Loss: 1.3060
Epoch 9, accuracy: 0.6250
Epoch 10, accuracy: 0.6210
Epoch 10, Train Loss: 0.2897, Val Loss: 1.4106
Epoch 11, accuracy: 0.6190
Epoch 12, accuracy: 0.6210
Epoch 12, Train Loss: 0.2377, Val Loss: 1.4177
Epoch 13, accuracy: 0.6190
Epoch 14, accuracy: 0.6170
Epoch 14, Train Loss: 0.2381, Val Loss: 1.4222
Epoch 15, accuracy: 0.6180
Epoch 16, accuracy: 0.6160
Epoch 16, Train Loss: 0.2963, Val Loss: 1.4220
Epoch 17, accuracy: 0.6160
Epoch 18, accuracy: 0.6150
Epoch 18, Train Loss: 0.2045, Val Loss: 1.4216
Epoch 19, accuracy: 0.6160
Epoch 20, accuracy: 0.6160
Epoch 20, Train Loss: 0.2649, Val Loss: 1.4208
Epoch 21, accuracy: 0.6170
Epoch 22, accuracy: 0.6170
Epoch 22, Train Loss: 0.2431, Val Loss: 1.4204
Epoch 23, accuracy: 0.6170
Epoch 24, accuracy: 0.6170
Epoch 24, Train Loss: 0.1899, Val Loss: 1.4203
Epoch 25, accuracy: 0.6170
Epoch 26, accuracy: 0.6170
Epoch 26, Train Loss: 0.2621, Val Loss: 1.4203
Epoch 27, accuracy: 0.6170
Epoch 28, accuracy: 0.6170
Epoch 28, Train Loss: 0.2379, Val Loss: 1.4202
Epoch 29, accuracy: 0.6170
Epoch 30, accuracy: 0.6170
Epoch 30, Train Loss: 0.2590, Val Loss: 1.4202
Epoch 31, accuracy: 0.6170
Epoch 32, accuracy: 0.6170
Epoch 32, Train Loss: 0.3095, Val Loss: 1.4202
Epoch 33, accuracy: 0.6170
Epoch 34, accuracy: 0.6170
Epoch 34, Train Loss: 0.2477, Val Loss: 1.4202
Epoch 35, accuracy: 0.6170
Epoch 36, accuracy: 0.6170
Epoch 36, Train Loss: 0.1748, Val Loss: 1.4202
Epoch 37, accuracy: 0.6170
Epoch 38, accuracy: 0.6170
Epoch 38, Train Loss: 0.2958, Val Loss: 1.4202
Epoch 39, accuracy: 0.6170
Epoch 40, accuracy: 0.6170
Epoch 40, Train Loss: 0.2309, Val Loss: 1.4202
Epoch 41, accuracy: 0.6170
Epoch 42, accuracy: 0.6170
Epoch 42, Train Loss: 0.2098, Val Loss: 1.4202
Epoch 43, accuracy: 0.6170
Epoch 44, accuracy: 0.6170
Epoch 44, Train Loss: 0.2175, Val Loss: 1.4202
Epoch 45, accuracy: 0.6170
Epoch 46, accuracy: 0.6170
Epoch 46, Train Loss: 0.2674, Val Loss: 1.4202
Epoch 47, accuracy: 0.6170
Epoch 48, accuracy: 0.6170
Epoch 48, Train Loss: 0.3237, Val Loss: 1.4202
Epoch 49, accuracy: 0.6170
Epoch 50, accuracy: 0.6170
Epoch 50, Train Loss: 0.2581, Val Loss: 1.4202
Epoch 51, accuracy: 0.6170
Epoch 52, accuracy: 0.6170
Epoch 52, Train Loss: 0.2781, Val Loss: 1.4202
Epoch 53, accuracy: 0.6170
Epoch 54, accuracy: 0.6170
Epoch 54, Train Loss: 0.2728, Val Loss: 1.4202
Epoch 55, accuracy: 0.6170
Epoch 56, accuracy: 0.6170
Epoch 56, Train Loss: 0.2205, Val Loss: 1.4202
Epoch 57, accuracy: 0.6170
Epoch 58, accuracy: 0.6170
Epoch 58, Train Loss: 0.2805, Val Loss: 1.4202
Epoch 59, accuracy: 0.6170
Epoch 60, accuracy: 0.6170
Epoch 60, Train Loss: 0.1986, Val Loss: 1.4202
Epoch 61, accuracy: 0.6170
Epoch 62, accuracy: 0.6170
Epoch 62, Train Loss: 0.3194, Val Loss: 1.4202
Epoch 63, accuracy: 0.6170
Epoch 64, accuracy: 0.6170
Epoch 64, Train Loss: 0.2615, Val Loss: 1.4202
Epoch 65, accuracy: 0.6170
Epoch 66, accuracy: 0.6170
Epoch 66, Train Loss: 0.2354, Val Loss: 1.4202
Epoch 67, accuracy: 0.6170
Epoch 68, accuracy: 0.6170
Epoch 68, Train Loss: 0.2519, Val Loss: 1.4202
Epoch 69, accuracy: 0.6170
Epoch 70, accuracy: 0.6170
Epoch 70, Train Loss: 0.2195, Val Loss: 1.4202
Epoch 71, accuracy: 0.6170
Epoch 72, accuracy: 0.6170
Epoch 72, Train Loss: 0.2332, Val Loss: 1.4202
Epoch 73, accuracy: 0.6170
Epoch 74, accuracy: 0.6170
Epoch 74, Train Loss: 0.1767, Val Loss: 1.4202
Epoch 75, accuracy: 0.6170
Epoch 76, accuracy: 0.6170
Epoch 76, Train Loss: 0.2613, Val Loss: 1.4202
Epoch 77, accuracy: 0.6170
Epoch 78, accuracy: 0.6170
Epoch 78, Train Loss: 0.2147, Val Loss: 1.4202
Epoch 79, accuracy: 0.6170
Epoch 80, accuracy: 0.6170
Epoch 80, Train Loss: 0.2578, Val Loss: 1.4202
Epoch 81, accuracy: 0.6170
Epoch 82, accuracy: 0.6170
Epoch 82, Train Loss: 0.2993, Val Loss: 1.4202
Epoch 83, accuracy: 0.6170
Epoch 84, accuracy: 0.6170
Epoch 84, Train Loss: 0.2699, Val Loss: 1.4202
Epoch 85, accuracy: 0.6170
Epoch 86, accuracy: 0.6170
Epoch 86, Train Loss: 0.2204, Val Loss: 1.4202
Epoch 87, accuracy: 0.6170
Epoch 88, accuracy: 0.6170
Epoch 88, Train Loss: 0.2503, Val Loss: 1.4202
Epoch 89, accuracy: 0.6170
Epoch 90, accuracy: 0.6170
Epoch 90, Train Loss: 0.2550, Val Loss: 1.4202
Epoch 91, accuracy: 0.6170
Epoch 92, accuracy: 0.6170
Epoch 92, Train Loss: 0.1996, Val Loss: 1.4202
Epoch 93, accuracy: 0.6170
Epoch 94, accuracy: 0.6170
Epoch 94, Train Loss: 0.2458, Val Loss: 1.4202
Epoch 95, accuracy: 0.6170
Epoch 96, accuracy: 0.6170
Epoch 96, Train Loss: 0.2598, Val Loss: 1.4202
Epoch 97, accuracy: 0.6170
Epoch 98, accuracy: 0.6170
Epoch 98, Train Loss: 0.3477, Val Loss: 1.4202
Epoch 99, accuracy: 0.6170
Epoch 100, accuracy: 0.6170
Epoch 100, Train Loss: 0.2364, Val Loss: 1.4202
Epoch 101, accuracy: 0.6170
Epoch 102, accuracy: 0.6170
Epoch 102, Train Loss: 0.2465, Val Loss: 1.4202
Epoch 103, accuracy: 0.6170
Epoch 104, accuracy: 0.6170
Epoch 104, Train Loss: 0.2947, Val Loss: 1.4202
Epoch 105, accuracy: 0.6170
Epoch 106, accuracy: 0.6170
Epoch 106, Train Loss: 0.3094, Val Loss: 1.4202
Epoch 107, accuracy: 0.6170
Epoch 108, accuracy: 0.6170
Epoch 108, Train Loss: 0.2529, Val Loss: 1.4202
Epoch 109, accuracy: 0.6170
Epoch 110, accuracy: 0.6170
Epoch 110, Train Loss: 0.1938, Val Loss: 1.4202
Epoch 111, accuracy: 0.6170
Epoch 112, accuracy: 0.6170
Epoch 112, Train Loss: 0.2239, Val Loss: 1.4202
Epoch 113, accuracy: 0.6170
Epoch 114, accuracy: 0.6170
Epoch 114, Train Loss: 0.3055, Val Loss: 1.4202
Epoch 115, accuracy: 0.6170
Epoch 116, accuracy: 0.6170
Epoch 116, Train Loss: 0.2099, Val Loss: 1.4202
Epoch 117, accuracy: 0.6170
Epoch 118, accuracy: 0.6170
Epoch 118, Train Loss: 0.2573, Val Loss: 1.4202
Epoch 119, accuracy: 0.6170
Epoch 120, accuracy: 0.6170
Epoch 120, Train Loss: 0.2428, Val Loss: 1.4202
Epoch 121, accuracy: 0.6170
Epoch 122, accuracy: 0.6170
Epoch 122, Train Loss: 0.3006, Val Loss: 1.4202
Epoch 123, accuracy: 0.6170
Epoch 124, accuracy: 0.6170
Epoch 124, Train Loss: 0.2831, Val Loss: 1.4202
Epoch 125, accuracy: 0.6170
Epoch 126, accuracy: 0.6170
Epoch 126, Train Loss: 0.2722, Val Loss: 1.4202
Epoch 127, accuracy: 0.6170
Epoch 128, accuracy: 0.6170
Epoch 128, Train Loss: 0.2820, Val Loss: 1.4202
Epoch 129, accuracy: 0.6170
Epoch 130, accuracy: 0.6170
Epoch 130, Train Loss: 0.2387, Val Loss: 1.4202
Epoch 131, accuracy: 0.6170
Epoch 132, accuracy: 0.6170
Epoch 132, Train Loss: 0.2717, Val Loss: 1.4202
Epoch 133, accuracy: 0.6170
Epoch 134, accuracy: 0.6170
Epoch 134, Train Loss: 0.1983, Val Loss: 1.4202
Epoch 135, accuracy: 0.6170
Epoch 136, accuracy: 0.6170
Epoch 136, Train Loss: 0.2702, Val Loss: 1.4202
Epoch 137, accuracy: 0.6170
Epoch 138, accuracy: 0.6170
Epoch 138, Train Loss: 0.2519, Val Loss: 1.4202
Epoch 139, accuracy: 0.6170
Epoch 140, accuracy: 0.6170
Epoch 140, Train Loss: 0.2406, Val Loss: 1.4202
Epoch 141, accuracy: 0.6170
Epoch 142, accuracy: 0.6170
Epoch 142, Train Loss: 0.2543, Val Loss: 1.4202
Epoch 143, accuracy: 0.6170
Epoch 144, accuracy: 0.6170
Epoch 144, Train Loss: 0.2717, Val Loss: 1.4202
Epoch 145, accuracy: 0.6170
Epoch 146, accuracy: 0.6170
Epoch 146, Train Loss: 0.2620, Val Loss: 1.4202
Epoch 147, accuracy: 0.6170
Epoch 148, accuracy: 0.6170
Epoch 148, Train Loss: 0.2499, Val Loss: 1.4202
Epoch 149, accuracy: 0.6170
Epoch 150, accuracy: 0.6170
Epoch 150, Train Loss: 0.2521, Val Loss: 1.4202
Epoch 151, accuracy: 0.6170
Epoch 152, accuracy: 0.6170
Epoch 152, Train Loss: 0.2476, Val Loss: 1.4202
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 153, accuracy: 0.6170
Epoch 154, accuracy: 0.6170
Epoch 154, Train Loss: 0.3077, Val Loss: 1.4202
Epoch 155, accuracy: 0.6170
Epoch 156, accuracy: 0.6170
Epoch 156, Train Loss: 0.2543, Val Loss: 1.4202
Epoch 157, accuracy: 0.6170
Epoch 158, accuracy: 0.6170
Epoch 158, Train Loss: 0.2135, Val Loss: 1.4202
Epoch 159, accuracy: 0.6170
Epoch 160, accuracy: 0.6170
Epoch 160, Train Loss: 0.2568, Val Loss: 1.4202
Epoch 161, accuracy: 0.6170
Epoch 162, accuracy: 0.6170
Epoch 162, Train Loss: 0.2469, Val Loss: 1.4202
Epoch 163, accuracy: 0.6170
Epoch 164, accuracy: 0.6170
Epoch 164, Train Loss: 0.2053, Val Loss: 1.4202
Epoch 165, accuracy: 0.6170
Epoch 166, accuracy: 0.6170
Epoch 166, Train Loss: 0.2220, Val Loss: 1.4202
Epoch 167, accuracy: 0.6170
Epoch 168, accuracy: 0.6170
Epoch 168, Train Loss: 0.2669, Val Loss: 1.4202
Epoch 169, accuracy: 0.6170
Epoch 170, accuracy: 0.6170
Epoch 170, Train Loss: 0.3022, Val Loss: 1.4202
Epoch 171, accuracy: 0.6170
Epoch 172, accuracy: 0.6170
Epoch 172, Train Loss: 0.2529, Val Loss: 1.4202
Epoch 173, accuracy: 0.6170
Epoch 174, accuracy: 0.6170
Epoch 174, Train Loss: 0.2463, Val Loss: 1.4202
Epoch 175, accuracy: 0.6170
Epoch 176, accuracy: 0.6170
Epoch 176, Train Loss: 0.2029, Val Loss: 1.4202
Epoch 177, accuracy: 0.6170
Epoch 178, accuracy: 0.6170
Epoch 178, Train Loss: 0.2569, Val Loss: 1.4202
Epoch 179, accuracy: 0.6170
Epoch 180, accuracy: 0.6170
Epoch 180, Train Loss: 0.2808, Val Loss: 1.4202
Epoch 181, accuracy: 0.6170
Epoch 182, accuracy: 0.6170
Epoch 182, Train Loss: 0.2229, Val Loss: 1.4202
Epoch 183, accuracy: 0.6170
Epoch 184, accuracy: 0.6170
Epoch 184, Train Loss: 0.2751, Val Loss: 1.4202
Epoch 185, accuracy: 0.6170
Epoch 186, accuracy: 0.6170
Epoch 186, Train Loss: 0.3402, Val Loss: 1.4202
Epoch 187, accuracy: 0.6170
Epoch 188, accuracy: 0.6170
Epoch 188, Train Loss: 0.3054, Val Loss: 1.4202
Epoch 189, accuracy: 0.6170
Epoch 190, accuracy: 0.6170
Epoch 190, Train Loss: 0.2250, Val Loss: 1.4202
Epoch 191, accuracy: 0.6170
Epoch 192, accuracy: 0.6170
Epoch 192, Train Loss: 0.2412, Val Loss: 1.4202
Epoch 193, accuracy: 0.6170
Epoch 194, accuracy: 0.6170
Epoch 194, Train Loss: 0.2830, Val Loss: 1.4202
Epoch 195, accuracy: 0.6170
Epoch 196, accuracy: 0.6170
Epoch 196, Train Loss: 0.1943, Val Loss: 1.4202
Epoch 197, accuracy: 0.6170
Epoch 198, accuracy: 0.6170
Epoch 198, Train Loss: 0.1660, Val Loss: 1.4202
Epoch 199, accuracy: 0.6170
Loaded best model with val_loss = 1.222571849822998
test :accuracy 0.5890, f1_macro: 0.5589, f1_micro: 0.5890, auc: 0.8508
Training mamba3 with 8 layers...
可训练参数: 1983422_mamba3
不可训练参数: 0
✅ Epoch 0: New best model saved with val_loss = 1.7382
Epoch 0, accuracy: 0.3120
Epoch 0, Train Loss: 1.9251, Val Loss: 1.7382
Epoch 1, accuracy: 0.3090
✅ Epoch 2: New best model saved with val_loss = 1.3606
Epoch 2, accuracy: 0.4850
Epoch 2, Train Loss: 1.8225, Val Loss: 1.3606
✅ Epoch 3: New best model saved with val_loss = 1.2584
Epoch 3, accuracy: 0.5480
✅ Epoch 4: New best model saved with val_loss = 1.1831
Epoch 4, accuracy: 0.5910
Epoch 4, Train Loss: 0.9499, Val Loss: 1.1831
✅ Epoch 5: New best model saved with val_loss = 1.1563
Epoch 5, accuracy: 0.6020
✅ Epoch 6: New best model saved with val_loss = 1.1521
Epoch 6, accuracy: 0.6010
Epoch 6, Train Loss: 0.7865, Val Loss: 1.1521
✅ Epoch 7: New best model saved with val_loss = 1.1442
Epoch 7, accuracy: 0.5960
Epoch 8, accuracy: 0.5970
Epoch 8, Train Loss: 0.5877, Val Loss: 1.1619
Epoch 9, accuracy: 0.5850
Epoch 10, accuracy: 0.5790
Epoch 10, Train Loss: 0.5815, Val Loss: 1.2254
Epoch 11, accuracy: 0.5800
Epoch 12, accuracy: 0.5860
Epoch 12, Train Loss: 0.5444, Val Loss: 1.2625
Epoch 13, accuracy: 0.5900
Epoch 14, accuracy: 0.5910
Epoch 14, Train Loss: 0.3690, Val Loss: 1.2989
Epoch 15, accuracy: 0.5900
Epoch 16, accuracy: 0.5900
Epoch 16, Train Loss: 0.4475, Val Loss: 1.3038
Epoch 17, accuracy: 0.5900
Epoch 18, accuracy: 0.5860
Epoch 18, Train Loss: 0.4812, Val Loss: 1.3073
Epoch 19, accuracy: 0.5880
Epoch 20, accuracy: 0.5880
Epoch 20, Train Loss: 0.5080, Val Loss: 1.3098
Epoch 21, accuracy: 0.5870
Epoch 22, accuracy: 0.5860
Epoch 22, Train Loss: 0.4662, Val Loss: 1.3102
Epoch 23, accuracy: 0.5860
Epoch 24, accuracy: 0.5860
Epoch 24, Train Loss: 0.3759, Val Loss: 1.3106
Epoch 25, accuracy: 0.5860
Epoch 26, accuracy: 0.5860
Epoch 26, Train Loss: 0.3731, Val Loss: 1.3109
Epoch 27, accuracy: 0.5860
Epoch 28, accuracy: 0.5870
Epoch 28, Train Loss: 0.4269, Val Loss: 1.3109
Epoch 29, accuracy: 0.5870
Epoch 30, accuracy: 0.5870
Epoch 30, Train Loss: 0.3401, Val Loss: 1.3110
Epoch 31, accuracy: 0.5880
Epoch 32, accuracy: 0.5880
Epoch 32, Train Loss: 0.4576, Val Loss: 1.3110
Epoch 33, accuracy: 0.5880
Epoch 34, accuracy: 0.5880
Epoch 34, Train Loss: 0.4031, Val Loss: 1.3110
Epoch 35, accuracy: 0.5880
Epoch 36, accuracy: 0.5880
Epoch 36, Train Loss: 0.4282, Val Loss: 1.3110
Epoch 37, accuracy: 0.5880
Epoch 38, accuracy: 0.5880
Epoch 38, Train Loss: 0.3948, Val Loss: 1.3110
Epoch 39, accuracy: 0.5880
Epoch 40, accuracy: 0.5880
Epoch 40, Train Loss: 0.3738, Val Loss: 1.3110
Epoch 41, accuracy: 0.5880
Epoch 42, accuracy: 0.5880
Epoch 42, Train Loss: 0.4825, Val Loss: 1.3110
Epoch 43, accuracy: 0.5880
Epoch 44, accuracy: 0.5880
Epoch 44, Train Loss: 0.4548, Val Loss: 1.3110
Epoch 45, accuracy: 0.5880
Epoch 46, accuracy: 0.5880
Epoch 46, Train Loss: 0.4063, Val Loss: 1.3110
Epoch 47, accuracy: 0.5880
Epoch 48, accuracy: 0.5880
Epoch 48, Train Loss: 0.3856, Val Loss: 1.3110
Epoch 49, accuracy: 0.5880
Epoch 50, accuracy: 0.5880
Epoch 50, Train Loss: 0.4687, Val Loss: 1.3110
Epoch 51, accuracy: 0.5880
Epoch 52, accuracy: 0.5880
Epoch 52, Train Loss: 0.4635, Val Loss: 1.3110
Epoch 53, accuracy: 0.5880
Epoch 54, accuracy: 0.5880
Epoch 54, Train Loss: 0.4660, Val Loss: 1.3110
Epoch 55, accuracy: 0.5880
Epoch 56, accuracy: 0.5880
Epoch 56, Train Loss: 0.3726, Val Loss: 1.3110
Epoch 57, accuracy: 0.5880
Epoch 58, accuracy: 0.5880
Epoch 58, Train Loss: 0.3827, Val Loss: 1.3110
Epoch 59, accuracy: 0.5880
Epoch 60, accuracy: 0.5880
Epoch 60, Train Loss: 0.4423, Val Loss: 1.3110
Epoch 61, accuracy: 0.5880
Epoch 62, accuracy: 0.5880
Epoch 62, Train Loss: 0.4832, Val Loss: 1.3110
Epoch 63, accuracy: 0.5880
Epoch 64, accuracy: 0.5880
Epoch 64, Train Loss: 0.4449, Val Loss: 1.3110
Epoch 65, accuracy: 0.5880
Epoch 66, accuracy: 0.5880
Epoch 66, Train Loss: 0.3845, Val Loss: 1.3110
Epoch 67, accuracy: 0.5880
Epoch 68, accuracy: 0.5880
Epoch 68, Train Loss: 0.4907, Val Loss: 1.3110
Epoch 69, accuracy: 0.5880
Epoch 70, accuracy: 0.5880
Epoch 70, Train Loss: 0.4543, Val Loss: 1.3110
Epoch 71, accuracy: 0.5880
Epoch 72, accuracy: 0.5880
Epoch 72, Train Loss: 0.4061, Val Loss: 1.3110
Epoch 73, accuracy: 0.5880
Epoch 74, accuracy: 0.5880
Epoch 74, Train Loss: 0.4041, Val Loss: 1.3110
Epoch 75, accuracy: 0.5880
Epoch 76, accuracy: 0.5880
Epoch 76, Train Loss: 0.4468, Val Loss: 1.3110
Epoch 77, accuracy: 0.5880
Epoch 78, accuracy: 0.5880
Epoch 78, Train Loss: 0.4610, Val Loss: 1.3110
Epoch 79, accuracy: 0.5880
Epoch 80, accuracy: 0.5880
Epoch 80, Train Loss: 0.4775, Val Loss: 1.3110
Epoch 81, accuracy: 0.5880
Epoch 82, accuracy: 0.5880
Epoch 82, Train Loss: 0.3990, Val Loss: 1.3110
Epoch 83, accuracy: 0.5880
Epoch 84, accuracy: 0.5880
Epoch 84, Train Loss: 0.4105, Val Loss: 1.3110
Epoch 85, accuracy: 0.5880
Epoch 86, accuracy: 0.5880
Epoch 86, Train Loss: 0.4937, Val Loss: 1.3110
Epoch 87, accuracy: 0.5880
Epoch 88, accuracy: 0.5880
Epoch 88, Train Loss: 0.4233, Val Loss: 1.3110
Epoch 89, accuracy: 0.5880
Epoch 90, accuracy: 0.5880
Epoch 90, Train Loss: 0.4921, Val Loss: 1.3110
Epoch 91, accuracy: 0.5880
Epoch 92, accuracy: 0.5880
Epoch 92, Train Loss: 0.4263, Val Loss: 1.3110
Epoch 93, accuracy: 0.5880
Epoch 94, accuracy: 0.5880
Epoch 94, Train Loss: 0.4238, Val Loss: 1.3110
Epoch 95, accuracy: 0.5880
Epoch 96, accuracy: 0.5880
Epoch 96, Train Loss: 0.4901, Val Loss: 1.3110
Epoch 97, accuracy: 0.5880
Epoch 98, accuracy: 0.5880
Epoch 98, Train Loss: 0.4675, Val Loss: 1.3110
Epoch 99, accuracy: 0.5880
Epoch 100, accuracy: 0.5880
Epoch 100, Train Loss: 0.4322, Val Loss: 1.3110
Epoch 101, accuracy: 0.5880
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 102, accuracy: 0.5880
Epoch 102, Train Loss: 0.4664, Val Loss: 1.3110
Epoch 103, accuracy: 0.5880
Epoch 104, accuracy: 0.5880
Epoch 104, Train Loss: 0.4420, Val Loss: 1.3110
Epoch 105, accuracy: 0.5880
Epoch 106, accuracy: 0.5880
Epoch 106, Train Loss: 0.4413, Val Loss: 1.3110
Epoch 107, accuracy: 0.5880
Epoch 108, accuracy: 0.5880
Epoch 108, Train Loss: 0.4312, Val Loss: 1.3110
Epoch 109, accuracy: 0.5880
Epoch 110, accuracy: 0.5880
Epoch 110, Train Loss: 0.5141, Val Loss: 1.3110
Epoch 111, accuracy: 0.5880
Epoch 112, accuracy: 0.5880
Epoch 112, Train Loss: 0.5459, Val Loss: 1.3110
Epoch 113, accuracy: 0.5880
Epoch 114, accuracy: 0.5880
Epoch 114, Train Loss: 0.3463, Val Loss: 1.3110
Epoch 115, accuracy: 0.5880
Epoch 116, accuracy: 0.5880
Epoch 116, Train Loss: 0.4865, Val Loss: 1.3110
Epoch 117, accuracy: 0.5880
Epoch 118, accuracy: 0.5880
Epoch 118, Train Loss: 0.4528, Val Loss: 1.3110
Epoch 119, accuracy: 0.5880
Epoch 120, accuracy: 0.5880
Epoch 120, Train Loss: 0.4653, Val Loss: 1.3110
Epoch 121, accuracy: 0.5880
Epoch 122, accuracy: 0.5880
Epoch 122, Train Loss: 0.5054, Val Loss: 1.3110
Epoch 123, accuracy: 0.5880
Epoch 124, accuracy: 0.5880
Epoch 124, Train Loss: 0.4678, Val Loss: 1.3110
Epoch 125, accuracy: 0.5880
Epoch 126, accuracy: 0.5880
Epoch 126, Train Loss: 0.4814, Val Loss: 1.3110
Epoch 127, accuracy: 0.5880
Epoch 128, accuracy: 0.5880
Epoch 128, Train Loss: 0.4328, Val Loss: 1.3110
Epoch 129, accuracy: 0.5880
Epoch 130, accuracy: 0.5880
Epoch 130, Train Loss: 0.5036, Val Loss: 1.3110
Epoch 131, accuracy: 0.5880
Epoch 132, accuracy: 0.5880
Epoch 132, Train Loss: 0.4333, Val Loss: 1.3110
Epoch 133, accuracy: 0.5880
Epoch 134, accuracy: 0.5880
Epoch 134, Train Loss: 0.5269, Val Loss: 1.3110
Epoch 135, accuracy: 0.5880
Epoch 136, accuracy: 0.5880
Epoch 136, Train Loss: 0.5841, Val Loss: 1.3110
Epoch 137, accuracy: 0.5880
Epoch 138, accuracy: 0.5880
Epoch 138, Train Loss: 0.4259, Val Loss: 1.3110
Epoch 139, accuracy: 0.5880
Epoch 140, accuracy: 0.5880
Epoch 140, Train Loss: 0.4333, Val Loss: 1.3110
Epoch 141, accuracy: 0.5880
Epoch 142, accuracy: 0.5880
Epoch 142, Train Loss: 0.5119, Val Loss: 1.3110
Epoch 143, accuracy: 0.5880
Epoch 144, accuracy: 0.5880
Epoch 144, Train Loss: 0.3621, Val Loss: 1.3110
Epoch 145, accuracy: 0.5880
Epoch 146, accuracy: 0.5880
Epoch 146, Train Loss: 0.5320, Val Loss: 1.3110
Epoch 147, accuracy: 0.5880
Epoch 148, accuracy: 0.5880
Epoch 148, Train Loss: 0.3500, Val Loss: 1.3110
Epoch 149, accuracy: 0.5880
Epoch 150, accuracy: 0.5880
Epoch 150, Train Loss: 0.4681, Val Loss: 1.3110
Epoch 151, accuracy: 0.5880
Epoch 152, accuracy: 0.5880
Epoch 152, Train Loss: 0.4481, Val Loss: 1.3110
Epoch 153, accuracy: 0.5880
Epoch 154, accuracy: 0.5880
Epoch 154, Train Loss: 0.5060, Val Loss: 1.3110
Epoch 155, accuracy: 0.5880
Epoch 156, accuracy: 0.5880
Epoch 156, Train Loss: 0.4711, Val Loss: 1.3110
Epoch 157, accuracy: 0.5880
Epoch 158, accuracy: 0.5880
Epoch 158, Train Loss: 0.4959, Val Loss: 1.3110
Epoch 159, accuracy: 0.5880
Epoch 160, accuracy: 0.5880
Epoch 160, Train Loss: 0.4372, Val Loss: 1.3110
Epoch 161, accuracy: 0.5880
Epoch 162, accuracy: 0.5880
Epoch 162, Train Loss: 0.3493, Val Loss: 1.3110
Epoch 163, accuracy: 0.5880
Epoch 164, accuracy: 0.5880
Epoch 164, Train Loss: 0.5206, Val Loss: 1.3110
Epoch 165, accuracy: 0.5880
Epoch 166, accuracy: 0.5880
Epoch 166, Train Loss: 0.4435, Val Loss: 1.3110
Epoch 167, accuracy: 0.5880
Epoch 168, accuracy: 0.5880
Epoch 168, Train Loss: 0.3926, Val Loss: 1.3110
Epoch 169, accuracy: 0.5880
Epoch 170, accuracy: 0.5880
Epoch 170, Train Loss: 0.4383, Val Loss: 1.3110
Epoch 171, accuracy: 0.5880
Epoch 172, accuracy: 0.5880
Epoch 172, Train Loss: 0.3861, Val Loss: 1.3110
Epoch 173, accuracy: 0.5880
Epoch 174, accuracy: 0.5880
Epoch 174, Train Loss: 0.4130, Val Loss: 1.3110
Epoch 175, accuracy: 0.5880
Epoch 176, accuracy: 0.5880
Epoch 176, Train Loss: 0.4641, Val Loss: 1.3110
Epoch 177, accuracy: 0.5880
Epoch 178, accuracy: 0.5880
Epoch 178, Train Loss: 0.4576, Val Loss: 1.3110
Epoch 179, accuracy: 0.5880
Epoch 180, accuracy: 0.5880
Epoch 180, Train Loss: 0.3830, Val Loss: 1.3110
Epoch 181, accuracy: 0.5880
Epoch 182, accuracy: 0.5880
Epoch 182, Train Loss: 0.4262, Val Loss: 1.3110
Epoch 183, accuracy: 0.5880
Epoch 184, accuracy: 0.5880
Epoch 184, Train Loss: 0.5217, Val Loss: 1.3110
Epoch 185, accuracy: 0.5880
Epoch 186, accuracy: 0.5880
Epoch 186, Train Loss: 0.3509, Val Loss: 1.3110
Epoch 187, accuracy: 0.5880
Epoch 188, accuracy: 0.5880
Epoch 188, Train Loss: 0.4463, Val Loss: 1.3110
Epoch 189, accuracy: 0.5880
Epoch 190, accuracy: 0.5880
Epoch 190, Train Loss: 0.4288, Val Loss: 1.3110
Epoch 191, accuracy: 0.5880
Epoch 192, accuracy: 0.5880
Epoch 192, Train Loss: 0.4833, Val Loss: 1.3110
Epoch 193, accuracy: 0.5880
Epoch 194, accuracy: 0.5880
Epoch 194, Train Loss: 0.4687, Val Loss: 1.3110
Epoch 195, accuracy: 0.5880
Epoch 196, accuracy: 0.5880
Epoch 196, Train Loss: 0.4351, Val Loss: 1.3110
Epoch 197, accuracy: 0.5880
Epoch 198, accuracy: 0.5880
Epoch 198, Train Loss: 0.4537, Val Loss: 1.3110
Epoch 199, accuracy: 0.5880
Loaded best model with val_loss = 1.144229769706726
test :accuracy 0.5960, f1_macro: 0.5502, f1_micro: 0.5960, auc: 0.8419
Training mamba3 with 32 layers...
可训练参数: 3974078_mamba3
不可训练参数: 0
✅ Epoch 0: New best model saved with val_loss = 1.7583
Epoch 0, accuracy: 0.3710
Epoch 0, Train Loss: 16878.6660, Val Loss: 1.7583
Epoch 1, accuracy: 0.2760
Epoch 2, accuracy: 0.2620
Epoch 2, Train Loss: 479.2129, Val Loss: 1.7967
✅ Epoch 3: New best model saved with val_loss = 1.5747
Epoch 3, accuracy: 0.3950
✅ Epoch 4: New best model saved with val_loss = 1.4582
Epoch 4, accuracy: 0.4940
Epoch 4, Train Loss: 1.2745, Val Loss: 1.4582
✅ Epoch 5: New best model saved with val_loss = 1.2887
Epoch 5, accuracy: 0.5930
✅ Epoch 6: New best model saved with val_loss = 1.1711
Epoch 6, accuracy: 0.6260
Epoch 6, Train Loss: 1.0167, Val Loss: 1.1711
✅ Epoch 7: New best model saved with val_loss = 1.1345
Epoch 7, accuracy: 0.6190
Epoch 8, accuracy: 0.6030
Epoch 8, Train Loss: 0.8566, Val Loss: 1.1545
Epoch 9, accuracy: 0.5890
Epoch 10, accuracy: 0.5870
Epoch 10, Train Loss: 0.8878, Val Loss: 1.2288
Epoch 11, accuracy: 0.5850
Epoch 12, accuracy: 0.5950
Epoch 12, Train Loss: 0.7661, Val Loss: 1.2528
Epoch 13, accuracy: 0.6100
Epoch 14, accuracy: 0.6110
Epoch 14, Train Loss: 0.5759, Val Loss: 1.2433
Epoch 15, accuracy: 0.6120
Epoch 16, accuracy: 0.6210
Epoch 16, Train Loss: 0.5334, Val Loss: 1.2390
Epoch 17, accuracy: 0.6230
Epoch 18, accuracy: 0.6260
Epoch 18, Train Loss: 0.5796, Val Loss: 1.2354
Epoch 19, accuracy: 0.6280
Epoch 20, accuracy: 0.6270
Epoch 20, Train Loss: 0.6265, Val Loss: 1.2338
Epoch 21, accuracy: 0.6270
Epoch 22, accuracy: 0.6260
Epoch 22, Train Loss: 0.5487, Val Loss: 1.2335
Epoch 23, accuracy: 0.6260
Epoch 24, accuracy: 0.6260
Epoch 24, Train Loss: 0.5920, Val Loss: 1.2331
Epoch 25, accuracy: 0.6240
Epoch 26, accuracy: 0.6240
Epoch 26, Train Loss: 0.6217, Val Loss: 1.2329
Epoch 27, accuracy: 0.6250
Epoch 28, accuracy: 0.6250
Epoch 28, Train Loss: 0.6244, Val Loss: 1.2328
Epoch 29, accuracy: 0.6250
Epoch 30, accuracy: 0.6250
Epoch 30, Train Loss: 0.6312, Val Loss: 1.2327
Epoch 31, accuracy: 0.6250
Epoch 32, accuracy: 0.6250
Epoch 32, Train Loss: 0.6410, Val Loss: 1.2327
Epoch 33, accuracy: 0.6250
Epoch 34, accuracy: 0.6250
Epoch 34, Train Loss: 0.5118, Val Loss: 1.2327
Epoch 35, accuracy: 0.6250
Epoch 36, accuracy: 0.6250
Epoch 36, Train Loss: 0.5954, Val Loss: 1.2327
Epoch 37, accuracy: 0.6250
Epoch 38, accuracy: 0.6250
Epoch 38, Train Loss: 0.5548, Val Loss: 1.2327
Epoch 39, accuracy: 0.6250
Epoch 40, accuracy: 0.6250
Epoch 40, Train Loss: 0.5400, Val Loss: 1.2327
Epoch 41, accuracy: 0.6250
Epoch 42, accuracy: 0.6250
Epoch 42, Train Loss: 0.5743, Val Loss: 1.2327
Epoch 43, accuracy: 0.6250
Epoch 44, accuracy: 0.6250
Epoch 44, Train Loss: 0.5780, Val Loss: 1.2327
Epoch 45, accuracy: 0.6250
Epoch 46, accuracy: 0.6250
Epoch 46, Train Loss: 0.5282, Val Loss: 1.2327
Epoch 47, accuracy: 0.6250
Epoch 48, accuracy: 0.6250
Epoch 48, Train Loss: 0.6551, Val Loss: 1.2327
Epoch 49, accuracy: 0.6250
Epoch 50, accuracy: 0.6250
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
/root/miniconda3/lib/python3.12/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling
  warnings.warn(f"Using '{self.__class__.__name__}' without a "
Epoch 50, Train Loss: 0.6247, Val Loss: 1.2327
Epoch 51, accuracy: 0.6250
Epoch 52, accuracy: 0.6250
Epoch 52, Train Loss: 0.5755, Val Loss: 1.2327
Epoch 53, accuracy: 0.6250
Epoch 54, accuracy: 0.6250
Epoch 54, Train Loss: 0.5101, Val Loss: 1.2327
Epoch 55, accuracy: 0.6250
Epoch 56, accuracy: 0.6250
Epoch 56, Train Loss: 0.5842, Val Loss: 1.2327
Epoch 57, accuracy: 0.6250
Epoch 58, accuracy: 0.6250
Epoch 58, Train Loss: 0.5516, Val Loss: 1.2327
Epoch 59, accuracy: 0.6250
Epoch 60, accuracy: 0.6250
Epoch 60, Train Loss: 0.4841, Val Loss: 1.2327
Epoch 61, accuracy: 0.6250
Epoch 62, accuracy: 0.6250
Epoch 62, Train Loss: 0.6377, Val Loss: 1.2327
Epoch 63, accuracy: 0.6250
Epoch 64, accuracy: 0.6250
Epoch 64, Train Loss: 0.6081, Val Loss: 1.2327
Epoch 65, accuracy: 0.6250
Epoch 66, accuracy: 0.6250
Epoch 66, Train Loss: 0.5455, Val Loss: 1.2327
Epoch 67, accuracy: 0.6250
Epoch 68, accuracy: 0.6250
Epoch 68, Train Loss: 0.6624, Val Loss: 1.2327
Epoch 69, accuracy: 0.6250
Epoch 70, accuracy: 0.6250
Epoch 70, Train Loss: 0.6558, Val Loss: 1.2327
Epoch 71, accuracy: 0.6250
Epoch 72, accuracy: 0.6250
Epoch 72, Train Loss: 0.5877, Val Loss: 1.2327
Epoch 73, accuracy: 0.6250
Epoch 74, accuracy: 0.6250
Epoch 74, Train Loss: 0.7007, Val Loss: 1.2327
Epoch 75, accuracy: 0.6250
Epoch 76, accuracy: 0.6250
Epoch 76, Train Loss: 0.5464, Val Loss: 1.2327
Epoch 77, accuracy: 0.6250
Epoch 78, accuracy: 0.6250
Epoch 78, Train Loss: 0.5135, Val Loss: 1.2327
Epoch 79, accuracy: 0.6250
Epoch 80, accuracy: 0.6250
Epoch 80, Train Loss: 0.5727, Val Loss: 1.2327
Epoch 81, accuracy: 0.6250
Epoch 82, accuracy: 0.6250
Epoch 82, Train Loss: 0.5899, Val Loss: 1.2327
Epoch 83, accuracy: 0.6250
Epoch 84, accuracy: 0.6250
Epoch 84, Train Loss: 0.6725, Val Loss: 1.2327
Epoch 85, accuracy: 0.6250
Epoch 86, accuracy: 0.6250
Epoch 86, Train Loss: 0.5751, Val Loss: 1.2327
Epoch 87, accuracy: 0.6250
Epoch 88, accuracy: 0.6250
Epoch 88, Train Loss: 0.6321, Val Loss: 1.2327
Epoch 89, accuracy: 0.6250
Epoch 90, accuracy: 0.6250
Epoch 90, Train Loss: 0.6009, Val Loss: 1.2327
Epoch 91, accuracy: 0.6250
Epoch 92, accuracy: 0.6250
Epoch 92, Train Loss: 0.6695, Val Loss: 1.2327
Epoch 93, accuracy: 0.6250
Epoch 94, accuracy: 0.6250
Epoch 94, Train Loss: 0.6481, Val Loss: 1.2327
Epoch 95, accuracy: 0.6250
Epoch 96, accuracy: 0.6250
Epoch 96, Train Loss: 0.6356, Val Loss: 1.2327
Epoch 97, accuracy: 0.6250
Epoch 98, accuracy: 0.6250
Epoch 98, Train Loss: 0.5767, Val Loss: 1.2327
Epoch 99, accuracy: 0.6250
Epoch 100, accuracy: 0.6250
Epoch 100, Train Loss: 0.5796, Val Loss: 1.2327
Epoch 101, accuracy: 0.6250
Epoch 102, accuracy: 0.6250
Epoch 102, Train Loss: 0.5849, Val Loss: 1.2327
Epoch 103, accuracy: 0.6250
Epoch 104, accuracy: 0.6250
Epoch 104, Train Loss: 0.6002, Val Loss: 1.2327
Epoch 105, accuracy: 0.6250
Epoch 106, accuracy: 0.6250
Epoch 106, Train Loss: 0.6403, Val Loss: 1.2327
Epoch 107, accuracy: 0.6250
Epoch 108, accuracy: 0.6250
Epoch 108, Train Loss: 0.5219, Val Loss: 1.2327
Epoch 109, accuracy: 0.6250
Epoch 110, accuracy: 0.6250
Epoch 110, Train Loss: 0.5494, Val Loss: 1.2327
Epoch 111, accuracy: 0.6250
Epoch 112, accuracy: 0.6250
Epoch 112, Train Loss: 0.6541, Val Loss: 1.2327
Epoch 113, accuracy: 0.6250
Epoch 114, accuracy: 0.6250
Epoch 114, Train Loss: 0.6091, Val Loss: 1.2327
Epoch 115, accuracy: 0.6250
Epoch 116, accuracy: 0.6250
Epoch 116, Train Loss: 0.7167, Val Loss: 1.2327
Epoch 117, accuracy: 0.6250
Epoch 118, accuracy: 0.6250
Epoch 118, Train Loss: 0.6008, Val Loss: 1.2327
Epoch 119, accuracy: 0.6250
Epoch 120, accuracy: 0.6250
Epoch 120, Train Loss: 0.5068, Val Loss: 1.2327
Epoch 121, accuracy: 0.6250
Epoch 122, accuracy: 0.6250
Epoch 122, Train Loss: 0.5949, Val Loss: 1.2327
Epoch 123, accuracy: 0.6250
Epoch 124, accuracy: 0.6250
Epoch 124, Train Loss: 0.6781, Val Loss: 1.2327
Epoch 125, accuracy: 0.6250
Epoch 126, accuracy: 0.6250
Epoch 126, Train Loss: 0.5205, Val Loss: 1.2327
Epoch 127, accuracy: 0.6250
Epoch 128, accuracy: 0.6250
Epoch 128, Train Loss: 0.6324, Val Loss: 1.2327
Epoch 129, accuracy: 0.6250
Epoch 130, accuracy: 0.6250
Epoch 130, Train Loss: 0.5947, Val Loss: 1.2327
Epoch 131, accuracy: 0.6250
Epoch 132, accuracy: 0.6250
Epoch 132, Train Loss: 0.6463, Val Loss: 1.2327
Epoch 133, accuracy: 0.6250
Epoch 134, accuracy: 0.6250
Epoch 134, Train Loss: 0.5130, Val Loss: 1.2327
Epoch 135, accuracy: 0.6250
Epoch 136, accuracy: 0.6250
Epoch 136, Train Loss: 0.6149, Val Loss: 1.2327
Epoch 137, accuracy: 0.6250
Epoch 138, accuracy: 0.6250
Epoch 138, Train Loss: 0.5922, Val Loss: 1.2327
Epoch 139, accuracy: 0.6250
Epoch 140, accuracy: 0.6250
Epoch 140, Train Loss: 0.5228, Val Loss: 1.2327
Epoch 141, accuracy: 0.6250
Epoch 142, accuracy: 0.6250
Epoch 142, Train Loss: 0.6400, Val Loss: 1.2327
Epoch 143, accuracy: 0.6250
Epoch 144, accuracy: 0.6250
Epoch 144, Train Loss: 0.6070, Val Loss: 1.2327
Epoch 145, accuracy: 0.6250
Epoch 146, accuracy: 0.6250
Epoch 146, Train Loss: 0.5725, Val Loss: 1.2327
Epoch 147, accuracy: 0.6250
Epoch 148, accuracy: 0.6250
Epoch 148, Train Loss: 0.6429, Val Loss: 1.2327
Epoch 149, accuracy: 0.6250
Epoch 150, accuracy: 0.6250
Epoch 150, Train Loss: 0.5845, Val Loss: 1.2327
Epoch 151, accuracy: 0.6250
Epoch 152, accuracy: 0.6250
Epoch 152, Train Loss: 0.5387, Val Loss: 1.2327
Epoch 153, accuracy: 0.6250
Epoch 154, accuracy: 0.6250
Epoch 154, Train Loss: 0.5438, Val Loss: 1.2327
Epoch 155, accuracy: 0.6250
Epoch 156, accuracy: 0.6250
Epoch 156, Train Loss: 0.6641, Val Loss: 1.2327
Epoch 157, accuracy: 0.6250
Epoch 158, accuracy: 0.6250
Epoch 158, Train Loss: 0.5698, Val Loss: 1.2327
Epoch 159, accuracy: 0.6250
Epoch 160, accuracy: 0.6250
Epoch 160, Train Loss: 0.6609, Val Loss: 1.2327
Epoch 161, accuracy: 0.6250
Epoch 162, accuracy: 0.6250
Epoch 162, Train Loss: 0.6443, Val Loss: 1.2327
Epoch 163, accuracy: 0.6250
Epoch 164, accuracy: 0.6250
Epoch 164, Train Loss: 0.5975, Val Loss: 1.2327
Epoch 165, accuracy: 0.6250
Epoch 166, accuracy: 0.6250
Epoch 166, Train Loss: 0.6702, Val Loss: 1.2327
Epoch 167, accuracy: 0.6250
Epoch 168, accuracy: 0.6250
Epoch 168, Train Loss: 0.5242, Val Loss: 1.2327
Epoch 169, accuracy: 0.6250
Epoch 170, accuracy: 0.6250
Epoch 170, Train Loss: 0.6699, Val Loss: 1.2327
Epoch 171, accuracy: 0.6250
Epoch 172, accuracy: 0.6250
Epoch 172, Train Loss: 0.6215, Val Loss: 1.2327
Epoch 173, accuracy: 0.6250
Epoch 174, accuracy: 0.6250
Epoch 174, Train Loss: 0.6534, Val Loss: 1.2327
Epoch 175, accuracy: 0.6250
Epoch 176, accuracy: 0.6250
Epoch 176, Train Loss: 0.6773, Val Loss: 1.2327
Epoch 177, accuracy: 0.6250
Epoch 178, accuracy: 0.6250
Epoch 178, Train Loss: 0.5144, Val Loss: 1.2327
Epoch 179, accuracy: 0.6250
Epoch 180, accuracy: 0.6250
Epoch 180, Train Loss: 0.4782, Val Loss: 1.2327
Epoch 181, accuracy: 0.6250
Epoch 182, accuracy: 0.6250
Epoch 182, Train Loss: 0.6182, Val Loss: 1.2327
Epoch 183, accuracy: 0.6250
Epoch 184, accuracy: 0.6250
Epoch 184, Train Loss: 0.6186, Val Loss: 1.2327
Epoch 185, accuracy: 0.6250
Epoch 186, accuracy: 0.6250
Epoch 186, Train Loss: 0.6017, Val Loss: 1.2327
Epoch 187, accuracy: 0.6250
Epoch 188, accuracy: 0.6250
Epoch 188, Train Loss: 0.6170, Val Loss: 1.2327
Epoch 189, accuracy: 0.6250
Epoch 190, accuracy: 0.6250
Epoch 190, Train Loss: 0.5860, Val Loss: 1.2327
Epoch 191, accuracy: 0.6250
Epoch 192, accuracy: 0.6250
Epoch 192, Train Loss: 0.5082, Val Loss: 1.2327
Epoch 193, accuracy: 0.6250
Epoch 194, accuracy: 0.6250
Epoch 194, Train Loss: 0.6129, Val Loss: 1.2327
Epoch 195, accuracy: 0.6250
Epoch 196, accuracy: 0.6250
Epoch 196, Train Loss: 0.5365, Val Loss: 1.2327
Epoch 197, accuracy: 0.6250
Epoch 198, accuracy: 0.6250
Epoch 198, Train Loss: 0.5877, Val Loss: 1.2327
Epoch 199, accuracy: 0.6250
Loaded best model with val_loss = 1.134495496749878
test :accuracy 0.6190, f1_macro: 0.5852, f1_micro: 0.6190, auc: 0.8551
Final Results: {'mamba3_2_Citeseer': np.float64(0.589), 'mamba3_8_Citeseer': np.float64(0.596), 'mamba3_32_Citeseer': np.float64(0.619)} ['1485758_mamba3_0', '1983422_mamba3_0', '3974078_mamba3_0']
========== Running baseline 2/3 ==========
Training mamba3 with 2 layers...
可训练参数: 1485758_mamba3
不可训练参数: 0
✅ Epoch 0: New best model saved with val_loss = 1.5435
Epoch 0, accuracy: 0.4050
Epoch 0, Train Loss: 1.9154, Val Loss: 1.5435
Epoch 1, accuracy: 0.1430
✅ Epoch 2: New best model saved with val_loss = 1.2484
Epoch 2, accuracy: 0.5340
Epoch 2, Train Loss: 1.6913, Val Loss: 1.2484
✅ Epoch 3: New best model saved with val_loss = 1.0685
Epoch 3, accuracy: 0.6280
✅ Epoch 4: New best model saved with val_loss = 1.0539
Epoch 4, accuracy: 0.6370
Epoch 4, Train Loss: 0.6594, Val Loss: 1.0539
Epoch 5, accuracy: 0.6400
Epoch 6, accuracy: 0.6510
Epoch 6, Train Loss: 0.6343, Val Loss: 1.1007
Epoch 7, accuracy: 0.6450
Epoch 8, accuracy: 0.6470
Epoch 8, Train Loss: 0.4388, Val Loss: 1.1293
Epoch 9, accuracy: 0.6530
Epoch 10, accuracy: 0.6580
Epoch 10, Train Loss: 0.4659, Val Loss: 1.1486
Epoch 11, accuracy: 0.6590
Epoch 12, accuracy: 0.6600
Epoch 12, Train Loss: 0.3565, Val Loss: 1.1538
Epoch 13, accuracy: 0.6610
Epoch 14, accuracy: 0.6600
Epoch 14, Train Loss: 0.3950, Val Loss: 1.1593
Epoch 15, accuracy: 0.6610
Epoch 16, accuracy: 0.6620
Epoch 16, Train Loss: 0.3506, Val Loss: 1.1652
Epoch 17, accuracy: 0.6620
Epoch 18, accuracy: 0.6610
Epoch 18, Train Loss: 0.3724, Val Loss: 1.1659
Epoch 19, accuracy: 0.6620
Epoch 20, accuracy: 0.6620
Epoch 20, Train Loss: 0.3322, Val Loss: 1.1666
Epoch 21, accuracy: 0.6610
Epoch 22, accuracy: 0.6630
Epoch 22, Train Loss: 0.3188, Val Loss: 1.1672
Epoch 23, accuracy: 0.6630
Epoch 24, accuracy: 0.6630
Epoch 24, Train Loss: 0.4241, Val Loss: 1.1672
Epoch 25, accuracy: 0.6630
Epoch 26, accuracy: 0.6630
Epoch 26, Train Loss: 0.3447, Val Loss: 1.1673
Epoch 27, accuracy: 0.6630
Epoch 28, accuracy: 0.6630
Epoch 28, Train Loss: 0.2999, Val Loss: 1.1673
Epoch 29, accuracy: 0.6630
Epoch 30, accuracy: 0.6630
Epoch 30, Train Loss: 0.3364, Val Loss: 1.1673
Epoch 31, accuracy: 0.6630
Epoch 32, accuracy: 0.6630
Epoch 32, Train Loss: 0.3766, Val Loss: 1.1673
Epoch 33, accuracy: 0.6630
Epoch 34, accuracy: 0.6630
Epoch 34, Train Loss: 0.3384, Val Loss: 1.1674
Epoch 35, accuracy: 0.6630
Epoch 36, accuracy: 0.6630
Epoch 36, Train Loss: 0.3123, Val Loss: 1.1674
Epoch 37, accuracy: 0.6630
Epoch 38, accuracy: 0.6630
Epoch 38, Train Loss: 0.3507, Val Loss: 1.1674
Epoch 39, accuracy: 0.6630
Epoch 40, accuracy: 0.6630
Epoch 40, Train Loss: 0.3547, Val Loss: 1.1674
Epoch 41, accuracy: 0.6630
Epoch 42, accuracy: 0.6630
Epoch 42, Train Loss: 0.3468, Val Loss: 1.1674
Epoch 43, accuracy: 0.6630
Epoch 44, accuracy: 0.6630
Epoch 44, Train Loss: 0.3244, Val Loss: 1.1674
Epoch 45, accuracy: 0.6630
Epoch 46, accuracy: 0.6630
Epoch 46, Train Loss: 0.3948, Val Loss: 1.1674
Epoch 47, accuracy: 0.6630
Epoch 48, accuracy: 0.6630
Epoch 48, Train Loss: 0.2987, Val Loss: 1.1674
Epoch 49, accuracy: 0.6630
Epoch 50, accuracy: 0.6630
Epoch 50, Train Loss: 0.3543, Val Loss: 1.1674
Epoch 51, accuracy: 0.6630
Epoch 52, accuracy: 0.6630
Epoch 52, Train Loss: 0.2769, Val Loss: 1.1674
Epoch 53, accuracy: 0.6630
Epoch 54, accuracy: 0.6630
Epoch 54, Train Loss: 0.2702, Val Loss: 1.1674
Epoch 55, accuracy: 0.6630
Epoch 56, accuracy: 0.6630
Epoch 56, Train Loss: 0.3588, Val Loss: 1.1674
Epoch 57, accuracy: 0.6630
Epoch 58, accuracy: 0.6630
Epoch 58, Train Loss: 0.3513, Val Loss: 1.1674
Epoch 59, accuracy: 0.6630
Epoch 60, accuracy: 0.6630
Epoch 60, Train Loss: 0.3386, Val Loss: 1.1674
Epoch 61, accuracy: 0.6630
Epoch 62, accuracy: 0.6630
Epoch 62, Train Loss: 0.3217, Val Loss: 1.1674
Epoch 63, accuracy: 0.6630
Epoch 64, accuracy: 0.6630
Epoch 64, Train Loss: 0.3398, Val Loss: 1.1674
Epoch 65, accuracy: 0.6630
Epoch 66, accuracy: 0.6630
Epoch 66, Train Loss: 0.2823, Val Loss: 1.1674
Epoch 67, accuracy: 0.6630
Epoch 68, accuracy: 0.6630
Epoch 68, Train Loss: 0.3184, Val Loss: 1.1674
Epoch 69, accuracy: 0.6630
Epoch 70, accuracy: 0.6630
Epoch 70, Train Loss: 0.3559, Val Loss: 1.1674
Epoch 71, accuracy: 0.6630
Epoch 72, accuracy: 0.6630
Epoch 72, Train Loss: 0.3394, Val Loss: 1.1674
Epoch 73, accuracy: 0.6630
Epoch 74, accuracy: 0.6630
Epoch 74, Train Loss: 0.3861, Val Loss: 1.1674
Epoch 75, accuracy: 0.6630
Epoch 76, accuracy: 0.6630
Epoch 76, Train Loss: 0.3985, Val Loss: 1.1674
Epoch 77, accuracy: 0.6630
Epoch 78, accuracy: 0.6630
Epoch 78, Train Loss: 0.3432, Val Loss: 1.1674
Epoch 79, accuracy: 0.6630
Epoch 80, accuracy: 0.6630
Epoch 80, Train Loss: 0.3577, Val Loss: 1.1674
Epoch 81, accuracy: 0.6630
Epoch 82, accuracy: 0.6630
Epoch 82, Train Loss: 0.3639, Val Loss: 1.1674
Epoch 83, accuracy: 0.6630
Epoch 84, accuracy: 0.6630
Epoch 84, Train Loss: 0.4178, Val Loss: 1.1674
Epoch 85, accuracy: 0.6630
Epoch 86, accuracy: 0.6630
Epoch 86, Train Loss: 0.2794, Val Loss: 1.1674
Epoch 87, accuracy: 0.6630
Epoch 88, accuracy: 0.6630
Epoch 88, Train Loss: 0.3008, Val Loss: 1.1674
Epoch 89, accuracy: 0.6630
Epoch 90, accuracy: 0.6630
Epoch 90, Train Loss: 0.2971, Val Loss: 1.1674
Epoch 91, accuracy: 0.6630
Epoch 92, accuracy: 0.6630
Epoch 92, Train Loss: 0.2701, Val Loss: 1.1674
Epoch 93, accuracy: 0.6630
Epoch 94, accuracy: 0.6630
Epoch 94, Train Loss: 0.2690, Val Loss: 1.1674
Epoch 95, accuracy: 0.6630
Epoch 96, accuracy: 0.6630
Epoch 96, Train Loss: 0.3248, Val Loss: 1.1674
Epoch 97, accuracy: 0.6630
Epoch 98, accuracy: 0.6630
Epoch 98, Train Loss: 0.3066, Val Loss: 1.1674
Epoch 99, accuracy: 0.6630
Epoch 100, accuracy: 0.6630
Epoch 100, Train Loss: 0.3617, Val Loss: 1.1674
Epoch 101, accuracy: 0.6630
Epoch 102, accuracy: 0.6630
Epoch 102, Train Loss: 0.3309, Val Loss: 1.1674
Epoch 103, accuracy: 0.6630
Epoch 104, accuracy: 0.6630
Epoch 104, Train Loss: 0.3135, Val Loss: 1.1674
Epoch 105, accuracy: 0.6630
Epoch 106, accuracy: 0.6630
Epoch 106, Train Loss: 0.3481, Val Loss: 1.1674
Epoch 107, accuracy: 0.6630
Epoch 108, accuracy: 0.6630
Epoch 108, Train Loss: 0.2746, Val Loss: 1.1674
Epoch 109, accuracy: 0.6630
Epoch 110, accuracy: 0.6630
Epoch 110, Train Loss: 0.3409, Val Loss: 1.1674
Epoch 111, accuracy: 0.6630
Epoch 112, accuracy: 0.6630
Epoch 112, Train Loss: 0.3093, Val Loss: 1.1674
Epoch 113, accuracy: 0.6630
Epoch 114, accuracy: 0.6630
Epoch 114, Train Loss: 0.3624, Val Loss: 1.1674
Epoch 115, accuracy: 0.6630
Epoch 116, accuracy: 0.6630
Epoch 116, Train Loss: 0.3411, Val Loss: 1.1674
Epoch 117, accuracy: 0.6630
Epoch 118, accuracy: 0.6630
Epoch 118, Train Loss: 0.3624, Val Loss: 1.1674
Epoch 119, accuracy: 0.6630
Epoch 120, accuracy: 0.6630
Epoch 120, Train Loss: 0.3560, Val Loss: 1.1674
Epoch 121, accuracy: 0.6630
Epoch 122, accuracy: 0.6630
Epoch 122, Train Loss: 0.2787, Val Loss: 1.1674
Epoch 123, accuracy: 0.6630
Epoch 124, accuracy: 0.6630
Epoch 124, Train Loss: 0.3367, Val Loss: 1.1674
Epoch 125, accuracy: 0.6630
Epoch 126, accuracy: 0.6630
Epoch 126, Train Loss: 0.3339, Val Loss: 1.1674
Epoch 127, accuracy: 0.6630
Epoch 128, accuracy: 0.6630
Epoch 128, Train Loss: 0.3710, Val Loss: 1.1674
Epoch 129, accuracy: 0.6630
Epoch 130, accuracy: 0.6630
Epoch 130, Train Loss: 0.3563, Val Loss: 1.1674
Epoch 131, accuracy: 0.6630
Epoch 132, accuracy: 0.6630
Epoch 132, Train Loss: 0.3974, Val Loss: 1.1674
Epoch 133, accuracy: 0.6630
Epoch 134, accuracy: 0.6630
Epoch 134, Train Loss: 0.4120, Val Loss: 1.1674
Epoch 135, accuracy: 0.6630
Epoch 136, accuracy: 0.6630
Epoch 136, Train Loss: 0.3925, Val Loss: 1.1674
Epoch 137, accuracy: 0.6630
Epoch 138, accuracy: 0.6630
Epoch 138, Train Loss: 0.3045, Val Loss: 1.1674
Epoch 139, accuracy: 0.6630
Epoch 140, accuracy: 0.6630
Epoch 140, Train Loss: 0.3369, Val Loss: 1.1674
Epoch 141, accuracy: 0.6630
Epoch 142, accuracy: 0.6630
Epoch 142, Train Loss: 0.3644, Val Loss: 1.1674
Epoch 143, accuracy: 0.6630
Epoch 144, accuracy: 0.6630
Epoch 144, Train Loss: 0.2907, Val Loss: 1.1674
Epoch 145, accuracy: 0.6630
Epoch 146, accuracy: 0.6630
Epoch 146, Train Loss: 0.3431, Val Loss: 1.1674
Epoch 147, accuracy: 0.6630
Epoch 148, accuracy: 0.6630
Epoch 148, Train Loss: 0.3090, Val Loss: 1.1674
Epoch 149, accuracy: 0.6630
Epoch 150, accuracy: 0.6630
Epoch 150, Train Loss: 0.3420, Val Loss: 1.1674
Epoch 151, accuracy: 0.6630
Epoch 152, accuracy: 0.6630
Epoch 152, Train Loss: 0.3220, Val Loss: 1.1674
Epoch 153, accuracy: 0.6630
Epoch 154, accuracy: 0.6630
Epoch 154, Train Loss: 0.3050, Val Loss: 1.1674
Epoch 155, accuracy: 0.6630
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 156, accuracy: 0.6630
Epoch 156, Train Loss: 0.3937, Val Loss: 1.1674
Epoch 157, accuracy: 0.6630
Epoch 158, accuracy: 0.6630
Epoch 158, Train Loss: 0.2777, Val Loss: 1.1674
Epoch 159, accuracy: 0.6630
Epoch 160, accuracy: 0.6630
Epoch 160, Train Loss: 0.2906, Val Loss: 1.1674
Epoch 161, accuracy: 0.6630
Epoch 162, accuracy: 0.6630
Epoch 162, Train Loss: 0.3413, Val Loss: 1.1674
Epoch 163, accuracy: 0.6630
Epoch 164, accuracy: 0.6630
Epoch 164, Train Loss: 0.3272, Val Loss: 1.1674
Epoch 165, accuracy: 0.6630
Epoch 166, accuracy: 0.6630
Epoch 166, Train Loss: 0.3739, Val Loss: 1.1674
Epoch 167, accuracy: 0.6630
Epoch 168, accuracy: 0.6630
Epoch 168, Train Loss: 0.3988, Val Loss: 1.1674
Epoch 169, accuracy: 0.6630
Epoch 170, accuracy: 0.6630
Epoch 170, Train Loss: 0.3113, Val Loss: 1.1674
Epoch 171, accuracy: 0.6630
Epoch 172, accuracy: 0.6630
Epoch 172, Train Loss: 0.4046, Val Loss: 1.1674
Epoch 173, accuracy: 0.6630
Epoch 174, accuracy: 0.6630
Epoch 174, Train Loss: 0.2799, Val Loss: 1.1674
Epoch 175, accuracy: 0.6630
Epoch 176, accuracy: 0.6630
Epoch 176, Train Loss: 0.2710, Val Loss: 1.1674
Epoch 177, accuracy: 0.6630
Epoch 178, accuracy: 0.6630
Epoch 178, Train Loss: 0.2990, Val Loss: 1.1674
Epoch 179, accuracy: 0.6630
Epoch 180, accuracy: 0.6630
Epoch 180, Train Loss: 0.3523, Val Loss: 1.1674
Epoch 181, accuracy: 0.6630
Epoch 182, accuracy: 0.6630
Epoch 182, Train Loss: 0.3069, Val Loss: 1.1674
Epoch 183, accuracy: 0.6630
Epoch 184, accuracy: 0.6630
Epoch 184, Train Loss: 0.3890, Val Loss: 1.1674
Epoch 185, accuracy: 0.6630
Epoch 186, accuracy: 0.6630
Epoch 186, Train Loss: 0.3535, Val Loss: 1.1674
Epoch 187, accuracy: 0.6630
Epoch 188, accuracy: 0.6630
Epoch 188, Train Loss: 0.3488, Val Loss: 1.1674
Epoch 189, accuracy: 0.6630
Epoch 190, accuracy: 0.6630
Epoch 190, Train Loss: 0.2885, Val Loss: 1.1674
Epoch 191, accuracy: 0.6630
Epoch 192, accuracy: 0.6630
Epoch 192, Train Loss: 0.3402, Val Loss: 1.1674
Epoch 193, accuracy: 0.6630
Epoch 194, accuracy: 0.6630
Epoch 194, Train Loss: 0.4058, Val Loss: 1.1674
Epoch 195, accuracy: 0.6630
Epoch 196, accuracy: 0.6630
Epoch 196, Train Loss: 0.3889, Val Loss: 1.1674
Epoch 197, accuracy: 0.6630
Epoch 198, accuracy: 0.6630
Epoch 198, Train Loss: 0.3021, Val Loss: 1.1674
Epoch 199, accuracy: 0.6630
Loaded best model with val_loss = 1.053856372833252
test :accuracy 0.6370, f1_macro: 0.5843, f1_micro: 0.6370, auc: 0.8707
Training mamba3 with 8 layers...
可训练参数: 1983422_mamba3
不可训练参数: 0
✅ Epoch 0: New best model saved with val_loss = 1.4706
Epoch 0, accuracy: 0.5300
Epoch 0, Train Loss: 1.8361, Val Loss: 1.4706
✅ Epoch 1: New best model saved with val_loss = 1.2612
Epoch 1, accuracy: 0.5900
✅ Epoch 2: New best model saved with val_loss = 1.1296
Epoch 2, accuracy: 0.6310
Epoch 2, Train Loss: 1.0363, Val Loss: 1.1296
Epoch 3, accuracy: 0.6240
✅ Epoch 4: New best model saved with val_loss = 1.1201
Epoch 4, accuracy: 0.6460
Epoch 4, Train Loss: 0.6793, Val Loss: 1.1201
Epoch 5, accuracy: 0.6460
Epoch 6, accuracy: 0.6490
Epoch 6, Train Loss: 0.4769, Val Loss: 1.1979
Epoch 7, accuracy: 0.6230
Epoch 8, accuracy: 0.6060
Epoch 8, Train Loss: 0.5489, Val Loss: 1.3912
Epoch 9, accuracy: 0.5930
Epoch 10, accuracy: 0.6090
Epoch 10, Train Loss: 0.4564, Val Loss: 1.4467
Epoch 11, accuracy: 0.6110
Epoch 12, accuracy: 0.6140
Epoch 12, Train Loss: 0.4317, Val Loss: 1.4370
Epoch 13, accuracy: 0.6220
Epoch 14, accuracy: 0.6310
Epoch 14, Train Loss: 0.3729, Val Loss: 1.4190
Epoch 15, accuracy: 0.6330
Epoch 16, accuracy: 0.6370
Epoch 16, Train Loss: 0.4692, Val Loss: 1.4066
Epoch 17, accuracy: 0.6360
Epoch 18, accuracy: 0.6360
Epoch 18, Train Loss: 0.4864, Val Loss: 1.4059
Epoch 19, accuracy: 0.6360
Epoch 20, accuracy: 0.6360
Epoch 20, Train Loss: 0.4448, Val Loss: 1.4053
Epoch 21, accuracy: 0.6360
Epoch 22, accuracy: 0.6360
Epoch 22, Train Loss: 0.3778, Val Loss: 1.4045
Epoch 23, accuracy: 0.6360
Epoch 24, accuracy: 0.6360
Epoch 24, Train Loss: 0.4745, Val Loss: 1.4044
Epoch 25, accuracy: 0.6360
Epoch 26, accuracy: 0.6360
Epoch 26, Train Loss: 0.5138, Val Loss: 1.4044
Epoch 27, accuracy: 0.6360
Epoch 28, accuracy: 0.6360
Epoch 28, Train Loss: 0.3792, Val Loss: 1.4043
Epoch 29, accuracy: 0.6360
Epoch 30, accuracy: 0.6360
Epoch 30, Train Loss: 0.5667, Val Loss: 1.4043
Epoch 31, accuracy: 0.6360
Epoch 32, accuracy: 0.6360
Epoch 32, Train Loss: 0.4692, Val Loss: 1.4043
Epoch 33, accuracy: 0.6360
Epoch 34, accuracy: 0.6360
Epoch 34, Train Loss: 0.4488, Val Loss: 1.4043
Epoch 35, accuracy: 0.6360
Epoch 36, accuracy: 0.6360
Epoch 36, Train Loss: 0.4834, Val Loss: 1.4043
Epoch 37, accuracy: 0.6360
Epoch 38, accuracy: 0.6360
Epoch 38, Train Loss: 0.3178, Val Loss: 1.4043
Epoch 39, accuracy: 0.6360
Epoch 40, accuracy: 0.6360
Epoch 40, Train Loss: 0.4257, Val Loss: 1.4043
Epoch 41, accuracy: 0.6360
Epoch 42, accuracy: 0.6360
Epoch 42, Train Loss: 0.5046, Val Loss: 1.4043
Epoch 43, accuracy: 0.6360
Epoch 44, accuracy: 0.6360
Epoch 44, Train Loss: 0.4065, Val Loss: 1.4043
Epoch 45, accuracy: 0.6360
Epoch 46, accuracy: 0.6360
Epoch 46, Train Loss: 0.5913, Val Loss: 1.4043
Epoch 47, accuracy: 0.6360
Epoch 48, accuracy: 0.6360
Epoch 48, Train Loss: 0.4514, Val Loss: 1.4043
Epoch 49, accuracy: 0.6360
Epoch 50, accuracy: 0.6360
Epoch 50, Train Loss: 0.4394, Val Loss: 1.4043
Epoch 51, accuracy: 0.6360
Epoch 52, accuracy: 0.6360
Epoch 52, Train Loss: 0.5045, Val Loss: 1.4043
Epoch 53, accuracy: 0.6360
Epoch 54, accuracy: 0.6360
Epoch 54, Train Loss: 0.4880, Val Loss: 1.4043
Epoch 55, accuracy: 0.6360
Epoch 56, accuracy: 0.6360
Epoch 56, Train Loss: 0.3803, Val Loss: 1.4043
Epoch 57, accuracy: 0.6360
Epoch 58, accuracy: 0.6360
Epoch 58, Train Loss: 0.4284, Val Loss: 1.4043
Epoch 59, accuracy: 0.6360
Epoch 60, accuracy: 0.6360
Epoch 60, Train Loss: 0.4535, Val Loss: 1.4043
Epoch 61, accuracy: 0.6360
Epoch 62, accuracy: 0.6360
Epoch 62, Train Loss: 0.4035, Val Loss: 1.4043
Epoch 63, accuracy: 0.6360
Epoch 64, accuracy: 0.6360
Epoch 64, Train Loss: 0.3390, Val Loss: 1.4043
Epoch 65, accuracy: 0.6360
Epoch 66, accuracy: 0.6360
Epoch 66, Train Loss: 0.4451, Val Loss: 1.4043
Epoch 67, accuracy: 0.6360
Epoch 68, accuracy: 0.6360
Epoch 68, Train Loss: 0.4060, Val Loss: 1.4043
Epoch 69, accuracy: 0.6360
Epoch 70, accuracy: 0.6360
Epoch 70, Train Loss: 0.4466, Val Loss: 1.4043
Epoch 71, accuracy: 0.6360
Epoch 72, accuracy: 0.6360
Epoch 72, Train Loss: 0.4549, Val Loss: 1.4043
Epoch 73, accuracy: 0.6360
Epoch 74, accuracy: 0.6360
Epoch 74, Train Loss: 0.3846, Val Loss: 1.4043
Epoch 75, accuracy: 0.6360
Epoch 76, accuracy: 0.6360
Epoch 76, Train Loss: 0.3690, Val Loss: 1.4043
Epoch 77, accuracy: 0.6360
Epoch 78, accuracy: 0.6360
Epoch 78, Train Loss: 0.4805, Val Loss: 1.4043
Epoch 79, accuracy: 0.6360
Epoch 80, accuracy: 0.6360
Epoch 80, Train Loss: 0.4421, Val Loss: 1.4043
Epoch 81, accuracy: 0.6360
Epoch 82, accuracy: 0.6360
Epoch 82, Train Loss: 0.4425, Val Loss: 1.4043
Epoch 83, accuracy: 0.6360
Epoch 84, accuracy: 0.6360
Epoch 84, Train Loss: 0.4547, Val Loss: 1.4043
Epoch 85, accuracy: 0.6360
Epoch 86, accuracy: 0.6360
Epoch 86, Train Loss: 0.5690, Val Loss: 1.4043
Epoch 87, accuracy: 0.6360
Epoch 88, accuracy: 0.6360
Epoch 88, Train Loss: 0.3628, Val Loss: 1.4043
Epoch 89, accuracy: 0.6360
Epoch 90, accuracy: 0.6360
Epoch 90, Train Loss: 0.3608, Val Loss: 1.4043
Epoch 91, accuracy: 0.6360
Epoch 92, accuracy: 0.6360
Epoch 92, Train Loss: 0.3809, Val Loss: 1.4043
Epoch 93, accuracy: 0.6360
Epoch 94, accuracy: 0.6360
Epoch 94, Train Loss: 0.4139, Val Loss: 1.4043
Epoch 95, accuracy: 0.6360
Epoch 96, accuracy: 0.6360
Epoch 96, Train Loss: 0.4581, Val Loss: 1.4043
Epoch 97, accuracy: 0.6360
Epoch 98, accuracy: 0.6360
Epoch 98, Train Loss: 0.4780, Val Loss: 1.4043
Epoch 99, accuracy: 0.6360
Epoch 100, accuracy: 0.6360
Epoch 100, Train Loss: 0.4956, Val Loss: 1.4043
Epoch 101, accuracy: 0.6360
Epoch 102, accuracy: 0.6360
Epoch 102, Train Loss: 0.4767, Val Loss: 1.4043
Epoch 103, accuracy: 0.6360
Epoch 104, accuracy: 0.6360
Epoch 104, Train Loss: 0.4974, Val Loss: 1.4043
Epoch 105, accuracy: 0.6360
Epoch 106, accuracy: 0.6360
Epoch 106, Train Loss: 0.4205, Val Loss: 1.4044
Epoch 107, accuracy: 0.6360
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 108, accuracy: 0.6360
Epoch 108, Train Loss: 0.4403, Val Loss: 1.4043
Epoch 109, accuracy: 0.6360
Epoch 110, accuracy: 0.6360
Epoch 110, Train Loss: 0.4384, Val Loss: 1.4043
Epoch 111, accuracy: 0.6360
Epoch 112, accuracy: 0.6360
Epoch 112, Train Loss: 0.4436, Val Loss: 1.4043
Epoch 113, accuracy: 0.6360
Epoch 114, accuracy: 0.6360
Epoch 114, Train Loss: 0.5253, Val Loss: 1.4043
Epoch 115, accuracy: 0.6360
Epoch 116, accuracy: 0.6360
Epoch 116, Train Loss: 0.4427, Val Loss: 1.4043
Epoch 117, accuracy: 0.6360
Epoch 118, accuracy: 0.6360
Epoch 118, Train Loss: 0.5305, Val Loss: 1.4043
Epoch 119, accuracy: 0.6360
Epoch 120, accuracy: 0.6360
Epoch 120, Train Loss: 0.4535, Val Loss: 1.4043
Epoch 121, accuracy: 0.6360
Epoch 122, accuracy: 0.6360
Epoch 122, Train Loss: 0.3775, Val Loss: 1.4043
Epoch 123, accuracy: 0.6360
Epoch 124, accuracy: 0.6360
Epoch 124, Train Loss: 0.4454, Val Loss: 1.4043
Epoch 125, accuracy: 0.6360
Epoch 126, accuracy: 0.6360
Epoch 126, Train Loss: 0.3127, Val Loss: 1.4043
Epoch 127, accuracy: 0.6360
Epoch 128, accuracy: 0.6360
Epoch 128, Train Loss: 0.5008, Val Loss: 1.4043
Epoch 129, accuracy: 0.6360
Epoch 130, accuracy: 0.6360
Epoch 130, Train Loss: 0.4137, Val Loss: 1.4043
Epoch 131, accuracy: 0.6360
Epoch 132, accuracy: 0.6360
Epoch 132, Train Loss: 0.4765, Val Loss: 1.4043
Epoch 133, accuracy: 0.6360
Epoch 134, accuracy: 0.6360
Epoch 134, Train Loss: 0.4410, Val Loss: 1.4044
Epoch 135, accuracy: 0.6360
Epoch 136, accuracy: 0.6360
Epoch 136, Train Loss: 0.4745, Val Loss: 1.4044
Epoch 137, accuracy: 0.6360
Epoch 138, accuracy: 0.6360
Epoch 138, Train Loss: 0.4655, Val Loss: 1.4043
Epoch 139, accuracy: 0.6360
Epoch 140, accuracy: 0.6360
Epoch 140, Train Loss: 0.4655, Val Loss: 1.4043
Epoch 141, accuracy: 0.6360
Epoch 142, accuracy: 0.6360
Epoch 142, Train Loss: 0.5043, Val Loss: 1.4044
Epoch 143, accuracy: 0.6360
Epoch 144, accuracy: 0.6360
Epoch 144, Train Loss: 0.4878, Val Loss: 1.4044
Epoch 145, accuracy: 0.6360
Epoch 146, accuracy: 0.6360
Epoch 146, Train Loss: 0.3147, Val Loss: 1.4044
Epoch 147, accuracy: 0.6360
Epoch 148, accuracy: 0.6360
Epoch 148, Train Loss: 0.4608, Val Loss: 1.4044
Epoch 149, accuracy: 0.6360
Epoch 150, accuracy: 0.6360
Epoch 150, Train Loss: 0.4638, Val Loss: 1.4044
Epoch 151, accuracy: 0.6360
Epoch 152, accuracy: 0.6360
Epoch 152, Train Loss: 0.4655, Val Loss: 1.4044
Epoch 153, accuracy: 0.6360
Epoch 154, accuracy: 0.6360
Epoch 154, Train Loss: 0.4676, Val Loss: 1.4044
Epoch 155, accuracy: 0.6360
Epoch 156, accuracy: 0.6360
Epoch 156, Train Loss: 0.4682, Val Loss: 1.4043
Epoch 157, accuracy: 0.6360
Epoch 158, accuracy: 0.6360
Epoch 158, Train Loss: 0.3842, Val Loss: 1.4044
Epoch 159, accuracy: 0.6360
Epoch 160, accuracy: 0.6360
Epoch 160, Train Loss: 0.4644, Val Loss: 1.4044
Epoch 161, accuracy: 0.6360
Epoch 162, accuracy: 0.6360
Epoch 162, Train Loss: 0.3930, Val Loss: 1.4043
Epoch 163, accuracy: 0.6360
Epoch 164, accuracy: 0.6360
Epoch 164, Train Loss: 0.4349, Val Loss: 1.4043
Epoch 165, accuracy: 0.6360
Epoch 166, accuracy: 0.6360
Epoch 166, Train Loss: 0.3401, Val Loss: 1.4043
Epoch 167, accuracy: 0.6360
Epoch 168, accuracy: 0.6360
Epoch 168, Train Loss: 0.4692, Val Loss: 1.4043
Epoch 169, accuracy: 0.6360
Epoch 170, accuracy: 0.6360
Epoch 170, Train Loss: 0.4612, Val Loss: 1.4043
Epoch 171, accuracy: 0.6360
Epoch 172, accuracy: 0.6360
Epoch 172, Train Loss: 0.4553, Val Loss: 1.4043
Epoch 173, accuracy: 0.6360
Epoch 174, accuracy: 0.6360
Epoch 174, Train Loss: 0.3872, Val Loss: 1.4043
Epoch 175, accuracy: 0.6360
Epoch 176, accuracy: 0.6360
Epoch 176, Train Loss: 0.3925, Val Loss: 1.4043
Epoch 177, accuracy: 0.6360
Epoch 178, accuracy: 0.6360
Epoch 178, Train Loss: 0.5032, Val Loss: 1.4043
Epoch 179, accuracy: 0.6360
Epoch 180, accuracy: 0.6360
Epoch 180, Train Loss: 0.5102, Val Loss: 1.4043
Epoch 181, accuracy: 0.6360
Epoch 182, accuracy: 0.6360
Epoch 182, Train Loss: 0.4142, Val Loss: 1.4043
Epoch 183, accuracy: 0.6360
Epoch 184, accuracy: 0.6360
Epoch 184, Train Loss: 0.3931, Val Loss: 1.4043
Epoch 185, accuracy: 0.6360
Epoch 186, accuracy: 0.6360
Epoch 186, Train Loss: 0.5304, Val Loss: 1.4043
Epoch 187, accuracy: 0.6360
Epoch 188, accuracy: 0.6360
Epoch 188, Train Loss: 0.5248, Val Loss: 1.4043
Epoch 189, accuracy: 0.6360
Epoch 190, accuracy: 0.6360
Epoch 190, Train Loss: 0.4693, Val Loss: 1.4043
Epoch 191, accuracy: 0.6360
Epoch 192, accuracy: 0.6360
Epoch 192, Train Loss: 0.2933, Val Loss: 1.4043
Epoch 193, accuracy: 0.6360
Epoch 194, accuracy: 0.6360
Epoch 194, Train Loss: 0.4927, Val Loss: 1.4043
Epoch 195, accuracy: 0.6360
Epoch 196, accuracy: 0.6360
Epoch 196, Train Loss: 0.4298, Val Loss: 1.4043
Epoch 197, accuracy: 0.6360
Epoch 198, accuracy: 0.6360
Epoch 198, Train Loss: 0.4839, Val Loss: 1.4043
Epoch 199, accuracy: 0.6360
Loaded best model with val_loss = 1.1200629472732544
test :accuracy 0.6460, f1_macro: 0.6080, f1_micro: 0.6460, auc: 0.8608
Training mamba3 with 32 layers...
可训练参数: 3974078_mamba3
不可训练参数: 0
✅ Epoch 0: New best model saved with val_loss = 1.8030
Epoch 0, accuracy: 0.3380
Epoch 0, Train Loss: 32037.4648, Val Loss: 1.8030
Epoch 1, accuracy: 0.2310
Epoch 2, accuracy: 0.2500
Epoch 2, Train Loss: 887.3982, Val Loss: 2.8309
✅ Epoch 3: New best model saved with val_loss = 1.5563
Epoch 3, accuracy: 0.4190
Epoch 4, accuracy: 0.3570
Epoch 4, Train Loss: 121.7219, Val Loss: 2.2149
Epoch 5, accuracy: 0.4020
Epoch 6, accuracy: 0.3130
Epoch 6, Train Loss: 1.4560, Val Loss: 1.8352
Epoch 7, accuracy: 0.3180
Epoch 8, accuracy: 0.3750
Epoch 8, Train Loss: 1.1316, Val Loss: 1.6147
Epoch 9, accuracy: 0.4000
Epoch 10, accuracy: 0.3980
Epoch 10, Train Loss: 1.0632, Val Loss: 1.5962
Epoch 11, accuracy: 0.4030
Epoch 12, accuracy: 0.4010
Epoch 12, Train Loss: 1.0864, Val Loss: 1.5944
Epoch 13, accuracy: 0.3970
Epoch 14, accuracy: 0.3940
Epoch 14, Train Loss: 1.0034, Val Loss: 1.5791
Epoch 15, accuracy: 0.3910
Epoch 16, accuracy: 0.3910
Epoch 16, Train Loss: 0.9569, Val Loss: 1.5699
Epoch 17, accuracy: 0.3910
Epoch 18, accuracy: 0.3910
Epoch 18, Train Loss: 0.9120, Val Loss: 1.5684
Epoch 19, accuracy: 0.3920
Epoch 20, accuracy: 0.3920
Epoch 20, Train Loss: 0.9058, Val Loss: 1.5664
Epoch 21, accuracy: 0.3930
Epoch 22, accuracy: 0.3930
Epoch 22, Train Loss: 1.0642, Val Loss: 1.5652
Epoch 23, accuracy: 0.3930
Epoch 24, accuracy: 0.3930
Epoch 24, Train Loss: 1.0580, Val Loss: 1.5650
Epoch 25, accuracy: 0.3930
Epoch 26, accuracy: 0.3930
Epoch 26, Train Loss: 1.0053, Val Loss: 1.5648
Epoch 27, accuracy: 0.3930
Epoch 28, accuracy: 0.3930
Epoch 28, Train Loss: 1.0065, Val Loss: 1.5646
Epoch 29, accuracy: 0.3930
Epoch 30, accuracy: 0.3930
Epoch 30, Train Loss: 0.8238, Val Loss: 1.5646
Epoch 31, accuracy: 0.3930
Epoch 32, accuracy: 0.3930
Epoch 32, Train Loss: 4.2380, Val Loss: 1.5645
Epoch 33, accuracy: 0.3930
Epoch 34, accuracy: 0.3930
Epoch 34, Train Loss: 0.9751, Val Loss: 1.5645
Epoch 35, accuracy: 0.3930
Epoch 36, accuracy: 0.3930
Epoch 36, Train Loss: 0.9524, Val Loss: 1.5645
Epoch 37, accuracy: 0.3930
Epoch 38, accuracy: 0.3930
Epoch 38, Train Loss: 0.9784, Val Loss: 1.5645
Epoch 39, accuracy: 0.3930
Epoch 40, accuracy: 0.3930
Epoch 40, Train Loss: 0.9782, Val Loss: 1.5645
Epoch 41, accuracy: 0.3930
Epoch 42, accuracy: 0.3930
Epoch 42, Train Loss: 1.0112, Val Loss: 1.5645
Epoch 43, accuracy: 0.3930
Epoch 44, accuracy: 0.3930
Epoch 44, Train Loss: 0.9922, Val Loss: 1.5645
Epoch 45, accuracy: 0.3930
Epoch 46, accuracy: 0.3930
Epoch 46, Train Loss: 0.9852, Val Loss: 1.5645
Epoch 47, accuracy: 0.3930
Epoch 48, accuracy: 0.3930
Epoch 48, Train Loss: 1.0664, Val Loss: 1.5645
Epoch 49, accuracy: 0.3930
Epoch 50, accuracy: 0.3930
Epoch 50, Train Loss: 0.9643, Val Loss: 1.5645
Epoch 51, accuracy: 0.3930
Epoch 52, accuracy: 0.3930
Epoch 52, Train Loss: 0.9726, Val Loss: 1.5645
Epoch 53, accuracy: 0.3930
Epoch 54, accuracy: 0.3930
Epoch 54, Train Loss: 0.8974, Val Loss: 1.5645
Epoch 55, accuracy: 0.3930
Epoch 56, accuracy: 0.3930
Epoch 56, Train Loss: 0.9985, Val Loss: 1.5645
Epoch 57, accuracy: 0.3930
Epoch 58, accuracy: 0.3930
Epoch 58, Train Loss: 0.9927, Val Loss: 1.5645
Epoch 59, accuracy: 0.3930
Epoch 60, accuracy: 0.3930
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
/root/miniconda3/lib/python3.12/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling
  warnings.warn(f"Using '{self.__class__.__name__}' without a "
Epoch 60, Train Loss: 1.0803, Val Loss: 1.5645
Epoch 61, accuracy: 0.3930
Epoch 62, accuracy: 0.3930
Epoch 62, Train Loss: 1.0847, Val Loss: 1.5645
Epoch 63, accuracy: 0.3930
Epoch 64, accuracy: 0.3930
Epoch 64, Train Loss: 0.9852, Val Loss: 1.5645
Epoch 65, accuracy: 0.3930
Epoch 66, accuracy: 0.3930
Epoch 66, Train Loss: 0.9438, Val Loss: 1.5645
Epoch 67, accuracy: 0.3930
Epoch 68, accuracy: 0.3930
Epoch 68, Train Loss: 1.1020, Val Loss: 1.5645
Epoch 69, accuracy: 0.3930
Epoch 70, accuracy: 0.3930
Epoch 70, Train Loss: 0.8977, Val Loss: 1.5645
Epoch 71, accuracy: 0.3930
Epoch 72, accuracy: 0.3930
Epoch 72, Train Loss: 0.9144, Val Loss: 1.5645
Epoch 73, accuracy: 0.3930
Epoch 74, accuracy: 0.3930
Epoch 74, Train Loss: 0.9623, Val Loss: 1.5645
Epoch 75, accuracy: 0.3930
Epoch 76, accuracy: 0.3930
Epoch 76, Train Loss: 0.8720, Val Loss: 1.5645
Epoch 77, accuracy: 0.3930
Epoch 78, accuracy: 0.3930
Epoch 78, Train Loss: 0.9889, Val Loss: 1.5645
Epoch 79, accuracy: 0.3930
Epoch 80, accuracy: 0.3930
Epoch 80, Train Loss: 0.9432, Val Loss: 1.5645
Epoch 81, accuracy: 0.3930
Epoch 82, accuracy: 0.3930
Epoch 82, Train Loss: 0.9712, Val Loss: 1.5645
Epoch 83, accuracy: 0.3930
Epoch 84, accuracy: 0.3930
Epoch 84, Train Loss: 0.9440, Val Loss: 1.5645
Epoch 85, accuracy: 0.3930
Epoch 86, accuracy: 0.3930
Epoch 86, Train Loss: 1.0370, Val Loss: 1.5645
Epoch 87, accuracy: 0.3930
Epoch 88, accuracy: 0.3930
Epoch 88, Train Loss: 0.9746, Val Loss: 1.5645
Epoch 89, accuracy: 0.3930
Epoch 90, accuracy: 0.3930
Epoch 90, Train Loss: 0.9561, Val Loss: 1.5645
Epoch 91, accuracy: 0.3930
Epoch 92, accuracy: 0.3930
Epoch 92, Train Loss: 0.9294, Val Loss: 1.5645
Epoch 93, accuracy: 0.3930
Epoch 94, accuracy: 0.3930
Epoch 94, Train Loss: 1.0220, Val Loss: 1.5645
Epoch 95, accuracy: 0.3930
Epoch 96, accuracy: 0.3930
Epoch 96, Train Loss: 1.0282, Val Loss: 1.5645
Epoch 97, accuracy: 0.3930
Epoch 98, accuracy: 0.3930
Epoch 98, Train Loss: 0.9344, Val Loss: 1.5645
Epoch 99, accuracy: 0.3930
Epoch 100, accuracy: 0.3930
Epoch 100, Train Loss: 0.9345, Val Loss: 1.5645
Epoch 101, accuracy: 0.3930
Epoch 102, accuracy: 0.3930
Epoch 102, Train Loss: 0.9197, Val Loss: 1.5645
Epoch 103, accuracy: 0.3930
Epoch 104, accuracy: 0.3930
Epoch 104, Train Loss: 0.9724, Val Loss: 1.5645
Epoch 105, accuracy: 0.3930
Epoch 106, accuracy: 0.3930
Epoch 106, Train Loss: 0.9876, Val Loss: 1.5645
Epoch 107, accuracy: 0.3930
Epoch 108, accuracy: 0.3930
Epoch 108, Train Loss: 0.9507, Val Loss: 1.5645
Epoch 109, accuracy: 0.3930
Epoch 110, accuracy: 0.3930
Epoch 110, Train Loss: 0.9815, Val Loss: 1.5645
Epoch 111, accuracy: 0.3930
Epoch 112, accuracy: 0.3930
Epoch 112, Train Loss: 0.9560, Val Loss: 1.5645
Epoch 113, accuracy: 0.3930
Epoch 114, accuracy: 0.3930
Epoch 114, Train Loss: 0.9423, Val Loss: 1.5645
Epoch 115, accuracy: 0.3930
Epoch 116, accuracy: 0.3930
Epoch 116, Train Loss: 0.9491, Val Loss: 1.5645
Epoch 117, accuracy: 0.3930
Epoch 118, accuracy: 0.3930
Epoch 118, Train Loss: 1.0182, Val Loss: 1.5645
Epoch 119, accuracy: 0.3930
Epoch 120, accuracy: 0.3930
Epoch 120, Train Loss: 0.8913, Val Loss: 1.5645
Epoch 121, accuracy: 0.3930
Epoch 122, accuracy: 0.3930
Epoch 122, Train Loss: 0.9618, Val Loss: 1.5645
Epoch 123, accuracy: 0.3930
Epoch 124, accuracy: 0.3930
Epoch 124, Train Loss: 1.0245, Val Loss: 1.5645
Epoch 125, accuracy: 0.3930
Epoch 126, accuracy: 0.3930
Epoch 126, Train Loss: 1.0206, Val Loss: 1.5645
Epoch 127, accuracy: 0.3930
Epoch 128, accuracy: 0.3930
Epoch 128, Train Loss: 1.0283, Val Loss: 1.5645
Epoch 129, accuracy: 0.3930
Epoch 130, accuracy: 0.3930
Epoch 130, Train Loss: 1.0219, Val Loss: 1.5645
Epoch 131, accuracy: 0.3930
Epoch 132, accuracy: 0.3930
Epoch 132, Train Loss: 0.9555, Val Loss: 1.5645
Epoch 133, accuracy: 0.3930
Epoch 134, accuracy: 0.3930
Epoch 134, Train Loss: 6.5216, Val Loss: 1.5645
Epoch 135, accuracy: 0.3930
Epoch 136, accuracy: 0.3930
Epoch 136, Train Loss: 0.9299, Val Loss: 1.5645
Epoch 137, accuracy: 0.3930
Epoch 138, accuracy: 0.3930
Epoch 138, Train Loss: 0.9908, Val Loss: 1.5645
Epoch 139, accuracy: 0.3930
Epoch 140, accuracy: 0.3930
Epoch 140, Train Loss: 1.0095, Val Loss: 1.5645
Epoch 141, accuracy: 0.3930
Epoch 142, accuracy: 0.3930
Epoch 142, Train Loss: 0.9335, Val Loss: 1.5645
Epoch 143, accuracy: 0.3930
Epoch 144, accuracy: 0.3930
Epoch 144, Train Loss: 0.9368, Val Loss: 1.5645
Epoch 145, accuracy: 0.3930
Epoch 146, accuracy: 0.3930
Epoch 146, Train Loss: 1.0154, Val Loss: 1.5645
Epoch 147, accuracy: 0.3930
Epoch 148, accuracy: 0.3930
Epoch 148, Train Loss: 0.9126, Val Loss: 1.5645
Epoch 149, accuracy: 0.3930
Epoch 150, accuracy: 0.3930
Epoch 150, Train Loss: 1.1262, Val Loss: 1.5645
Epoch 151, accuracy: 0.3930
Epoch 152, accuracy: 0.3930
Epoch 152, Train Loss: 0.9496, Val Loss: 1.5645
Epoch 153, accuracy: 0.3930
Epoch 154, accuracy: 0.3930
Epoch 154, Train Loss: 0.9146, Val Loss: 1.5645
Epoch 155, accuracy: 0.3930
Epoch 156, accuracy: 0.3930
Epoch 156, Train Loss: 1.0419, Val Loss: 1.5645
Epoch 157, accuracy: 0.3930
Epoch 158, accuracy: 0.3930
Epoch 158, Train Loss: 1.0015, Val Loss: 1.5645
Epoch 159, accuracy: 0.3930
Epoch 160, accuracy: 0.3930
Epoch 160, Train Loss: 0.9085, Val Loss: 1.5645
Epoch 161, accuracy: 0.3930
Epoch 162, accuracy: 0.3930
Epoch 162, Train Loss: 0.9375, Val Loss: 1.5645
Epoch 163, accuracy: 0.3930
Epoch 164, accuracy: 0.3930
Epoch 164, Train Loss: 0.9965, Val Loss: 1.5644
Epoch 165, accuracy: 0.3930
Epoch 166, accuracy: 0.3930
Epoch 166, Train Loss: 0.9598, Val Loss: 1.5644
Epoch 167, accuracy: 0.3930
Epoch 168, accuracy: 0.3930
Epoch 168, Train Loss: 0.9610, Val Loss: 1.5644
Epoch 169, accuracy: 0.3930
Epoch 170, accuracy: 0.3930
Epoch 170, Train Loss: 0.9607, Val Loss: 1.5644
Epoch 171, accuracy: 0.3930
Epoch 172, accuracy: 0.3930
Epoch 172, Train Loss: 1.0543, Val Loss: 1.5644
Epoch 173, accuracy: 0.3930
Epoch 174, accuracy: 0.3930
Epoch 174, Train Loss: 1.0086, Val Loss: 1.5644
Epoch 175, accuracy: 0.3930
Epoch 176, accuracy: 0.3930
Epoch 176, Train Loss: 0.9482, Val Loss: 1.5644
Epoch 177, accuracy: 0.3930
Epoch 178, accuracy: 0.3930
Epoch 178, Train Loss: 1.0469, Val Loss: 1.5644
Epoch 179, accuracy: 0.3930
Epoch 180, accuracy: 0.3930
Epoch 180, Train Loss: 1.0216, Val Loss: 1.5644
Epoch 181, accuracy: 0.3930
Epoch 182, accuracy: 0.3930
Epoch 182, Train Loss: 1.0161, Val Loss: 1.5644
Epoch 183, accuracy: 0.3930
Epoch 184, accuracy: 0.3930
Epoch 184, Train Loss: 1.0282, Val Loss: 1.5644
Epoch 185, accuracy: 0.3930
Epoch 186, accuracy: 0.3930
Epoch 186, Train Loss: 1.0059, Val Loss: 1.5644
Epoch 187, accuracy: 0.3930
Epoch 188, accuracy: 0.3930
Epoch 188, Train Loss: 0.8823, Val Loss: 1.5644
Epoch 189, accuracy: 0.3930
Epoch 190, accuracy: 0.3930
Epoch 190, Train Loss: 0.9982, Val Loss: 1.5644
Epoch 191, accuracy: 0.3930
Epoch 192, accuracy: 0.3930
Epoch 192, Train Loss: 0.9225, Val Loss: 1.5644
Epoch 193, accuracy: 0.3930
Epoch 194, accuracy: 0.3930
Epoch 194, Train Loss: 1.0121, Val Loss: 1.5644
Epoch 195, accuracy: 0.3930
Epoch 196, accuracy: 0.3930
Epoch 196, Train Loss: 0.9868, Val Loss: 1.5644
Epoch 197, accuracy: 0.3930
Epoch 198, accuracy: 0.3930
Epoch 198, Train Loss: 0.8850, Val Loss: 1.5644
Epoch 199, accuracy: 0.3930
Loaded best model with val_loss = 1.5562951564788818
test :accuracy 0.4190, f1_macro: 0.2649, f1_micro: 0.4190, auc: 0.7999
Final Results: {'mamba3_2_Citeseer': np.float64(0.637), 'mamba3_8_Citeseer': np.float64(0.646), 'mamba3_32_Citeseer': np.float64(0.419)} ['1485758_mamba3_0', '1983422_mamba3_0', '3974078_mamba3_0']
========== Running baseline 3/3 ==========
Training mamba3 with 2 layers...
可训练参数: 1485758_mamba3
不可训练参数: 0
✅ Epoch 0: New best model saved with val_loss = 1.5524
Epoch 0, accuracy: 0.3790
Epoch 0, Train Loss: 1.8723, Val Loss: 1.5524
✅ Epoch 1: New best model saved with val_loss = 1.2934
Epoch 1, accuracy: 0.5810
Epoch 2, accuracy: 0.6230
Epoch 2, Train Loss: 1.0142, Val Loss: 1.3273
✅ Epoch 3: New best model saved with val_loss = 1.2135
Epoch 3, accuracy: 0.6390
Epoch 4, accuracy: 0.6240
Epoch 4, Train Loss: 0.5863, Val Loss: 1.2349
Epoch 5, accuracy: 0.6250
Epoch 6, accuracy: 0.6230
Epoch 6, Train Loss: 0.4672, Val Loss: 1.3766
Epoch 7, accuracy: 0.6160
Epoch 8, accuracy: 0.6160
Epoch 8, Train Loss: 0.3447, Val Loss: 1.5115
Epoch 9, accuracy: 0.6450
Epoch 10, accuracy: 0.6460
Epoch 10, Train Loss: 0.3357, Val Loss: 1.5179
Epoch 11, accuracy: 0.6450
Epoch 12, accuracy: 0.6510
Epoch 12, Train Loss: 0.3287, Val Loss: 1.5160
Epoch 13, accuracy: 0.6550
Epoch 14, accuracy: 0.6520
Epoch 14, Train Loss: 0.2177, Val Loss: 1.5140
Epoch 15, accuracy: 0.6530
Epoch 16, accuracy: 0.6540
Epoch 16, Train Loss: 0.2654, Val Loss: 1.5123
Epoch 17, accuracy: 0.6530
Epoch 18, accuracy: 0.6530
Epoch 18, Train Loss: 0.2707, Val Loss: 1.5122
Epoch 19, accuracy: 0.6520
Epoch 20, accuracy: 0.6520
Epoch 20, Train Loss: 0.2540, Val Loss: 1.5119
Epoch 21, accuracy: 0.6520
Epoch 22, accuracy: 0.6520
Epoch 22, Train Loss: 0.2618, Val Loss: 1.5119
Epoch 23, accuracy: 0.6520
Epoch 24, accuracy: 0.6520
Epoch 24, Train Loss: 0.2689, Val Loss: 1.5119
Epoch 25, accuracy: 0.6520
Epoch 26, accuracy: 0.6520
Epoch 26, Train Loss: 0.3250, Val Loss: 1.5120
Epoch 27, accuracy: 0.6520
Epoch 28, accuracy: 0.6520
Epoch 28, Train Loss: 0.2592, Val Loss: 1.5120
Epoch 29, accuracy: 0.6520
Epoch 30, accuracy: 0.6520
Epoch 30, Train Loss: 0.2041, Val Loss: 1.5120
Epoch 31, accuracy: 0.6520
Epoch 32, accuracy: 0.6520
Epoch 32, Train Loss: 0.2879, Val Loss: 1.5120
Epoch 33, accuracy: 0.6520
Epoch 34, accuracy: 0.6520
Epoch 34, Train Loss: 0.2883, Val Loss: 1.5120
Epoch 35, accuracy: 0.6520
Epoch 36, accuracy: 0.6520
Epoch 36, Train Loss: 0.3275, Val Loss: 1.5120
Epoch 37, accuracy: 0.6520
Epoch 38, accuracy: 0.6520
Epoch 38, Train Loss: 0.2707, Val Loss: 1.5120
Epoch 39, accuracy: 0.6520
Epoch 40, accuracy: 0.6520
Epoch 40, Train Loss: 0.2504, Val Loss: 1.5120
Epoch 41, accuracy: 0.6520
Epoch 42, accuracy: 0.6520
Epoch 42, Train Loss: 0.2564, Val Loss: 1.5120
Epoch 43, accuracy: 0.6520
Epoch 44, accuracy: 0.6520
Epoch 44, Train Loss: 0.2358, Val Loss: 1.5120
Epoch 45, accuracy: 0.6520
Epoch 46, accuracy: 0.6520
Epoch 46, Train Loss: 0.2897, Val Loss: 1.5120
Epoch 47, accuracy: 0.6520
Epoch 48, accuracy: 0.6520
Epoch 48, Train Loss: 0.3303, Val Loss: 1.5120
Epoch 49, accuracy: 0.6520
Epoch 50, accuracy: 0.6520
Epoch 50, Train Loss: 0.2554, Val Loss: 1.5120
Epoch 51, accuracy: 0.6520
Epoch 52, accuracy: 0.6520
Epoch 52, Train Loss: 0.3267, Val Loss: 1.5120
Epoch 53, accuracy: 0.6520
Epoch 54, accuracy: 0.6520
Epoch 54, Train Loss: 0.2830, Val Loss: 1.5120
Epoch 55, accuracy: 0.6520
Epoch 56, accuracy: 0.6520
Epoch 56, Train Loss: 0.2639, Val Loss: 1.5120
Epoch 57, accuracy: 0.6520
Epoch 58, accuracy: 0.6520
Epoch 58, Train Loss: 0.3141, Val Loss: 1.5120
Epoch 59, accuracy: 0.6520
Epoch 60, accuracy: 0.6520
Epoch 60, Train Loss: 0.2516, Val Loss: 1.5120
Epoch 61, accuracy: 0.6520
Epoch 62, accuracy: 0.6520
Epoch 62, Train Loss: 0.2803, Val Loss: 1.5120
Epoch 63, accuracy: 0.6520
Epoch 64, accuracy: 0.6520
Epoch 64, Train Loss: 0.2779, Val Loss: 1.5120
Epoch 65, accuracy: 0.6520
Epoch 66, accuracy: 0.6520
Epoch 66, Train Loss: 0.2588, Val Loss: 1.5120
Epoch 67, accuracy: 0.6520
Epoch 68, accuracy: 0.6520
Epoch 68, Train Loss: 0.2400, Val Loss: 1.5120
Epoch 69, accuracy: 0.6520
Epoch 70, accuracy: 0.6520
Epoch 70, Train Loss: 0.1696, Val Loss: 1.5120
Epoch 71, accuracy: 0.6520
Epoch 72, accuracy: 0.6520
Epoch 72, Train Loss: 0.2792, Val Loss: 1.5120
Epoch 73, accuracy: 0.6520
Epoch 74, accuracy: 0.6520
Epoch 74, Train Loss: 0.2607, Val Loss: 1.5120
Epoch 75, accuracy: 0.6520
Epoch 76, accuracy: 0.6520
Epoch 76, Train Loss: 0.3052, Val Loss: 1.5120
Epoch 77, accuracy: 0.6520
Epoch 78, accuracy: 0.6520
Epoch 78, Train Loss: 0.3094, Val Loss: 1.5120
Epoch 79, accuracy: 0.6520
Epoch 80, accuracy: 0.6520
Epoch 80, Train Loss: 0.2405, Val Loss: 1.5120
Epoch 81, accuracy: 0.6520
Epoch 82, accuracy: 0.6520
Epoch 82, Train Loss: 0.3149, Val Loss: 1.5120
Epoch 83, accuracy: 0.6520
Epoch 84, accuracy: 0.6520
Epoch 84, Train Loss: 0.3223, Val Loss: 1.5120
Epoch 85, accuracy: 0.6520
Epoch 86, accuracy: 0.6520
Epoch 86, Train Loss: 0.2639, Val Loss: 1.5120
Epoch 87, accuracy: 0.6520
Epoch 88, accuracy: 0.6520
Epoch 88, Train Loss: 0.2668, Val Loss: 1.5120
Epoch 89, accuracy: 0.6520
Epoch 90, accuracy: 0.6520
Epoch 90, Train Loss: 0.3295, Val Loss: 1.5120
Epoch 91, accuracy: 0.6520
Epoch 92, accuracy: 0.6520
Epoch 92, Train Loss: 0.2356, Val Loss: 1.5120
Epoch 93, accuracy: 0.6520
Epoch 94, accuracy: 0.6520
Epoch 94, Train Loss: 0.3179, Val Loss: 1.5120
Epoch 95, accuracy: 0.6520
Epoch 96, accuracy: 0.6520
Epoch 96, Train Loss: 0.2701, Val Loss: 1.5120
Epoch 97, accuracy: 0.6520
Epoch 98, accuracy: 0.6520
Epoch 98, Train Loss: 0.2797, Val Loss: 1.5120
Epoch 99, accuracy: 0.6520
Epoch 100, accuracy: 0.6520
Epoch 100, Train Loss: 0.2378, Val Loss: 1.5120
Epoch 101, accuracy: 0.6520
Epoch 102, accuracy: 0.6520
Epoch 102, Train Loss: 0.2196, Val Loss: 1.5120
Epoch 103, accuracy: 0.6520
Epoch 104, accuracy: 0.6520
Epoch 104, Train Loss: 0.2681, Val Loss: 1.5120
Epoch 105, accuracy: 0.6520
Epoch 106, accuracy: 0.6520
Epoch 106, Train Loss: 0.2438, Val Loss: 1.5120
Epoch 107, accuracy: 0.6520
Epoch 108, accuracy: 0.6520
Epoch 108, Train Loss: 0.3162, Val Loss: 1.5120
Epoch 109, accuracy: 0.6520
Epoch 110, accuracy: 0.6520
Epoch 110, Train Loss: 0.2087, Val Loss: 1.5120
Epoch 111, accuracy: 0.6520
Epoch 112, accuracy: 0.6520
Epoch 112, Train Loss: 0.2243, Val Loss: 1.5120
Epoch 113, accuracy: 0.6520
Epoch 114, accuracy: 0.6520
Epoch 114, Train Loss: 0.2461, Val Loss: 1.5120
Epoch 115, accuracy: 0.6520
Epoch 116, accuracy: 0.6520
Epoch 116, Train Loss: 0.2638, Val Loss: 1.5120
Epoch 117, accuracy: 0.6520
Epoch 118, accuracy: 0.6520
Epoch 118, Train Loss: 0.1967, Val Loss: 1.5120
Epoch 119, accuracy: 0.6520
Epoch 120, accuracy: 0.6520
Epoch 120, Train Loss: 0.2449, Val Loss: 1.5120
Epoch 121, accuracy: 0.6520
Epoch 122, accuracy: 0.6520
Epoch 122, Train Loss: 0.2478, Val Loss: 1.5120
Epoch 123, accuracy: 0.6520
Epoch 124, accuracy: 0.6520
Epoch 124, Train Loss: 0.2399, Val Loss: 1.5120
Epoch 125, accuracy: 0.6520
Epoch 126, accuracy: 0.6520
Epoch 126, Train Loss: 0.2247, Val Loss: 1.5120
Epoch 127, accuracy: 0.6520
Epoch 128, accuracy: 0.6520
Epoch 128, Train Loss: 0.2659, Val Loss: 1.5120
Epoch 129, accuracy: 0.6520
Epoch 130, accuracy: 0.6520
Epoch 130, Train Loss: 0.2391, Val Loss: 1.5120
Epoch 131, accuracy: 0.6520
Epoch 132, accuracy: 0.6520
Epoch 132, Train Loss: 0.2518, Val Loss: 1.5120
Epoch 133, accuracy: 0.6520
Epoch 134, accuracy: 0.6520
Epoch 134, Train Loss: 0.2431, Val Loss: 1.5120
Epoch 135, accuracy: 0.6520
Epoch 136, accuracy: 0.6520
Epoch 136, Train Loss: 0.1937, Val Loss: 1.5120
Epoch 137, accuracy: 0.6520
Epoch 138, accuracy: 0.6520
Epoch 138, Train Loss: 0.3354, Val Loss: 1.5120
Epoch 139, accuracy: 0.6520
Epoch 140, accuracy: 0.6520
Epoch 140, Train Loss: 0.2671, Val Loss: 1.5120
Epoch 141, accuracy: 0.6520
Epoch 142, accuracy: 0.6520
Epoch 142, Train Loss: 0.2123, Val Loss: 1.5120
Epoch 143, accuracy: 0.6520
Epoch 144, accuracy: 0.6520
Epoch 144, Train Loss: 0.3129, Val Loss: 1.5120
Epoch 145, accuracy: 0.6520
Epoch 146, accuracy: 0.6520
Epoch 146, Train Loss: 0.2933, Val Loss: 1.5120
Epoch 147, accuracy: 0.6520
Epoch 148, accuracy: 0.6520
Epoch 148, Train Loss: 0.2242, Val Loss: 1.5120
Epoch 149, accuracy: 0.6520
Epoch 150, accuracy: 0.6520
Epoch 150, Train Loss: 0.2987, Val Loss: 1.5120
Epoch 151, accuracy: 0.6520
Epoch 152, accuracy: 0.6520
Epoch 152, Train Loss: 0.2936, Val Loss: 1.5120
Epoch 153, accuracy: 0.6520
Epoch 154, accuracy: 0.6520
Epoch 154, Train Loss: 0.2254, Val Loss: 1.5120
Epoch 155, accuracy: 0.6520
Epoch 156, accuracy: 0.6520
Epoch 156, Train Loss: 0.2405, Val Loss: 1.5120
Epoch 157, accuracy: 0.6520
Epoch 158, accuracy: 0.6520
Epoch 158, Train Loss: 0.3421, Val Loss: 1.5120
Epoch 159, accuracy: 0.6520
Epoch 160, accuracy: 0.6520
Epoch 160, Train Loss: 0.2755, Val Loss: 1.5120
Epoch 161, accuracy: 0.6520
Epoch 162, accuracy: 0.6520
Epoch 162, Train Loss: 0.2886, Val Loss: 1.5120
Epoch 163, accuracy: 0.6520
Epoch 164, accuracy: 0.6520
Epoch 164, Train Loss: 0.2290, Val Loss: 1.5120
Epoch 165, accuracy: 0.6520
Epoch 166, accuracy: 0.6520
Epoch 166, Train Loss: 0.2996, Val Loss: 1.5120
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 167, accuracy: 0.6520
Epoch 168, accuracy: 0.6520
Epoch 168, Train Loss: 0.3941, Val Loss: 1.5120
Epoch 169, accuracy: 0.6520
Epoch 170, accuracy: 0.6520
Epoch 170, Train Loss: 0.3026, Val Loss: 1.5120
Epoch 171, accuracy: 0.6520
Epoch 172, accuracy: 0.6520
Epoch 172, Train Loss: 0.2663, Val Loss: 1.5120
Epoch 173, accuracy: 0.6520
Epoch 174, accuracy: 0.6520
Epoch 174, Train Loss: 0.2469, Val Loss: 1.5120
Epoch 175, accuracy: 0.6520
Epoch 176, accuracy: 0.6520
Epoch 176, Train Loss: 0.3121, Val Loss: 1.5120
Epoch 177, accuracy: 0.6520
Epoch 178, accuracy: 0.6520
Epoch 178, Train Loss: 0.3631, Val Loss: 1.5120
Epoch 179, accuracy: 0.6520
Epoch 180, accuracy: 0.6520
Epoch 180, Train Loss: 0.2373, Val Loss: 1.5120
Epoch 181, accuracy: 0.6520
Epoch 182, accuracy: 0.6520
Epoch 182, Train Loss: 0.3909, Val Loss: 1.5120
Epoch 183, accuracy: 0.6520
Epoch 184, accuracy: 0.6520
Epoch 184, Train Loss: 0.2099, Val Loss: 1.5120
Epoch 185, accuracy: 0.6520
Epoch 186, accuracy: 0.6520
Epoch 186, Train Loss: 0.2318, Val Loss: 1.5120
Epoch 187, accuracy: 0.6520
Epoch 188, accuracy: 0.6520
Epoch 188, Train Loss: 0.2934, Val Loss: 1.5120
Epoch 189, accuracy: 0.6520
Epoch 190, accuracy: 0.6520
Epoch 190, Train Loss: 0.2493, Val Loss: 1.5120
Epoch 191, accuracy: 0.6520
Epoch 192, accuracy: 0.6520
Epoch 192, Train Loss: 0.2564, Val Loss: 1.5120
Epoch 193, accuracy: 0.6520
Epoch 194, accuracy: 0.6520
Epoch 194, Train Loss: 0.2866, Val Loss: 1.5120
Epoch 195, accuracy: 0.6520
Epoch 196, accuracy: 0.6520
Epoch 196, Train Loss: 0.2728, Val Loss: 1.5120
Epoch 197, accuracy: 0.6520
Epoch 198, accuracy: 0.6520
Epoch 198, Train Loss: 0.2372, Val Loss: 1.5120
Epoch 199, accuracy: 0.6520
Loaded best model with val_loss = 1.2134921550750732
test :accuracy 0.6390, f1_macro: 0.5803, f1_micro: 0.6390, auc: 0.8380
Training mamba3 with 8 layers...
可训练参数: 1983422_mamba3
不可训练参数: 0
✅ Epoch 0: New best model saved with val_loss = 1.5521
Epoch 0, accuracy: 0.3400
Epoch 0, Train Loss: 1.9003, Val Loss: 1.5521
✅ Epoch 1: New best model saved with val_loss = 1.4420
Epoch 1, accuracy: 0.4800
✅ Epoch 2: New best model saved with val_loss = 1.2348
Epoch 2, accuracy: 0.5700
Epoch 2, Train Loss: 1.5487, Val Loss: 1.2348
✅ Epoch 3: New best model saved with val_loss = 1.1410
Epoch 3, accuracy: 0.6020
Epoch 4, accuracy: 0.6160
Epoch 4, Train Loss: 0.8331, Val Loss: 1.1433
✅ Epoch 5: New best model saved with val_loss = 1.0894
Epoch 5, accuracy: 0.6360
✅ Epoch 6: New best model saved with val_loss = 1.0639
Epoch 6, accuracy: 0.6460
Epoch 6, Train Loss: 0.7581, Val Loss: 1.0639
✅ Epoch 7: New best model saved with val_loss = 1.0559
Epoch 7, accuracy: 0.6490
Epoch 8, accuracy: 0.6530
Epoch 8, Train Loss: 0.4612, Val Loss: 1.0624
Epoch 9, accuracy: 0.6540
Epoch 10, accuracy: 0.6690
Epoch 10, Train Loss: 0.5709, Val Loss: 1.0837
Epoch 11, accuracy: 0.6650
Epoch 12, accuracy: 0.6690
Epoch 12, Train Loss: 0.4472, Val Loss: 1.1309
Epoch 13, accuracy: 0.6710
Epoch 14, accuracy: 0.6680
Epoch 14, Train Loss: 0.3495, Val Loss: 1.1552
Epoch 15, accuracy: 0.6650
Epoch 16, accuracy: 0.6660
Epoch 16, Train Loss: 0.2862, Val Loss: 1.1574
Epoch 17, accuracy: 0.6650
Epoch 18, accuracy: 0.6650
Epoch 18, Train Loss: 0.3862, Val Loss: 1.1601
Epoch 19, accuracy: 0.6670
Epoch 20, accuracy: 0.6660
Epoch 20, Train Loss: 0.3336, Val Loss: 1.1621
Epoch 21, accuracy: 0.6660
Epoch 22, accuracy: 0.6660
Epoch 22, Train Loss: 0.4071, Val Loss: 1.1624
Epoch 23, accuracy: 0.6660
Epoch 24, accuracy: 0.6670
Epoch 24, Train Loss: 0.2425, Val Loss: 1.1628
Epoch 25, accuracy: 0.6670
Epoch 26, accuracy: 0.6670
Epoch 26, Train Loss: 0.3680, Val Loss: 1.1630
Epoch 27, accuracy: 0.6670
Epoch 28, accuracy: 0.6670
Epoch 28, Train Loss: 0.3247, Val Loss: 1.1630
Epoch 29, accuracy: 0.6670
Epoch 30, accuracy: 0.6670
Epoch 30, Train Loss: 0.3467, Val Loss: 1.1631
Epoch 31, accuracy: 0.6670
Epoch 32, accuracy: 0.6670
Epoch 32, Train Loss: 0.3404, Val Loss: 1.1631
Epoch 33, accuracy: 0.6670
Epoch 34, accuracy: 0.6670
Epoch 34, Train Loss: 0.3407, Val Loss: 1.1631
Epoch 35, accuracy: 0.6670
Epoch 36, accuracy: 0.6670
Epoch 36, Train Loss: 0.3595, Val Loss: 1.1631
Epoch 37, accuracy: 0.6670
Epoch 38, accuracy: 0.6670
Epoch 38, Train Loss: 0.3494, Val Loss: 1.1631
Epoch 39, accuracy: 0.6670
Epoch 40, accuracy: 0.6670
Epoch 40, Train Loss: 0.3939, Val Loss: 1.1631
Epoch 41, accuracy: 0.6670
Epoch 42, accuracy: 0.6670
Epoch 42, Train Loss: 0.4154, Val Loss: 1.1631
Epoch 43, accuracy: 0.6670
Epoch 44, accuracy: 0.6670
Epoch 44, Train Loss: 0.3647, Val Loss: 1.1631
Epoch 45, accuracy: 0.6670
Epoch 46, accuracy: 0.6670
Epoch 46, Train Loss: 0.3473, Val Loss: 1.1631
Epoch 47, accuracy: 0.6670
Epoch 48, accuracy: 0.6670
Epoch 48, Train Loss: 0.3484, Val Loss: 1.1631
Epoch 49, accuracy: 0.6670
Epoch 50, accuracy: 0.6670
Epoch 50, Train Loss: 0.3356, Val Loss: 1.1631
Epoch 51, accuracy: 0.6670
Epoch 52, accuracy: 0.6670
Epoch 52, Train Loss: 0.3343, Val Loss: 1.1631
Epoch 53, accuracy: 0.6670
Epoch 54, accuracy: 0.6670
Epoch 54, Train Loss: 0.3924, Val Loss: 1.1631
Epoch 55, accuracy: 0.6670
Epoch 56, accuracy: 0.6670
Epoch 56, Train Loss: 0.2682, Val Loss: 1.1631
Epoch 57, accuracy: 0.6670
Epoch 58, accuracy: 0.6670
Epoch 58, Train Loss: 0.3798, Val Loss: 1.1631
Epoch 59, accuracy: 0.6670
Epoch 60, accuracy: 0.6670
Epoch 60, Train Loss: 0.3076, Val Loss: 1.1631
Epoch 61, accuracy: 0.6670
Epoch 62, accuracy: 0.6670
Epoch 62, Train Loss: 0.3290, Val Loss: 1.1631
Epoch 63, accuracy: 0.6670
Epoch 64, accuracy: 0.6670
Epoch 64, Train Loss: 0.4084, Val Loss: 1.1631
Epoch 65, accuracy: 0.6670
Epoch 66, accuracy: 0.6670
Epoch 66, Train Loss: 0.3286, Val Loss: 1.1631
Epoch 67, accuracy: 0.6670
Epoch 68, accuracy: 0.6670
Epoch 68, Train Loss: 0.4230, Val Loss: 1.1631
Epoch 69, accuracy: 0.6670
Epoch 70, accuracy: 0.6670
Epoch 70, Train Loss: 0.3877, Val Loss: 1.1631
Epoch 71, accuracy: 0.6670
Epoch 72, accuracy: 0.6670
Epoch 72, Train Loss: 0.3745, Val Loss: 1.1631
Epoch 73, accuracy: 0.6670
Epoch 74, accuracy: 0.6670
Epoch 74, Train Loss: 0.3988, Val Loss: 1.1631
Epoch 75, accuracy: 0.6670
Epoch 76, accuracy: 0.6670
Epoch 76, Train Loss: 0.4363, Val Loss: 1.1631
Epoch 77, accuracy: 0.6670
Epoch 78, accuracy: 0.6670
Epoch 78, Train Loss: 0.3438, Val Loss: 1.1631
Epoch 79, accuracy: 0.6670
Epoch 80, accuracy: 0.6670
Epoch 80, Train Loss: 0.3247, Val Loss: 1.1631
Epoch 81, accuracy: 0.6670
Epoch 82, accuracy: 0.6670
Epoch 82, Train Loss: 0.4020, Val Loss: 1.1631
Epoch 83, accuracy: 0.6670
Epoch 84, accuracy: 0.6670
Epoch 84, Train Loss: 0.3449, Val Loss: 1.1631
Epoch 85, accuracy: 0.6670
Epoch 86, accuracy: 0.6670
Epoch 86, Train Loss: 0.4274, Val Loss: 1.1631
Epoch 87, accuracy: 0.6670
Epoch 88, accuracy: 0.6670
Epoch 88, Train Loss: 0.3404, Val Loss: 1.1631
Epoch 89, accuracy: 0.6670
Epoch 90, accuracy: 0.6670
Epoch 90, Train Loss: 0.3264, Val Loss: 1.1631
Epoch 91, accuracy: 0.6670
Epoch 92, accuracy: 0.6670
Epoch 92, Train Loss: 0.3141, Val Loss: 1.1631
Epoch 93, accuracy: 0.6670
Epoch 94, accuracy: 0.6670
Epoch 94, Train Loss: 0.3265, Val Loss: 1.1631
Epoch 95, accuracy: 0.6670
Epoch 96, accuracy: 0.6670
Epoch 96, Train Loss: 0.4303, Val Loss: 1.1631
Epoch 97, accuracy: 0.6670
Epoch 98, accuracy: 0.6670
Epoch 98, Train Loss: 0.3430, Val Loss: 1.1631
Epoch 99, accuracy: 0.6670
Epoch 100, accuracy: 0.6670
Epoch 100, Train Loss: 0.2913, Val Loss: 1.1631
Epoch 101, accuracy: 0.6670
Epoch 102, accuracy: 0.6670
Epoch 102, Train Loss: 0.4000, Val Loss: 1.1631
Epoch 103, accuracy: 0.6670
Epoch 104, accuracy: 0.6670
Epoch 104, Train Loss: 0.3443, Val Loss: 1.1631
Epoch 105, accuracy: 0.6670
Epoch 106, accuracy: 0.6670
Epoch 106, Train Loss: 0.3974, Val Loss: 1.1631
Epoch 107, accuracy: 0.6670
Epoch 108, accuracy: 0.6670
Epoch 108, Train Loss: 0.3035, Val Loss: 1.1631
Epoch 109, accuracy: 0.6670
Epoch 110, accuracy: 0.6670
Epoch 110, Train Loss: 0.3617, Val Loss: 1.1631
Epoch 111, accuracy: 0.6670
Epoch 112, accuracy: 0.6670
Epoch 112, Train Loss: 0.2644, Val Loss: 1.1631
Epoch 113, accuracy: 0.6670
Epoch 114, accuracy: 0.6670
Epoch 114, Train Loss: 0.3603, Val Loss: 1.1631
Epoch 115, accuracy: 0.6670
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 116, accuracy: 0.6670
Epoch 116, Train Loss: 0.2976, Val Loss: 1.1631
Epoch 117, accuracy: 0.6670
Epoch 118, accuracy: 0.6670
Epoch 118, Train Loss: 0.3673, Val Loss: 1.1631
Epoch 119, accuracy: 0.6670
Epoch 120, accuracy: 0.6670
Epoch 120, Train Loss: 0.4022, Val Loss: 1.1631
Epoch 121, accuracy: 0.6670
Epoch 122, accuracy: 0.6670
Epoch 122, Train Loss: 0.3487, Val Loss: 1.1631
Epoch 123, accuracy: 0.6670
Epoch 124, accuracy: 0.6670
Epoch 124, Train Loss: 0.3353, Val Loss: 1.1631
Epoch 125, accuracy: 0.6670
Epoch 126, accuracy: 0.6670
Epoch 126, Train Loss: 0.4079, Val Loss: 1.1631
Epoch 127, accuracy: 0.6670
Epoch 128, accuracy: 0.6670
Epoch 128, Train Loss: 0.3286, Val Loss: 1.1631
Epoch 129, accuracy: 0.6670
Epoch 130, accuracy: 0.6670
Epoch 130, Train Loss: 0.2893, Val Loss: 1.1631
Epoch 131, accuracy: 0.6670
Epoch 132, accuracy: 0.6670
Epoch 132, Train Loss: 0.3090, Val Loss: 1.1631
Epoch 133, accuracy: 0.6670
Epoch 134, accuracy: 0.6670
Epoch 134, Train Loss: 0.3029, Val Loss: 1.1631
Epoch 135, accuracy: 0.6670
Epoch 136, accuracy: 0.6670
Epoch 136, Train Loss: 0.4808, Val Loss: 1.1631
Epoch 137, accuracy: 0.6670
Epoch 138, accuracy: 0.6670
Epoch 138, Train Loss: 0.3554, Val Loss: 1.1631
Epoch 139, accuracy: 0.6670
Epoch 140, accuracy: 0.6670
Epoch 140, Train Loss: 0.3606, Val Loss: 1.1631
Epoch 141, accuracy: 0.6670
Epoch 142, accuracy: 0.6670
Epoch 142, Train Loss: 0.3330, Val Loss: 1.1631
Epoch 143, accuracy: 0.6670
Epoch 144, accuracy: 0.6670
Epoch 144, Train Loss: 0.4081, Val Loss: 1.1631
Epoch 145, accuracy: 0.6670
Epoch 146, accuracy: 0.6670
Epoch 146, Train Loss: 0.2823, Val Loss: 1.1631
Epoch 147, accuracy: 0.6670
Epoch 148, accuracy: 0.6670
Epoch 148, Train Loss: 0.4058, Val Loss: 1.1631
Epoch 149, accuracy: 0.6670
Epoch 150, accuracy: 0.6670
Epoch 150, Train Loss: 0.3323, Val Loss: 1.1631
Epoch 151, accuracy: 0.6670
Epoch 152, accuracy: 0.6670
Epoch 152, Train Loss: 0.2887, Val Loss: 1.1631
Epoch 153, accuracy: 0.6670
Epoch 154, accuracy: 0.6670
Epoch 154, Train Loss: 0.4430, Val Loss: 1.1631
Epoch 155, accuracy: 0.6670
Epoch 156, accuracy: 0.6670
Epoch 156, Train Loss: 0.4087, Val Loss: 1.1631
Epoch 157, accuracy: 0.6670
Epoch 158, accuracy: 0.6670
Epoch 158, Train Loss: 0.3692, Val Loss: 1.1631
Epoch 159, accuracy: 0.6670
Epoch 160, accuracy: 0.6670
Epoch 160, Train Loss: 0.4399, Val Loss: 1.1631
Epoch 161, accuracy: 0.6670
Epoch 162, accuracy: 0.6670
Epoch 162, Train Loss: 0.2940, Val Loss: 1.1631
Epoch 163, accuracy: 0.6670
Epoch 164, accuracy: 0.6670
Epoch 164, Train Loss: 0.3156, Val Loss: 1.1631
Epoch 165, accuracy: 0.6670
Epoch 166, accuracy: 0.6670
Epoch 166, Train Loss: 0.4188, Val Loss: 1.1631
Epoch 167, accuracy: 0.6670
Epoch 168, accuracy: 0.6670
Epoch 168, Train Loss: 0.3702, Val Loss: 1.1631
Epoch 169, accuracy: 0.6670
Epoch 170, accuracy: 0.6670
Epoch 170, Train Loss: 0.3283, Val Loss: 1.1631
Epoch 171, accuracy: 0.6670
Epoch 172, accuracy: 0.6670
Epoch 172, Train Loss: 0.3527, Val Loss: 1.1631
Epoch 173, accuracy: 0.6670
Epoch 174, accuracy: 0.6670
Epoch 174, Train Loss: 0.2959, Val Loss: 1.1631
Epoch 175, accuracy: 0.6670
Epoch 176, accuracy: 0.6670
Epoch 176, Train Loss: 0.3117, Val Loss: 1.1631
Epoch 177, accuracy: 0.6670
Epoch 178, accuracy: 0.6670
Epoch 178, Train Loss: 0.3374, Val Loss: 1.1631
Epoch 179, accuracy: 0.6670
Epoch 180, accuracy: 0.6670
Epoch 180, Train Loss: 0.4502, Val Loss: 1.1631
Epoch 181, accuracy: 0.6670
Epoch 182, accuracy: 0.6670
Epoch 182, Train Loss: 0.3683, Val Loss: 1.1631
Epoch 183, accuracy: 0.6670
Epoch 184, accuracy: 0.6670
Epoch 184, Train Loss: 0.3432, Val Loss: 1.1631
Epoch 185, accuracy: 0.6670
Epoch 186, accuracy: 0.6670
Epoch 186, Train Loss: 0.2665, Val Loss: 1.1631
Epoch 187, accuracy: 0.6670
Epoch 188, accuracy: 0.6670
Epoch 188, Train Loss: 0.3652, Val Loss: 1.1631
Epoch 189, accuracy: 0.6670
Epoch 190, accuracy: 0.6670
Epoch 190, Train Loss: 0.3621, Val Loss: 1.1631
Epoch 191, accuracy: 0.6670
Epoch 192, accuracy: 0.6670
Epoch 192, Train Loss: 0.4463, Val Loss: 1.1631
Epoch 193, accuracy: 0.6670
Epoch 194, accuracy: 0.6670
Epoch 194, Train Loss: 0.3281, Val Loss: 1.1631
Epoch 195, accuracy: 0.6670
Epoch 196, accuracy: 0.6670
Epoch 196, Train Loss: 0.4051, Val Loss: 1.1631
Epoch 197, accuracy: 0.6670
Epoch 198, accuracy: 0.6670
Epoch 198, Train Loss: 0.4688, Val Loss: 1.1631
Epoch 199, accuracy: 0.6670
Loaded best model with val_loss = 1.055911660194397
test :accuracy 0.6490, f1_macro: 0.6122, f1_micro: 0.6490, auc: 0.8685
Training mamba3 with 32 layers...
可训练参数: 3974078_mamba3
不可训练参数: 0
✅ Epoch 0: New best model saved with val_loss = 2.3051
Epoch 0, accuracy: 0.2120
Epoch 0, Train Loss: 27216.6445, Val Loss: 2.3051
✅ Epoch 1: New best model saved with val_loss = 2.0226
Epoch 1, accuracy: 0.2550
✅ Epoch 2: New best model saved with val_loss = 1.5100
Epoch 2, accuracy: 0.4040
Epoch 2, Train Loss: 369.6382, Val Loss: 1.5100
✅ Epoch 3: New best model saved with val_loss = 1.5037
Epoch 3, accuracy: 0.3480
✅ Epoch 4: New best model saved with val_loss = 1.3265
Epoch 4, accuracy: 0.4790
Epoch 4, Train Loss: 24.8137, Val Loss: 1.3265
✅ Epoch 5: New best model saved with val_loss = 1.2846
Epoch 5, accuracy: 0.5970
Epoch 6, accuracy: 0.6090
Epoch 6, Train Loss: 0.9787, Val Loss: 1.3098
Epoch 7, accuracy: 0.5710
Epoch 8, accuracy: 0.5350
Epoch 8, Train Loss: 0.8734, Val Loss: 1.4132
Epoch 9, accuracy: 0.5260
Epoch 10, accuracy: 0.5460
Epoch 10, Train Loss: 0.7914, Val Loss: 1.4008
Epoch 11, accuracy: 0.5790
Epoch 12, accuracy: 0.5830
Epoch 12, Train Loss: 0.6694, Val Loss: 1.3253
Epoch 13, accuracy: 0.5910
Epoch 14, accuracy: 0.6010
Epoch 14, Train Loss: 0.7253, Val Loss: 1.3104
Epoch 15, accuracy: 0.6020
Epoch 16, accuracy: 0.6020
Epoch 16, Train Loss: 0.6955, Val Loss: 1.2961
Epoch 17, accuracy: 0.6030
Epoch 18, accuracy: 0.6030
Epoch 18, Train Loss: 0.7314, Val Loss: 1.2900
Epoch 19, accuracy: 0.6020
Epoch 20, accuracy: 0.6030
Epoch 20, Train Loss: 0.6344, Val Loss: 1.2892
Epoch 21, accuracy: 0.6030
Epoch 22, accuracy: 0.6040
Epoch 22, Train Loss: 0.6303, Val Loss: 1.2884
Epoch 23, accuracy: 0.6040
Epoch 24, accuracy: 0.6040
Epoch 24, Train Loss: 0.7169, Val Loss: 1.2879
Epoch 25, accuracy: 0.6030
Epoch 26, accuracy: 0.6030
Epoch 26, Train Loss: 0.7541, Val Loss: 1.2878
Epoch 27, accuracy: 0.6030
Epoch 28, accuracy: 0.6030
Epoch 28, Train Loss: 0.6485, Val Loss: 1.2877
Epoch 29, accuracy: 0.6030
Epoch 30, accuracy: 0.6030
Epoch 30, Train Loss: 0.6636, Val Loss: 1.2876
Epoch 31, accuracy: 0.6030
Epoch 32, accuracy: 0.6030
Epoch 32, Train Loss: 38.5908, Val Loss: 1.2876
Epoch 33, accuracy: 0.6030
Epoch 34, accuracy: 0.6030
Epoch 34, Train Loss: 0.6519, Val Loss: 1.2876
Epoch 35, accuracy: 0.6030
Epoch 36, accuracy: 0.6030
Epoch 36, Train Loss: 0.7005, Val Loss: 1.2876
Epoch 37, accuracy: 0.6030
Epoch 38, accuracy: 0.6030
Epoch 38, Train Loss: 0.6947, Val Loss: 1.2876
Epoch 39, accuracy: 0.6030
Epoch 40, accuracy: 0.6030
Epoch 40, Train Loss: 0.6784, Val Loss: 1.2876
Epoch 41, accuracy: 0.6030
Epoch 42, accuracy: 0.6030
Epoch 42, Train Loss: 0.6750, Val Loss: 1.2876
Epoch 43, accuracy: 0.6030
Epoch 44, accuracy: 0.6030
Epoch 44, Train Loss: 0.6765, Val Loss: 1.2876
Epoch 45, accuracy: 0.6030
Epoch 46, accuracy: 0.6030
Epoch 46, Train Loss: 0.6741, Val Loss: 1.2876
Epoch 47, accuracy: 0.6030
Epoch 48, accuracy: 0.6030
Epoch 48, Train Loss: 0.6552, Val Loss: 1.2876
Epoch 49, accuracy: 0.6030
Epoch 50, accuracy: 0.6030
Epoch 50, Train Loss: 0.6721, Val Loss: 1.2876
Epoch 51, accuracy: 0.6030
Epoch 52, accuracy: 0.6030
Epoch 52, Train Loss: 0.6694, Val Loss: 1.2876
Epoch 53, accuracy: 0.6030
Epoch 54, accuracy: 0.6030
Epoch 54, Train Loss: 0.7280, Val Loss: 1.2876
Epoch 55, accuracy: 0.6030
Epoch 56, accuracy: 0.6030
Epoch 56, Train Loss: 0.7273, Val Loss: 1.2876
Epoch 57, accuracy: 0.6030
Epoch 58, accuracy: 0.6030
Epoch 58, Train Loss: 0.6649, Val Loss: 1.2876
Epoch 59, accuracy: 0.6030
Epoch 60, accuracy: 0.6030
Epoch 60, Train Loss: 0.7603, Val Loss: 1.2876
Epoch 61, accuracy: 0.6030
Epoch 62, accuracy: 0.6030
Epoch 62, Train Loss: 0.6983, Val Loss: 1.2876
Epoch 63, accuracy: 0.6030
Epoch 64, accuracy: 0.6030
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
/root/miniconda3/lib/python3.12/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling
  warnings.warn(f"Using '{self.__class__.__name__}' without a "
Epoch 64, Train Loss: 0.6136, Val Loss: 1.2876
Epoch 65, accuracy: 0.6030
Epoch 66, accuracy: 0.6030
Epoch 66, Train Loss: 0.6298, Val Loss: 1.2876
Epoch 67, accuracy: 0.6030
Epoch 68, accuracy: 0.6030
Epoch 68, Train Loss: 0.5678, Val Loss: 1.2876
Epoch 69, accuracy: 0.6030
Epoch 70, accuracy: 0.6030
Epoch 70, Train Loss: 0.6555, Val Loss: 1.2876
Epoch 71, accuracy: 0.6030
Epoch 72, accuracy: 0.6030
Epoch 72, Train Loss: 0.6647, Val Loss: 1.2876
Epoch 73, accuracy: 0.6030
Epoch 74, accuracy: 0.6030
Epoch 74, Train Loss: 0.7134, Val Loss: 1.2876
Epoch 75, accuracy: 0.6030
Epoch 76, accuracy: 0.6030
Epoch 76, Train Loss: 0.6381, Val Loss: 1.2876
Epoch 77, accuracy: 0.6030
Epoch 78, accuracy: 0.6030
Epoch 78, Train Loss: 0.7122, Val Loss: 1.2876
Epoch 79, accuracy: 0.6030
Epoch 80, accuracy: 0.6030
Epoch 80, Train Loss: 0.6562, Val Loss: 1.2876
Epoch 81, accuracy: 0.6030
Epoch 82, accuracy: 0.6030
Epoch 82, Train Loss: 0.6875, Val Loss: 1.2876
Epoch 83, accuracy: 0.6030
Epoch 84, accuracy: 0.6030
Epoch 84, Train Loss: 0.6630, Val Loss: 1.2876
Epoch 85, accuracy: 0.6030
Epoch 86, accuracy: 0.6030
Epoch 86, Train Loss: 0.6365, Val Loss: 1.2876
Epoch 87, accuracy: 0.6030
Epoch 88, accuracy: 0.6030
Epoch 88, Train Loss: 0.6707, Val Loss: 1.2876
Epoch 89, accuracy: 0.6030
Epoch 90, accuracy: 0.6030
Epoch 90, Train Loss: 0.7563, Val Loss: 1.2876
Epoch 91, accuracy: 0.6030
Epoch 92, accuracy: 0.6030
Epoch 92, Train Loss: 0.7140, Val Loss: 1.2876
Epoch 93, accuracy: 0.6030
Epoch 94, accuracy: 0.6030
Epoch 94, Train Loss: 0.6543, Val Loss: 1.2876
Epoch 95, accuracy: 0.6030
Epoch 96, accuracy: 0.6030
Epoch 96, Train Loss: 0.7952, Val Loss: 1.2876
Epoch 97, accuracy: 0.6030
Epoch 98, accuracy: 0.6030
Epoch 98, Train Loss: 0.6978, Val Loss: 1.2876
Epoch 99, accuracy: 0.6030
Epoch 100, accuracy: 0.6030
Epoch 100, Train Loss: 0.6500, Val Loss: 1.2876
Epoch 101, accuracy: 0.6030
Epoch 102, accuracy: 0.6030
Epoch 102, Train Loss: 0.6987, Val Loss: 1.2876
Epoch 103, accuracy: 0.6030
Epoch 104, accuracy: 0.6030
Epoch 104, Train Loss: 0.7395, Val Loss: 1.2876
Epoch 105, accuracy: 0.6030
Epoch 106, accuracy: 0.6030
Epoch 106, Train Loss: 0.6393, Val Loss: 1.2876
Epoch 107, accuracy: 0.6030
Epoch 108, accuracy: 0.6030
Epoch 108, Train Loss: 0.7057, Val Loss: 1.2876
Epoch 109, accuracy: 0.6030
Epoch 110, accuracy: 0.6030
Epoch 110, Train Loss: 0.6859, Val Loss: 1.2876
Epoch 111, accuracy: 0.6030
Epoch 112, accuracy: 0.6030
Epoch 112, Train Loss: 0.7947, Val Loss: 1.2876
Epoch 113, accuracy: 0.6030
Epoch 114, accuracy: 0.6030
Epoch 114, Train Loss: 0.7023, Val Loss: 1.2876
Epoch 115, accuracy: 0.6030
Epoch 116, accuracy: 0.6030
Epoch 116, Train Loss: 0.7375, Val Loss: 1.2876
Epoch 117, accuracy: 0.6030
Epoch 118, accuracy: 0.6030
Epoch 118, Train Loss: 0.7824, Val Loss: 1.2876
Epoch 119, accuracy: 0.6030
Epoch 120, accuracy: 0.6030
Epoch 120, Train Loss: 0.6410, Val Loss: 1.2876
Epoch 121, accuracy: 0.6030
Epoch 122, accuracy: 0.6030
Epoch 122, Train Loss: 0.6758, Val Loss: 1.2876
Epoch 123, accuracy: 0.6030
Epoch 124, accuracy: 0.6030
Epoch 124, Train Loss: 0.7335, Val Loss: 1.2876
Epoch 125, accuracy: 0.6030
Epoch 126, accuracy: 0.6030
Epoch 126, Train Loss: 0.5881, Val Loss: 1.2876
Epoch 127, accuracy: 0.6030
Epoch 128, accuracy: 0.6030
Epoch 128, Train Loss: 0.6708, Val Loss: 1.2876
Epoch 129, accuracy: 0.6030
Epoch 130, accuracy: 0.6030
Epoch 130, Train Loss: 0.6636, Val Loss: 1.2876
Epoch 131, accuracy: 0.6030
Epoch 132, accuracy: 0.6030
Epoch 132, Train Loss: 0.7625, Val Loss: 1.2876
Epoch 133, accuracy: 0.6030
Epoch 134, accuracy: 0.6030
Epoch 134, Train Loss: 0.6902, Val Loss: 1.2876
Epoch 135, accuracy: 0.6030
Epoch 136, accuracy: 0.6030
Epoch 136, Train Loss: 0.6501, Val Loss: 1.2876
Epoch 137, accuracy: 0.6030
Epoch 138, accuracy: 0.6030
Epoch 138, Train Loss: 0.6856, Val Loss: 1.2876
Epoch 139, accuracy: 0.6030
Epoch 140, accuracy: 0.6030
Epoch 140, Train Loss: 0.6304, Val Loss: 1.2876
Epoch 141, accuracy: 0.6030
Epoch 142, accuracy: 0.6030
Epoch 142, Train Loss: 0.7772, Val Loss: 1.2876
Epoch 143, accuracy: 0.6030
Epoch 144, accuracy: 0.6030
Epoch 144, Train Loss: 0.6685, Val Loss: 1.2876
Epoch 145, accuracy: 0.6030
Epoch 146, accuracy: 0.6030
Epoch 146, Train Loss: 0.7084, Val Loss: 1.2876
Epoch 147, accuracy: 0.6030
Epoch 148, accuracy: 0.6030
Epoch 148, Train Loss: 0.7147, Val Loss: 1.2876
Epoch 149, accuracy: 0.6030
Epoch 150, accuracy: 0.6030
Epoch 150, Train Loss: 0.6723, Val Loss: 1.2876
Epoch 151, accuracy: 0.6030
Epoch 152, accuracy: 0.6030
Epoch 152, Train Loss: 0.6764, Val Loss: 1.2876
Epoch 153, accuracy: 0.6030
Epoch 154, accuracy: 0.6030
Epoch 154, Train Loss: 0.6376, Val Loss: 1.2876
Epoch 155, accuracy: 0.6030
Epoch 156, accuracy: 0.6030
Epoch 156, Train Loss: 0.6364, Val Loss: 1.2876
Epoch 157, accuracy: 0.6030
Epoch 158, accuracy: 0.6030
Epoch 158, Train Loss: 0.6553, Val Loss: 1.2876
Epoch 159, accuracy: 0.6030
Epoch 160, accuracy: 0.6030
Epoch 160, Train Loss: 0.5895, Val Loss: 1.2876
Epoch 161, accuracy: 0.6030
Epoch 162, accuracy: 0.6030
Epoch 162, Train Loss: 0.6823, Val Loss: 1.2876
Epoch 163, accuracy: 0.6030
Epoch 164, accuracy: 0.6030
Epoch 164, Train Loss: 0.7846, Val Loss: 1.2876
Epoch 165, accuracy: 0.6030
Epoch 166, accuracy: 0.6030
Epoch 166, Train Loss: 0.7047, Val Loss: 1.2876
Epoch 167, accuracy: 0.6030
Epoch 168, accuracy: 0.6030
Epoch 168, Train Loss: 0.7256, Val Loss: 1.2876
Epoch 169, accuracy: 0.6030
Epoch 170, accuracy: 0.6030
Epoch 170, Train Loss: 0.7905, Val Loss: 1.2876
Epoch 171, accuracy: 0.6030
Epoch 172, accuracy: 0.6030
Epoch 172, Train Loss: 0.7005, Val Loss: 1.2876
Epoch 173, accuracy: 0.6030
Epoch 174, accuracy: 0.6030
Epoch 174, Train Loss: 0.6735, Val Loss: 1.2876
Epoch 175, accuracy: 0.6030
Epoch 176, accuracy: 0.6030
Epoch 176, Train Loss: 0.5726, Val Loss: 1.2876
Epoch 177, accuracy: 0.6030
Epoch 178, accuracy: 0.6030
Epoch 178, Train Loss: 0.7229, Val Loss: 1.2876
Epoch 179, accuracy: 0.6030
Epoch 180, accuracy: 0.6030
Epoch 180, Train Loss: 0.6937, Val Loss: 1.2876
Epoch 181, accuracy: 0.6030
Epoch 182, accuracy: 0.6030
Epoch 182, Train Loss: 0.7639, Val Loss: 1.2876
Epoch 183, accuracy: 0.6030
Epoch 184, accuracy: 0.6030
Epoch 184, Train Loss: 0.6752, Val Loss: 1.2876
Epoch 185, accuracy: 0.6030
Epoch 186, accuracy: 0.6030
Epoch 186, Train Loss: 0.7396, Val Loss: 1.2876
Epoch 187, accuracy: 0.6030
Epoch 188, accuracy: 0.6030
Epoch 188, Train Loss: 0.7069, Val Loss: 1.2876
Epoch 189, accuracy: 0.6030
Epoch 190, accuracy: 0.6030
Epoch 190, Train Loss: 0.6910, Val Loss: 1.2876
Epoch 191, accuracy: 0.6030
Epoch 192, accuracy: 0.6030
Epoch 192, Train Loss: 0.6841, Val Loss: 1.2876
Epoch 193, accuracy: 0.6030
Epoch 194, accuracy: 0.6030
Epoch 194, Train Loss: 0.7012, Val Loss: 1.2876
Epoch 195, accuracy: 0.6030
Epoch 196, accuracy: 0.6030
Epoch 196, Train Loss: 0.7194, Val Loss: 1.2876
Epoch 197, accuracy: 0.6030
Epoch 198, accuracy: 0.6030
Epoch 198, Train Loss: 0.6552, Val Loss: 1.2876
Epoch 199, accuracy: 0.6030
Loaded best model with val_loss = 1.2845512628555298
test :accuracy 0.5970, f1_macro: 0.5332, f1_micro: 0.5970, auc: 0.8550
Final Results: {'mamba3_2_Citeseer': np.float64(0.639), 'mamba3_8_Citeseer': np.float64(0.649), 'mamba3_32_Citeseer': np.float64(0.597)} ['1485758_mamba3_0', '1983422_mamba3_0', '3974078_mamba3_0']
mamba3_2_Citeseer: Accuracy = 0.6217 ± 0.0283
mamba3_8_Citeseer: Accuracy = 0.6303 ± 0.0298
mamba3_32_Citeseer: Accuracy = 0.5450 ± 0.1097
========== Running baseline 1/3 ==========
Training mamba3 with 2 layers...
可训练参数: 560885_mamba3
不可训练参数: 0
batch size: (901, 901)
✅ Epoch 0: New best model saved with val_loss = 1.0775
Epoch 0, accuracy: 0.4756
Epoch 0, Train Loss: 1.2548, Val Loss: 1.0775
batch size: (898, 898)
Epoch 1, accuracy: 0.1685
batch size: (875, 875)
Epoch 2, accuracy: 0.1774
Epoch 2, Train Loss: 2.2052, Val Loss: 1.6186
batch size: (900, 900)
✅ Epoch 3: New best model saved with val_loss = 0.9125
Epoch 3, accuracy: 0.4035
batch size: (900, 900)
Epoch 4, accuracy: 0.5002
Epoch 4, Train Loss: 0.8424, Val Loss: 0.9690
batch size: (894, 894)
Epoch 5, accuracy: 0.5072
batch size: (932, 932)
✅ Epoch 6: New best model saved with val_loss = 0.8300
Epoch 6, accuracy: 0.4936
Epoch 6, Train Loss: 0.7524, Val Loss: 0.8300
batch size: (902, 902)
Epoch 7, accuracy: 0.4675
batch size: (893, 893)
Epoch 8, accuracy: 0.4583
Epoch 8, Train Loss: 0.6408, Val Loss: 0.9300
batch size: (907, 907)
Epoch 9, accuracy: 0.4787
batch size: (892, 892)
Epoch 10, accuracy: 0.5186
Epoch 10, Train Loss: 0.6485, Val Loss: 0.9002
batch size: (899, 899)
Epoch 11, accuracy: 0.5522
batch size: (893, 893)
✅ Epoch 12: New best model saved with val_loss = 0.7996
Epoch 12, accuracy: 0.5804
Epoch 12, Train Loss: 0.6210, Val Loss: 0.7996
batch size: (911, 911)
✅ Epoch 13: New best model saved with val_loss = 0.7516
Epoch 13, accuracy: 0.5844
batch size: (901, 901)
Epoch 14, accuracy: 0.5769
Epoch 14, Train Loss: 0.4686, Val Loss: 0.7559
batch size: (915, 915)
Epoch 15, accuracy: 0.5744
batch size: (904, 904)
Epoch 16, accuracy: 0.5638
Epoch 16, Train Loss: 0.4572, Val Loss: 0.7663
batch size: (905, 905)
Epoch 17, accuracy: 0.5525
batch size: (893, 893)
Epoch 18, accuracy: 0.5498
Epoch 18, Train Loss: 0.3868, Val Loss: 0.7757
batch size: (890, 890)
Epoch 19, accuracy: 0.5585
batch size: (888, 888)
Epoch 20, accuracy: 0.5621
Epoch 20, Train Loss: 0.3927, Val Loss: 0.7821
batch size: (893, 893)
Epoch 21, accuracy: 0.5628
batch size: (887, 887)
Epoch 22, accuracy: 0.5686
Epoch 22, Train Loss: 0.3973, Val Loss: 0.7901
batch size: (902, 902)
Epoch 23, accuracy: 0.5781
batch size: (899, 899)
Epoch 24, accuracy: 0.5836
Epoch 24, Train Loss: 0.3424, Val Loss: 0.7680
batch size: (883, 883)
✅ Epoch 25: New best model saved with val_loss = 0.7490
Epoch 25, accuracy: 0.5979
batch size: (890, 890)
Epoch 26, accuracy: 0.6014
Epoch 26, Train Loss: 0.4477, Val Loss: 0.7561
batch size: (894, 894)
✅ Epoch 27: New best model saved with val_loss = 0.7390
Epoch 27, accuracy: 0.6091
batch size: (902, 902)
✅ Epoch 28: New best model saved with val_loss = 0.7288
Epoch 28, accuracy: 0.6146
Epoch 28, Train Loss: 0.3268, Val Loss: 0.7288
batch size: (917, 917)
✅ Epoch 29: New best model saved with val_loss = 0.7107
Epoch 29, accuracy: 0.6228
batch size: (888, 888)
✅ Epoch 30: New best model saved with val_loss = 0.7058
Epoch 30, accuracy: 0.6244
Epoch 30, Train Loss: 0.3279, Val Loss: 0.7058
batch size: (892, 892)
✅ Epoch 31: New best model saved with val_loss = 0.6996
Epoch 31, accuracy: 0.6290
batch size: (911, 911)
Epoch 32, accuracy: 0.6370
Epoch 32, Train Loss: 0.3317, Val Loss: 0.7036
batch size: (900, 900)
Epoch 33, accuracy: 0.6394
batch size: (905, 905)
Epoch 34, accuracy: 0.6395
Epoch 34, Train Loss: 0.2743, Val Loss: 0.7001
batch size: (901, 901)
✅ Epoch 35: New best model saved with val_loss = 0.6981
Epoch 35, accuracy: 0.6438
batch size: (888, 888)
Epoch 36, accuracy: 0.6448
Epoch 36, Train Loss: 0.2647, Val Loss: 0.7061
batch size: (904, 904)
✅ Epoch 37: New best model saved with val_loss = 0.6936
Epoch 37, accuracy: 0.6421
batch size: (888, 888)
✅ Epoch 38: New best model saved with val_loss = 0.6787
Epoch 38, accuracy: 0.6482
Epoch 38, Train Loss: 0.2710, Val Loss: 0.6787
batch size: (913, 913)
Epoch 39, accuracy: 0.6499
batch size: (911, 911)
Epoch 40, accuracy: 0.6475
Epoch 40, Train Loss: 0.3396, Val Loss: 0.7086
batch size: (897, 897)
Epoch 41, accuracy: 0.6485
batch size: (902, 902)
Epoch 42, accuracy: 0.6473
Epoch 42, Train Loss: 0.4052, Val Loss: 0.6981
batch size: (897, 897)
Epoch 43, accuracy: 0.6464
batch size: (893, 893)
Epoch 44, accuracy: 0.6462
Epoch 44, Train Loss: 0.3114, Val Loss: 0.7068
batch size: (906, 906)
Epoch 45, accuracy: 0.6457
batch size: (904, 904)
Epoch 46, accuracy: 0.6451
Epoch 46, Train Loss: 0.3389, Val Loss: 0.6924
batch size: (918, 918)
Epoch 47, accuracy: 0.6482
batch size: (899, 899)
Epoch 48, accuracy: 0.6444
Epoch 48, Train Loss: 0.2847, Val Loss: 0.6921
batch size: (884, 884)
Epoch 49, accuracy: 0.6451
batch size: (919, 919)
Epoch 50, accuracy: 0.6475
Epoch 50, Train Loss: 0.3538, Val Loss: 0.6948
batch size: (883, 883)
Epoch 51, accuracy: 0.6481
batch size: (913, 913)
Epoch 52, accuracy: 0.6474
Epoch 52, Train Loss: 0.2942, Val Loss: 0.6928
batch size: (904, 904)
Epoch 53, accuracy: 0.6458
batch size: (900, 900)
Epoch 54, accuracy: 0.6478
Epoch 54, Train Loss: 0.3889, Val Loss: 0.7017
batch size: (897, 897)
Epoch 55, accuracy: 0.6472
batch size: (922, 922)
Epoch 56, accuracy: 0.6453
Epoch 56, Train Loss: 0.2812, Val Loss: 0.7095
batch size: (903, 903)
Epoch 57, accuracy: 0.6466
batch size: (894, 894)
Epoch 58, accuracy: 0.6480
Epoch 58, Train Loss: 0.3646, Val Loss: 0.7161
batch size: (881, 881)
Epoch 59, accuracy: 0.6449
batch size: (910, 910)
Epoch 60, accuracy: 0.6457
Epoch 60, Train Loss: 0.3849, Val Loss: 0.7037
batch size: (900, 900)
Epoch 61, accuracy: 0.6450
batch size: (885, 885)
Epoch 62, accuracy: 0.6457
Epoch 62, Train Loss: 0.3354, Val Loss: 0.7093
batch size: (895, 895)
Epoch 63, accuracy: 0.6451
batch size: (887, 887)
Epoch 64, accuracy: 0.6454
Epoch 64, Train Loss: 0.2994, Val Loss: 0.6951
batch size: (902, 902)
Epoch 65, accuracy: 0.6438
batch size: (916, 916)
Epoch 66, accuracy: 0.6464
Epoch 66, Train Loss: 0.2479, Val Loss: 0.6942
batch size: (903, 903)
Epoch 67, accuracy: 0.6476
batch size: (897, 897)
Epoch 68, accuracy: 0.6454
Epoch 68, Train Loss: 0.3142, Val Loss: 0.6973
batch size: (912, 912)
Epoch 69, accuracy: 0.6468
batch size: (903, 903)
Epoch 70, accuracy: 0.6458
Epoch 70, Train Loss: 0.2749, Val Loss: 0.6987
batch size: (889, 889)
Epoch 71, accuracy: 0.6473
batch size: (898, 898)
Epoch 72, accuracy: 0.6468
Epoch 72, Train Loss: 0.3968, Val Loss: 0.6984
batch size: (901, 901)
Epoch 73, accuracy: 0.6449
batch size: (912, 912)
Epoch 74, accuracy: 0.6439
Epoch 74, Train Loss: 0.2339, Val Loss: 0.7105
batch size: (887, 887)
Epoch 75, accuracy: 0.6466
batch size: (927, 927)
Epoch 76, accuracy: 0.6453
Epoch 76, Train Loss: 0.2942, Val Loss: 0.7181
batch size: (892, 892)
Epoch 77, accuracy: 0.6449
batch size: (905, 905)
Epoch 78, accuracy: 0.6453
Epoch 78, Train Loss: 0.3147, Val Loss: 0.7116
batch size: (886, 886)
Epoch 79, accuracy: 0.6452
batch size: (920, 920)
Epoch 80, accuracy: 0.6436
Epoch 80, Train Loss: 0.3516, Val Loss: 0.7106
batch size: (912, 912)
Epoch 81, accuracy: 0.6467
batch size: (922, 922)
Epoch 82, accuracy: 0.6437
Epoch 82, Train Loss: 0.3391, Val Loss: 0.7087
batch size: (895, 895)
Epoch 83, accuracy: 0.6457
batch size: (895, 895)
Epoch 84, accuracy: 0.6433
Epoch 84, Train Loss: 0.2129, Val Loss: 0.6976
batch size: (899, 899)
Epoch 85, accuracy: 0.6495
batch size: (890, 890)
Epoch 86, accuracy: 0.6479
Epoch 86, Train Loss: 0.3120, Val Loss: 0.7165
batch size: (896, 896)
Epoch 87, accuracy: 0.6468
batch size: (887, 887)
Epoch 88, accuracy: 0.6451
Epoch 88, Train Loss: 0.3338, Val Loss: 0.7055
batch size: (885, 885)
Epoch 89, accuracy: 0.6440
batch size: (895, 895)
Epoch 90, accuracy: 0.6482
Epoch 90, Train Loss: 0.3324, Val Loss: 0.7028
batch size: (897, 897)
Epoch 91, accuracy: 0.6436
batch size: (904, 904)
Epoch 92, accuracy: 0.6460
Epoch 92, Train Loss: 0.3080, Val Loss: 0.6816
batch size: (901, 901)
Epoch 93, accuracy: 0.6451
batch size: (910, 910)
Epoch 94, accuracy: 0.6464
Epoch 94, Train Loss: 0.3690, Val Loss: 0.7057
batch size: (898, 898)
Epoch 95, accuracy: 0.6453
batch size: (881, 881)
Epoch 96, accuracy: 0.6478
Epoch 96, Train Loss: 0.3894, Val Loss: 0.7042
batch size: (908, 908)
Epoch 97, accuracy: 0.6438
batch size: (923, 923)
Epoch 98, accuracy: 0.6469
Epoch 98, Train Loss: 0.3148, Val Loss: 0.7101
batch size: (910, 910)
Epoch 99, accuracy: 0.6458
batch size: (893, 893)
Epoch 100, accuracy: 0.6454
Epoch 100, Train Loss: 0.2438, Val Loss: 0.7074
batch size: (907, 907)
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 101, accuracy: 0.6447
batch size: (897, 897)
Epoch 102, accuracy: 0.6456
Epoch 102, Train Loss: 0.3337, Val Loss: 0.6920
batch size: (911, 911)
Epoch 103, accuracy: 0.6437
batch size: (905, 905)
Epoch 104, accuracy: 0.6484
Epoch 104, Train Loss: 0.2644, Val Loss: 0.6980
batch size: (909, 909)
Epoch 105, accuracy: 0.6439
batch size: (923, 923)
Epoch 106, accuracy: 0.6457
Epoch 106, Train Loss: 0.2861, Val Loss: 0.6979
batch size: (903, 903)
Epoch 107, accuracy: 0.6471
batch size: (920, 920)
Epoch 108, accuracy: 0.6467
Epoch 108, Train Loss: 0.3242, Val Loss: 0.6905
batch size: (905, 905)
Epoch 109, accuracy: 0.6498
batch size: (907, 907)
Epoch 110, accuracy: 0.6456
Epoch 110, Train Loss: 0.2764, Val Loss: 0.7077
batch size: (907, 907)
Epoch 111, accuracy: 0.6428
batch size: (899, 899)
Epoch 112, accuracy: 0.6467
Epoch 112, Train Loss: 0.3520, Val Loss: 0.7171
batch size: (903, 903)
Epoch 113, accuracy: 0.6460
batch size: (882, 882)
Epoch 114, accuracy: 0.6467
Epoch 114, Train Loss: 0.3632, Val Loss: 0.7156
batch size: (899, 899)
Epoch 115, accuracy: 0.6429
batch size: (898, 898)
Epoch 116, accuracy: 0.6474
Epoch 116, Train Loss: 0.3304, Val Loss: 0.6966
batch size: (894, 894)
Epoch 117, accuracy: 0.6438
batch size: (904, 904)
Epoch 118, accuracy: 0.6446
Epoch 118, Train Loss: 0.3762, Val Loss: 0.7070
batch size: (901, 901)
Epoch 119, accuracy: 0.6458
batch size: (906, 906)
Epoch 120, accuracy: 0.6470
Epoch 120, Train Loss: 0.3129, Val Loss: 0.6888
batch size: (876, 876)
Epoch 121, accuracy: 0.6457
batch size: (913, 913)
Epoch 122, accuracy: 0.6450
Epoch 122, Train Loss: 0.3684, Val Loss: 0.7116
batch size: (901, 901)
Epoch 123, accuracy: 0.6464
batch size: (911, 911)
Epoch 124, accuracy: 0.6469
Epoch 124, Train Loss: 0.4528, Val Loss: 0.7010
batch size: (894, 894)
Epoch 125, accuracy: 0.6454
batch size: (877, 877)
Epoch 126, accuracy: 0.6458
Epoch 126, Train Loss: 0.3469, Val Loss: 0.6916
batch size: (903, 903)
Epoch 127, accuracy: 0.6469
batch size: (893, 893)
Epoch 128, accuracy: 0.6470
Epoch 128, Train Loss: 0.3101, Val Loss: 0.7064
batch size: (895, 895)
Epoch 129, accuracy: 0.6448
batch size: (907, 907)
Epoch 130, accuracy: 0.6447
Epoch 130, Train Loss: 0.2779, Val Loss: 0.7061
batch size: (894, 894)
Epoch 131, accuracy: 0.6438
batch size: (907, 907)
Epoch 132, accuracy: 0.6425
Epoch 132, Train Loss: 0.2501, Val Loss: 0.6976
batch size: (889, 889)
Epoch 133, accuracy: 0.6456
batch size: (894, 894)
✅ Epoch 134: New best model saved with val_loss = 0.6693
Epoch 134, accuracy: 0.6472
Epoch 134, Train Loss: 0.2330, Val Loss: 0.6693
batch size: (891, 891)
Epoch 135, accuracy: 0.6488
batch size: (890, 890)
Epoch 136, accuracy: 0.6454
Epoch 136, Train Loss: 0.3578, Val Loss: 0.7025
batch size: (901, 901)
Epoch 137, accuracy: 0.6488
batch size: (885, 885)
Epoch 138, accuracy: 0.6455
Epoch 138, Train Loss: 0.3603, Val Loss: 0.7064
batch size: (894, 894)
Epoch 139, accuracy: 0.6447
batch size: (898, 898)
Epoch 140, accuracy: 0.6468
Epoch 140, Train Loss: 0.2935, Val Loss: 0.7058
batch size: (893, 893)
Epoch 141, accuracy: 0.6444
batch size: (897, 897)
Epoch 142, accuracy: 0.6458
Epoch 142, Train Loss: 0.3188, Val Loss: 0.6949
batch size: (896, 896)
Epoch 143, accuracy: 0.6467
batch size: (890, 890)
Epoch 144, accuracy: 0.6458
Epoch 144, Train Loss: 0.3144, Val Loss: 0.6884
batch size: (913, 913)
Epoch 145, accuracy: 0.6470
batch size: (917, 917)
Epoch 146, accuracy: 0.6462
Epoch 146, Train Loss: 0.3254, Val Loss: 0.6952
batch size: (914, 914)
Epoch 147, accuracy: 0.6469
batch size: (899, 899)
Epoch 148, accuracy: 0.6451
Epoch 148, Train Loss: 0.2928, Val Loss: 0.7035
batch size: (893, 893)
Epoch 149, accuracy: 0.6463
batch size: (893, 893)
Epoch 150, accuracy: 0.6458
Epoch 150, Train Loss: 0.2973, Val Loss: 0.6926
batch size: (913, 913)
Epoch 151, accuracy: 0.6468
batch size: (908, 908)
Epoch 152, accuracy: 0.6441
Epoch 152, Train Loss: 0.3671, Val Loss: 0.7090
batch size: (892, 892)
Epoch 153, accuracy: 0.6453
batch size: (896, 896)
Epoch 154, accuracy: 0.6463
Epoch 154, Train Loss: 0.2378, Val Loss: 0.6989
batch size: (907, 907)
Epoch 155, accuracy: 0.6459
batch size: (917, 917)
Epoch 156, accuracy: 0.6472
Epoch 156, Train Loss: 0.2603, Val Loss: 0.7096
batch size: (892, 892)
Epoch 157, accuracy: 0.6469
batch size: (896, 896)
Epoch 158, accuracy: 0.6455
Epoch 158, Train Loss: 0.3422, Val Loss: 0.6952
batch size: (900, 900)
Epoch 159, accuracy: 0.6459
batch size: (904, 904)
Epoch 160, accuracy: 0.6449
Epoch 160, Train Loss: 0.3212, Val Loss: 0.7132
batch size: (901, 901)
Epoch 161, accuracy: 0.6468
batch size: (913, 913)
Epoch 162, accuracy: 0.6431
Epoch 162, Train Loss: 0.3294, Val Loss: 0.7096
batch size: (899, 899)
Epoch 163, accuracy: 0.6444
batch size: (901, 901)
Epoch 164, accuracy: 0.6494
Epoch 164, Train Loss: 0.3865, Val Loss: 0.7049
batch size: (893, 893)
Epoch 165, accuracy: 0.6436
batch size: (898, 898)
Epoch 166, accuracy: 0.6452
Epoch 166, Train Loss: 0.3261, Val Loss: 0.7229
batch size: (895, 895)
Epoch 167, accuracy: 0.6448
batch size: (901, 901)
Epoch 168, accuracy: 0.6471
Epoch 168, Train Loss: 0.2378, Val Loss: 0.7046
batch size: (899, 899)
Epoch 169, accuracy: 0.6441
batch size: (897, 897)
Epoch 170, accuracy: 0.6469
Epoch 170, Train Loss: 0.2006, Val Loss: 0.7016
batch size: (895, 895)
Epoch 171, accuracy: 0.6457
batch size: (893, 893)
Epoch 172, accuracy: 0.6434
Epoch 172, Train Loss: 0.3376, Val Loss: 0.6964
batch size: (932, 932)
Epoch 173, accuracy: 0.6463
batch size: (886, 886)
Epoch 174, accuracy: 0.6456
Epoch 174, Train Loss: 0.3850, Val Loss: 0.6963
batch size: (897, 897)
Epoch 175, accuracy: 0.6466
batch size: (907, 907)
Epoch 176, accuracy: 0.6462
Epoch 176, Train Loss: 0.3839, Val Loss: 0.6992
batch size: (884, 884)
Epoch 177, accuracy: 0.6480
batch size: (901, 901)
Epoch 178, accuracy: 0.6412
Epoch 178, Train Loss: 0.2661, Val Loss: 0.6928
batch size: (910, 910)
Epoch 179, accuracy: 0.6454
batch size: (899, 899)
Epoch 180, accuracy: 0.6453
Epoch 180, Train Loss: 0.2819, Val Loss: 0.7102
batch size: (886, 886)
Epoch 181, accuracy: 0.6483
batch size: (905, 905)
Epoch 182, accuracy: 0.6458
Epoch 182, Train Loss: 0.2916, Val Loss: 0.7052
batch size: (881, 881)
Epoch 183, accuracy: 0.6464
batch size: (912, 912)
Epoch 184, accuracy: 0.6468
Epoch 184, Train Loss: 0.2573, Val Loss: 0.6955
batch size: (899, 899)
Epoch 185, accuracy: 0.6497
batch size: (899, 899)
Epoch 186, accuracy: 0.6467
Epoch 186, Train Loss: 0.3281, Val Loss: 0.7106
batch size: (911, 911)
Epoch 187, accuracy: 0.6483
batch size: (896, 896)
Epoch 188, accuracy: 0.6460
Epoch 188, Train Loss: 0.3332, Val Loss: 0.7116
batch size: (892, 892)
Epoch 189, accuracy: 0.6442
batch size: (926, 926)
Epoch 190, accuracy: 0.6475
Epoch 190, Train Loss: 0.2643, Val Loss: 0.7210
batch size: (899, 899)
Epoch 191, accuracy: 0.6471
batch size: (885, 885)
Epoch 192, accuracy: 0.6443
Epoch 192, Train Loss: 0.4004, Val Loss: 0.6977
batch size: (875, 875)
Epoch 193, accuracy: 0.6468
batch size: (915, 915)
Epoch 194, accuracy: 0.6470
Epoch 194, Train Loss: 0.2350, Val Loss: 0.7074
batch size: (906, 906)
Epoch 195, accuracy: 0.6456
batch size: (902, 902)
Epoch 196, accuracy: 0.6451
Epoch 196, Train Loss: 0.3221, Val Loss: 0.7100
batch size: (910, 910)
Epoch 197, accuracy: 0.6479
batch size: (907, 907)
Epoch 198, accuracy: 0.6456
Epoch 198, Train Loss: 0.2618, Val Loss: 0.6974
batch size: (897, 897)
Epoch 199, accuracy: 0.6486
Loaded best model with val_loss = 0.6693045496940613
test :accuracy 0.6456, f1_macro: 0.6420, f1_micro: 0.6456, auc: 0.8383
Training mamba3 with 8 layers...
可训练参数: 1056245_mamba3
不可训练参数: 0
batch size: (891, 891)
✅ Epoch 0: New best model saved with val_loss = 1.0124
Epoch 0, accuracy: 0.4679
Epoch 0, Train Loss: 1.0825, Val Loss: 1.0124
batch size: (903, 903)
Epoch 1, accuracy: 0.1729
batch size: (908, 908)
Epoch 2, accuracy: 0.1681
Epoch 2, Train Loss: 1.4554, Val Loss: 1.6156
batch size: (894, 894)
Epoch 3, accuracy: 0.3142
batch size: (904, 904)
Epoch 4, accuracy: 0.4268
Epoch 4, Train Loss: 0.9927, Val Loss: 1.0206
batch size: (891, 891)
✅ Epoch 5: New best model saved with val_loss = 0.9034
Epoch 5, accuracy: 0.4871
batch size: (914, 914)
Epoch 6, accuracy: 0.5007
Epoch 6, Train Loss: 0.9107, Val Loss: 0.9257
batch size: (908, 908)
Epoch 7, accuracy: 0.4373
batch size: (898, 898)
Epoch 8, accuracy: 0.3996
Epoch 8, Train Loss: 0.9106, Val Loss: 0.9930
batch size: (906, 906)
Epoch 9, accuracy: 0.4020
batch size: (902, 902)
✅ Epoch 10: New best model saved with val_loss = 0.8776
Epoch 10, accuracy: 0.4278
Epoch 10, Train Loss: 0.8465, Val Loss: 0.8776
batch size: (895, 895)
✅ Epoch 11: New best model saved with val_loss = 0.8241
Epoch 11, accuracy: 0.4500
batch size: (886, 886)
✅ Epoch 12: New best model saved with val_loss = 0.8096
Epoch 12, accuracy: 0.4675
Epoch 12, Train Loss: 0.7040, Val Loss: 0.8096
batch size: (900, 900)
Epoch 13, accuracy: 0.4777
batch size: (889, 889)
Epoch 14, accuracy: 0.5113
Epoch 14, Train Loss: 0.6888, Val Loss: 0.8218
batch size: (895, 895)
✅ Epoch 15: New best model saved with val_loss = 0.7687
Epoch 15, accuracy: 0.5737
batch size: (916, 916)
✅ Epoch 16: New best model saved with val_loss = 0.7056
Epoch 16, accuracy: 0.6398
Epoch 16, Train Loss: 0.6029, Val Loss: 0.7056
batch size: (913, 913)
✅ Epoch 17: New best model saved with val_loss = 0.6510
Epoch 17, accuracy: 0.6638
batch size: (885, 885)
✅ Epoch 18: New best model saved with val_loss = 0.6435
Epoch 18, accuracy: 0.6640
Epoch 18, Train Loss: 0.4808, Val Loss: 0.6435
batch size: (891, 891)
✅ Epoch 19: New best model saved with val_loss = 0.6337
Epoch 19, accuracy: 0.6516
batch size: (898, 898)
Epoch 20, accuracy: 0.6170
Epoch 20, Train Loss: 0.2916, Val Loss: 0.6483
batch size: (887, 887)
Epoch 21, accuracy: 0.5956
batch size: (906, 906)
Epoch 22, accuracy: 0.5708
Epoch 22, Train Loss: 0.3588, Val Loss: 0.7261
batch size: (880, 880)
Epoch 23, accuracy: 0.5731
batch size: (896, 896)
Epoch 24, accuracy: 0.6146
Epoch 24, Train Loss: 0.3612, Val Loss: 0.7246
batch size: (885, 885)
Epoch 25, accuracy: 0.6469
batch size: (903, 903)
Epoch 26, accuracy: 0.6438
Epoch 26, Train Loss: 0.3493, Val Loss: 0.7328
batch size: (883, 883)
✅ Epoch 27: New best model saved with val_loss = 0.6179
Epoch 27, accuracy: 0.6454
batch size: (883, 883)
Epoch 28, accuracy: 0.6432
Epoch 28, Train Loss: 0.3147, Val Loss: 0.6528
batch size: (886, 886)
Epoch 29, accuracy: 0.6424
batch size: (902, 902)
Epoch 30, accuracy: 0.6385
Epoch 30, Train Loss: 0.2590, Val Loss: 0.6558
batch size: (894, 894)
Epoch 31, accuracy: 0.6365
batch size: (902, 902)
Epoch 32, accuracy: 0.6351
Epoch 32, Train Loss: 0.3207, Val Loss: 0.6328
batch size: (913, 913)
Epoch 33, accuracy: 0.6343
batch size: (895, 895)
Epoch 34, accuracy: 0.6291
Epoch 34, Train Loss: 0.2817, Val Loss: 0.6624
batch size: (901, 901)
Epoch 35, accuracy: 0.6345
batch size: (899, 899)
Epoch 36, accuracy: 0.6311
Epoch 36, Train Loss: 0.2594, Val Loss: 0.6570
batch size: (894, 894)
Epoch 37, accuracy: 0.6323
batch size: (905, 905)
Epoch 38, accuracy: 0.6308
Epoch 38, Train Loss: 0.3930, Val Loss: 0.6557
batch size: (911, 911)
Epoch 39, accuracy: 0.6300
batch size: (897, 897)
Epoch 40, accuracy: 0.6294
Epoch 40, Train Loss: 0.3540, Val Loss: 0.6642
batch size: (898, 898)
Epoch 41, accuracy: 0.6329
batch size: (886, 886)
Epoch 42, accuracy: 0.6310
Epoch 42, Train Loss: 0.2648, Val Loss: 0.7050
batch size: (886, 886)
Epoch 43, accuracy: 0.6320
batch size: (897, 897)
Epoch 44, accuracy: 0.6297
Epoch 44, Train Loss: 0.2292, Val Loss: 0.6894
batch size: (888, 888)
Epoch 45, accuracy: 0.6331
batch size: (893, 893)
Epoch 46, accuracy: 0.6323
Epoch 46, Train Loss: 0.2813, Val Loss: 0.6360
batch size: (908, 908)
Epoch 47, accuracy: 0.6308
batch size: (889, 889)
Epoch 48, accuracy: 0.6315
Epoch 48, Train Loss: 0.3440, Val Loss: 0.6519
batch size: (900, 900)
Epoch 49, accuracy: 0.6304
batch size: (885, 885)
Epoch 50, accuracy: 0.6277
Epoch 50, Train Loss: 0.2937, Val Loss: 0.6774
batch size: (895, 895)
Epoch 51, accuracy: 0.6338
batch size: (915, 915)
Epoch 52, accuracy: 0.6324
Epoch 52, Train Loss: 0.2009, Val Loss: 0.6386
batch size: (900, 900)
Epoch 53, accuracy: 0.6295
batch size: (892, 892)
✅ Epoch 54: New best model saved with val_loss = 0.6076
Epoch 54, accuracy: 0.6338
Epoch 54, Train Loss: 0.3618, Val Loss: 0.6076
batch size: (903, 903)
Epoch 55, accuracy: 0.6316
batch size: (893, 893)
Epoch 56, accuracy: 0.6319
Epoch 56, Train Loss: 0.3353, Val Loss: 0.6247
batch size: (917, 917)
Epoch 57, accuracy: 0.6292
batch size: (893, 893)
Epoch 58, accuracy: 0.6318
Epoch 58, Train Loss: 0.2403, Val Loss: 0.6586
batch size: (907, 907)
Epoch 59, accuracy: 0.6310
batch size: (917, 917)
Epoch 60, accuracy: 0.6306
Epoch 60, Train Loss: 0.3358, Val Loss: 0.6367
batch size: (890, 890)
Epoch 61, accuracy: 0.6315
batch size: (902, 902)
Epoch 62, accuracy: 0.6320
Epoch 62, Train Loss: 0.3621, Val Loss: 0.6760
batch size: (910, 910)
Epoch 63, accuracy: 0.6311
batch size: (907, 907)
Epoch 64, accuracy: 0.6301
Epoch 64, Train Loss: 0.3244, Val Loss: 0.6583
batch size: (884, 884)
Epoch 65, accuracy: 0.6315
batch size: (906, 906)
Epoch 66, accuracy: 0.6344
Epoch 66, Train Loss: 0.2241, Val Loss: 0.6617
batch size: (930, 930)
Epoch 67, accuracy: 0.6324
batch size: (911, 911)
Epoch 68, accuracy: 0.6290
Epoch 68, Train Loss: 0.2810, Val Loss: 0.6690
batch size: (881, 881)
Epoch 69, accuracy: 0.6323
batch size: (900, 900)
Epoch 70, accuracy: 0.6317
Epoch 70, Train Loss: 0.2585, Val Loss: 0.6481
batch size: (886, 886)
Epoch 71, accuracy: 0.6300
batch size: (908, 908)
Epoch 72, accuracy: 0.6299
Epoch 72, Train Loss: 0.2773, Val Loss: 0.6386
batch size: (891, 891)
Epoch 73, accuracy: 0.6284
batch size: (901, 901)
Epoch 74, accuracy: 0.6312
Epoch 74, Train Loss: 0.2797, Val Loss: 0.6331
batch size: (906, 906)
Epoch 75, accuracy: 0.6299
batch size: (899, 899)
Epoch 76, accuracy: 0.6310
Epoch 76, Train Loss: 0.2689, Val Loss: 0.6494
batch size: (896, 896)
Epoch 77, accuracy: 0.6302
batch size: (896, 896)
Epoch 78, accuracy: 0.6299
Epoch 78, Train Loss: 0.3031, Val Loss: 0.6644
batch size: (917, 917)
Epoch 79, accuracy: 0.6327
batch size: (893, 893)
Epoch 80, accuracy: 0.6309
Epoch 80, Train Loss: 0.1591, Val Loss: 0.6197
batch size: (888, 888)
Epoch 81, accuracy: 0.6300
batch size: (887, 887)
Epoch 82, accuracy: 0.6320
Epoch 82, Train Loss: 0.3060, Val Loss: 0.6707
batch size: (887, 887)
Epoch 83, accuracy: 0.6302
batch size: (906, 906)
Epoch 84, accuracy: 0.6306
Epoch 84, Train Loss: 0.3149, Val Loss: 0.6540
batch size: (895, 895)
Epoch 85, accuracy: 0.6295
batch size: (914, 914)
Epoch 86, accuracy: 0.6321
Epoch 86, Train Loss: 0.4841, Val Loss: 0.6751
batch size: (894, 894)
Epoch 87, accuracy: 0.6308
batch size: (904, 904)
Epoch 88, accuracy: 0.6293
Epoch 88, Train Loss: 0.2821, Val Loss: 0.6470
batch size: (902, 902)
Epoch 89, accuracy: 0.6333
batch size: (917, 917)
Epoch 90, accuracy: 0.6291
Epoch 90, Train Loss: 0.2650, Val Loss: 0.6493
batch size: (896, 896)
Epoch 91, accuracy: 0.6334
batch size: (883, 883)
Epoch 92, accuracy: 0.6325
Epoch 92, Train Loss: 0.2464, Val Loss: 0.6591
batch size: (901, 901)
Epoch 93, accuracy: 0.6320
batch size: (910, 910)
Epoch 94, accuracy: 0.6302
Epoch 94, Train Loss: 0.2439, Val Loss: 0.6279
batch size: (905, 905)
Epoch 95, accuracy: 0.6307
batch size: (917, 917)
✅ Epoch 96: New best model saved with val_loss = 0.6064
Epoch 96, accuracy: 0.6304
Epoch 96, Train Loss: 0.3351, Val Loss: 0.6064
batch size: (908, 908)
Epoch 97, accuracy: 0.6297
batch size: (905, 905)
Epoch 98, accuracy: 0.6314
Epoch 98, Train Loss: 0.3487, Val Loss: 0.6745
batch size: (896, 896)
Epoch 99, accuracy: 0.6308
batch size: (887, 887)
Epoch 100, accuracy: 0.6324
Epoch 100, Train Loss: 0.2786, Val Loss: 0.6423
batch size: (895, 895)
Epoch 101, accuracy: 0.6315
batch size: (883, 883)
Epoch 102, accuracy: 0.6302
Epoch 102, Train Loss: 0.3765, Val Loss: 0.6673
batch size: (886, 886)
Epoch 103, accuracy: 0.6297
batch size: (900, 900)
Epoch 104, accuracy: 0.6318
Epoch 104, Train Loss: 0.3074, Val Loss: 0.6508
batch size: (902, 902)
Epoch 105, accuracy: 0.6294
batch size: (923, 923)
Epoch 106, accuracy: 0.6312
Epoch 106, Train Loss: 0.2915, Val Loss: 0.6518
batch size: (893, 893)
Epoch 107, accuracy: 0.6321
batch size: (906, 906)
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 108, accuracy: 0.6308
Epoch 108, Train Loss: 0.3024, Val Loss: 0.6694
batch size: (894, 894)
Epoch 109, accuracy: 0.6293
batch size: (898, 898)
Epoch 110, accuracy: 0.6311
Epoch 110, Train Loss: 0.2867, Val Loss: 0.6475
batch size: (910, 910)
Epoch 111, accuracy: 0.6294
batch size: (888, 888)
Epoch 112, accuracy: 0.6307
Epoch 112, Train Loss: 0.3106, Val Loss: 0.6489
batch size: (895, 895)
Epoch 113, accuracy: 0.6304
batch size: (883, 883)
Epoch 114, accuracy: 0.6326
Epoch 114, Train Loss: 0.3704, Val Loss: 0.6441
batch size: (914, 914)
Epoch 115, accuracy: 0.6306
batch size: (906, 906)
Epoch 116, accuracy: 0.6303
Epoch 116, Train Loss: 0.3102, Val Loss: 0.6782
batch size: (920, 920)
Epoch 117, accuracy: 0.6318
batch size: (907, 907)
Epoch 118, accuracy: 0.6311
Epoch 118, Train Loss: 0.4026, Val Loss: 0.6457
batch size: (899, 899)
Epoch 119, accuracy: 0.6296
batch size: (887, 887)
Epoch 120, accuracy: 0.6325
Epoch 120, Train Loss: 0.2362, Val Loss: 0.6574
batch size: (883, 883)
Epoch 121, accuracy: 0.6313
batch size: (893, 893)
Epoch 122, accuracy: 0.6316
Epoch 122, Train Loss: 0.3449, Val Loss: 0.6487
batch size: (926, 926)
Epoch 123, accuracy: 0.6321
batch size: (889, 889)
Epoch 124, accuracy: 0.6301
Epoch 124, Train Loss: 0.2664, Val Loss: 0.6599
batch size: (909, 909)
Epoch 125, accuracy: 0.6304
batch size: (881, 881)
Epoch 126, accuracy: 0.6309
Epoch 126, Train Loss: 0.2423, Val Loss: 0.6832
batch size: (892, 892)
Epoch 127, accuracy: 0.6312
batch size: (906, 906)
Epoch 128, accuracy: 0.6277
Epoch 128, Train Loss: 0.2496, Val Loss: 0.6382
batch size: (904, 904)
Epoch 129, accuracy: 0.6291
batch size: (906, 906)
Epoch 130, accuracy: 0.6294
Epoch 130, Train Loss: 0.2112, Val Loss: 0.6635
batch size: (916, 916)
Epoch 131, accuracy: 0.6316
batch size: (921, 921)
Epoch 132, accuracy: 0.6301
Epoch 132, Train Loss: 0.3414, Val Loss: 0.6583
batch size: (917, 917)
Epoch 133, accuracy: 0.6298
batch size: (882, 882)
Epoch 134, accuracy: 0.6316
Epoch 134, Train Loss: 0.3656, Val Loss: 0.6687
batch size: (903, 903)
Epoch 135, accuracy: 0.6316
batch size: (895, 895)
Epoch 136, accuracy: 0.6308
Epoch 136, Train Loss: 0.2855, Val Loss: 0.6755
batch size: (908, 908)
Epoch 137, accuracy: 0.6274
batch size: (892, 892)
Epoch 138, accuracy: 0.6296
Epoch 138, Train Loss: 0.2571, Val Loss: 0.6473
batch size: (883, 883)
Epoch 139, accuracy: 0.6297
batch size: (902, 902)
Epoch 140, accuracy: 0.6311
Epoch 140, Train Loss: 0.3336, Val Loss: 0.6598
batch size: (898, 898)
Epoch 141, accuracy: 0.6302
batch size: (876, 876)
Epoch 142, accuracy: 0.6320
Epoch 142, Train Loss: 0.2674, Val Loss: 0.6738
batch size: (886, 886)
Epoch 143, accuracy: 0.6316
batch size: (896, 896)
Epoch 144, accuracy: 0.6310
Epoch 144, Train Loss: 0.3933, Val Loss: 0.6315
batch size: (910, 910)
Epoch 145, accuracy: 0.6325
batch size: (901, 901)
Epoch 146, accuracy: 0.6328
Epoch 146, Train Loss: 0.3322, Val Loss: 0.6787
batch size: (902, 902)
Epoch 147, accuracy: 0.6297
batch size: (899, 899)
Epoch 148, accuracy: 0.6281
Epoch 148, Train Loss: 0.2534, Val Loss: 0.6293
batch size: (893, 893)
Epoch 149, accuracy: 0.6319
batch size: (894, 894)
Epoch 150, accuracy: 0.6320
Epoch 150, Train Loss: 0.3706, Val Loss: 0.6754
batch size: (908, 908)
Epoch 151, accuracy: 0.6317
batch size: (909, 909)
Epoch 152, accuracy: 0.6316
Epoch 152, Train Loss: 0.3312, Val Loss: 0.6464
batch size: (895, 895)
Epoch 153, accuracy: 0.6317
batch size: (906, 906)
Epoch 154, accuracy: 0.6300
Epoch 154, Train Loss: 0.3082, Val Loss: 0.6269
batch size: (909, 909)
Epoch 155, accuracy: 0.6308
batch size: (893, 893)
Epoch 156, accuracy: 0.6300
Epoch 156, Train Loss: 0.2407, Val Loss: 0.6714
batch size: (909, 909)
Epoch 157, accuracy: 0.6304
batch size: (920, 920)
Epoch 158, accuracy: 0.6300
Epoch 158, Train Loss: 0.3113, Val Loss: 0.6433
batch size: (896, 896)
Epoch 159, accuracy: 0.6308
batch size: (904, 904)
Epoch 160, accuracy: 0.6323
Epoch 160, Train Loss: 0.2738, Val Loss: 0.6504
batch size: (909, 909)
Epoch 161, accuracy: 0.6298
batch size: (903, 903)
Epoch 162, accuracy: 0.6301
Epoch 162, Train Loss: 0.4045, Val Loss: 0.6609
batch size: (904, 904)
Epoch 163, accuracy: 0.6308
batch size: (882, 882)
Epoch 164, accuracy: 0.6325
Epoch 164, Train Loss: 0.2739, Val Loss: 0.7086
batch size: (906, 906)
Epoch 165, accuracy: 0.6296
batch size: (901, 901)
Epoch 166, accuracy: 0.6319
Epoch 166, Train Loss: 0.3014, Val Loss: 0.6607
batch size: (907, 907)
Epoch 167, accuracy: 0.6310
batch size: (900, 900)
Epoch 168, accuracy: 0.6304
Epoch 168, Train Loss: 0.2626, Val Loss: 0.6318
batch size: (896, 896)
Epoch 169, accuracy: 0.6301
batch size: (897, 897)
Epoch 170, accuracy: 0.6291
Epoch 170, Train Loss: 0.3632, Val Loss: 0.6742
batch size: (896, 896)
Epoch 171, accuracy: 0.6290
batch size: (900, 900)
Epoch 172, accuracy: 0.6293
Epoch 172, Train Loss: 0.2767, Val Loss: 0.6716
batch size: (917, 917)
Epoch 173, accuracy: 0.6307
batch size: (894, 894)
Epoch 174, accuracy: 0.6306
Epoch 174, Train Loss: 0.2173, Val Loss: 0.6240
batch size: (908, 908)
Epoch 175, accuracy: 0.6303
batch size: (890, 890)
Epoch 176, accuracy: 0.6286
Epoch 176, Train Loss: 0.2780, Val Loss: 0.6527
batch size: (897, 897)
Epoch 177, accuracy: 0.6297
batch size: (902, 902)
Epoch 178, accuracy: 0.6285
Epoch 178, Train Loss: 0.2271, Val Loss: 0.6315
batch size: (910, 910)
Epoch 179, accuracy: 0.6313
batch size: (911, 911)
Epoch 180, accuracy: 0.6280
Epoch 180, Train Loss: 0.2459, Val Loss: 0.6874
batch size: (901, 901)
Epoch 181, accuracy: 0.6312
batch size: (891, 891)
Epoch 182, accuracy: 0.6286
Epoch 182, Train Loss: 0.3246, Val Loss: 0.6658
batch size: (884, 884)
Epoch 183, accuracy: 0.6290
batch size: (910, 910)
Epoch 184, accuracy: 0.6298
Epoch 184, Train Loss: 0.2753, Val Loss: 0.6526
batch size: (908, 908)
Epoch 185, accuracy: 0.6326
batch size: (893, 893)
Epoch 186, accuracy: 0.6318
Epoch 186, Train Loss: 0.2769, Val Loss: 0.6575
batch size: (906, 906)
Epoch 187, accuracy: 0.6311
batch size: (895, 895)
Epoch 188, accuracy: 0.6321
Epoch 188, Train Loss: 0.3631, Val Loss: 0.6504
batch size: (891, 891)
Epoch 189, accuracy: 0.6284
batch size: (897, 897)
Epoch 190, accuracy: 0.6329
Epoch 190, Train Loss: 0.3516, Val Loss: 0.6667
batch size: (898, 898)
Epoch 191, accuracy: 0.6312
batch size: (895, 895)
Epoch 192, accuracy: 0.6338
Epoch 192, Train Loss: 0.3551, Val Loss: 0.6602
batch size: (896, 896)
Epoch 193, accuracy: 0.6294
batch size: (893, 893)
Epoch 194, accuracy: 0.6330
Epoch 194, Train Loss: 0.1974, Val Loss: 0.6449
batch size: (907, 907)
Epoch 195, accuracy: 0.6308
batch size: (905, 905)
Epoch 196, accuracy: 0.6290
Epoch 196, Train Loss: 0.3426, Val Loss: 0.6855
batch size: (891, 891)
Epoch 197, accuracy: 0.6277
batch size: (909, 909)
Epoch 198, accuracy: 0.6304
Epoch 198, Train Loss: 0.3536, Val Loss: 0.6350
batch size: (890, 890)
Epoch 199, accuracy: 0.6300
Loaded best model with val_loss = 0.6063700914382935
test :accuracy 0.6295, f1_macro: 0.6274, f1_micro: 0.6295, auc: 0.8458
Training mamba3 with 32 layers...
可训练参数: 3037685_mamba3
不可训练参数: 0
batch size: (900, 900)
✅ Epoch 0: New best model saved with val_loss = 1.2591
Epoch 0, accuracy: 0.2584
Epoch 0, Train Loss: 1559.9116, Val Loss: 1.2591
batch size: (903, 903)
Epoch 1, accuracy: 0.4439
batch size: (904, 904)
✅ Epoch 2: New best model saved with val_loss = 0.7592
Epoch 2, accuracy: 0.4355
Epoch 2, Train Loss: 1.5318, Val Loss: 0.7592
batch size: (921, 921)
Epoch 3, accuracy: 0.2504
batch size: (899, 899)
Epoch 4, accuracy: 0.4070
Epoch 4, Train Loss: 0.9640, Val Loss: 0.8571
batch size: (915, 915)
✅ Epoch 5: New best model saved with val_loss = 0.7020
Epoch 5, accuracy: 0.5817
batch size: (890, 890)
Epoch 6, accuracy: 0.6103
Epoch 6, Train Loss: 0.6836, Val Loss: 0.7724
batch size: (900, 900)
Epoch 7, accuracy: 0.6132
batch size: (902, 902)
✅ Epoch 8: New best model saved with val_loss = 0.6859
Epoch 8, accuracy: 0.6089
Epoch 8, Train Loss: 0.6301, Val Loss: 0.6859
batch size: (908, 908)
✅ Epoch 9: New best model saved with val_loss = 0.6642
Epoch 9, accuracy: 0.5831
batch size: (905, 905)
Epoch 10, accuracy: 0.5480
Epoch 10, Train Loss: 0.5375, Val Loss: 0.6812
batch size: (907, 907)
Epoch 11, accuracy: 0.5393
batch size: (887, 887)
Epoch 12, accuracy: 0.5806
Epoch 12, Train Loss: 0.7451, Val Loss: 0.6796
batch size: (904, 904)
✅ Epoch 13: New best model saved with val_loss = 0.6576
Epoch 13, accuracy: 0.6212
batch size: (911, 911)
✅ Epoch 14: New best model saved with val_loss = 0.6346
Epoch 14, accuracy: 0.6563
Epoch 14, Train Loss: 0.4689, Val Loss: 0.6346
batch size: (902, 902)
✅ Epoch 15: New best model saved with val_loss = 0.6273
Epoch 15, accuracy: 0.6685
batch size: (880, 880)
Epoch 16, accuracy: 0.6670
Epoch 16, Train Loss: 0.4203, Val Loss: 0.6350
batch size: (895, 895)
Epoch 17, accuracy: 0.6672
batch size: (923, 923)
Epoch 18, accuracy: 0.6618
Epoch 18, Train Loss: 0.4346, Val Loss: 0.6546
batch size: (902, 902)
Epoch 19, accuracy: 0.6619
batch size: (912, 912)
Epoch 20, accuracy: 0.6629
Epoch 20, Train Loss: 0.4785, Val Loss: 0.6757
batch size: (881, 881)
Epoch 21, accuracy: 0.6546
batch size: (886, 886)
Epoch 22, accuracy: 0.6536
Epoch 22, Train Loss: 0.3577, Val Loss: 0.6902
batch size: (902, 902)
Epoch 23, accuracy: 0.6536
batch size: (883, 883)
Epoch 24, accuracy: 0.6530
Epoch 24, Train Loss: 0.4653, Val Loss: 0.6681
batch size: (900, 900)
Epoch 25, accuracy: 0.6551
batch size: (900, 900)
Epoch 26, accuracy: 0.6563
Epoch 26, Train Loss: 0.3460, Val Loss: 0.6753
batch size: (911, 911)
Epoch 27, accuracy: 0.6611
batch size: (901, 901)
Epoch 28, accuracy: 0.6592
Epoch 28, Train Loss: 0.3895, Val Loss: 0.6704
batch size: (912, 912)
Epoch 29, accuracy: 0.6598
batch size: (910, 910)
Epoch 30, accuracy: 0.6583
Epoch 30, Train Loss: 0.3330, Val Loss: 0.6846
batch size: (890, 890)
Epoch 31, accuracy: 0.6596
batch size: (900, 900)
Epoch 32, accuracy: 0.6581
Epoch 32, Train Loss: 0.4086, Val Loss: 0.6843
batch size: (899, 899)
Epoch 33, accuracy: 0.6606
batch size: (892, 892)
Epoch 34, accuracy: 0.6614
Epoch 34, Train Loss: 0.4105, Val Loss: 0.6726
batch size: (900, 900)
Epoch 35, accuracy: 0.6582
batch size: (906, 906)
Epoch 36, accuracy: 0.6614
Epoch 36, Train Loss: 0.4124, Val Loss: 0.6730
batch size: (891, 891)
Epoch 37, accuracy: 0.6581
batch size: (895, 895)
Epoch 38, accuracy: 0.6579
Epoch 38, Train Loss: 0.3886, Val Loss: 0.6702
batch size: (878, 878)
Epoch 39, accuracy: 0.6579
batch size: (904, 904)
Epoch 40, accuracy: 0.6607
Epoch 40, Train Loss: 0.3910, Val Loss: 0.6773
batch size: (894, 894)
Epoch 41, accuracy: 0.6570
batch size: (895, 895)
Epoch 42, accuracy: 0.6615
Epoch 42, Train Loss: 0.2854, Val Loss: 0.6777
batch size: (890, 890)
Epoch 43, accuracy: 0.6615
batch size: (885, 885)
Epoch 44, accuracy: 0.6599
Epoch 44, Train Loss: 0.3021, Val Loss: 0.6745
batch size: (899, 899)
Epoch 45, accuracy: 0.6596
batch size: (890, 890)
Epoch 46, accuracy: 0.6600
Epoch 46, Train Loss: 0.3517, Val Loss: 0.6841
batch size: (899, 899)
Epoch 47, accuracy: 0.6589
batch size: (902, 902)
Epoch 48, accuracy: 0.6619
Epoch 48, Train Loss: 0.3743, Val Loss: 0.6725
batch size: (894, 894)
Epoch 49, accuracy: 0.6592
batch size: (897, 897)
Epoch 50, accuracy: 0.6617
Epoch 50, Train Loss: 0.3691, Val Loss: 0.6857
batch size: (895, 895)
Epoch 51, accuracy: 0.6602
batch size: (896, 896)
Epoch 52, accuracy: 0.6606
Epoch 52, Train Loss: 0.4124, Val Loss: 0.6640
batch size: (896, 896)
Epoch 53, accuracy: 0.6590
batch size: (894, 894)
Epoch 54, accuracy: 0.6596
Epoch 54, Train Loss: 0.4546, Val Loss: 0.6965
batch size: (920, 920)
Epoch 55, accuracy: 0.6613
batch size: (892, 892)
Epoch 56, accuracy: 0.6599
Epoch 56, Train Loss: 0.4290, Val Loss: 0.6743
batch size: (913, 913)
Epoch 57, accuracy: 0.6620
batch size: (897, 897)
Epoch 58, accuracy: 0.6592
Epoch 58, Train Loss: 0.3425, Val Loss: 0.6942
batch size: (895, 895)
Epoch 59, accuracy: 0.6614
batch size: (903, 903)
Epoch 60, accuracy: 0.6597
Epoch 60, Train Loss: 0.3438, Val Loss: 0.6691
batch size: (892, 892)
Epoch 61, accuracy: 0.6572
batch size: (900, 900)
Epoch 62, accuracy: 0.6584
Epoch 62, Train Loss: 0.4429, Val Loss: 0.6611
batch size: (894, 894)
Epoch 63, accuracy: 0.6584
batch size: (905, 905)
Epoch 64, accuracy: 0.6581
Epoch 64, Train Loss: 0.3842, Val Loss: 0.6687
batch size: (908, 908)
Epoch 65, accuracy: 0.6599
batch size: (892, 892)
Epoch 66, accuracy: 0.6610
Epoch 66, Train Loss: 0.3033, Val Loss: 0.6715
batch size: (892, 892)
Epoch 67, accuracy: 0.6590
batch size: (900, 900)
Epoch 68, accuracy: 0.6587
Epoch 68, Train Loss: 0.3462, Val Loss: 0.6770
batch size: (894, 894)
Epoch 69, accuracy: 0.6588
batch size: (898, 898)
Epoch 70, accuracy: 0.6616
Epoch 70, Train Loss: 0.3828, Val Loss: 0.6809
batch size: (887, 887)
Epoch 71, accuracy: 0.6612
batch size: (888, 888)
Epoch 72, accuracy: 0.6602
Epoch 72, Train Loss: 0.3951, Val Loss: 0.6725
batch size: (927, 927)
Epoch 73, accuracy: 0.6582
batch size: (879, 879)
Epoch 74, accuracy: 0.6580
Epoch 74, Train Loss: 0.4154, Val Loss: 0.6774
batch size: (908, 908)
Epoch 75, accuracy: 0.6580
batch size: (901, 901)
Epoch 76, accuracy: 0.6556
Epoch 76, Train Loss: 0.4222, Val Loss: 0.6633
batch size: (900, 900)
Epoch 77, accuracy: 0.6580
batch size: (893, 893)
Epoch 78, accuracy: 0.6618
Epoch 78, Train Loss: 0.4318, Val Loss: 0.6620
batch size: (925, 925)
Epoch 79, accuracy: 0.6618
batch size: (901, 901)
Epoch 80, accuracy: 0.6611
Epoch 80, Train Loss: 0.3043, Val Loss: 0.6679
batch size: (908, 908)
Epoch 81, accuracy: 0.6596
batch size: (913, 913)
Epoch 82, accuracy: 0.6566
Epoch 82, Train Loss: 0.4616, Val Loss: 0.6758
batch size: (893, 893)
Epoch 83, accuracy: 0.6593
batch size: (911, 911)
Epoch 84, accuracy: 0.6610
Epoch 84, Train Loss: 0.3870, Val Loss: 0.6731
batch size: (884, 884)
Epoch 85, accuracy: 0.6617
batch size: (879, 879)
Epoch 86, accuracy: 0.6593
Epoch 86, Train Loss: 0.3419, Val Loss: 0.6838
batch size: (907, 907)
Epoch 87, accuracy: 0.6607
batch size: (887, 887)
Epoch 88, accuracy: 0.6620
Epoch 88, Train Loss: 0.5022, Val Loss: 0.6922
batch size: (880, 880)
Epoch 89, accuracy: 0.6605
batch size: (899, 899)
Epoch 90, accuracy: 0.6601
Epoch 90, Train Loss: 0.3719, Val Loss: 0.6550
batch size: (886, 886)
Epoch 91, accuracy: 0.6606
batch size: (901, 901)
Epoch 92, accuracy: 0.6582
Epoch 92, Train Loss: 0.3503, Val Loss: 0.6844
batch size: (887, 887)
Epoch 93, accuracy: 0.6600
batch size: (886, 886)
Epoch 94, accuracy: 0.6611
Epoch 94, Train Loss: 0.3642, Val Loss: 0.6822
batch size: (900, 900)
Epoch 95, accuracy: 0.6605
batch size: (899, 899)
Epoch 96, accuracy: 0.6606
Epoch 96, Train Loss: 0.3891, Val Loss: 0.6751
batch size: (890, 890)
Epoch 97, accuracy: 0.6611
batch size: (905, 905)
Epoch 98, accuracy: 0.6598
Epoch 98, Train Loss: 0.4292, Val Loss: 0.6784
batch size: (896, 896)
Epoch 99, accuracy: 0.6603
batch size: (902, 902)
Epoch 100, accuracy: 0.6603
Epoch 100, Train Loss: 0.3352, Val Loss: 0.6803
batch size: (878, 878)
Epoch 101, accuracy: 0.6575
batch size: (882, 882)
Epoch 102, accuracy: 0.6584
Epoch 102, Train Loss: 0.3546, Val Loss: 0.6817
batch size: (907, 907)
Epoch 103, accuracy: 0.6595
batch size: (885, 885)
Epoch 104, accuracy: 0.6609
Epoch 104, Train Loss: 0.3367, Val Loss: 0.6751
batch size: (883, 883)
Epoch 105, accuracy: 0.6582
batch size: (891, 891)
Epoch 106, accuracy: 0.6612
Epoch 106, Train Loss: 0.3347, Val Loss: 0.6788
batch size: (876, 876)
Epoch 107, accuracy: 0.6607
batch size: (882, 882)
Epoch 108, accuracy: 0.6607
Epoch 108, Train Loss: 0.3325, Val Loss: 0.6608
batch size: (923, 923)
Epoch 109, accuracy: 0.6606
batch size: (904, 904)
Epoch 110, accuracy: 0.6584
Epoch 110, Train Loss: 0.3336, Val Loss: 0.7051
batch size: (897, 897)
Epoch 111, accuracy: 0.6623
batch size: (891, 891)
Epoch 112, accuracy: 0.6601
Epoch 112, Train Loss: 0.4362, Val Loss: 0.6783
batch size: (918, 918)
Epoch 113, accuracy: 0.6611
batch size: (891, 891)
Epoch 114, accuracy: 0.6594
Epoch 114, Train Loss: 0.3676, Val Loss: 0.6724
batch size: (904, 904)
Epoch 115, accuracy: 0.6580
batch size: (886, 886)
Epoch 116, accuracy: 0.6592
Epoch 116, Train Loss: 0.4048, Val Loss: 0.6591
batch size: (906, 906)
Epoch 117, accuracy: 0.6611
batch size: (889, 889)
Epoch 118, accuracy: 0.6594
Epoch 118, Train Loss: 0.3225, Val Loss: 0.6826
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
/root/miniconda3/lib/python3.12/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling
  warnings.warn(f"Using '{self.__class__.__name__}' without a "
batch size: (897, 897)
Epoch 119, accuracy: 0.6589
batch size: (895, 895)
Epoch 120, accuracy: 0.6600
Epoch 120, Train Loss: 0.4035, Val Loss: 0.6700
batch size: (891, 891)
Epoch 121, accuracy: 0.6592
batch size: (899, 899)
Epoch 122, accuracy: 0.6602
Epoch 122, Train Loss: 0.4549, Val Loss: 0.6707
batch size: (888, 888)
Epoch 123, accuracy: 0.6616
batch size: (909, 909)
Epoch 124, accuracy: 0.6591
Epoch 124, Train Loss: 0.3681, Val Loss: 0.6872
batch size: (909, 909)
Epoch 125, accuracy: 0.6596
batch size: (901, 901)
Epoch 126, accuracy: 0.6618
Epoch 126, Train Loss: 0.4414, Val Loss: 0.6816
batch size: (907, 907)
Epoch 127, accuracy: 0.6628
batch size: (893, 893)
Epoch 128, accuracy: 0.6607
Epoch 128, Train Loss: 0.3425, Val Loss: 0.6725
batch size: (905, 905)
Epoch 129, accuracy: 0.6600
batch size: (894, 894)
Epoch 130, accuracy: 0.6590
Epoch 130, Train Loss: 0.4619, Val Loss: 0.6829
batch size: (884, 884)
Epoch 131, accuracy: 0.6574
batch size: (907, 907)
Epoch 132, accuracy: 0.6597
Epoch 132, Train Loss: 0.4306, Val Loss: 0.6700
batch size: (891, 891)
Epoch 133, accuracy: 0.6571
batch size: (889, 889)
Epoch 134, accuracy: 0.6609
Epoch 134, Train Loss: 0.4481, Val Loss: 0.6692
batch size: (898, 898)
Epoch 135, accuracy: 0.6591
batch size: (906, 906)
Epoch 136, accuracy: 0.6590
Epoch 136, Train Loss: 0.4487, Val Loss: 0.6720
batch size: (884, 884)
Epoch 137, accuracy: 0.6603
batch size: (901, 901)
Epoch 138, accuracy: 0.6622
Epoch 138, Train Loss: 0.3395, Val Loss: 0.6790
batch size: (910, 910)
Epoch 139, accuracy: 0.6596
batch size: (907, 907)
Epoch 140, accuracy: 0.6608
Epoch 140, Train Loss: 0.3706, Val Loss: 0.6782
batch size: (886, 886)
Epoch 141, accuracy: 0.6607
batch size: (878, 878)
Epoch 142, accuracy: 0.6604
Epoch 142, Train Loss: 0.4059, Val Loss: 0.6667
batch size: (898, 898)
Epoch 143, accuracy: 0.6623
batch size: (895, 895)
Epoch 144, accuracy: 0.6611
Epoch 144, Train Loss: 0.4538, Val Loss: 0.6755
batch size: (896, 896)
Epoch 145, accuracy: 0.6582
batch size: (889, 889)
Epoch 146, accuracy: 0.6625
Epoch 146, Train Loss: 0.4364, Val Loss: 0.6657
batch size: (893, 893)
Epoch 147, accuracy: 0.6600
batch size: (903, 903)
Epoch 148, accuracy: 0.6599
Epoch 148, Train Loss: 0.4418, Val Loss: 0.6705
batch size: (919, 919)
Epoch 149, accuracy: 0.6590
batch size: (912, 912)
Epoch 150, accuracy: 0.6577
Epoch 150, Train Loss: 0.3939, Val Loss: 0.6772
batch size: (911, 911)
Epoch 151, accuracy: 0.6607
batch size: (904, 904)
Epoch 152, accuracy: 0.6601
Epoch 152, Train Loss: 0.2992, Val Loss: 0.6864
batch size: (899, 899)
Epoch 153, accuracy: 0.6634
batch size: (911, 911)
Epoch 154, accuracy: 0.6603
Epoch 154, Train Loss: 0.3357, Val Loss: 0.6787
batch size: (902, 902)
Epoch 155, accuracy: 0.6597
batch size: (893, 893)
Epoch 156, accuracy: 0.6588
Epoch 156, Train Loss: 0.3690, Val Loss: 0.6643
batch size: (909, 909)
Epoch 157, accuracy: 0.6615
batch size: (885, 885)
Epoch 158, accuracy: 0.6603
Epoch 158, Train Loss: 0.3977, Val Loss: 0.6756
batch size: (917, 917)
Epoch 159, accuracy: 0.6598
batch size: (886, 886)
Epoch 160, accuracy: 0.6616
Epoch 160, Train Loss: 0.3573, Val Loss: 0.6733
batch size: (896, 896)
Epoch 161, accuracy: 0.6610
batch size: (899, 899)
Epoch 162, accuracy: 0.6575
Epoch 162, Train Loss: 0.3696, Val Loss: 0.6771
batch size: (905, 905)
Epoch 163, accuracy: 0.6588
batch size: (895, 895)
Epoch 164, accuracy: 0.6595
Epoch 164, Train Loss: 0.3598, Val Loss: 0.6860
batch size: (895, 895)
Epoch 165, accuracy: 0.6588
batch size: (886, 886)
Epoch 166, accuracy: 0.6584
Epoch 166, Train Loss: 0.3249, Val Loss: 0.6796
batch size: (911, 911)
Epoch 167, accuracy: 0.6577
batch size: (888, 888)
Epoch 168, accuracy: 0.6600
Epoch 168, Train Loss: 0.4574, Val Loss: 0.6779
batch size: (896, 896)
Epoch 169, accuracy: 0.6594
batch size: (907, 907)
Epoch 170, accuracy: 0.6590
Epoch 170, Train Loss: 0.4334, Val Loss: 0.6676
batch size: (899, 899)
Epoch 171, accuracy: 0.6612
batch size: (900, 900)
Epoch 172, accuracy: 0.6590
Epoch 172, Train Loss: 0.4358, Val Loss: 0.6700
batch size: (881, 881)
Epoch 173, accuracy: 0.6596
batch size: (910, 910)
Epoch 174, accuracy: 0.6609
Epoch 174, Train Loss: 0.3270, Val Loss: 0.6888
batch size: (907, 907)
Epoch 175, accuracy: 0.6609
batch size: (912, 912)
Epoch 176, accuracy: 0.6578
Epoch 176, Train Loss: 0.3010, Val Loss: 0.6801
batch size: (902, 902)
Epoch 177, accuracy: 0.6618
batch size: (872, 872)
Epoch 178, accuracy: 0.6623
Epoch 178, Train Loss: 0.3417, Val Loss: 0.6721
batch size: (893, 893)
Epoch 179, accuracy: 0.6599
batch size: (892, 892)
Epoch 180, accuracy: 0.6595
Epoch 180, Train Loss: 0.3022, Val Loss: 0.6737
batch size: (878, 878)
Epoch 181, accuracy: 0.6595
batch size: (903, 903)
Epoch 182, accuracy: 0.6612
Epoch 182, Train Loss: 0.3295, Val Loss: 0.6761
batch size: (906, 906)
Epoch 183, accuracy: 0.6608
batch size: (887, 887)
Epoch 184, accuracy: 0.6586
Epoch 184, Train Loss: 0.3292, Val Loss: 0.6574
batch size: (894, 894)
Epoch 185, accuracy: 0.6586
batch size: (887, 887)
Epoch 186, accuracy: 0.6580
Epoch 186, Train Loss: 0.4279, Val Loss: 0.6764
batch size: (902, 902)
Epoch 187, accuracy: 0.6591
batch size: (895, 895)
Epoch 188, accuracy: 0.6602
Epoch 188, Train Loss: 0.3754, Val Loss: 0.6758
batch size: (882, 882)
Epoch 189, accuracy: 0.6569
batch size: (892, 892)
Epoch 190, accuracy: 0.6622
Epoch 190, Train Loss: 0.4154, Val Loss: 0.6810
batch size: (892, 892)
Epoch 191, accuracy: 0.6630
batch size: (910, 910)
Epoch 192, accuracy: 0.6589
Epoch 192, Train Loss: 0.4059, Val Loss: 0.6756
batch size: (899, 899)
Epoch 193, accuracy: 0.6606
batch size: (906, 906)
Epoch 194, accuracy: 0.6611
Epoch 194, Train Loss: 0.3083, Val Loss: 0.6757
batch size: (869, 869)
Epoch 195, accuracy: 0.6592
batch size: (896, 896)
Epoch 196, accuracy: 0.6612
Epoch 196, Train Loss: 0.3475, Val Loss: 0.6671
batch size: (906, 906)
Epoch 197, accuracy: 0.6613
batch size: (903, 903)
Epoch 198, accuracy: 0.6620
Epoch 198, Train Loss: 0.3364, Val Loss: 0.6729
batch size: (897, 897)
Epoch 199, accuracy: 0.6621
Loaded best model with val_loss = 0.6273477673530579
test :accuracy 0.6674, f1_macro: 0.6629, f1_micro: 0.6674, auc: 0.8343
Final Results: {'mamba3_2_Pubmed': np.float64(0.6456323568457386), 'mamba3_8_Pubmed': np.float64(0.6295207553892749), 'mamba3_32_Pubmed': np.float64(0.6673883267794812)} ['560885_mamba3_0', '1056245_mamba3_0', '3037685_mamba3_0']
========== Running baseline 2/3 ==========
Training mamba3 with 2 layers...
可训练参数: 560885_mamba3
不可训练参数: 0
batch size: (908, 908)
✅ Epoch 0: New best model saved with val_loss = 0.8516
Epoch 0, accuracy: 0.5416
Epoch 0, Train Loss: 1.1584, Val Loss: 0.8516
batch size: (889, 889)
✅ Epoch 1: New best model saved with val_loss = 0.8185
Epoch 1, accuracy: 0.5794
batch size: (887, 887)
✅ Epoch 2: New best model saved with val_loss = 0.6780
Epoch 2, accuracy: 0.6402
Epoch 2, Train Loss: 0.7869, Val Loss: 0.6780
batch size: (886, 886)
✅ Epoch 3: New best model saved with val_loss = 0.5886
Epoch 3, accuracy: 0.6759
batch size: (891, 891)
Epoch 4, accuracy: 0.6075
Epoch 4, Train Loss: 0.5657, Val Loss: 0.8298
batch size: (900, 900)
Epoch 5, accuracy: 0.6573
batch size: (902, 902)
Epoch 6, accuracy: 0.6730
Epoch 6, Train Loss: 0.4849, Val Loss: 0.6134
batch size: (893, 893)
Epoch 7, accuracy: 0.6817
batch size: (883, 883)
Epoch 8, accuracy: 0.6763
Epoch 8, Train Loss: 0.3143, Val Loss: 0.7217
batch size: (888, 888)
Epoch 9, accuracy: 0.6722
batch size: (898, 898)
Epoch 10, accuracy: 0.6735
Epoch 10, Train Loss: 0.3644, Val Loss: 0.7827
batch size: (890, 890)
Epoch 11, accuracy: 0.6764
batch size: (896, 896)
Epoch 12, accuracy: 0.6779
Epoch 12, Train Loss: 0.4477, Val Loss: 0.7672
batch size: (889, 889)
Epoch 13, accuracy: 0.6841
batch size: (909, 909)
Epoch 14, accuracy: 0.6845
Epoch 14, Train Loss: 0.5080, Val Loss: 0.7617
batch size: (903, 903)
Epoch 15, accuracy: 0.6885
batch size: (902, 902)
Epoch 16, accuracy: 0.6873
Epoch 16, Train Loss: 0.4401, Val Loss: 0.7631
batch size: (895, 895)
Epoch 17, accuracy: 0.6883
batch size: (914, 914)
Epoch 18, accuracy: 0.6861
Epoch 18, Train Loss: 0.4430, Val Loss: 0.7526
batch size: (909, 909)
Epoch 19, accuracy: 0.6871
batch size: (901, 901)
Epoch 20, accuracy: 0.6857
Epoch 20, Train Loss: 0.4514, Val Loss: 0.7682
batch size: (899, 899)
Epoch 21, accuracy: 0.6870
batch size: (911, 911)
Epoch 22, accuracy: 0.6849
Epoch 22, Train Loss: 0.3888, Val Loss: 0.7671
batch size: (898, 898)
Epoch 23, accuracy: 0.6862
batch size: (903, 903)
Epoch 24, accuracy: 0.6853
Epoch 24, Train Loss: 0.5033, Val Loss: 0.7489
batch size: (898, 898)
Epoch 25, accuracy: 0.6892
batch size: (887, 887)
Epoch 26, accuracy: 0.6895
Epoch 26, Train Loss: 0.3963, Val Loss: 0.7633
batch size: (902, 902)
Epoch 27, accuracy: 0.6876
batch size: (913, 913)
Epoch 28, accuracy: 0.6863
Epoch 28, Train Loss: 0.3698, Val Loss: 0.7498
batch size: (900, 900)
Epoch 29, accuracy: 0.6842
batch size: (908, 908)
Epoch 30, accuracy: 0.6855
Epoch 30, Train Loss: 0.4938, Val Loss: 0.7594
batch size: (887, 887)
Epoch 31, accuracy: 0.6875
batch size: (878, 878)
Epoch 32, accuracy: 0.6829
Epoch 32, Train Loss: 0.4018, Val Loss: 0.7466
batch size: (907, 907)
Epoch 33, accuracy: 0.6874
batch size: (903, 903)
Epoch 34, accuracy: 0.6854
Epoch 34, Train Loss: 0.3233, Val Loss: 0.7752
batch size: (874, 874)
Epoch 35, accuracy: 0.6866
batch size: (884, 884)
Epoch 36, accuracy: 0.6863
Epoch 36, Train Loss: 0.4060, Val Loss: 0.7622
batch size: (904, 904)
Epoch 37, accuracy: 0.6871
batch size: (895, 895)
Epoch 38, accuracy: 0.6865
Epoch 38, Train Loss: 0.3370, Val Loss: 0.7557
batch size: (902, 902)
Epoch 39, accuracy: 0.6867
batch size: (877, 877)
Epoch 40, accuracy: 0.6901
Epoch 40, Train Loss: 0.3672, Val Loss: 0.7421
batch size: (891, 891)
Epoch 41, accuracy: 0.6865
batch size: (901, 901)
Epoch 42, accuracy: 0.6890
Epoch 42, Train Loss: 0.2306, Val Loss: 0.7598
batch size: (906, 906)
Epoch 43, accuracy: 0.6881
batch size: (900, 900)
Epoch 44, accuracy: 0.6890
Epoch 44, Train Loss: 0.3578, Val Loss: 0.7546
batch size: (870, 870)
Epoch 45, accuracy: 0.6849
batch size: (917, 917)
Epoch 46, accuracy: 0.6878
Epoch 46, Train Loss: 0.3448, Val Loss: 0.7572
batch size: (888, 888)
Epoch 47, accuracy: 0.6865
batch size: (902, 902)
Epoch 48, accuracy: 0.6864
Epoch 48, Train Loss: 0.3616, Val Loss: 0.7684
batch size: (896, 896)
Epoch 49, accuracy: 0.6872
batch size: (907, 907)
Epoch 50, accuracy: 0.6883
Epoch 50, Train Loss: 0.2941, Val Loss: 0.7731
batch size: (892, 892)
Epoch 51, accuracy: 0.6889
batch size: (916, 916)
Epoch 52, accuracy: 0.6881
Epoch 52, Train Loss: 0.3981, Val Loss: 0.7427
batch size: (887, 887)
Epoch 53, accuracy: 0.6858
batch size: (886, 886)
Epoch 54, accuracy: 0.6859
Epoch 54, Train Loss: 0.2660, Val Loss: 0.7564
batch size: (900, 900)
Epoch 55, accuracy: 0.6889
batch size: (890, 890)
Epoch 56, accuracy: 0.6857
Epoch 56, Train Loss: 0.4509, Val Loss: 0.7573
batch size: (887, 887)
Epoch 57, accuracy: 0.6862
batch size: (891, 891)
Epoch 58, accuracy: 0.6877
Epoch 58, Train Loss: 0.4140, Val Loss: 0.7589
batch size: (875, 875)
Epoch 59, accuracy: 0.6879
batch size: (893, 893)
Epoch 60, accuracy: 0.6862
Epoch 60, Train Loss: 0.3609, Val Loss: 0.7584
batch size: (905, 905)
Epoch 61, accuracy: 0.6863
batch size: (914, 914)
Epoch 62, accuracy: 0.6887
Epoch 62, Train Loss: 0.3079, Val Loss: 0.7621
batch size: (887, 887)
Epoch 63, accuracy: 0.6881
batch size: (898, 898)
Epoch 64, accuracy: 0.6864
Epoch 64, Train Loss: 0.3784, Val Loss: 0.7579
batch size: (894, 894)
Epoch 65, accuracy: 0.6860
batch size: (899, 899)
Epoch 66, accuracy: 0.6865
Epoch 66, Train Loss: 0.3343, Val Loss: 0.7544
batch size: (903, 903)
Epoch 67, accuracy: 0.6854
batch size: (885, 885)
Epoch 68, accuracy: 0.6875
Epoch 68, Train Loss: 0.5793, Val Loss: 0.7767
batch size: (912, 912)
Epoch 69, accuracy: 0.6876
batch size: (890, 890)
Epoch 70, accuracy: 0.6867
Epoch 70, Train Loss: 0.4743, Val Loss: 0.7595
batch size: (929, 929)
Epoch 71, accuracy: 0.6874
batch size: (894, 894)
Epoch 72, accuracy: 0.6858
Epoch 72, Train Loss: 0.4155, Val Loss: 0.7622
batch size: (921, 921)
Epoch 73, accuracy: 0.6874
batch size: (905, 905)
Epoch 74, accuracy: 0.6869
Epoch 74, Train Loss: 0.4406, Val Loss: 0.7548
batch size: (890, 890)
Epoch 75, accuracy: 0.6861
batch size: (914, 914)
Epoch 76, accuracy: 0.6867
Epoch 76, Train Loss: 0.4249, Val Loss: 0.7550
batch size: (892, 892)
Epoch 77, accuracy: 0.6848
batch size: (889, 889)
Epoch 78, accuracy: 0.6867
Epoch 78, Train Loss: 0.4179, Val Loss: 0.7601
batch size: (890, 890)
Epoch 79, accuracy: 0.6869
batch size: (897, 897)
Epoch 80, accuracy: 0.6865
Epoch 80, Train Loss: 0.4749, Val Loss: 0.7665
batch size: (913, 913)
Epoch 81, accuracy: 0.6854
batch size: (897, 897)
Epoch 82, accuracy: 0.6861
Epoch 82, Train Loss: 0.3574, Val Loss: 0.7578
batch size: (902, 902)
Epoch 83, accuracy: 0.6867
batch size: (915, 915)
Epoch 84, accuracy: 0.6877
Epoch 84, Train Loss: 0.4718, Val Loss: 0.7607
batch size: (889, 889)
Epoch 85, accuracy: 0.6865
batch size: (923, 923)
Epoch 86, accuracy: 0.6867
Epoch 86, Train Loss: 0.4531, Val Loss: 0.7429
batch size: (898, 898)
Epoch 87, accuracy: 0.6884
batch size: (917, 917)
Epoch 88, accuracy: 0.6883
Epoch 88, Train Loss: 0.3631, Val Loss: 0.7707
batch size: (920, 920)
Epoch 89, accuracy: 0.6848
batch size: (884, 884)
Epoch 90, accuracy: 0.6879
Epoch 90, Train Loss: 0.4655, Val Loss: 0.7586
batch size: (909, 909)
Epoch 91, accuracy: 0.6864
batch size: (886, 886)
Epoch 92, accuracy: 0.6860
Epoch 92, Train Loss: 0.3808, Val Loss: 0.7778
batch size: (905, 905)
Epoch 93, accuracy: 0.6864
batch size: (897, 897)
Epoch 94, accuracy: 0.6872
Epoch 94, Train Loss: 0.3586, Val Loss: 0.7565
batch size: (893, 893)
Epoch 95, accuracy: 0.6833
batch size: (896, 896)
Epoch 96, accuracy: 0.6871
Epoch 96, Train Loss: 0.4901, Val Loss: 0.7662
batch size: (909, 909)
Epoch 97, accuracy: 0.6844
batch size: (894, 894)
Epoch 98, accuracy: 0.6877
Epoch 98, Train Loss: 0.3454, Val Loss: 0.7624
batch size: (893, 893)
Epoch 99, accuracy: 0.6857
batch size: (879, 879)
Epoch 100, accuracy: 0.6854
Epoch 100, Train Loss: 0.3122, Val Loss: 0.7628
batch size: (911, 911)
Epoch 101, accuracy: 0.6863
batch size: (919, 919)
Epoch 102, accuracy: 0.6881
Epoch 102, Train Loss: 0.3533, Val Loss: 0.7686
batch size: (880, 880)
Epoch 103, accuracy: 0.6858
batch size: (883, 883)
Epoch 104, accuracy: 0.6879
Epoch 104, Train Loss: 0.2670, Val Loss: 0.7728
batch size: (889, 889)
Epoch 105, accuracy: 0.6885
batch size: (894, 894)
Epoch 106, accuracy: 0.6864
Epoch 106, Train Loss: 0.2940, Val Loss: 0.7515
batch size: (907, 907)
Epoch 107, accuracy: 0.6855
batch size: (887, 887)
Epoch 108, accuracy: 0.6876
Epoch 108, Train Loss: 0.3860, Val Loss: 0.7583
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
batch size: (897, 897)
Epoch 109, accuracy: 0.6880
batch size: (911, 911)
Epoch 110, accuracy: 0.6853
Epoch 110, Train Loss: 0.3413, Val Loss: 0.7669
batch size: (882, 882)
Epoch 111, accuracy: 0.6863
batch size: (905, 905)
Epoch 112, accuracy: 0.6832
Epoch 112, Train Loss: 0.4357, Val Loss: 0.7667
batch size: (904, 904)
Epoch 113, accuracy: 0.6863
batch size: (890, 890)
Epoch 114, accuracy: 0.6879
Epoch 114, Train Loss: 0.3261, Val Loss: 0.7516
batch size: (902, 902)
Epoch 115, accuracy: 0.6862
batch size: (889, 889)
Epoch 116, accuracy: 0.6874
Epoch 116, Train Loss: 0.4112, Val Loss: 0.7653
batch size: (896, 896)
Epoch 117, accuracy: 0.6875
batch size: (903, 903)
Epoch 118, accuracy: 0.6882
Epoch 118, Train Loss: 0.3311, Val Loss: 0.7541
batch size: (915, 915)
Epoch 119, accuracy: 0.6853
batch size: (896, 896)
Epoch 120, accuracy: 0.6893
Epoch 120, Train Loss: 0.3007, Val Loss: 0.7375
batch size: (912, 912)
Epoch 121, accuracy: 0.6862
batch size: (903, 903)
Epoch 122, accuracy: 0.6866
Epoch 122, Train Loss: 0.4119, Val Loss: 0.7433
batch size: (921, 921)
Epoch 123, accuracy: 0.6869
batch size: (893, 893)
Epoch 124, accuracy: 0.6861
Epoch 124, Train Loss: 0.2478, Val Loss: 0.7652
batch size: (894, 894)
Epoch 125, accuracy: 0.6853
batch size: (909, 909)
Epoch 126, accuracy: 0.6855
Epoch 126, Train Loss: 0.3354, Val Loss: 0.7522
batch size: (897, 897)
Epoch 127, accuracy: 0.6871
batch size: (888, 888)
Epoch 128, accuracy: 0.6870
Epoch 128, Train Loss: 0.5570, Val Loss: 0.7565
batch size: (913, 913)
Epoch 129, accuracy: 0.6858
batch size: (894, 894)
Epoch 130, accuracy: 0.6883
Epoch 130, Train Loss: 0.3980, Val Loss: 0.7554
batch size: (913, 913)
Epoch 131, accuracy: 0.6884
batch size: (926, 926)
Epoch 132, accuracy: 0.6847
Epoch 132, Train Loss: 0.3730, Val Loss: 0.7635
batch size: (910, 910)
Epoch 133, accuracy: 0.6876
batch size: (902, 902)
Epoch 134, accuracy: 0.6883
Epoch 134, Train Loss: 0.4573, Val Loss: 0.7402
batch size: (894, 894)
Epoch 135, accuracy: 0.6872
batch size: (905, 905)
Epoch 136, accuracy: 0.6843
Epoch 136, Train Loss: 0.4269, Val Loss: 0.7559
batch size: (897, 897)
Epoch 137, accuracy: 0.6860
batch size: (892, 892)
Epoch 138, accuracy: 0.6846
Epoch 138, Train Loss: 0.3302, Val Loss: 0.7474
batch size: (888, 888)
Epoch 139, accuracy: 0.6873
batch size: (914, 914)
Epoch 140, accuracy: 0.6829
Epoch 140, Train Loss: 0.4205, Val Loss: 0.7545
batch size: (910, 910)
Epoch 141, accuracy: 0.6864
batch size: (898, 898)
Epoch 142, accuracy: 0.6877
Epoch 142, Train Loss: 0.3491, Val Loss: 0.7500
batch size: (890, 890)
Epoch 143, accuracy: 0.6879
batch size: (895, 895)
Epoch 144, accuracy: 0.6856
Epoch 144, Train Loss: 0.3199, Val Loss: 0.7599
batch size: (903, 903)
Epoch 145, accuracy: 0.6862
batch size: (904, 904)
Epoch 146, accuracy: 0.6883
Epoch 146, Train Loss: 0.4152, Val Loss: 0.7580
batch size: (917, 917)
Epoch 147, accuracy: 0.6862
batch size: (898, 898)
Epoch 148, accuracy: 0.6870
Epoch 148, Train Loss: 0.2834, Val Loss: 0.7671
batch size: (912, 912)
Epoch 149, accuracy: 0.6886
batch size: (887, 887)
Epoch 150, accuracy: 0.6860
Epoch 150, Train Loss: 0.4181, Val Loss: 0.7587
batch size: (909, 909)
Epoch 151, accuracy: 0.6875
batch size: (897, 897)
Epoch 152, accuracy: 0.6878
Epoch 152, Train Loss: 0.4441, Val Loss: 0.7609
batch size: (912, 912)
Epoch 153, accuracy: 0.6856
batch size: (896, 896)
Epoch 154, accuracy: 0.6887
Epoch 154, Train Loss: 0.3258, Val Loss: 0.7433
batch size: (915, 915)
Epoch 155, accuracy: 0.6890
batch size: (901, 901)
Epoch 156, accuracy: 0.6871
Epoch 156, Train Loss: 0.3921, Val Loss: 0.7554
batch size: (899, 899)
Epoch 157, accuracy: 0.6869
batch size: (913, 913)
Epoch 158, accuracy: 0.6864
Epoch 158, Train Loss: 0.4590, Val Loss: 0.7488
batch size: (912, 912)
Epoch 159, accuracy: 0.6883
batch size: (901, 901)
Epoch 160, accuracy: 0.6880
Epoch 160, Train Loss: 0.4778, Val Loss: 0.7653
batch size: (900, 900)
Epoch 161, accuracy: 0.6871
batch size: (904, 904)
Epoch 162, accuracy: 0.6876
Epoch 162, Train Loss: 0.3455, Val Loss: 0.7369
batch size: (901, 901)
Epoch 163, accuracy: 0.6845
batch size: (905, 905)
Epoch 164, accuracy: 0.6876
Epoch 164, Train Loss: 0.3695, Val Loss: 0.7577
batch size: (884, 884)
Epoch 165, accuracy: 0.6863
batch size: (894, 894)
Epoch 166, accuracy: 0.6861
Epoch 166, Train Loss: 0.4836, Val Loss: 0.7506
batch size: (887, 887)
Epoch 167, accuracy: 0.6882
batch size: (905, 905)
Epoch 168, accuracy: 0.6877
Epoch 168, Train Loss: 0.4447, Val Loss: 0.7670
batch size: (892, 892)
Epoch 169, accuracy: 0.6858
batch size: (896, 896)
Epoch 170, accuracy: 0.6865
Epoch 170, Train Loss: 0.3627, Val Loss: 0.7706
batch size: (889, 889)
Epoch 171, accuracy: 0.6897
batch size: (897, 897)
Epoch 172, accuracy: 0.6875
Epoch 172, Train Loss: 0.3536, Val Loss: 0.7604
batch size: (893, 893)
Epoch 173, accuracy: 0.6859
batch size: (895, 895)
Epoch 174, accuracy: 0.6864
Epoch 174, Train Loss: 0.4469, Val Loss: 0.7565
batch size: (902, 902)
Epoch 175, accuracy: 0.6872
batch size: (899, 899)
Epoch 176, accuracy: 0.6881
Epoch 176, Train Loss: 0.3331, Val Loss: 0.7723
batch size: (902, 902)
Epoch 177, accuracy: 0.6887
batch size: (906, 906)
Epoch 178, accuracy: 0.6865
Epoch 178, Train Loss: 0.4278, Val Loss: 0.7541
batch size: (904, 904)
Epoch 179, accuracy: 0.6881
batch size: (906, 906)
Epoch 180, accuracy: 0.6866
Epoch 180, Train Loss: 0.3680, Val Loss: 0.7768
batch size: (926, 926)
Epoch 181, accuracy: 0.6883
batch size: (917, 917)
Epoch 182, accuracy: 0.6861
Epoch 182, Train Loss: 0.5495, Val Loss: 0.7530
batch size: (888, 888)
Epoch 183, accuracy: 0.6853
batch size: (914, 914)
Epoch 184, accuracy: 0.6851
Epoch 184, Train Loss: 0.4300, Val Loss: 0.7673
batch size: (884, 884)
Epoch 185, accuracy: 0.6869
batch size: (894, 894)
Epoch 186, accuracy: 0.6871
Epoch 186, Train Loss: 0.4234, Val Loss: 0.7490
batch size: (887, 887)
Epoch 187, accuracy: 0.6875
batch size: (899, 899)
Epoch 188, accuracy: 0.6873
Epoch 188, Train Loss: 0.4973, Val Loss: 0.7546
batch size: (888, 888)
Epoch 189, accuracy: 0.6872
batch size: (899, 899)
Epoch 190, accuracy: 0.6858
Epoch 190, Train Loss: 0.5058, Val Loss: 0.7511
batch size: (886, 886)
Epoch 191, accuracy: 0.6866
batch size: (898, 898)
Epoch 192, accuracy: 0.6894
Epoch 192, Train Loss: 0.4730, Val Loss: 0.7384
batch size: (889, 889)
Epoch 193, accuracy: 0.6884
batch size: (899, 899)
Epoch 194, accuracy: 0.6862
Epoch 194, Train Loss: 0.3984, Val Loss: 0.7719
batch size: (911, 911)
Epoch 195, accuracy: 0.6866
batch size: (911, 911)
Epoch 196, accuracy: 0.6859
Epoch 196, Train Loss: 0.3928, Val Loss: 0.7548
batch size: (877, 877)
Epoch 197, accuracy: 0.6888
batch size: (899, 899)
Epoch 198, accuracy: 0.6837
Epoch 198, Train Loss: 0.3216, Val Loss: 0.7560
batch size: (890, 890)
Epoch 199, accuracy: 0.6881
Loaded best model with val_loss = 0.5886437296867371
test :accuracy 0.6763, f1_macro: 0.6830, f1_micro: 0.6763, auc: 0.8463
Training mamba3 with 8 layers...
可训练参数: 1056245_mamba3
不可训练参数: 0
batch size: (926, 926)
✅ Epoch 0: New best model saved with val_loss = 0.9946
Epoch 0, accuracy: 0.5886
Epoch 0, Train Loss: 1.1428, Val Loss: 0.9946
batch size: (893, 893)
Epoch 1, accuracy: 0.4774
batch size: (907, 907)
✅ Epoch 2: New best model saved with val_loss = 0.8647
Epoch 2, accuracy: 0.4725
Epoch 2, Train Loss: 1.4617, Val Loss: 0.8647
batch size: (899, 899)
✅ Epoch 3: New best model saved with val_loss = 0.8219
Epoch 3, accuracy: 0.5474
batch size: (904, 904)
✅ Epoch 4: New best model saved with val_loss = 0.6842
Epoch 4, accuracy: 0.5905
Epoch 4, Train Loss: 0.9946, Val Loss: 0.6842
batch size: (910, 910)
✅ Epoch 5: New best model saved with val_loss = 0.6496
Epoch 5, accuracy: 0.5900
batch size: (915, 915)
Epoch 6, accuracy: 0.5624
Epoch 6, Train Loss: 0.5218, Val Loss: 0.7053
batch size: (888, 888)
Epoch 7, accuracy: 0.5749
batch size: (899, 899)
Epoch 8, accuracy: 0.6014
Epoch 8, Train Loss: 0.4824, Val Loss: 0.6798
batch size: (886, 886)
Epoch 9, accuracy: 0.6255
batch size: (900, 900)
✅ Epoch 10: New best model saved with val_loss = 0.6418
Epoch 10, accuracy: 0.6601
Epoch 10, Train Loss: 0.4297, Val Loss: 0.6418
batch size: (905, 905)
✅ Epoch 11: New best model saved with val_loss = 0.5824
Epoch 11, accuracy: 0.6865
batch size: (879, 879)
✅ Epoch 12: New best model saved with val_loss = 0.5767
Epoch 12, accuracy: 0.6973
Epoch 12, Train Loss: 0.3529, Val Loss: 0.5767
batch size: (901, 901)
✅ Epoch 13: New best model saved with val_loss = 0.5690
Epoch 13, accuracy: 0.6997
batch size: (898, 898)
Epoch 14, accuracy: 0.7045
Epoch 14, Train Loss: 0.4079, Val Loss: 0.5807
batch size: (903, 903)
Epoch 15, accuracy: 0.6962
batch size: (890, 890)
Epoch 16, accuracy: 0.6843
Epoch 16, Train Loss: 0.5013, Val Loss: 0.5840
batch size: (901, 901)
Epoch 17, accuracy: 0.6737
batch size: (868, 868)
Epoch 18, accuracy: 0.6607
Epoch 18, Train Loss: 0.4682, Val Loss: 0.6542
batch size: (892, 892)
Epoch 19, accuracy: 0.6456
batch size: (901, 901)
Epoch 20, accuracy: 0.6423
Epoch 20, Train Loss: 0.3507, Val Loss: 0.6797
batch size: (901, 901)
Epoch 21, accuracy: 0.6459
batch size: (900, 900)
Epoch 22, accuracy: 0.6441
Epoch 22, Train Loss: 0.3356, Val Loss: 0.6866
batch size: (897, 897)
Epoch 23, accuracy: 0.6489
batch size: (923, 923)
Epoch 24, accuracy: 0.6481
Epoch 24, Train Loss: 0.3515, Val Loss: 0.6825
batch size: (923, 923)
Epoch 25, accuracy: 0.6536
batch size: (905, 905)
Epoch 26, accuracy: 0.6539
Epoch 26, Train Loss: 0.2709, Val Loss: 0.6734
batch size: (889, 889)
Epoch 27, accuracy: 0.6532
batch size: (900, 900)
Epoch 28, accuracy: 0.6535
Epoch 28, Train Loss: 0.3295, Val Loss: 0.6676
batch size: (896, 896)
Epoch 29, accuracy: 0.6508
batch size: (899, 899)
Epoch 30, accuracy: 0.6519
Epoch 30, Train Loss: 0.3467, Val Loss: 0.6600
batch size: (903, 903)
Epoch 31, accuracy: 0.6519
batch size: (892, 892)
Epoch 32, accuracy: 0.6549
Epoch 32, Train Loss: 0.3695, Val Loss: 0.6536
batch size: (900, 900)
Epoch 33, accuracy: 0.6524
batch size: (890, 890)
Epoch 34, accuracy: 0.6511
Epoch 34, Train Loss: 0.4264, Val Loss: 0.6544
batch size: (892, 892)
Epoch 35, accuracy: 0.6532
batch size: (899, 899)
Epoch 36, accuracy: 0.6536
Epoch 36, Train Loss: 0.3747, Val Loss: 0.6582
batch size: (901, 901)
Epoch 37, accuracy: 0.6517
batch size: (914, 914)
Epoch 38, accuracy: 0.6572
Epoch 38, Train Loss: 0.3183, Val Loss: 0.6615
batch size: (897, 897)
Epoch 39, accuracy: 0.6550
batch size: (904, 904)
Epoch 40, accuracy: 0.6539
Epoch 40, Train Loss: 0.3304, Val Loss: 0.6574
batch size: (896, 896)
Epoch 41, accuracy: 0.6514
batch size: (928, 928)
Epoch 42, accuracy: 0.6525
Epoch 42, Train Loss: 0.4088, Val Loss: 0.6648
batch size: (903, 903)
Epoch 43, accuracy: 0.6521
batch size: (905, 905)
Epoch 44, accuracy: 0.6533
Epoch 44, Train Loss: 0.3431, Val Loss: 0.6728
batch size: (902, 902)
Epoch 45, accuracy: 0.6526
batch size: (894, 894)
Epoch 46, accuracy: 0.6516
Epoch 46, Train Loss: 0.3816, Val Loss: 0.6659
batch size: (908, 908)
Epoch 47, accuracy: 0.6550
batch size: (886, 886)
Epoch 48, accuracy: 0.6549
Epoch 48, Train Loss: 0.2641, Val Loss: 0.6660
batch size: (887, 887)
Epoch 49, accuracy: 0.6556
batch size: (903, 903)
Epoch 50, accuracy: 0.6523
Epoch 50, Train Loss: 0.4212, Val Loss: 0.6592
batch size: (909, 909)
Epoch 51, accuracy: 0.6516
batch size: (906, 906)
Epoch 52, accuracy: 0.6529
Epoch 52, Train Loss: 0.3675, Val Loss: 0.6545
batch size: (904, 904)
Epoch 53, accuracy: 0.6512
batch size: (915, 915)
Epoch 54, accuracy: 0.6541
Epoch 54, Train Loss: 0.3525, Val Loss: 0.6583
batch size: (892, 892)
Epoch 55, accuracy: 0.6515
batch size: (893, 893)
Epoch 56, accuracy: 0.6549
Epoch 56, Train Loss: 0.4518, Val Loss: 0.6639
batch size: (897, 897)
Epoch 57, accuracy: 0.6523
batch size: (900, 900)
Epoch 58, accuracy: 0.6543
Epoch 58, Train Loss: 0.3656, Val Loss: 0.6488
batch size: (880, 880)
Epoch 59, accuracy: 0.6534
batch size: (899, 899)
Epoch 60, accuracy: 0.6514
Epoch 60, Train Loss: 0.5149, Val Loss: 0.6611
batch size: (896, 896)
Epoch 61, accuracy: 0.6521
batch size: (917, 917)
Epoch 62, accuracy: 0.6545
Epoch 62, Train Loss: 0.3660, Val Loss: 0.6540
batch size: (901, 901)
Epoch 63, accuracy: 0.6537
batch size: (893, 893)
Epoch 64, accuracy: 0.6512
Epoch 64, Train Loss: 0.3865, Val Loss: 0.6420
batch size: (905, 905)
Epoch 65, accuracy: 0.6526
batch size: (903, 903)
Epoch 66, accuracy: 0.6512
Epoch 66, Train Loss: 0.4336, Val Loss: 0.6501
batch size: (883, 883)
Epoch 67, accuracy: 0.6534
batch size: (901, 901)
Epoch 68, accuracy: 0.6544
Epoch 68, Train Loss: 0.3555, Val Loss: 0.6713
batch size: (911, 911)
Epoch 69, accuracy: 0.6519
batch size: (889, 889)
Epoch 70, accuracy: 0.6557
Epoch 70, Train Loss: 0.3408, Val Loss: 0.6542
batch size: (916, 916)
Epoch 71, accuracy: 0.6547
batch size: (914, 914)
Epoch 72, accuracy: 0.6503
Epoch 72, Train Loss: 0.3777, Val Loss: 0.6577
batch size: (888, 888)
Epoch 73, accuracy: 0.6514
batch size: (886, 886)
Epoch 74, accuracy: 0.6540
Epoch 74, Train Loss: 0.4050, Val Loss: 0.6738
batch size: (898, 898)
Epoch 75, accuracy: 0.6539
batch size: (909, 909)
Epoch 76, accuracy: 0.6536
Epoch 76, Train Loss: 0.3523, Val Loss: 0.6610
batch size: (904, 904)
Epoch 77, accuracy: 0.6547
batch size: (908, 908)
Epoch 78, accuracy: 0.6524
Epoch 78, Train Loss: 0.4294, Val Loss: 0.6578
batch size: (904, 904)
Epoch 79, accuracy: 0.6514
batch size: (882, 882)
Epoch 80, accuracy: 0.6523
Epoch 80, Train Loss: 0.4432, Val Loss: 0.6607
batch size: (916, 916)
Epoch 81, accuracy: 0.6541
batch size: (900, 900)
Epoch 82, accuracy: 0.6536
Epoch 82, Train Loss: 0.2908, Val Loss: 0.6638
batch size: (886, 886)
Epoch 83, accuracy: 0.6550
batch size: (891, 891)
Epoch 84, accuracy: 0.6566
Epoch 84, Train Loss: 0.3988, Val Loss: 0.6564
batch size: (910, 910)
Epoch 85, accuracy: 0.6534
batch size: (888, 888)
Epoch 86, accuracy: 0.6536
Epoch 86, Train Loss: 0.4751, Val Loss: 0.6654
batch size: (916, 916)
Epoch 87, accuracy: 0.6524
batch size: (898, 898)
Epoch 88, accuracy: 0.6505
Epoch 88, Train Loss: 0.4260, Val Loss: 0.6619
batch size: (902, 902)
Epoch 89, accuracy: 0.6530
batch size: (912, 912)
Epoch 90, accuracy: 0.6533
Epoch 90, Train Loss: 0.4394, Val Loss: 0.6601
batch size: (893, 893)
Epoch 91, accuracy: 0.6534
batch size: (900, 900)
Epoch 92, accuracy: 0.6548
Epoch 92, Train Loss: 0.3227, Val Loss: 0.6654
batch size: (898, 898)
Epoch 93, accuracy: 0.6527
batch size: (910, 910)
Epoch 94, accuracy: 0.6545
Epoch 94, Train Loss: 0.4252, Val Loss: 0.6645
batch size: (906, 906)
Epoch 95, accuracy: 0.6521
batch size: (909, 909)
Epoch 96, accuracy: 0.6508
Epoch 96, Train Loss: 0.4110, Val Loss: 0.6620
batch size: (920, 920)
Epoch 97, accuracy: 0.6537
batch size: (901, 901)
Epoch 98, accuracy: 0.6530
Epoch 98, Train Loss: 0.3143, Val Loss: 0.6617
batch size: (892, 892)
Epoch 99, accuracy: 0.6505
batch size: (906, 906)
Epoch 100, accuracy: 0.6549
Epoch 100, Train Loss: 0.4414, Val Loss: 0.6577
batch size: (890, 890)
Epoch 101, accuracy: 0.6540
batch size: (898, 898)
Epoch 102, accuracy: 0.6544
Epoch 102, Train Loss: 0.3562, Val Loss: 0.6622
batch size: (904, 904)
Epoch 103, accuracy: 0.6537
batch size: (905, 905)
Epoch 104, accuracy: 0.6555
Epoch 104, Train Loss: 0.4034, Val Loss: 0.6525
batch size: (894, 894)
Epoch 105, accuracy: 0.6512
batch size: (916, 916)
Epoch 106, accuracy: 0.6537
Epoch 106, Train Loss: 0.2917, Val Loss: 0.6605
batch size: (909, 909)
Epoch 107, accuracy: 0.6513
batch size: (903, 903)
Epoch 108, accuracy: 0.6500
Epoch 108, Train Loss: 0.3539, Val Loss: 0.6578
batch size: (906, 906)
Epoch 109, accuracy: 0.6515
batch size: (893, 893)
Epoch 110, accuracy: 0.6514
Epoch 110, Train Loss: 0.4668, Val Loss: 0.6670
batch size: (889, 889)
Epoch 111, accuracy: 0.6526
batch size: (900, 900)
Epoch 112, accuracy: 0.6528
Epoch 112, Train Loss: 0.3914, Val Loss: 0.6572
batch size: (904, 904)
Epoch 113, accuracy: 0.6513
batch size: (912, 912)
Epoch 114, accuracy: 0.6548
Epoch 114, Train Loss: 0.4623, Val Loss: 0.6713
batch size: (914, 914)
Epoch 115, accuracy: 0.6539
batch size: (890, 890)
Epoch 116, accuracy: 0.6538
Epoch 116, Train Loss: 0.4005, Val Loss: 0.6727
batch size: (890, 890)
Epoch 117, accuracy: 0.6539
batch size: (886, 886)
Epoch 118, accuracy: 0.6531
Epoch 118, Train Loss: 0.3497, Val Loss: 0.6556
batch size: (895, 895)
Epoch 119, accuracy: 0.6540
batch size: /root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
(893, 893)
Epoch 120, accuracy: 0.6517
Epoch 120, Train Loss: 0.5743, Val Loss: 0.6736
batch size: (897, 897)
Epoch 121, accuracy: 0.6541
batch size: (911, 911)
Epoch 122, accuracy: 0.6529
Epoch 122, Train Loss: 0.3346, Val Loss: 0.6620
batch size: (896, 896)
Epoch 123, accuracy: 0.6553
batch size: (921, 921)
Epoch 124, accuracy: 0.6504
Epoch 124, Train Loss: 0.3629, Val Loss: 0.6436
batch size: (889, 889)
Epoch 125, accuracy: 0.6533
batch size: (880, 880)
Epoch 126, accuracy: 0.6526
Epoch 126, Train Loss: 0.3946, Val Loss: 0.6537
batch size: (911, 911)
Epoch 127, accuracy: 0.6535
batch size: (894, 894)
Epoch 128, accuracy: 0.6527
Epoch 128, Train Loss: 0.3553, Val Loss: 0.6610
batch size: (907, 907)
Epoch 129, accuracy: 0.6552
batch size: (897, 897)
Epoch 130, accuracy: 0.6510
Epoch 130, Train Loss: 0.3547, Val Loss: 0.6604
batch size: (882, 882)
Epoch 131, accuracy: 0.6545
batch size: (906, 906)
Epoch 132, accuracy: 0.6519
Epoch 132, Train Loss: 0.2960, Val Loss: 0.6406
batch size: (899, 899)
Epoch 133, accuracy: 0.6524
batch size: (900, 900)
Epoch 134, accuracy: 0.6538
Epoch 134, Train Loss: 0.3895, Val Loss: 0.6770
batch size: (887, 887)
Epoch 135, accuracy: 0.6527
batch size: (914, 914)
Epoch 136, accuracy: 0.6544
Epoch 136, Train Loss: 0.3217, Val Loss: 0.6651
batch size: (892, 892)
Epoch 137, accuracy: 0.6523
batch size: (912, 912)
Epoch 138, accuracy: 0.6528
Epoch 138, Train Loss: 0.4741, Val Loss: 0.6701
batch size: (883, 883)
Epoch 139, accuracy: 0.6549
batch size: (905, 905)
Epoch 140, accuracy: 0.6516
Epoch 140, Train Loss: 0.4380, Val Loss: 0.6534
batch size: (886, 886)
Epoch 141, accuracy: 0.6523
batch size: (902, 902)
Epoch 142, accuracy: 0.6540
Epoch 142, Train Loss: 0.4559, Val Loss: 0.6463
batch size: (921, 921)
Epoch 143, accuracy: 0.6539
batch size: (905, 905)
Epoch 144, accuracy: 0.6514
Epoch 144, Train Loss: 0.3538, Val Loss: 0.6583
batch size: (883, 883)
Epoch 145, accuracy: 0.6538
batch size: (910, 910)
Epoch 146, accuracy: 0.6522
Epoch 146, Train Loss: 0.4741, Val Loss: 0.6639
batch size: (916, 916)
Epoch 147, accuracy: 0.6525
batch size: (889, 889)
Epoch 148, accuracy: 0.6541
Epoch 148, Train Loss: 0.3894, Val Loss: 0.6515
batch size: (906, 906)
Epoch 149, accuracy: 0.6535
batch size: (886, 886)
Epoch 150, accuracy: 0.6545
Epoch 150, Train Loss: 0.4027, Val Loss: 0.6518
batch size: (902, 902)
Epoch 151, accuracy: 0.6533
batch size: (912, 912)
Epoch 152, accuracy: 0.6530
Epoch 152, Train Loss: 0.3160, Val Loss: 0.6388
batch size: (905, 905)
Epoch 153, accuracy: 0.6535
batch size: (922, 922)
Epoch 154, accuracy: 0.6510
Epoch 154, Train Loss: 0.3236, Val Loss: 0.6760
batch size: (903, 903)
Epoch 155, accuracy: 0.6527
batch size: (890, 890)
Epoch 156, accuracy: 0.6524
Epoch 156, Train Loss: 0.3512, Val Loss: 0.6634
batch size: (902, 902)
Epoch 157, accuracy: 0.6527
batch size: (885, 885)
Epoch 158, accuracy: 0.6538
Epoch 158, Train Loss: 0.2937, Val Loss: 0.6631
batch size: (918, 918)
Epoch 159, accuracy: 0.6536
batch size: (883, 883)
Epoch 160, accuracy: 0.6513
Epoch 160, Train Loss: 0.2791, Val Loss: 0.6679
batch size: (923, 923)
Epoch 161, accuracy: 0.6546
batch size: (944, 944)
Epoch 162, accuracy: 0.6563
Epoch 162, Train Loss: 0.4460, Val Loss: 0.6609
batch size: (886, 886)
Epoch 163, accuracy: 0.6503
batch size: (894, 894)
Epoch 164, accuracy: 0.6541
Epoch 164, Train Loss: 0.3344, Val Loss: 0.6451
batch size: (887, 887)
Epoch 165, accuracy: 0.6510
batch size: (907, 907)
Epoch 166, accuracy: 0.6552
Epoch 166, Train Loss: 0.2849, Val Loss: 0.6525
batch size: (901, 901)
Epoch 167, accuracy: 0.6529
batch size: (901, 901)
Epoch 168, accuracy: 0.6533
Epoch 168, Train Loss: 0.4278, Val Loss: 0.6595
batch size: (899, 899)
Epoch 169, accuracy: 0.6542
batch size: (905, 905)
Epoch 170, accuracy: 0.6525
Epoch 170, Train Loss: 0.5099, Val Loss: 0.6556
batch size: (907, 907)
Epoch 171, accuracy: 0.6510
batch size: (916, 916)
Epoch 172, accuracy: 0.6531
Epoch 172, Train Loss: 0.4635, Val Loss: 0.6584
batch size: (893, 893)
Epoch 173, accuracy: 0.6531
batch size: (889, 889)
Epoch 174, accuracy: 0.6546
Epoch 174, Train Loss: 0.4121, Val Loss: 0.6503
batch size: (892, 892)
Epoch 175, accuracy: 0.6515
batch size: (892, 892)
Epoch 176, accuracy: 0.6512
Epoch 176, Train Loss: 0.3855, Val Loss: 0.6638
batch size: (884, 884)
Epoch 177, accuracy: 0.6537
batch size: (909, 909)
Epoch 178, accuracy: 0.6532
Epoch 178, Train Loss: 0.4547, Val Loss: 0.6623
batch size: (899, 899)
Epoch 179, accuracy: 0.6543
batch size: (918, 918)
Epoch 180, accuracy: 0.6542
Epoch 180, Train Loss: 0.3614, Val Loss: 0.6442
batch size: (884, 884)
Epoch 181, accuracy: 0.6516
batch size: (893, 893)
Epoch 182, accuracy: 0.6526
Epoch 182, Train Loss: 0.4414, Val Loss: 0.6635
batch size: (902, 902)
Epoch 183, accuracy: 0.6552
batch size: (890, 890)
Epoch 184, accuracy: 0.6512
Epoch 184, Train Loss: 0.3461, Val Loss: 0.6620
batch size: (901, 901)
Epoch 185, accuracy: 0.6522
batch size: (903, 903)
Epoch 186, accuracy: 0.6538
Epoch 186, Train Loss: 0.4630, Val Loss: 0.6607
batch size: (904, 904)
Epoch 187, accuracy: 0.6541
batch size: (886, 886)
Epoch 188, accuracy: 0.6509
Epoch 188, Train Loss: 0.4095, Val Loss: 0.6542
batch size: (915, 915)
Epoch 189, accuracy: 0.6532
batch size: (905, 905)
Epoch 190, accuracy: 0.6554
Epoch 190, Train Loss: 0.3148, Val Loss: 0.6695
batch size: (912, 912)
Epoch 191, accuracy: 0.6527
batch size: (900, 900)
Epoch 192, accuracy: 0.6535
Epoch 192, Train Loss: 0.3792, Val Loss: 0.6514
batch size: (914, 914)
Epoch 193, accuracy: 0.6547
batch size: (903, 903)
Epoch 194, accuracy: 0.6529
Epoch 194, Train Loss: 0.4038, Val Loss: 0.6671
batch size: (916, 916)
Epoch 195, accuracy: 0.6535
batch size: (878, 878)
Epoch 196, accuracy: 0.6506
Epoch 196, Train Loss: 0.3189, Val Loss: 0.6578
batch size: (905, 905)
Epoch 197, accuracy: 0.6548
batch size: (898, 898)
Epoch 198, accuracy: 0.6531
Epoch 198, Train Loss: 0.3553, Val Loss: 0.6500
batch size: (886, 886)
Epoch 199, accuracy: 0.6524
Loaded best model with val_loss = 0.5689650774002075
test :accuracy 0.7022, f1_macro: 0.7025, f1_micro: 0.7022, auc: 0.8461
Training mamba3 with 32 layers...
可训练参数: 3037685_mamba3
不可训练参数: 0
batch size: (879, 879)
✅ Epoch 0: New best model saved with val_loss = 1.4204
Epoch 0, accuracy: 0.4386
Epoch 0, Train Loss: 1599.8420, Val Loss: 1.4204
batch size: (905, 905)
✅ Epoch 1: New best model saved with val_loss = 1.3268
Epoch 1, accuracy: 0.4553
batch size: (903, 903)
✅ Epoch 2: New best model saved with val_loss = 0.8898
Epoch 2, accuracy: 0.5747
Epoch 2, Train Loss: 1.0017, Val Loss: 0.8898
batch size: (912, 912)
✅ Epoch 3: New best model saved with val_loss = 0.8099
Epoch 3, accuracy: 0.5699
batch size: (906, 906)
Epoch 4, accuracy: 0.6607
Epoch 4, Train Loss: 0.8125, Val Loss: 0.8288
batch size: (880, 880)
Epoch 5, accuracy: 0.5771
batch size: (916, 916)
✅ Epoch 6: New best model saved with val_loss = 0.7585
Epoch 6, accuracy: 0.6649
Epoch 6, Train Loss: 0.6939, Val Loss: 0.7585
batch size: (913, 913)
✅ Epoch 7: New best model saved with val_loss = 0.6232
Epoch 7, accuracy: 0.6600
batch size: (900, 900)
Epoch 8, accuracy: 0.5999
Epoch 8, Train Loss: 0.4871, Val Loss: 0.6917
batch size: (907, 907)
Epoch 9, accuracy: 0.5782
batch size: (906, 906)
Epoch 10, accuracy: 0.5916
Epoch 10, Train Loss: 0.5373, Val Loss: 0.7267
batch size: (911, 911)
Epoch 11, accuracy: 0.6226
batch size: (914, 914)
Epoch 12, accuracy: 0.6645
Epoch 12, Train Loss: 0.4292, Val Loss: 0.6235
batch size: (887, 887)
✅ Epoch 13: New best model saved with val_loss = 0.6154
Epoch 13, accuracy: 0.6935
batch size: (895, 895)
Epoch 14, accuracy: 0.7097
Epoch 14, Train Loss: 0.4852, Val Loss: 0.6384
batch size: (905, 905)
Epoch 15, accuracy: 0.7140
batch size: (880, 880)
Epoch 16, accuracy: 0.7125
Epoch 16, Train Loss: 0.5257, Val Loss: 0.6623
batch size: (907, 907)
Epoch 17, accuracy: 0.7104
batch size: (896, 896)
Epoch 18, accuracy: 0.7061
Epoch 18, Train Loss: 0.4666, Val Loss: 0.6797
batch size: (887, 887)
Epoch 19, accuracy: 0.7008
batch size: (905, 905)
Epoch 20, accuracy: 0.6993
Epoch 20, Train Loss: 0.2790, Val Loss: 0.6760
batch size: (902, 902)
Epoch 21, accuracy: 0.6998
batch size: (900, 900)
Epoch 22, accuracy: 0.6975
Epoch 22, Train Loss: 0.4072, Val Loss: 0.6833
batch size: (887, 887)
Epoch 23, accuracy: 0.6964
batch size: (911, 911)
Epoch 24, accuracy: 0.6966
Epoch 24, Train Loss: 0.3433, Val Loss: 0.6844
batch size: (905, 905)
Epoch 25, accuracy: 0.6934
batch size: (899, 899)
Epoch 26, accuracy: 0.6957
Epoch 26, Train Loss: 0.4115, Val Loss: 0.6839
batch size: (891, 891)
Epoch 27, accuracy: 0.6955
batch size: (903, 903)
Epoch 28, accuracy: 0.6949
Epoch 28, Train Loss: 0.3750, Val Loss: 0.6840
batch size: (893, 893)
Epoch 29, accuracy: 0.6961
batch size: (912, 912)
Epoch 30, accuracy: 0.6948
Epoch 30, Train Loss: 0.4503, Val Loss: 0.6768
batch size: (909, 909)
Epoch 31, accuracy: 0.6954
batch size: (904, 904)
Epoch 32, accuracy: 0.6956
Epoch 32, Train Loss: 0.3821, Val Loss: 0.6756
batch size: (894, 894)
Epoch 33, accuracy: 0.6956
batch size: (906, 906)
Epoch 34, accuracy: 0.6938
Epoch 34, Train Loss: 0.3667, Val Loss: 0.6666
batch size: (898, 898)
Epoch 35, accuracy: 0.6939
batch size: (898, 898)
Epoch 36, accuracy: 0.6920
Epoch 36, Train Loss: 0.3858, Val Loss: 0.6770
batch size: (892, 892)
Epoch 37, accuracy: 0.6915
batch size: (911, 911)
Epoch 38, accuracy: 0.6961
Epoch 38, Train Loss: 0.4100, Val Loss: 0.6774
batch size: (908, 908)
Epoch 39, accuracy: 0.6954
batch size: (902, 902)
Epoch 40, accuracy: 0.6935
Epoch 40, Train Loss: 0.5237, Val Loss: 0.6828
batch size: (894, 894)
Epoch 41, accuracy: 0.6937
batch size: (895, 895)
Epoch 42, accuracy: 0.6913
Epoch 42, Train Loss: 0.3750, Val Loss: 0.6644
batch size: (910, 910)
Epoch 43, accuracy: 0.6938
batch size: (892, 892)
Epoch 44, accuracy: 0.6933
Epoch 44, Train Loss: 0.3174, Val Loss: 0.6770
batch size: (883, 883)
Epoch 45, accuracy: 0.6963
batch size: (900, 900)
Epoch 46, accuracy: 0.6966
Epoch 46, Train Loss: 0.3426, Val Loss: 0.6750
batch size: (901, 901)
Epoch 47, accuracy: 0.6962
batch size: (900, 900)
Epoch 48, accuracy: 0.6946
Epoch 48, Train Loss: 0.4680, Val Loss: 0.6663
batch size: (928, 928)
Epoch 49, accuracy: 0.6953
batch size: (903, 903)
Epoch 50, accuracy: 0.6943
Epoch 50, Train Loss: 0.4479, Val Loss: 0.6896
batch size: (872, 872)
Epoch 51, accuracy: 0.6940
batch size: (901, 901)
Epoch 52, accuracy: 0.6952
Epoch 52, Train Loss: 0.3770, Val Loss: 0.6875
batch size: (895, 895)
Epoch 53, accuracy: 0.6952
batch size: (898, 898)
Epoch 54, accuracy: 0.6937
Epoch 54, Train Loss: 0.3777, Val Loss: 0.6665
batch size: (886, 886)
Epoch 55, accuracy: 0.6956
batch size: (897, 897)
Epoch 56, accuracy: 0.6935
Epoch 56, Train Loss: 0.3498, Val Loss: 0.6721
batch size: (887, 887)
Epoch 57, accuracy: 0.6935
batch size: (895, 895)
Epoch 58, accuracy: 0.6949
Epoch 58, Train Loss: 0.3677, Val Loss: 0.6743
batch size: (883, 883)
Epoch 59, accuracy: 0.6941
batch size: (907, 907)
Epoch 60, accuracy: 0.6954
Epoch 60, Train Loss: 0.3956, Val Loss: 0.6775
batch size: (895, 895)
Epoch 61, accuracy: 0.6958
batch size: (897, 897)
Epoch 62, accuracy: 0.6951
Epoch 62, Train Loss: 0.2497, Val Loss: 0.6745
batch size: (898, 898)
Epoch 63, accuracy: 0.6942
batch size: (890, 890)
Epoch 64, accuracy: 0.6940
Epoch 64, Train Loss: 0.3693, Val Loss: 0.6717
batch size: (890, 890)
Epoch 65, accuracy: 0.6955
batch size: (912, 912)
Epoch 66, accuracy: 0.6983
Epoch 66, Train Loss: 0.4238, Val Loss: 0.6809
batch size: (891, 891)
Epoch 67, accuracy: 0.6953
batch size: (882, 882)
Epoch 68, accuracy: 0.6945
Epoch 68, Train Loss: 0.2587, Val Loss: 0.6772
batch size: (899, 899)
Epoch 69, accuracy: 0.6968
batch size: (866, 866)
Epoch 70, accuracy: 0.6939
Epoch 70, Train Loss: 0.4281, Val Loss: 0.6803
batch size: (893, 893)
Epoch 71, accuracy: 0.6961
batch size: (914, 914)
Epoch 72, accuracy: 0.6955
Epoch 72, Train Loss: 0.3651, Val Loss: 0.6849
batch size: (908, 908)
Epoch 73, accuracy: 0.6939
batch size: (896, 896)
Epoch 74, accuracy: 0.6964
Epoch 74, Train Loss: 0.4281, Val Loss: 0.6906
batch size: (877, 877)
Epoch 75, accuracy: 0.6955
batch size: (901, 901)
Epoch 76, accuracy: 0.6955
Epoch 76, Train Loss: 0.4378, Val Loss: 0.6783
batch size: (893, 893)
Epoch 77, accuracy: 0.6933
batch size: (898, 898)
Epoch 78, accuracy: 0.6945
Epoch 78, Train Loss: 0.3815, Val Loss: 0.6753
batch size: (905, 905)
Epoch 79, accuracy: 0.6959
batch size: (901, 901)
Epoch 80, accuracy: 0.6935
Epoch 80, Train Loss: 0.4185, Val Loss: 0.6839
batch size: (905, 905)
Epoch 81, accuracy: 0.6948
batch size: (909, 909)
Epoch 82, accuracy: 0.6961
Epoch 82, Train Loss: 0.3032, Val Loss: 0.6725
batch size: (887, 887)
Epoch 83, accuracy: 0.6960
batch size: (885, 885)
Epoch 84, accuracy: 0.6979
Epoch 84, Train Loss: 0.3852, Val Loss: 0.6903
batch size: (903, 903)
Epoch 85, accuracy: 0.6942
batch size: (911, 911)
Epoch 86, accuracy: 0.6944
Epoch 86, Train Loss: 0.4051, Val Loss: 0.6692
batch size: (902, 902)
Epoch 87, accuracy: 0.6942
batch size: (922, 922)
Epoch 88, accuracy: 0.6939
Epoch 88, Train Loss: 0.4122, Val Loss: 0.6757
batch size: (886, 886)
Epoch 89, accuracy: 0.6962
batch size: (901, 901)
Epoch 90, accuracy: 0.6963
Epoch 90, Train Loss: 0.4559, Val Loss: 0.6665
batch size: (922, 922)
Epoch 91, accuracy: 0.6951
batch size: (914, 914)
Epoch 92, accuracy: 0.6971
Epoch 92, Train Loss: 0.3673, Val Loss: 0.6768
batch size: (906, 906)
Epoch 93, accuracy: 0.6949
batch size: (910, 910)
Epoch 94, accuracy: 0.6948
Epoch 94, Train Loss: 0.3582, Val Loss: 0.6838
batch size: (902, 902)
Epoch 95, accuracy: 0.6957
batch size: (912, 912)
Epoch 96, accuracy: 0.6939
Epoch 96, Train Loss: 0.3603, Val Loss: 0.6885
batch size: (908, 908)
Epoch 97, accuracy: 0.6951
batch size: (912, 912)
Epoch 98, accuracy: 0.6959
Epoch 98, Train Loss: 0.4139, Val Loss: 0.6985
batch size: (904, 904)
Epoch 99, accuracy: 0.6943
batch size: (882, 882)
Epoch 100, accuracy: 0.6946
Epoch 100, Train Loss: 0.3009, Val Loss: 0.6736
batch size: (909, 909)
Epoch 101, accuracy: 0.6947
batch size: (894, 894)
Epoch 102, accuracy: 0.6956
Epoch 102, Train Loss: 0.4234, Val Loss: 0.6894
batch size: (915, 915)
Epoch 103, accuracy: 0.6941
batch size: (907, 907)
Epoch 104, accuracy: 0.6948
Epoch 104, Train Loss: 0.3813, Val Loss: 0.6816
batch size: (888, 888)
Epoch 105, accuracy: 0.6944
batch size: (875, 875)
Epoch 106, accuracy: 0.6950
Epoch 106, Train Loss: 0.4126, Val Loss: 0.6854
batch size: (906, 906)
Epoch 107, accuracy: 0.6941
batch size: (900, 900)
Epoch 108, accuracy: 0.6928
Epoch 108, Train Loss: 0.3151, Val Loss: 0.6817
batch size: (887, 887)
Epoch 109, accuracy: 0.6956
batch size: (908, 908)
Epoch 110, accuracy: 0.6932
Epoch 110, Train Loss: 0.3329, Val Loss: 0.6651
batch size: (913, 913)
Epoch 111, accuracy: 0.6952
batch size: (911, 911)
Epoch 112, accuracy: 0.6956
Epoch 112, Train Loss: 0.3330, Val Loss: 0.6763
batch size: (904, 904)
Epoch 113, accuracy: 0.6956
batch size: (895, 895)
Epoch 114, accuracy: 0.6931
Epoch 114, Train Loss: 0.3701, Val Loss: 0.6776
batch size: (895, 895)
Epoch 115, accuracy: 0.6958
batch size: (894, 894)
Epoch 116, accuracy: 0.6951
Epoch 116, Train Loss: 0.3435, Val Loss: 0.6868
batch size: (914, 914)
Epoch 117, accuracy: 0.6947
batch size: (925, 925)
Epoch 118, accuracy: 0.6955
Epoch 118, Train Loss: 0.3870, Val Loss: 0.6914
batch size: (911, 911)
Epoch 119, accuracy: 0.6956
batch size: (899, 899)
Epoch 120, accuracy: 0.6964
Epoch 120, Train Loss: 0.3142, Val Loss: 0.6693
batch size: (907, 907)
Epoch 121, accuracy: 0.6943
batch size: (914, 914)
Epoch 122, accuracy: 0.6938
Epoch 122, Train Loss: 0.5149, Val Loss: 0.6790
batch size: (909, 909)
Epoch 123, accuracy: 0.6936
batch size: (893, 893)
Epoch 124, accuracy: 0.6963
Epoch 124, Train Loss: 0.3163, Val Loss: 0.6770
batch size: (909, 909)
Epoch 125, accuracy: 0.6934
batch size: (892, 892)
Epoch 126, accuracy: 0.6952
Epoch 126, Train Loss: 0.3967, Val Loss: 0.6785
batch size: (904, 904)
Epoch 127, accuracy: 0.6944
batch size: (895, 895)
Epoch 128, accuracy: 0.6960
Epoch 128, Train Loss: 0.3541, Val Loss: 0.6684
batch size: (901, 901)
Epoch 129, accuracy: 0.6949
batch size: (897, 897)
Epoch 130, accuracy: 0.6967
Epoch 130, Train Loss: 0.4389, Val Loss: 0.6752
batch size: (899, 899)
Epoch 131, accuracy: 0.6964
batch size: (894, 894)
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
/root/miniconda3/lib/python3.12/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling
  warnings.warn(f"Using '{self.__class__.__name__}' without a "
Epoch 132, accuracy: 0.6957
Epoch 132, Train Loss: 0.3979, Val Loss: 0.6893
batch size: (905, 905)
Epoch 133, accuracy: 0.6958
batch size: (879, 879)
Epoch 134, accuracy: 0.6944
Epoch 134, Train Loss: 0.2385, Val Loss: 0.6817
batch size: (898, 898)
Epoch 135, accuracy: 0.6957
batch size: (885, 885)
Epoch 136, accuracy: 0.6944
Epoch 136, Train Loss: 0.3153, Val Loss: 0.6856
batch size: (905, 905)
Epoch 137, accuracy: 0.6927
batch size: (890, 890)
Epoch 138, accuracy: 0.6954
Epoch 138, Train Loss: 0.3763, Val Loss: 0.6800
batch size: (888, 888)
Epoch 139, accuracy: 0.6956
batch size: (908, 908)
Epoch 140, accuracy: 0.6948
Epoch 140, Train Loss: 0.5012, Val Loss: 0.6796
batch size: (889, 889)
Epoch 141, accuracy: 0.6963
batch size: (906, 906)
Epoch 142, accuracy: 0.6921
Epoch 142, Train Loss: 0.3376, Val Loss: 0.6825
batch size: (900, 900)
Epoch 143, accuracy: 0.6938
batch size: (908, 908)
Epoch 144, accuracy: 0.6946
Epoch 144, Train Loss: 0.3930, Val Loss: 0.6878
batch size: (908, 908)
Epoch 145, accuracy: 0.6945
batch size: (909, 909)
Epoch 146, accuracy: 0.6950
Epoch 146, Train Loss: 0.4183, Val Loss: 0.6948
batch size: (896, 896)
Epoch 147, accuracy: 0.6955
batch size: (920, 920)
Epoch 148, accuracy: 0.6966
Epoch 148, Train Loss: 0.3983, Val Loss: 0.6753
batch size: (906, 906)
Epoch 149, accuracy: 0.6931
batch size: (912, 912)
Epoch 150, accuracy: 0.6954
Epoch 150, Train Loss: 0.3942, Val Loss: 0.6835
batch size: (871, 871)
Epoch 151, accuracy: 0.6955
batch size: (898, 898)
Epoch 152, accuracy: 0.6962
Epoch 152, Train Loss: 0.3638, Val Loss: 0.6659
batch size: (905, 905)
Epoch 153, accuracy: 0.6941
batch size: (900, 900)
Epoch 154, accuracy: 0.6942
Epoch 154, Train Loss: 0.4493, Val Loss: 0.6757
batch size: (896, 896)
Epoch 155, accuracy: 0.6963
batch size: (904, 904)
Epoch 156, accuracy: 0.6955
Epoch 156, Train Loss: 0.4286, Val Loss: 0.6827
batch size: (898, 898)
Epoch 157, accuracy: 0.6957
batch size: (899, 899)
Epoch 158, accuracy: 0.6936
Epoch 158, Train Loss: 0.3069, Val Loss: 0.6710
batch size: (889, 889)
Epoch 159, accuracy: 0.6959
batch size: (892, 892)
Epoch 160, accuracy: 0.6947
Epoch 160, Train Loss: 0.3201, Val Loss: 0.6753
batch size: (881, 881)
Epoch 161, accuracy: 0.6957
batch size: (900, 900)
Epoch 162, accuracy: 0.6937
Epoch 162, Train Loss: 0.3469, Val Loss: 0.6600
batch size: (892, 892)
Epoch 163, accuracy: 0.6956
batch size: (901, 901)
Epoch 164, accuracy: 0.6944
Epoch 164, Train Loss: 0.3041, Val Loss: 0.6706
batch size: (915, 915)
Epoch 165, accuracy: 0.6933
batch size: (892, 892)
Epoch 166, accuracy: 0.6960
Epoch 166, Train Loss: 0.4231, Val Loss: 0.6569
batch size: (895, 895)
Epoch 167, accuracy: 0.6948
batch size: (922, 922)
Epoch 168, accuracy: 0.6966
Epoch 168, Train Loss: 0.4506, Val Loss: 0.6899
batch size: (895, 895)
Epoch 169, accuracy: 0.6948
batch size: (891, 891)
Epoch 170, accuracy: 0.6970
Epoch 170, Train Loss: 0.4122, Val Loss: 0.6715
batch size: (907, 907)
Epoch 171, accuracy: 0.6945
batch size: (921, 921)
Epoch 172, accuracy: 0.6953
Epoch 172, Train Loss: 0.4517, Val Loss: 0.6760
batch size: (885, 885)
Epoch 173, accuracy: 0.6959
batch size: (907, 907)
Epoch 174, accuracy: 0.6929
Epoch 174, Train Loss: 0.4365, Val Loss: 0.6763
batch size: (893, 893)
Epoch 175, accuracy: 0.6967
batch size: (896, 896)
Epoch 176, accuracy: 0.6963
Epoch 176, Train Loss: 0.4202, Val Loss: 0.6689
batch size: (912, 912)
Epoch 177, accuracy: 0.6944
batch size: (879, 879)
Epoch 178, accuracy: 0.6948
Epoch 178, Train Loss: 0.2923, Val Loss: 0.6605
batch size: (909, 909)
Epoch 179, accuracy: 0.6949
batch size: (901, 901)
Epoch 180, accuracy: 0.6939
Epoch 180, Train Loss: 0.3357, Val Loss: 0.6725
batch size: (916, 916)
Epoch 181, accuracy: 0.6958
batch size: (901, 901)
Epoch 182, accuracy: 0.6964
Epoch 182, Train Loss: 0.3678, Val Loss: 0.6778
batch size: (908, 908)
Epoch 183, accuracy: 0.6958
batch size: (913, 913)
Epoch 184, accuracy: 0.6951
Epoch 184, Train Loss: 0.3903, Val Loss: 0.6740
batch size: (910, 910)
Epoch 185, accuracy: 0.6916
batch size: (900, 900)
Epoch 186, accuracy: 0.6959
Epoch 186, Train Loss: 0.4269, Val Loss: 0.6750
batch size: (907, 907)
Epoch 187, accuracy: 0.6953
batch size: (893, 893)
Epoch 188, accuracy: 0.6960
Epoch 188, Train Loss: 0.2748, Val Loss: 0.6710
batch size: (878, 878)
Epoch 189, accuracy: 0.6956
batch size: (903, 903)
Epoch 190, accuracy: 0.6952
Epoch 190, Train Loss: 0.4156, Val Loss: 0.6777
batch size: (888, 888)
Epoch 191, accuracy: 0.6935
batch size: (883, 883)
Epoch 192, accuracy: 0.6932
Epoch 192, Train Loss: 0.2901, Val Loss: 0.6800
batch size: (904, 904)
Epoch 193, accuracy: 0.6964
batch size: (902, 902)
Epoch 194, accuracy: 0.6954
Epoch 194, Train Loss: 0.3011, Val Loss: 0.6868
batch size: (887, 887)
Epoch 195, accuracy: 0.6939
batch size: (892, 892)
Epoch 196, accuracy: 0.6930
Epoch 196, Train Loss: 0.3612, Val Loss: 0.6759
batch size: (915, 915)
Epoch 197, accuracy: 0.6941
batch size: (911, 911)
Epoch 198, accuracy: 0.6930
Epoch 198, Train Loss: 0.2834, Val Loss: 0.6776
batch size: (896, 896)
Epoch 199, accuracy: 0.6922
Loaded best model with val_loss = 0.6154103875160217
test :accuracy 0.6943, f1_macro: 0.6966, f1_micro: 0.6943, auc: 0.8582
Final Results: {'mamba3_2_Pubmed': np.float64(0.676313683648973), 'mamba3_8_Pubmed': np.float64(0.7022196469958994), 'mamba3_32_Pubmed': np.float64(0.694299128101945)} ['560885_mamba3_0', '1056245_mamba3_0', '3037685_mamba3_0']
========== Running baseline 3/3 ==========
Training mamba3 with 2 layers...
可训练参数: 560885_mamba3
不可训练参数: 0
batch size: (905, 905)
✅ Epoch 0: New best model saved with val_loss = 2.1065
Epoch 0, accuracy: 0.4293
Epoch 0, Train Loss: 1.2403, Val Loss: 2.1065
batch size: (908, 908)
✅ Epoch 1: New best model saved with val_loss = 1.5818
Epoch 1, accuracy: 0.4317
batch size: (893, 893)
✅ Epoch 2: New best model saved with val_loss = 1.0271
Epoch 2, accuracy: 0.4571
Epoch 2, Train Loss: 1.4334, Val Loss: 1.0271
batch size: (900, 900)
✅ Epoch 3: New best model saved with val_loss = 0.8744
Epoch 3, accuracy: 0.4902
batch size: (897, 897)
Epoch 4, accuracy: 0.4545
Epoch 4, Train Loss: 0.8344, Val Loss: 0.9369
batch size: (882, 882)
✅ Epoch 5: New best model saved with val_loss = 0.8711
Epoch 5, accuracy: 0.4601
batch size: (886, 886)
✅ Epoch 6: New best model saved with val_loss = 0.7804
Epoch 6, accuracy: 0.4606
Epoch 6, Train Loss: 0.8914, Val Loss: 0.7804
batch size: (896, 896)
✅ Epoch 7: New best model saved with val_loss = 0.7765
Epoch 7, accuracy: 0.4294
batch size: (888, 888)
Epoch 8, accuracy: 0.4216
Epoch 8, Train Loss: 0.7064, Val Loss: 0.8106
batch size: (875, 875)
Epoch 9, accuracy: 0.4345
batch size: (884, 884)
Epoch 10, accuracy: 0.4455
Epoch 10, Train Loss: 0.8023, Val Loss: 0.8349
batch size: (891, 891)
Epoch 11, accuracy: 0.4484
batch size: (893, 893)
Epoch 12, accuracy: 0.4593
Epoch 12, Train Loss: 0.5214, Val Loss: 0.8248
batch size: (884, 884)
✅ Epoch 13: New best model saved with val_loss = 0.7656
Epoch 13, accuracy: 0.4902
batch size: (897, 897)
✅ Epoch 14: New best model saved with val_loss = 0.7394
Epoch 14, accuracy: 0.5081
Epoch 14, Train Loss: 0.4492, Val Loss: 0.7394
batch size: (906, 906)
✅ Epoch 15: New best model saved with val_loss = 0.7206
Epoch 15, accuracy: 0.5250
batch size: (910, 910)
Epoch 16, accuracy: 0.5391
Epoch 16, Train Loss: 0.3968, Val Loss: 0.7403
batch size: (907, 907)
Epoch 17, accuracy: 0.5438
batch size: (890, 890)
Epoch 18, accuracy: 0.5450
Epoch 18, Train Loss: 0.2966, Val Loss: 0.7537
batch size: (902, 902)
Epoch 19, accuracy: 0.5476
batch size: (895, 895)
Epoch 20, accuracy: 0.5407
Epoch 20, Train Loss: 0.4595, Val Loss: 0.7811
batch size: (888, 888)
Epoch 21, accuracy: 0.5346
batch size: (906, 906)
Epoch 22, accuracy: 0.5346
Epoch 22, Train Loss: 0.3118, Val Loss: 0.7668
batch size: (907, 907)
Epoch 23, accuracy: 0.5364
batch size: (885, 885)
Epoch 24, accuracy: 0.5343
Epoch 24, Train Loss: 0.4136, Val Loss: 0.8106
batch size: (887, 887)
Epoch 25, accuracy: 0.5361
batch size: (914, 914)
Epoch 26, accuracy: 0.5355
Epoch 26, Train Loss: 0.3899, Val Loss: 0.8075
batch size: (886, 886)
Epoch 27, accuracy: 0.5350
batch size: (904, 904)
Epoch 28, accuracy: 0.5325
Epoch 28, Train Loss: 0.4588, Val Loss: 0.8011
batch size: (900, 900)
Epoch 29, accuracy: 0.5317
batch size: (896, 896)
Epoch 30, accuracy: 0.5348
Epoch 30, Train Loss: 0.2604, Val Loss: 0.7906
batch size: (893, 893)
Epoch 31, accuracy: 0.5348
batch size: (902, 902)
Epoch 32, accuracy: 0.5369
Epoch 32, Train Loss: 0.3264, Val Loss: 0.8153
batch size: (897, 897)
Epoch 33, accuracy: 0.5355
batch size: (896, 896)
Epoch 34, accuracy: 0.5334
Epoch 34, Train Loss: 0.4570, Val Loss: 0.8544
batch size: (906, 906)
Epoch 35, accuracy: 0.5361
batch size: (896, 896)
Epoch 36, accuracy: 0.5336
Epoch 36, Train Loss: 0.3456, Val Loss: 0.8189
batch size: (901, 901)
Epoch 37, accuracy: 0.5337
batch size: (902, 902)
Epoch 38, accuracy: 0.5333
Epoch 38, Train Loss: 0.3175, Val Loss: 0.8186
batch size: (880, 880)
Epoch 39, accuracy: 0.5376
batch size: (890, 890)
Epoch 40, accuracy: 0.5329
Epoch 40, Train Loss: 0.2543, Val Loss: 0.8133
batch size: (884, 884)
Epoch 41, accuracy: 0.5331
batch size: (898, 898)
Epoch 42, accuracy: 0.5337
Epoch 42, Train Loss: 0.2692, Val Loss: 0.8225
batch size: (904, 904)
Epoch 43, accuracy: 0.5336
batch size: (901, 901)
Epoch 44, accuracy: 0.5347
Epoch 44, Train Loss: 0.4363, Val Loss: 0.8203
batch size: (909, 909)
Epoch 45, accuracy: 0.5325
batch size: (908, 908)
Epoch 46, accuracy: 0.5336
Epoch 46, Train Loss: 0.4139, Val Loss: 0.8088
batch size: (894, 894)
Epoch 47, accuracy: 0.5347
batch size: (883, 883)
Epoch 48, accuracy: 0.5325
Epoch 48, Train Loss: 0.3562, Val Loss: 0.8272
batch size: (894, 894)
Epoch 49, accuracy: 0.5338
batch size: (902, 902)
Epoch 50, accuracy: 0.5337
Epoch 50, Train Loss: 0.4027, Val Loss: 0.8131
batch size: (912, 912)
Epoch 51, accuracy: 0.5329
batch size: (916, 916)
Epoch 52, accuracy: 0.5333
Epoch 52, Train Loss: 0.3398, Val Loss: 0.8002
batch size: (896, 896)
Epoch 53, accuracy: 0.5353
batch size: (893, 893)
Epoch 54, accuracy: 0.5328
Epoch 54, Train Loss: 0.4079, Val Loss: 0.7987
batch size: (892, 892)
Epoch 55, accuracy: 0.5345
batch size: (903, 903)
Epoch 56, accuracy: 0.5358
Epoch 56, Train Loss: 0.3640, Val Loss: 0.8015
batch size: (883, 883)
Epoch 57, accuracy: 0.5340
batch size: (888, 888)
Epoch 58, accuracy: 0.5344
Epoch 58, Train Loss: 0.3537, Val Loss: 0.8653
batch size: (905, 905)
Epoch 59, accuracy: 0.5323
batch size: (905, 905)
Epoch 60, accuracy: 0.5341
Epoch 60, Train Loss: 0.2773, Val Loss: 0.7914
batch size: (906, 906)
Epoch 61, accuracy: 0.5308
batch size: (908, 908)
Epoch 62, accuracy: 0.5341
Epoch 62, Train Loss: 0.3510, Val Loss: 0.8631
batch size: (869, 869)
Epoch 63, accuracy: 0.5336
batch size: (896, 896)
Epoch 64, accuracy: 0.5364
Epoch 64, Train Loss: 0.2671, Val Loss: 0.8288
batch size: (897, 897)
Epoch 65, accuracy: 0.5333
batch size: (915, 915)
Epoch 66, accuracy: 0.5361
Epoch 66, Train Loss: 0.4132, Val Loss: 0.8041
batch size: (891, 891)
Epoch 67, accuracy: 0.5328
batch size: (906, 906)
Epoch 68, accuracy: 0.5346
Epoch 68, Train Loss: 0.2778, Val Loss: 0.7998
batch size: (898, 898)
Epoch 69, accuracy: 0.5328
batch size: (887, 887)
Epoch 70, accuracy: 0.5362
Epoch 70, Train Loss: 0.3269, Val Loss: 0.7902
batch size: (902, 902)
Epoch 71, accuracy: 0.5340
batch size: (894, 894)
Epoch 72, accuracy: 0.5337
Epoch 72, Train Loss: 0.4119, Val Loss: 0.8429
batch size: (888, 888)
Epoch 73, accuracy: 0.5341
batch size: (919, 919)
Epoch 74, accuracy: 0.5333
Epoch 74, Train Loss: 0.3263, Val Loss: 0.8015
batch size: (899, 899)
Epoch 75, accuracy: 0.5331
batch size: (908, 908)
Epoch 76, accuracy: 0.5334
Epoch 76, Train Loss: 0.4499, Val Loss: 0.8170
batch size: (905, 905)
Epoch 77, accuracy: 0.5360
batch size: (913, 913)
Epoch 78, accuracy: 0.5353
Epoch 78, Train Loss: 0.3619, Val Loss: 0.8033
batch size: (903, 903)
Epoch 79, accuracy: 0.5349
batch size: (889, 889)
Epoch 80, accuracy: 0.5317
Epoch 80, Train Loss: 0.2548, Val Loss: 0.8425
batch size: (870, 870)
Epoch 81, accuracy: 0.5346
batch size: (920, 920)
Epoch 82, accuracy: 0.5337
Epoch 82, Train Loss: 0.2209, Val Loss: 0.7996
batch size: (911, 911)
Epoch 83, accuracy: 0.5346
batch size: (906, 906)
Epoch 84, accuracy: 0.5321
Epoch 84, Train Loss: 0.3918, Val Loss: 0.8323
batch size: (882, 882)
Epoch 85, accuracy: 0.5347
batch size: (896, 896)
Epoch 86, accuracy: 0.5376
Epoch 86, Train Loss: 0.3082, Val Loss: 0.8097
batch size: (895, 895)
Epoch 87, accuracy: 0.5347
batch size: (887, 887)
Epoch 88, accuracy: 0.5364
Epoch 88, Train Loss: 0.3411, Val Loss: 0.7835
batch size: (896, 896)
Epoch 89, accuracy: 0.5335
batch size: (909, 909)
Epoch 90, accuracy: 0.5332
Epoch 90, Train Loss: 0.3638, Val Loss: 0.8146
batch size: (881, 881)
Epoch 91, accuracy: 0.5336
batch size: (891, 891)
Epoch 92, accuracy: 0.5342
Epoch 92, Train Loss: 0.3130, Val Loss: 0.8136
batch size: (913, 913)
Epoch 93, accuracy: 0.5341
batch size: (923, 923)
Epoch 94, accuracy: 0.5361
Epoch 94, Train Loss: 0.4201, Val Loss: 0.8184
batch size: (902, 902)
Epoch 95, accuracy: 0.5346
batch size: (912, 912)
Epoch 96, accuracy: 0.5371
Epoch 96, Train Loss: 0.4224, Val Loss: 0.8042
batch size: (885, 885)
Epoch 97, accuracy: 0.5350
batch size: (897, 897)
Epoch 98, accuracy: 0.5342
Epoch 98, Train Loss: 0.3931, Val Loss: 0.8158
batch size: (891, 891)
Epoch 99, accuracy: 0.5327
batch size: (890, 890)
Epoch 100, accuracy: 0.5347
Epoch 100, Train Loss: 0.3418, Val Loss: 0.8089
batch size: (890, 890)
Epoch 101, accuracy: 0.5333
batch size: (894, 894)
Epoch 102, accuracy: 0.5351
Epoch 102, Train Loss: 0.3966, Val Loss: 0.7843
batch size: (898, 898)
Epoch 103, accuracy: 0.5342
batch size: (891, 891)
Epoch 104, accuracy: 0.5351
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 104, Train Loss: 0.3472, Val Loss: 0.7895
batch size: (903, 903)
Epoch 105, accuracy: 0.5335
batch size: (898, 898)
Epoch 106, accuracy: 0.5334
Epoch 106, Train Loss: 0.3185, Val Loss: 0.8081
batch size: (886, 886)
Epoch 107, accuracy: 0.5342
batch size: (904, 904)
Epoch 108, accuracy: 0.5360
Epoch 108, Train Loss: 0.4038, Val Loss: 0.8015
batch size: (891, 891)
Epoch 109, accuracy: 0.5345
batch size: (904, 904)
Epoch 110, accuracy: 0.5341
Epoch 110, Train Loss: 0.2396, Val Loss: 0.7885
batch size: (906, 906)
Epoch 111, accuracy: 0.5352
batch size: (902, 902)
Epoch 112, accuracy: 0.5345
Epoch 112, Train Loss: 0.2795, Val Loss: 0.8441
batch size: (888, 888)
Epoch 113, accuracy: 0.5370
batch size: (916, 916)
Epoch 114, accuracy: 0.5315
Epoch 114, Train Loss: 0.4554, Val Loss: 0.8270
batch size: (903, 903)
Epoch 115, accuracy: 0.5359
batch size: (903, 903)
Epoch 116, accuracy: 0.5348
Epoch 116, Train Loss: 0.3297, Val Loss: 0.8631
batch size: (889, 889)
Epoch 117, accuracy: 0.5340
batch size: (899, 899)
Epoch 118, accuracy: 0.5369
Epoch 118, Train Loss: 0.3833, Val Loss: 0.8112
batch size: (891, 891)
Epoch 119, accuracy: 0.5332
batch size: (902, 902)
Epoch 120, accuracy: 0.5347
Epoch 120, Train Loss: 0.3232, Val Loss: 0.8305
batch size: (914, 914)
Epoch 121, accuracy: 0.5340
batch size: (898, 898)
Epoch 122, accuracy: 0.5343
Epoch 122, Train Loss: 0.3256, Val Loss: 0.8227
batch size: (910, 910)
Epoch 123, accuracy: 0.5332
batch size: (886, 886)
Epoch 124, accuracy: 0.5364
Epoch 124, Train Loss: 0.3659, Val Loss: 0.8218
batch size: (906, 906)
Epoch 125, accuracy: 0.5371
batch size: (904, 904)
Epoch 126, accuracy: 0.5344
Epoch 126, Train Loss: 0.2607, Val Loss: 0.7992
batch size: (909, 909)
Epoch 127, accuracy: 0.5338
batch size: (902, 902)
Epoch 128, accuracy: 0.5357
Epoch 128, Train Loss: 0.5005, Val Loss: 0.8186
batch size: (915, 915)
Epoch 129, accuracy: 0.5353
batch size: (915, 915)
Epoch 130, accuracy: 0.5342
Epoch 130, Train Loss: 0.2974, Val Loss: 0.7958
batch size: (899, 899)
Epoch 131, accuracy: 0.5339
batch size: (923, 923)
Epoch 132, accuracy: 0.5332
Epoch 132, Train Loss: 0.3928, Val Loss: 0.8268
batch size: (901, 901)
Epoch 133, accuracy: 0.5341
batch size: (906, 906)
Epoch 134, accuracy: 0.5332
Epoch 134, Train Loss: 0.4309, Val Loss: 0.8049
batch size: (896, 896)
Epoch 135, accuracy: 0.5335
batch size: (909, 909)
Epoch 136, accuracy: 0.5316
Epoch 136, Train Loss: 0.3236, Val Loss: 0.8123
batch size: (869, 869)
Epoch 137, accuracy: 0.5340
batch size: (899, 899)
Epoch 138, accuracy: 0.5334
Epoch 138, Train Loss: 0.2867, Val Loss: 0.8201
batch size: (896, 896)
Epoch 139, accuracy: 0.5342
batch size: (896, 896)
Epoch 140, accuracy: 0.5349
Epoch 140, Train Loss: 0.2892, Val Loss: 0.8272
batch size: (881, 881)
Epoch 141, accuracy: 0.5361
batch size: (912, 912)
Epoch 142, accuracy: 0.5312
Epoch 142, Train Loss: 0.3272, Val Loss: 0.8439
batch size: (899, 899)
Epoch 143, accuracy: 0.5352
batch size: (888, 888)
Epoch 144, accuracy: 0.5304
Epoch 144, Train Loss: 0.3144, Val Loss: 0.8261
batch size: (896, 896)
Epoch 145, accuracy: 0.5344
batch size: (896, 896)
Epoch 146, accuracy: 0.5317
Epoch 146, Train Loss: 0.3578, Val Loss: 0.8175
batch size: (913, 913)
Epoch 147, accuracy: 0.5328
batch size: (889, 889)
Epoch 148, accuracy: 0.5358
Epoch 148, Train Loss: 0.3261, Val Loss: 0.8297
batch size: (909, 909)
Epoch 149, accuracy: 0.5337
batch size: (909, 909)
Epoch 150, accuracy: 0.5366
Epoch 150, Train Loss: 0.2503, Val Loss: 0.8331
batch size: (896, 896)
Epoch 151, accuracy: 0.5350
batch size: (889, 889)
Epoch 152, accuracy: 0.5354
Epoch 152, Train Loss: 0.4436, Val Loss: 0.8065
batch size: (923, 923)
Epoch 153, accuracy: 0.5359
batch size: (897, 897)
Epoch 154, accuracy: 0.5324
Epoch 154, Train Loss: 0.4067, Val Loss: 0.8080
batch size: (899, 899)
Epoch 155, accuracy: 0.5331
batch size: (909, 909)
Epoch 156, accuracy: 0.5341
Epoch 156, Train Loss: 0.4242, Val Loss: 0.8213
batch size: (898, 898)
Epoch 157, accuracy: 0.5343
batch size: (888, 888)
Epoch 158, accuracy: 0.5344
Epoch 158, Train Loss: 0.4190, Val Loss: 0.8289
batch size: (903, 903)
Epoch 159, accuracy: 0.5337
batch size: (908, 908)
Epoch 160, accuracy: 0.5349
Epoch 160, Train Loss: 0.3856, Val Loss: 0.8113
batch size: (889, 889)
Epoch 161, accuracy: 0.5346
batch size: (895, 895)
Epoch 162, accuracy: 0.5350
Epoch 162, Train Loss: 0.3973, Val Loss: 0.8026
batch size: (886, 886)
Epoch 163, accuracy: 0.5345
batch size: (898, 898)
Epoch 164, accuracy: 0.5357
Epoch 164, Train Loss: 0.2850, Val Loss: 0.8394
batch size: (888, 888)
Epoch 165, accuracy: 0.5354
batch size: (903, 903)
Epoch 166, accuracy: 0.5336
Epoch 166, Train Loss: 0.4190, Val Loss: 0.8274
batch size: (885, 885)
Epoch 167, accuracy: 0.5337
batch size: (891, 891)
Epoch 168, accuracy: 0.5337
Epoch 168, Train Loss: 0.2961, Val Loss: 0.8184
batch size: (895, 895)
Epoch 169, accuracy: 0.5344
batch size: (910, 910)
Epoch 170, accuracy: 0.5355
Epoch 170, Train Loss: 0.4327, Val Loss: 0.7891
batch size: (925, 925)
Epoch 171, accuracy: 0.5354
batch size: (900, 900)
Epoch 172, accuracy: 0.5350
Epoch 172, Train Loss: 0.3490, Val Loss: 0.8316
batch size: (894, 894)
Epoch 173, accuracy: 0.5343
batch size: (898, 898)
Epoch 174, accuracy: 0.5349
Epoch 174, Train Loss: 0.3478, Val Loss: 0.8044
batch size: (902, 902)
Epoch 175, accuracy: 0.5342
batch size: (899, 899)
Epoch 176, accuracy: 0.5333
Epoch 176, Train Loss: 0.3888, Val Loss: 0.8815
batch size: (898, 898)
Epoch 177, accuracy: 0.5320
batch size: (902, 902)
Epoch 178, accuracy: 0.5363
Epoch 178, Train Loss: 0.3012, Val Loss: 0.8404
batch size: (886, 886)
Epoch 179, accuracy: 0.5346
batch size: (888, 888)
Epoch 180, accuracy: 0.5329
Epoch 180, Train Loss: 0.3493, Val Loss: 0.7915
batch size: (913, 913)
Epoch 181, accuracy: 0.5340
batch size: (895, 895)
Epoch 182, accuracy: 0.5361
Epoch 182, Train Loss: 0.3845, Val Loss: 0.8088
batch size: (897, 897)
Epoch 183, accuracy: 0.5348
batch size: (893, 893)
Epoch 184, accuracy: 0.5340
Epoch 184, Train Loss: 0.3274, Val Loss: 0.8194
batch size: (887, 887)
Epoch 185, accuracy: 0.5350
batch size: (903, 903)
Epoch 186, accuracy: 0.5341
Epoch 186, Train Loss: 0.2722, Val Loss: 0.8205
batch size: (881, 881)
Epoch 187, accuracy: 0.5345
batch size: (909, 909)
Epoch 188, accuracy: 0.5344
Epoch 188, Train Loss: 0.3590, Val Loss: 0.8294
batch size: (901, 901)
Epoch 189, accuracy: 0.5355
batch size: (897, 897)
Epoch 190, accuracy: 0.5361
Epoch 190, Train Loss: 0.4997, Val Loss: 0.8341
batch size: (894, 894)
Epoch 191, accuracy: 0.5357
batch size: (899, 899)
Epoch 192, accuracy: 0.5328
Epoch 192, Train Loss: 0.3657, Val Loss: 0.7757
batch size: (917, 917)
Epoch 193, accuracy: 0.5346
batch size: (871, 871)
Epoch 194, accuracy: 0.5353
Epoch 194, Train Loss: 0.3416, Val Loss: 0.7938
batch size: (911, 911)
Epoch 195, accuracy: 0.5311
batch size: (883, 883)
Epoch 196, accuracy: 0.5363
Epoch 196, Train Loss: 0.2490, Val Loss: 0.8088
batch size: (906, 906)
Epoch 197, accuracy: 0.5344
batch size: (884, 884)
Epoch 198, accuracy: 0.5322
Epoch 198, Train Loss: 0.2818, Val Loss: 0.8124
batch size: (899, 899)
Epoch 199, accuracy: 0.5356
Loaded best model with val_loss = 0.7205855846405029
test :accuracy 0.5270, f1_macro: 0.5404, f1_micro: 0.5270, auc: 0.7286
Training mamba3 with 8 layers...
可训练参数: 1056245_mamba3
不可训练参数: 0
batch size: (910, 910)
✅ Epoch 0: New best model saved with val_loss = 0.8175
Epoch 0, accuracy: 0.5128
Epoch 0, Train Loss: 1.2246, Val Loss: 0.8175
batch size: (911, 911)
Epoch 1, accuracy: 0.2382
batch size: (920, 920)
Epoch 2, accuracy: 0.5037
Epoch 2, Train Loss: 1.6495, Val Loss: 1.0619
batch size: (898, 898)
Epoch 3, accuracy: 0.4891
batch size: (893, 893)
✅ Epoch 4: New best model saved with val_loss = 0.8072
Epoch 4, accuracy: 0.5544
Epoch 4, Train Loss: 1.2878, Val Loss: 0.8072
batch size: (903, 903)
✅ Epoch 5: New best model saved with val_loss = 0.7539
Epoch 5, accuracy: 0.5587
batch size: (893, 893)
Epoch 6, accuracy: 0.3504
Epoch 6, Train Loss: 0.6954, Val Loss: 0.9030
batch size: (890, 890)
Epoch 7, accuracy: 0.3535
batch size: (891, 891)
Epoch 8, accuracy: 0.5096
Epoch 8, Train Loss: 0.8309, Val Loss: 0.7824
batch size: (880, 880)
✅ Epoch 9: New best model saved with val_loss = 0.7185
Epoch 9, accuracy: 0.5831
batch size: (915, 915)
Epoch 10, accuracy: 0.6046
Epoch 10, Train Loss: 0.5179, Val Loss: 0.7300
batch size: (896, 896)
Epoch 11, accuracy: 0.6170
batch size: (928, 928)
✅ Epoch 12: New best model saved with val_loss = 0.6896
Epoch 12, accuracy: 0.6423
Epoch 12, Train Loss: 0.5709, Val Loss: 0.6896
batch size: (907, 907)
✅ Epoch 13: New best model saved with val_loss = 0.6468
Epoch 13, accuracy: 0.6831
batch size: (892, 892)
✅ Epoch 14: New best model saved with val_loss = 0.6048
Epoch 14, accuracy: 0.7056
Epoch 14, Train Loss: 0.4511, Val Loss: 0.6048
batch size: (911, 911)
✅ Epoch 15: New best model saved with val_loss = 0.5890
Epoch 15, accuracy: 0.7079
batch size: (910, 910)
✅ Epoch 16: New best model saved with val_loss = 0.5873
Epoch 16, accuracy: 0.7000
Epoch 16, Train Loss: 0.4551, Val Loss: 0.5873
batch size: (906, 906)
Epoch 17, accuracy: 0.6932
batch size: (919, 919)
✅ Epoch 18: New best model saved with val_loss = 0.5872
Epoch 18, accuracy: 0.6975
Epoch 18, Train Loss: 0.4227, Val Loss: 0.5872
batch size: (900, 900)
Epoch 19, accuracy: 0.7054
batch size: (904, 904)
Epoch 20, accuracy: 0.7029
Epoch 20, Train Loss: 0.3689, Val Loss: 0.6117
batch size: (898, 898)
Epoch 21, accuracy: 0.7017
batch size: (896, 896)
Epoch 22, accuracy: 0.6962
Epoch 22, Train Loss: 0.4489, Val Loss: 0.6666
batch size: (896, 896)
Epoch 23, accuracy: 0.6892
batch size: (894, 894)
Epoch 24, accuracy: 0.6910
Epoch 24, Train Loss: 0.4373, Val Loss: 0.7169
batch size: (926, 926)
Epoch 25, accuracy: 0.6876
batch size: (888, 888)
Epoch 26, accuracy: 0.6880
Epoch 26, Train Loss: 0.4146, Val Loss: 0.6898
batch size: (905, 905)
Epoch 27, accuracy: 0.6909
batch size: (897, 897)
Epoch 28, accuracy: 0.6939
Epoch 28, Train Loss: 0.4512, Val Loss: 0.6890
batch size: (899, 899)
Epoch 29, accuracy: 0.6929
batch size: (879, 879)
Epoch 30, accuracy: 0.6939
Epoch 30, Train Loss: 0.3558, Val Loss: 0.6894
batch size: (898, 898)
Epoch 31, accuracy: 0.6935
batch size: (888, 888)
Epoch 32, accuracy: 0.6943
Epoch 32, Train Loss: 0.3252, Val Loss: 0.6837
batch size: (914, 914)
Epoch 33, accuracy: 0.6948
batch size: (895, 895)
Epoch 34, accuracy: 0.6922
Epoch 34, Train Loss: 0.3868, Val Loss: 0.6935
batch size: (898, 898)
Epoch 35, accuracy: 0.6927
batch size: (910, 910)
Epoch 36, accuracy: 0.6952
Epoch 36, Train Loss: 0.3280, Val Loss: 0.6951
batch size: (903, 903)
Epoch 37, accuracy: 0.6938
batch size: (914, 914)
Epoch 38, accuracy: 0.6936
Epoch 38, Train Loss: 0.3428, Val Loss: 0.6801
batch size: (909, 909)
Epoch 39, accuracy: 0.6945
batch size: (898, 898)
Epoch 40, accuracy: 0.6958
Epoch 40, Train Loss: 0.4029, Val Loss: 0.6868
batch size: (892, 892)
Epoch 41, accuracy: 0.6912
batch size: (897, 897)
Epoch 42, accuracy: 0.6924
Epoch 42, Train Loss: 0.3344, Val Loss: 0.6605
batch size: (917, 917)
Epoch 43, accuracy: 0.6953
batch size: (895, 895)
Epoch 44, accuracy: 0.6959
Epoch 44, Train Loss: 0.3498, Val Loss: 0.6764
batch size: (887, 887)
Epoch 45, accuracy: 0.6947
batch size: (886, 886)
Epoch 46, accuracy: 0.6930
Epoch 46, Train Loss: 0.4264, Val Loss: 0.6875
batch size: (911, 911)
Epoch 47, accuracy: 0.6946
batch size: (909, 909)
Epoch 48, accuracy: 0.6941
Epoch 48, Train Loss: 0.3524, Val Loss: 0.6761
batch size: (893, 893)
Epoch 49, accuracy: 0.6950
batch size: (890, 890)
Epoch 50, accuracy: 0.6918
Epoch 50, Train Loss: 0.4566, Val Loss: 0.6894
batch size: (906, 906)
Epoch 51, accuracy: 0.6948
batch size: (881, 881)
Epoch 52, accuracy: 0.6921
Epoch 52, Train Loss: 0.3767, Val Loss: 0.6873
batch size: (905, 905)
Epoch 53, accuracy: 0.6934
batch size: (896, 896)
Epoch 54, accuracy: 0.6947
Epoch 54, Train Loss: 0.3198, Val Loss: 0.6918
batch size: (898, 898)
Epoch 55, accuracy: 0.6933
batch size: (889, 889)
Epoch 56, accuracy: 0.6951
Epoch 56, Train Loss: 0.2401, Val Loss: 0.6929
batch size: (915, 915)
Epoch 57, accuracy: 0.6961
batch size: (882, 882)
Epoch 58, accuracy: 0.6950
Epoch 58, Train Loss: 0.4136, Val Loss: 0.6827
batch size: (900, 900)
Epoch 59, accuracy: 0.6944
batch size: (893, 893)
Epoch 60, accuracy: 0.6937
Epoch 60, Train Loss: 0.2336, Val Loss: 0.7001
batch size: (897, 897)
Epoch 61, accuracy: 0.6938
batch size: (888, 888)
Epoch 62, accuracy: 0.6972
Epoch 62, Train Loss: 0.5462, Val Loss: 0.6770
batch size: (908, 908)
Epoch 63, accuracy: 0.6945
batch size: (898, 898)
Epoch 64, accuracy: 0.6944
Epoch 64, Train Loss: 0.4188, Val Loss: 0.6738
batch size: (912, 912)
Epoch 65, accuracy: 0.6958
batch size: (922, 922)
Epoch 66, accuracy: 0.6957
Epoch 66, Train Loss: 0.3826, Val Loss: 0.7085
batch size: (878, 878)
Epoch 67, accuracy: 0.6962
batch size: (896, 896)
Epoch 68, accuracy: 0.6926
Epoch 68, Train Loss: 0.3600, Val Loss: 0.6996
batch size: (896, 896)
Epoch 69, accuracy: 0.6930
batch size: (917, 917)
Epoch 70, accuracy: 0.6940
Epoch 70, Train Loss: 0.3719, Val Loss: 0.6816
batch size: (890, 890)
Epoch 71, accuracy: 0.6928
batch size: (897, 897)
Epoch 72, accuracy: 0.6946
Epoch 72, Train Loss: 0.2708, Val Loss: 0.6906
batch size: (891, 891)
Epoch 73, accuracy: 0.6934
batch size: (916, 916)
Epoch 74, accuracy: 0.6937
Epoch 74, Train Loss: 0.4523, Val Loss: 0.6909
batch size: (918, 918)
Epoch 75, accuracy: 0.6936
batch size: (872, 872)
Epoch 76, accuracy: 0.6954
Epoch 76, Train Loss: 0.2876, Val Loss: 0.6857
batch size: (899, 899)
Epoch 77, accuracy: 0.6944
batch size: (895, 895)
Epoch 78, accuracy: 0.6944
Epoch 78, Train Loss: 0.4416, Val Loss: 0.7016
batch size: (896, 896)
Epoch 79, accuracy: 0.6956
batch size: (899, 899)
Epoch 80, accuracy: 0.6956
Epoch 80, Train Loss: 0.2885, Val Loss: 0.7036
batch size: (881, 881)
Epoch 81, accuracy: 0.6970
batch size: (885, 885)
Epoch 82, accuracy: 0.6938
Epoch 82, Train Loss: 0.3838, Val Loss: 0.6882
batch size: (901, 901)
Epoch 83, accuracy: 0.6934
batch size: (888, 888)
Epoch 84, accuracy: 0.6971
Epoch 84, Train Loss: 0.3899, Val Loss: 0.6886
batch size: (905, 905)
Epoch 85, accuracy: 0.6941
batch size: (909, 909)
Epoch 86, accuracy: 0.6926
Epoch 86, Train Loss: 0.4360, Val Loss: 0.6858
batch size: (895, 895)
Epoch 87, accuracy: 0.6941
batch size: (887, 887)
Epoch 88, accuracy: 0.6921
Epoch 88, Train Loss: 0.3744, Val Loss: 0.6692
batch size: (916, 916)
Epoch 89, accuracy: 0.6936
batch size: (906, 906)
Epoch 90, accuracy: 0.6952
Epoch 90, Train Loss: 0.3278, Val Loss: 0.6985
batch size: (911, 911)
Epoch 91, accuracy: 0.6943
batch size: (894, 894)
Epoch 92, accuracy: 0.6926
Epoch 92, Train Loss: 0.3810, Val Loss: 0.6930
batch size: (879, 879)
Epoch 93, accuracy: 0.6938
batch size: (914, 914)
Epoch 94, accuracy: 0.6945
Epoch 94, Train Loss: 0.4425, Val Loss: 0.7082
batch size: (902, 902)
Epoch 95, accuracy: 0.6944
batch size: (908, 908)
Epoch 96, accuracy: 0.6931
Epoch 96, Train Loss: 0.3362, Val Loss: 0.6989
batch size: (884, 884)
Epoch 97, accuracy: 0.6932
batch size: (891, 891)
Epoch 98, accuracy: 0.6931
Epoch 98, Train Loss: 0.3419, Val Loss: 0.6732
batch size: (893, 893)
Epoch 99, accuracy: 0.6931
batch size: (904, 904)
Epoch 100, accuracy: 0.6953
Epoch 100, Train Loss: 0.4587, Val Loss: 0.6772
batch size: (920, 920)
Epoch 101, accuracy: 0.6947
batch size: (915, 915)
Epoch 102, accuracy: 0.6965
Epoch 102, Train Loss: 0.3518, Val Loss: 0.6923
batch size: (913, 913)
Epoch 103, accuracy: 0.6949
batch size: (903, 903)
Epoch 104, accuracy: 0.6935
Epoch 104, Train Loss: 0.4107, Val Loss: 0.6828
batch size: (901, 901)
Epoch 105, accuracy: 0.6938
batch size: (891, 891)
Epoch 106, accuracy: 0.6933
Epoch 106, Train Loss: 0.2671, Val Loss: 0.6840
batch size: (899, 899)
Epoch 107, accuracy: 0.6922
batch size: (901, 901)
Epoch 108, accuracy: 0.6959
Epoch 108, Train Loss: 0.4515, Val Loss: 0.6765
batch size: (901, 901)
Epoch 109, accuracy: 0.6940
batch size: (885, 885)
Epoch 110, accuracy: 0.6958
Epoch 110, Train Loss: 0.3954, Val Loss: 0.6781
batch size: (899, 899)
Epoch 111, accuracy: 0.6920
batch size: (907, 907)
Epoch 112, accuracy: 0.6921
Epoch 112, Train Loss: 0.3562, Val Loss: 0.6913
batch size: (901, 901)
Epoch 113, accuracy: 0.6927
batch size: (907, 907)
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 114, accuracy: 0.6949
Epoch 114, Train Loss: 0.3907, Val Loss: 0.6820
batch size: (890, 890)
Epoch 115, accuracy: 0.6933
batch size: (897, 897)
Epoch 116, accuracy: 0.6936
Epoch 116, Train Loss: 0.4111, Val Loss: 0.6744
batch size: (915, 915)
Epoch 117, accuracy: 0.6965
batch size: (879, 879)
Epoch 118, accuracy: 0.6955
Epoch 118, Train Loss: 0.2936, Val Loss: 0.6915
batch size: (885, 885)
Epoch 119, accuracy: 0.6914
batch size: (896, 896)
Epoch 120, accuracy: 0.6972
Epoch 120, Train Loss: 0.3936, Val Loss: 0.6867
batch size: (913, 913)
Epoch 121, accuracy: 0.6956
batch size: (878, 878)
Epoch 122, accuracy: 0.6949
Epoch 122, Train Loss: 0.4622, Val Loss: 0.6862
batch size: (903, 903)
Epoch 123, accuracy: 0.6936
batch size: (896, 896)
Epoch 124, accuracy: 0.6973
Epoch 124, Train Loss: 0.3906, Val Loss: 0.7014
batch size: (900, 900)
Epoch 125, accuracy: 0.6937
batch size: (900, 900)
Epoch 126, accuracy: 0.6956
Epoch 126, Train Loss: 0.2748, Val Loss: 0.6829
batch size: (906, 906)
Epoch 127, accuracy: 0.6929
batch size: (913, 913)
Epoch 128, accuracy: 0.6938
Epoch 128, Train Loss: 0.4579, Val Loss: 0.6849
batch size: (902, 902)
Epoch 129, accuracy: 0.6936
batch size: (902, 902)
Epoch 130, accuracy: 0.6948
Epoch 130, Train Loss: 0.3562, Val Loss: 0.6796
batch size: (912, 912)
Epoch 131, accuracy: 0.6946
batch size: (894, 894)
Epoch 132, accuracy: 0.6944
Epoch 132, Train Loss: 0.3627, Val Loss: 0.6808
batch size: (905, 905)
Epoch 133, accuracy: 0.6922
batch size: (884, 884)
Epoch 134, accuracy: 0.6921
Epoch 134, Train Loss: 0.3983, Val Loss: 0.6850
batch size: (883, 883)
Epoch 135, accuracy: 0.6942
batch size: (911, 911)
Epoch 136, accuracy: 0.6939
Epoch 136, Train Loss: 0.4216, Val Loss: 0.6832
batch size: (904, 904)
Epoch 137, accuracy: 0.6933
batch size: (911, 911)
Epoch 138, accuracy: 0.6938
Epoch 138, Train Loss: 0.4830, Val Loss: 0.6805
batch size: (894, 894)
Epoch 139, accuracy: 0.6930
batch size: (911, 911)
Epoch 140, accuracy: 0.6968
Epoch 140, Train Loss: 0.4280, Val Loss: 0.6850
batch size: (881, 881)
Epoch 141, accuracy: 0.6942
batch size: (886, 886)
Epoch 142, accuracy: 0.6933
Epoch 142, Train Loss: 0.5085, Val Loss: 0.6891
batch size: (902, 902)
Epoch 143, accuracy: 0.6945
batch size: (893, 893)
Epoch 144, accuracy: 0.6954
Epoch 144, Train Loss: 0.4274, Val Loss: 0.6936
batch size: (890, 890)
Epoch 145, accuracy: 0.6943
batch size: (915, 915)
Epoch 146, accuracy: 0.6945
Epoch 146, Train Loss: 0.3543, Val Loss: 0.6851
batch size: (896, 896)
Epoch 147, accuracy: 0.6951
batch size: (897, 897)
Epoch 148, accuracy: 0.6952
Epoch 148, Train Loss: 0.3849, Val Loss: 0.6945
batch size: (891, 891)
Epoch 149, accuracy: 0.6940
batch size: (890, 890)
Epoch 150, accuracy: 0.6960
Epoch 150, Train Loss: 0.3882, Val Loss: 0.6952
batch size: (918, 918)
Epoch 151, accuracy: 0.6951
batch size: (901, 901)
Epoch 152, accuracy: 0.6947
Epoch 152, Train Loss: 0.4101, Val Loss: 0.6797
batch size: (922, 922)
Epoch 153, accuracy: 0.6947
batch size: (916, 916)
Epoch 154, accuracy: 0.6941
Epoch 154, Train Loss: 0.5494, Val Loss: 0.6917
batch size: (910, 910)
Epoch 155, accuracy: 0.6941
batch size: (901, 901)
Epoch 156, accuracy: 0.6952
Epoch 156, Train Loss: 0.3353, Val Loss: 0.6926
batch size: (931, 931)
Epoch 157, accuracy: 0.6936
batch size: (895, 895)
Epoch 158, accuracy: 0.6949
Epoch 158, Train Loss: 0.3246, Val Loss: 0.6840
batch size: (902, 902)
Epoch 159, accuracy: 0.6955
batch size: (901, 901)
Epoch 160, accuracy: 0.6967
Epoch 160, Train Loss: 0.4302, Val Loss: 0.6779
batch size: (912, 912)
Epoch 161, accuracy: 0.6947
batch size: (899, 899)
Epoch 162, accuracy: 0.6928
Epoch 162, Train Loss: 0.4435, Val Loss: 0.6908
batch size: (891, 891)
Epoch 163, accuracy: 0.6952
batch size: (898, 898)
Epoch 164, accuracy: 0.6951
Epoch 164, Train Loss: 0.4020, Val Loss: 0.6921
batch size: (899, 899)
Epoch 165, accuracy: 0.6945
batch size: (909, 909)
Epoch 166, accuracy: 0.6911
Epoch 166, Train Loss: 0.3635, Val Loss: 0.6820
batch size: (895, 895)
Epoch 167, accuracy: 0.6951
batch size: (893, 893)
Epoch 168, accuracy: 0.6943
Epoch 168, Train Loss: 0.4193, Val Loss: 0.6740
batch size: (918, 918)
Epoch 169, accuracy: 0.6946
batch size: (893, 893)
Epoch 170, accuracy: 0.6952
Epoch 170, Train Loss: 0.3744, Val Loss: 0.6930
batch size: (914, 914)
Epoch 171, accuracy: 0.6949
batch size: (902, 902)
Epoch 172, accuracy: 0.6943
Epoch 172, Train Loss: 0.2935, Val Loss: 0.7074
batch size: (887, 887)
Epoch 173, accuracy: 0.6944
batch size: (899, 899)
Epoch 174, accuracy: 0.6934
Epoch 174, Train Loss: 0.4219, Val Loss: 0.6911
batch size: (899, 899)
Epoch 175, accuracy: 0.6949
batch size: (915, 915)
Epoch 176, accuracy: 0.6941
Epoch 176, Train Loss: 0.3844, Val Loss: 0.6765
batch size: (899, 899)
Epoch 177, accuracy: 0.6956
batch size: (905, 905)
Epoch 178, accuracy: 0.6936
Epoch 178, Train Loss: 0.3063, Val Loss: 0.6718
batch size: (896, 896)
Epoch 179, accuracy: 0.6950
batch size: (906, 906)
Epoch 180, accuracy: 0.6952
Epoch 180, Train Loss: 0.3665, Val Loss: 0.6922
batch size: (893, 893)
Epoch 181, accuracy: 0.6944
batch size: (900, 900)
Epoch 182, accuracy: 0.6942
Epoch 182, Train Loss: 0.2699, Val Loss: 0.6800
batch size: (903, 903)
Epoch 183, accuracy: 0.6939
batch size: (922, 922)
Epoch 184, accuracy: 0.6942
Epoch 184, Train Loss: 0.3472, Val Loss: 0.6927
batch size: (925, 925)
Epoch 185, accuracy: 0.6952
batch size: (906, 906)
Epoch 186, accuracy: 0.6971
Epoch 186, Train Loss: 0.2964, Val Loss: 0.7063
batch size: (903, 903)
Epoch 187, accuracy: 0.6923
batch size: (915, 915)
Epoch 188, accuracy: 0.6937
Epoch 188, Train Loss: 0.3257, Val Loss: 0.6798
batch size: (922, 922)
Epoch 189, accuracy: 0.6941
batch size: (916, 916)
Epoch 190, accuracy: 0.6939
Epoch 190, Train Loss: 0.3353, Val Loss: 0.6896
batch size: (908, 908)
Epoch 191, accuracy: 0.6935
batch size: (906, 906)
Epoch 192, accuracy: 0.6937
Epoch 192, Train Loss: 0.3740, Val Loss: 0.6871
batch size: (892, 892)
Epoch 193, accuracy: 0.6949
batch size: (922, 922)
Epoch 194, accuracy: 0.6975
Epoch 194, Train Loss: 0.3634, Val Loss: 0.6972
batch size: (908, 908)
Epoch 195, accuracy: 0.6942
batch size: (903, 903)
Epoch 196, accuracy: 0.6949
Epoch 196, Train Loss: 0.2795, Val Loss: 0.6783
batch size: (886, 886)
Epoch 197, accuracy: 0.6937
batch size: (902, 902)
Epoch 198, accuracy: 0.6933
Epoch 198, Train Loss: 0.3210, Val Loss: 0.6935
batch size: (903, 903)
Epoch 199, accuracy: 0.6939
Loaded best model with val_loss = 0.587167501449585
test :accuracy 0.6964, f1_macro: 0.6951, f1_micro: 0.6964, auc: 0.8561
Training mamba3 with 32 layers...
可训练参数: 3037685_mamba3
不可训练参数: 0
batch size: (894, 894)
✅ Epoch 0: New best model saved with val_loss = 1.7205
Epoch 0, accuracy: 0.4913
Epoch 0, Train Loss: 4780.2915, Val Loss: 1.7205
batch size: (903, 903)
✅ Epoch 1: New best model saved with val_loss = 1.2415
Epoch 1, accuracy: 0.5043
batch size: (911, 911)
Epoch 2, accuracy: 0.4085
Epoch 2, Train Loss: 1.1810, Val Loss: 1.2628
batch size: (900, 900)
✅ Epoch 3: New best model saved with val_loss = 1.0463
Epoch 3, accuracy: 0.4796
batch size: (900, 900)
✅ Epoch 4: New best model saved with val_loss = 1.0275
Epoch 4, accuracy: 0.5212
Epoch 4, Train Loss: 0.5983, Val Loss: 1.0275
batch size: (885, 885)
✅ Epoch 5: New best model saved with val_loss = 0.9928
Epoch 5, accuracy: 0.5635
batch size: (910, 910)
✅ Epoch 6: New best model saved with val_loss = 0.9255
Epoch 6, accuracy: 0.5706
Epoch 6, Train Loss: 0.5356, Val Loss: 0.9255
batch size: (904, 904)
Epoch 7, accuracy: 0.5695
batch size: (908, 908)
Epoch 8, accuracy: 0.5596
Epoch 8, Train Loss: 0.6851, Val Loss: 1.2329
batch size: (911, 911)
Epoch 9, accuracy: 0.5822
batch size: (918, 918)
Epoch 10, accuracy: 0.5879
Epoch 10, Train Loss: 0.5550, Val Loss: 1.1625
batch size: (902, 902)
Epoch 11, accuracy: 0.5714
batch size: (894, 894)
Epoch 12, accuracy: 0.5631
Epoch 12, Train Loss: 0.5644, Val Loss: 1.4790
batch size: (904, 904)
Epoch 13, accuracy: 0.5596
batch size: (882, 882)
Epoch 14, accuracy: 0.5641
Epoch 14, Train Loss: 0.4473, Val Loss: 1.4879
batch size: (885, 885)
Epoch 15, accuracy: 0.5649
batch size: (909, 909)
Epoch 16, accuracy: 0.5656
Epoch 16, Train Loss: 0.4691, Val Loss: 1.4740
batch size: (890, 890)
Epoch 17, accuracy: 0.5701
batch size: (890, 890)
Epoch 18, accuracy: 0.5717
Epoch 18, Train Loss: 0.5392, Val Loss: 1.4674
batch size: (903, 903)
Epoch 19, accuracy: 0.5677
batch size: (904, 904)
Epoch 20, accuracy: 0.5670
Epoch 20, Train Loss: 0.4594, Val Loss: 1.4630
batch size: (904, 904)
Epoch 21, accuracy: 0.5699
batch size: (907, 907)
Epoch 22, accuracy: 0.5723
Epoch 22, Train Loss: 0.4409, Val Loss: 1.4610
batch size: (908, 908)
Epoch 23, accuracy: 0.5723
batch size: (907, 907)
Epoch 24, accuracy: 0.5723
Epoch 24, Train Loss: 0.4569, Val Loss: 1.5054
batch size: (904, 904)
Epoch 25, accuracy: 0.5716
batch size: (902, 902)
Epoch 26, accuracy: 0.5725
Epoch 26, Train Loss: 0.4891, Val Loss: 1.4181
batch size: (924, 924)
Epoch 27, accuracy: 0.5717
batch size: (895, 895)
Epoch 28, accuracy: 0.5709
Epoch 28, Train Loss: 0.5615, Val Loss: 1.4210
batch size: (888, 888)
Epoch 29, accuracy: 0.5736
batch size: (910, 910)
Epoch 30, accuracy: 0.5730
Epoch 30, Train Loss: 0.5166, Val Loss: 1.4670
batch size: (918, 918)
Epoch 31, accuracy: 0.5725
batch size: (893, 893)
Epoch 32, accuracy: 0.5724
Epoch 32, Train Loss: 0.4794, Val Loss: 1.4371
batch size: (894, 894)
Epoch 33, accuracy: 0.5749
batch size: (906, 906)
Epoch 34, accuracy: 0.5722
Epoch 34, Train Loss: 0.4259, Val Loss: 1.4392
batch size: (911, 911)
Epoch 35, accuracy: 0.5719
batch size: (903, 903)
Epoch 36, accuracy: 0.5724
Epoch 36, Train Loss: 0.4844, Val Loss: 1.4465
batch size: (882, 882)
Epoch 37, accuracy: 0.5769
batch size: (901, 901)
Epoch 38, accuracy: 0.5733
Epoch 38, Train Loss: 0.5070, Val Loss: 1.4363
batch size: (913, 913)
Epoch 39, accuracy: 0.5734
batch size: (910, 910)
Epoch 40, accuracy: 0.5718
Epoch 40, Train Loss: 0.3822, Val Loss: 1.4180
batch size: (889, 889)
Epoch 41, accuracy: 0.5741
batch size: (898, 898)
Epoch 42, accuracy: 0.5709
Epoch 42, Train Loss: 0.5209, Val Loss: 1.4676
batch size: (913, 913)
Epoch 43, accuracy: 0.5734
batch size: (891, 891)
Epoch 44, accuracy: 0.5720
Epoch 44, Train Loss: 0.4611, Val Loss: 1.4507
batch size: (902, 902)
Epoch 45, accuracy: 0.5737
batch size: (907, 907)
Epoch 46, accuracy: 0.5728
Epoch 46, Train Loss: 0.5150, Val Loss: 1.4294
batch size: (904, 904)
Epoch 47, accuracy: 0.5710
batch size: (883, 883)
Epoch 48, accuracy: 0.5710
Epoch 48, Train Loss: 0.4833, Val Loss: 1.3971
batch size: (917, 917)
Epoch 49, accuracy: 0.5760
batch size: (907, 907)
Epoch 50, accuracy: 0.5740
Epoch 50, Train Loss: 0.4217, Val Loss: 1.4162
batch size: (901, 901)
Epoch 51, accuracy: 0.5722
batch size: (907, 907)
Epoch 52, accuracy: 0.5740
Epoch 52, Train Loss: 0.6555, Val Loss: 1.4310
batch size: (902, 902)
Epoch 53, accuracy: 0.5745
batch size: (895, 895)
Epoch 54, accuracy: 0.5725
Epoch 54, Train Loss: 0.4267, Val Loss: 1.4726
batch size: (898, 898)
Epoch 55, accuracy: 0.5711
batch size: (902, 902)
Epoch 56, accuracy: 0.5726
Epoch 56, Train Loss: 0.4850, Val Loss: 1.4679
batch size: (920, 920)
Epoch 57, accuracy: 0.5734
batch size: (922, 922)
Epoch 58, accuracy: 0.5737
Epoch 58, Train Loss: 0.5071, Val Loss: 1.4214
batch size: (905, 905)
Epoch 59, accuracy: 0.5718
batch size: (914, 914)
Epoch 60, accuracy: 0.5731
Epoch 60, Train Loss: 0.5521, Val Loss: 1.4148
batch size: (903, 903)
Epoch 61, accuracy: 0.5747
batch size: (903, 903)
Epoch 62, accuracy: 0.5701
Epoch 62, Train Loss: 0.4685, Val Loss: 1.4066
batch size: (901, 901)
Epoch 63, accuracy: 0.5749
batch size: (915, 915)
Epoch 64, accuracy: 0.5706
Epoch 64, Train Loss: 0.5576, Val Loss: 1.4711
batch size: (907, 907)
Epoch 65, accuracy: 0.5726
batch size: (884, 884)
Epoch 66, accuracy: 0.5728
Epoch 66, Train Loss: 0.5059, Val Loss: 1.4539
batch size: (886, 886)
Epoch 67, accuracy: 0.5716
batch size: (904, 904)
Epoch 68, accuracy: 0.5717
Epoch 68, Train Loss: 0.5165, Val Loss: 1.4260
batch size: (890, 890)
Epoch 69, accuracy: 0.5749
batch size: (900, 900)
Epoch 70, accuracy: 0.5702
Epoch 70, Train Loss: 0.4690, Val Loss: 1.4346
batch size: (878, 878)
Epoch 71, accuracy: 0.5745
batch size: (880, 880)
Epoch 72, accuracy: 0.5752
Epoch 72, Train Loss: 0.4270, Val Loss: 1.4222
batch size: (893, 893)
Epoch 73, accuracy: 0.5706
batch size: (902, 902)
Epoch 74, accuracy: 0.5710
Epoch 74, Train Loss: 0.4613, Val Loss: 1.4511
batch size: (917, 917)
Epoch 75, accuracy: 0.5734
batch size: (890, 890)
Epoch 76, accuracy: 0.5753
Epoch 76, Train Loss: 0.4889, Val Loss: 1.4082
batch size: (914, 914)
Epoch 77, accuracy: 0.5710
batch size: (917, 917)
Epoch 78, accuracy: 0.5768
Epoch 78, Train Loss: 0.5731, Val Loss: 1.4438
batch size: (894, 894)
Epoch 79, accuracy: 0.5734
batch size: (903, 903)
Epoch 80, accuracy: 0.5728
Epoch 80, Train Loss: 0.4576, Val Loss: 1.4345
batch size: (910, 910)
Epoch 81, accuracy: 0.5712
batch size: (922, 922)
Epoch 82, accuracy: 0.5727
Epoch 82, Train Loss: 0.5877, Val Loss: 1.4478
batch size: (881, 881)
Epoch 83, accuracy: 0.5751
batch size: (890, 890)
Epoch 84, accuracy: 0.5756
Epoch 84, Train Loss: 0.5729, Val Loss: 1.4574
batch size: (902, 902)
Epoch 85, accuracy: 0.5700
batch size: (900, 900)
Epoch 86, accuracy: 0.5712
Epoch 86, Train Loss: 0.4676, Val Loss: 1.4476
batch size: (897, 897)
Epoch 87, accuracy: 0.5720
batch size: (900, 900)
Epoch 88, accuracy: 0.5737
Epoch 88, Train Loss: 0.6000, Val Loss: 1.3992
batch size: (890, 890)
Epoch 89, accuracy: 0.5713
batch size: (896, 896)
Epoch 90, accuracy: 0.5721
Epoch 90, Train Loss: 0.5135, Val Loss: 1.4137
batch size: (910, 910)
Epoch 91, accuracy: 0.5730
batch size: (900, 900)
Epoch 92, accuracy: 0.5720
Epoch 92, Train Loss: 0.4949, Val Loss: 1.4378
batch size: (887, 887)
Epoch 93, accuracy: 0.5727
batch size: (899, 899)
Epoch 94, accuracy: 0.5726
Epoch 94, Train Loss: 0.5235, Val Loss: 1.4347
batch size: (893, 893)
Epoch 95, accuracy: 0.5738
batch size: (915, 915)
Epoch 96, accuracy: 0.5736
Epoch 96, Train Loss: 0.4404, Val Loss: 1.4157
batch size: (897, 897)
Epoch 97, accuracy: 0.5753
batch size: (901, 901)
Epoch 98, accuracy: 0.5707
Epoch 98, Train Loss: 0.4225, Val Loss: 1.4581
batch size: (910, 910)
Epoch 99, accuracy: 0.5722
batch size: (897, 897)
Epoch 100, accuracy: 0.5750
Epoch 100, Train Loss: 0.5194, Val Loss: 1.4573
batch size: (879, 879)
Epoch 101, accuracy: 0.5708
batch size: (887, 887)
Epoch 102, accuracy: 0.5753
Epoch 102, Train Loss: 0.5437, Val Loss: 1.4537
batch size: (910, 910)
Epoch 103, accuracy: 0.5743
batch size: (888, 888)
Epoch 104, accuracy: 0.5729
Epoch 104, Train Loss: 0.4050, Val Loss: 1.4304
batch size: (886, 886)
Epoch 105, accuracy: 0.5729
batch size: (910, 910)
Epoch 106, accuracy: 0.5741
Epoch 106, Train Loss: 0.4393, Val Loss: 1.4417
batch size: (896, 896)
Epoch 107, accuracy: 0.5746
batch size: (897, 897)
Epoch 108, accuracy: 0.5738
Epoch 108, Train Loss: 0.4749, Val Loss: 1.4323
batch size: (913, 913)
Epoch 109, accuracy: 0.5732
batch size: (889, 889)
Epoch 110, accuracy: 0.5729
Epoch 110, Train Loss: 0.4746, Val Loss: 1.4287
batch size: (887, 887)
Epoch 111, accuracy: 0.5756
batch size: (911, 911)
Epoch 112, accuracy: 0.5762
Epoch 112, Train Loss: 0.5683, Val Loss: 1.4165
batch size: (897, 897)
Epoch 113, accuracy: 0.5733
batch size: (891, 891)
Epoch 114, accuracy: 0.5745
Epoch 114, Train Loss: 0.5531, Val Loss: 1.4051
batch size: (900, 900)
Epoch 115, accuracy: 0.5744
batch size: (899, 899)
Epoch 116, accuracy: 0.5748
Epoch 116, Train Loss: 0.5762, Val Loss: 1.4249
batch size: (882, 882)
Epoch 117, accuracy: 0.5726
batch size: (868, 868)
Epoch 118, accuracy: 0.5713
Epoch 118, Train Loss: 0.4536, Val Loss: 1.4568
batch size: (904, 904)
Epoch 119, accuracy: 0.5733
batch size: (890, 890)
Epoch 120, accuracy: 0.5712
Epoch 120, Train Loss: 0.4699, Val Loss: 1.4394
batch size: (896, 896)
Epoch 121, accuracy: 0.5711
batch size: (910, 910)
Epoch 122, accuracy: 0.5738
Epoch 122, Train Loss: 0.5152, Val Loss: 1.4112
batch size: (901, 901)
Epoch 123, accuracy: 0.5703
batch size: (904, 904)
Epoch 124, accuracy: 0.5718
Epoch 124, Train Loss: 0.4990, Val Loss: 1.4177
batch size: (919, 919)
Epoch 125, accuracy: 0.5720
batch size: (894, 894)
Epoch 126, accuracy: 0.5723
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Epoch 126, Train Loss: 0.5006, Val Loss: 1.4516
batch size: (892, 892)
Epoch 127, accuracy: 0.5678
batch size: (918, 918)
Epoch 128, accuracy: 0.5731
Epoch 128, Train Loss: 0.5931, Val Loss: 1.4194
batch size: (894, 894)
Epoch 129, accuracy: 0.5707
batch size: (922, 922)
Epoch 130, accuracy: 0.5761
Epoch 130, Train Loss: 0.4601, Val Loss: 1.4283
batch size: (898, 898)
Epoch 131, accuracy: 0.5738
batch size: (891, 891)
Epoch 132, accuracy: 0.5737
Epoch 132, Train Loss: 0.4966, Val Loss: 1.4395
batch size: (911, 911)
Epoch 133, accuracy: 0.5754
batch size: (897, 897)
Epoch 134, accuracy: 0.5722
Epoch 134, Train Loss: 0.5032, Val Loss: 1.4217
batch size: (898, 898)
Epoch 135, accuracy: 0.5704
batch size: (919, 919)
Epoch 136, accuracy: 0.5747
Epoch 136, Train Loss: 0.4776, Val Loss: 1.4633
batch size: (898, 898)
Epoch 137, accuracy: 0.5741
batch size: (893, 893)
Epoch 138, accuracy: 0.5715
Epoch 138, Train Loss: 0.4877, Val Loss: 1.4451
batch size: (899, 899)
Epoch 139, accuracy: 0.5722
batch size: (888, 888)
Epoch 140, accuracy: 0.5731
Epoch 140, Train Loss: 0.4546, Val Loss: 1.4099
batch size: (931, 931)
Epoch 141, accuracy: 0.5733
batch size: (905, 905)
Epoch 142, accuracy: 0.5695
Epoch 142, Train Loss: 0.5341, Val Loss: 1.4640
batch size: (888, 888)
Epoch 143, accuracy: 0.5723
batch size: (905, 905)
Epoch 144, accuracy: 0.5733
Epoch 144, Train Loss: 0.4808, Val Loss: 1.4118
batch size: (906, 906)
Epoch 145, accuracy: 0.5736
batch size: (893, 893)
Epoch 146, accuracy: 0.5729
Epoch 146, Train Loss: 0.4158, Val Loss: 1.4145
batch size: (889, 889)
Epoch 147, accuracy: 0.5710
batch size: (881, 881)
Epoch 148, accuracy: 0.5742
Epoch 148, Train Loss: 0.5002, Val Loss: 1.4592
batch size: (888, 888)
Epoch 149, accuracy: 0.5723
batch size: (908, 908)
Epoch 150, accuracy: 0.5735
Epoch 150, Train Loss: 0.5316, Val Loss: 1.4286
batch size: (881, 881)
Epoch 151, accuracy: 0.5694
batch size: (892, 892)
Epoch 152, accuracy: 0.5723
Epoch 152, Train Loss: 0.5747, Val Loss: 1.4511
batch size: (899, 899)
Epoch 153, accuracy: 0.5719
batch size: (923, 923)
Epoch 154, accuracy: 0.5712
Epoch 154, Train Loss: 0.4904, Val Loss: 1.4380
batch size: (897, 897)
Epoch 155, accuracy: 0.5706
batch size: (911, 911)
Epoch 156, accuracy: 0.5715
Epoch 156, Train Loss: 0.5049, Val Loss: 1.4069
batch size: (904, 904)
Epoch 157, accuracy: 0.5704
batch size: (895, 895)
Epoch 158, accuracy: 0.5745
Epoch 158, Train Loss: 0.5313, Val Loss: 1.4456
batch size: (888, 888)
Epoch 159, accuracy: 0.5758
batch size: (880, 880)
Epoch 160, accuracy: 0.5719
Epoch 160, Train Loss: 0.5301, Val Loss: 1.4327
batch size: (895, 895)
Epoch 161, accuracy: 0.5747
batch size: (890, 890)
Epoch 162, accuracy: 0.5713
Epoch 162, Train Loss: 0.4874, Val Loss: 1.4181
batch size: (897, 897)
Epoch 163, accuracy: 0.5749
batch size: (892, 892)
Epoch 164, accuracy: 0.5721
Epoch 164, Train Loss: 0.5616, Val Loss: 1.4113
batch size: (896, 896)
Epoch 165, accuracy: 0.5739
batch size: (898, 898)
Epoch 166, accuracy: 0.5724
Epoch 166, Train Loss: 0.5267, Val Loss: 1.4783
batch size: (924, 924)
Epoch 167, accuracy: 0.5720
batch size: (910, 910)
Epoch 168, accuracy: 0.5728
Epoch 168, Train Loss: 0.5148, Val Loss: 1.4573
batch size: (903, 903)
Epoch 169, accuracy: 0.5732
batch size: (888, 888)
Epoch 170, accuracy: 0.5710
Epoch 170, Train Loss: 0.4770, Val Loss: 1.4068
batch size: (894, 894)
Epoch 171, accuracy: 0.5738
batch size: (903, 903)
Epoch 172, accuracy: 0.5763
Epoch 172, Train Loss: 0.4899, Val Loss: 1.4436
batch size: (903, 903)
Epoch 173, accuracy: 0.5731
batch size: (894, 894)
Epoch 174, accuracy: 0.5740
Epoch 174, Train Loss: 0.4632, Val Loss: 1.4384
batch size: (916, 916)
Epoch 175, accuracy: 0.5742
batch size: (906, 906)
Epoch 176, accuracy: 0.5708
Epoch 176, Train Loss: 0.5173, Val Loss: 1.4501
batch size: (896, 896)
Epoch 177, accuracy: 0.5715
batch size: (894, 894)
Epoch 178, accuracy: 0.5733
Epoch 178, Train Loss: 0.4873, Val Loss: 1.4484
batch size: (917, 917)
Epoch 179, accuracy: 0.5707
batch size: (895, 895)
Epoch 180, accuracy: 0.5719
Epoch 180, Train Loss: 0.4484, Val Loss: 1.4135
batch size: (923, 923)
Epoch 181, accuracy: 0.5706
batch size: (905, 905)
Epoch 182, accuracy: 0.5722
Epoch 182, Train Loss: 0.4149, Val Loss: 1.4515
batch size: (920, 920)
Epoch 183, accuracy: 0.5720
batch size: (896, 896)
Epoch 184, accuracy: 0.5742
Epoch 184, Train Loss: 0.4620, Val Loss: 1.4552
batch size: (910, 910)
Epoch 185, accuracy: 0.5707
batch size: (896, 896)
Epoch 186, accuracy: 0.5742
Epoch 186, Train Loss: 0.4414, Val Loss: 1.4242
batch size: (889, 889)
Epoch 187, accuracy: 0.5729
batch size: (900, 900)
Epoch 188, accuracy: 0.5756
Epoch 188, Train Loss: 0.3670, Val Loss: 1.4606
batch size: (899, 899)
Epoch 189, accuracy: 0.5740
batch size: (889, 889)
Epoch 190, accuracy: 0.5726
Epoch 190, Train Loss: 0.4833, Val Loss: 1.4717
batch size: (893, 893)
Epoch 191, accuracy: 0.5735
batch size: (897, 897)
Epoch 192, accuracy: 0.5688
Epoch 192, Train Loss: 0.5839, Val Loss: 1.4573
batch size: (891, 891)
Epoch 193, accuracy: 0.5716
batch size: (896, 896)
Epoch 194, accuracy: 0.5718
Epoch 194, Train Loss: 0.5552, Val Loss: 1.4287
batch size: (887, 887)
Epoch 195, accuracy: 0.5733
batch size: (889, 889)
Epoch 196, accuracy: 0.5748
Epoch 196, Train Loss: 0.5332, Val Loss: 1.4021
batch size: (905, 905)
Epoch 197, accuracy: 0.5727
batch size: (899, 899)
Epoch 198, accuracy: 0.5736
Epoch 198, Train Loss: 0.4654, Val Loss: 1.4674
batch size: (908, 908)
Epoch 199, accuracy: 0.5734
Loaded best model with val_loss = 0.9255489706993103
test :accuracy 0.5704, f1_macro: 0.5364, f1_micro: 0.5704, auc: 0.7470
Final Results: {'mamba3_2_Pubmed': np.float64(0.5270282344531808), 'mamba3_8_Pubmed': np.float64(0.6964142717323605), 'mamba3_32_Pubmed': np.float64(0.5704046654498509)} ['560885_mamba3_0', '1056245_mamba3_0', '3037685_mamba3_0']
mamba3_2_Pubmed: Accuracy = 0.6163 ± 0.0788
mamba3_8_Pubmed: Accuracy = 0.6761 ± 0.0404
mamba3_32_Pubmed: Accuracy = 0.6440 ± 0.0652
