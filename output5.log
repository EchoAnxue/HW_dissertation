nohup: ignoring input
/root/miniconda3/lib/python3.12/site-packages/torch_geometric/graphgym/config.py:19: UserWarning: Could not define global config object. Please install 'yacs' via 'pip install yacs' in order to use GraphGym
  warnings.warn("Could not define global config object. Please install "
/root/miniconda3/lib/python3.12/site-packages/torch_geometric/graphgym/imports.py:14: UserWarning: Please install 'pytorch_lightning' via  'pip install pytorch_lightning' in order to use GraphGym
  warnings.warn("Please install 'pytorch_lightning' via  "
/root/miniconda3/lib/python3.12/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling
  warnings.warn(f"Using '{self.__class__.__name__}' without a "
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
/root/miniconda3/lib/python3.12/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling
  warnings.warn(f"Using '{self.__class__.__name__}' without a "
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
/root/miniconda3/lib/python3.12/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling
  warnings.warn(f"Using '{self.__class__.__name__}' without a "
/root/./pipeline/pipeline-Copy1.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
========== Running baseline 1/3 ==========
Training mamba3 with 2 layers...
可训练参数: 260406_mamba3
不可训练参数: 0
✅ Epoch 0: New best model saved with val_loss = 1.0266
Epoch 0, accuracy: 0.4890
Epoch 0, Train Loss: 1.1185, Val Loss: 1.0266
✅ Epoch 1: New best model saved with val_loss = 0.8723
Epoch 1, accuracy: 0.6100
✅ Epoch 2: New best model saved with val_loss = 0.7493
Epoch 2, accuracy: 0.6250
Epoch 2, Train Loss: 0.7988, Val Loss: 0.7493
✅ Epoch 3: New best model saved with val_loss = 0.6878
Epoch 3, accuracy: 0.6820
Epoch 4, accuracy: 0.7040
Epoch 4, Train Loss: 0.6514, Val Loss: 0.7232
Epoch 5, accuracy: 0.6680
Epoch 6, accuracy: 0.6600
Epoch 6, Train Loss: 0.5495, Val Loss: 0.7087
✅ Epoch 7: New best model saved with val_loss = 0.6812
Epoch 7, accuracy: 0.6390
Epoch 8, accuracy: 0.6320
Epoch 8, Train Loss: 0.5282, Val Loss: 0.6828
✅ Epoch 9: New best model saved with val_loss = 0.6754
Epoch 9, accuracy: 0.6540
Epoch 10, accuracy: 0.6640
Epoch 10, Train Loss: 0.3882, Val Loss: 0.6845
✅ Epoch 11: New best model saved with val_loss = 0.6724
Epoch 11, accuracy: 0.6670
✅ Epoch 12: New best model saved with val_loss = 0.6515
Epoch 12, accuracy: 0.6660
Epoch 12, Train Loss: 0.4425, Val Loss: 0.6515
✅ Epoch 13: New best model saved with val_loss = 0.6310
Epoch 13, accuracy: 0.6720
✅ Epoch 14: New best model saved with val_loss = 0.6198
Epoch 14, accuracy: 0.6880
Epoch 14, Train Loss: 0.4038, Val Loss: 0.6198
✅ Epoch 15: New best model saved with val_loss = 0.6192
Epoch 15, accuracy: 0.6970
✅ Epoch 16: New best model saved with val_loss = 0.6081
Epoch 16, accuracy: 0.7080
Epoch 16, Train Loss: 0.3013, Val Loss: 0.6081
✅ Epoch 17: New best model saved with val_loss = 0.6066
Epoch 17, accuracy: 0.7120
✅ Epoch 18: New best model saved with val_loss = 0.6037
Epoch 18, accuracy: 0.7190
Epoch 18, Train Loss: 0.2848, Val Loss: 0.6037
✅ Epoch 19: New best model saved with val_loss = 0.6003
Epoch 19, accuracy: 0.7260
Loaded best model with val_loss = 0.6003077626228333
test :accuracy 0.7260, f1_macro: 0.7248, f1_micro: 0.7260, auc: 0.8935
Final Results: {'mamba3_2_Pubmed': np.float64(0.726)} ['260406_mamba3_0']
========== Running baseline 2/3 ==========
Training mamba3 with 2 layers...
可训练参数: 260406_mamba3
不可训练参数: 0
✅ Epoch 0: New best model saved with val_loss = 1.1955
Epoch 0, accuracy: 0.4240
Epoch 0, Train Loss: 1.1551, Val Loss: 1.1955
✅ Epoch 1: New best model saved with val_loss = 1.0357
Epoch 1, accuracy: 0.5100
✅ Epoch 2: New best model saved with val_loss = 0.9363
Epoch 2, accuracy: 0.5180
Epoch 2, Train Loss: 0.8341, Val Loss: 0.9363
✅ Epoch 3: New best model saved with val_loss = 0.6939
Epoch 3, accuracy: 0.7000
✅ Epoch 4: New best model saved with val_loss = 0.6375
Epoch 4, accuracy: 0.7210
Epoch 4, Train Loss: 0.5896, Val Loss: 0.6375
✅ Epoch 5: New best model saved with val_loss = 0.6228
Epoch 5, accuracy: 0.7400
✅ Epoch 6: New best model saved with val_loss = 0.6059
Epoch 6, accuracy: 0.7460
Epoch 6, Train Loss: 0.6041, Val Loss: 0.6059
Epoch 7, accuracy: 0.7060
Epoch 8, accuracy: 0.6850
Epoch 8, Train Loss: 0.5515, Val Loss: 0.7210
Epoch 9, accuracy: 0.6930
Epoch 10, accuracy: 0.6960
Epoch 10, Train Loss: 0.4622, Val Loss: 0.6603
Epoch 11, accuracy: 0.7070
✅ Epoch 12: New best model saved with val_loss = 0.5794
Epoch 12, accuracy: 0.7430
Epoch 12, Train Loss: 0.5705, Val Loss: 0.5794
✅ Epoch 13: New best model saved with val_loss = 0.5534
Epoch 13, accuracy: 0.7580
✅ Epoch 14: New best model saved with val_loss = 0.5457
Epoch 14, accuracy: 0.7700
Epoch 14, Train Loss: 0.5546, Val Loss: 0.5457
Epoch 15, accuracy: 0.7660
Epoch 16, accuracy: 0.7680
Epoch 16, Train Loss: 0.2911, Val Loss: 0.5616
Epoch 17, accuracy: 0.7710
Epoch 18, accuracy: 0.7670
Epoch 18, Train Loss: 0.3552, Val Loss: 0.6025
Epoch 19, accuracy: 0.7590
Loaded best model with val_loss = 0.5457277894020081
test :accuracy 0.7700, f1_macro: 0.7639, f1_micro: 0.7700, auc: 0.8987
Final Results: {'mamba3_2_Pubmed': np.float64(0.77)} ['260406_mamba3_0']
========== Running baseline 3/3 ==========
Training mamba3 with 2 layers...
可训练参数: 260406_mamba3
不可训练参数: 0
✅ Epoch 0: New best model saved with val_loss = 1.0015
Epoch 0, accuracy: 0.4170
Epoch 0, Train Loss: 1.2381, Val Loss: 1.0015
✅ Epoch 1: New best model saved with val_loss = 0.8934
Epoch 1, accuracy: 0.5020
Epoch 2, accuracy: 0.4750
Epoch 2, Train Loss: 0.8401, Val Loss: 0.9283
✅ Epoch 3: New best model saved with val_loss = 0.8085
Epoch 3, accuracy: 0.5250
✅ Epoch 4: New best model saved with val_loss = 0.7684
Epoch 4, accuracy: 0.5220
Epoch 4, Train Loss: 0.7444, Val Loss: 0.7684
Epoch 5, accuracy: 0.5050
Epoch 6, accuracy: 0.5090
Epoch 6, Train Loss: 0.7875, Val Loss: 0.7962
✅ Epoch 7: New best model saved with val_loss = 0.7465
Epoch 7, accuracy: 0.5260
✅ Epoch 8: New best model saved with val_loss = 0.7046
Epoch 8, accuracy: 0.6340
Epoch 8, Train Loss: 0.8003, Val Loss: 0.7046
Epoch 9, accuracy: 0.7380
Epoch 10, accuracy: 0.7140
Epoch 10, Train Loss: 0.6422, Val Loss: 0.7305
Epoch 11, accuracy: 0.7090
Epoch 12, accuracy: 0.7150
Epoch 12, Train Loss: 0.5875, Val Loss: 0.7547
Epoch 13, accuracy: 0.7260
✅ Epoch 14: New best model saved with val_loss = 0.6918
Epoch 14, accuracy: 0.7560
Epoch 14, Train Loss: 0.7092, Val Loss: 0.6918
✅ Epoch 15: New best model saved with val_loss = 0.6764
Epoch 15, accuracy: 0.7650
✅ Epoch 16: New best model saved with val_loss = 0.6673
Epoch 16, accuracy: 0.7700
Epoch 16, Train Loss: 0.5354, Val Loss: 0.6673
✅ Epoch 17: New best model saved with val_loss = 0.6610
Epoch 17, accuracy: 0.7700
✅ Epoch 18: New best model saved with val_loss = 0.6566
Epoch 18, accuracy: 0.7670
Epoch 18, Train Loss: 0.5430, Val Loss: 0.6566
Epoch 19, accuracy: 0.7560
Loaded best model with val_loss = 0.6566183567047119
test :accuracy 0.7670, f1_macro: 0.7666, f1_micro: 0.7670, auc: 0.8946
Final Results: {'mamba3_2_Pubmed': np.float64(0.767)} ['260406_mamba3_0']
mamba3_2_Pubmed: Accuracy = 0.7543 ± 0.0246
